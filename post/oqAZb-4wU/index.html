<!DOCTYPE html>
<html>

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta content="yes" name="apple-mobile-web-app-capable" />
<meta content="black" name="apple-mobile-web-app-status-bar-style" />
<meta name="referrer" content="never">
<meta name="keywords" content="">
<meta name="description" content="欢迎访问[tianxia]的个人博客">
<meta name="author" content="kveln">
<title>k8s1.24.1安装（基于Centos 7） | tianxia</title>
<link rel="stylesheet" href="https://tinaxiawuhao.github.io/resource/bootstrap.min.css">
<link href="https://tinaxiawuhao.github.io/resource/all.min.css" rel="stylesheet">
<link rel="alternate" type="application/rss+xml" title="k8s1.24.1安装（基于Centos 7） | tianxia » Feed"
  href="https://tinaxiawuhao.github.io/atom.xml">
<link rel="stylesheet"
  href="https://tinaxiawuhao.github.io/resource/androidstudio.min.css">
<link href="https://tinaxiawuhao.github.io/styles/main.css" rel="stylesheet">
<script src="https://tinaxiawuhao.github.io/resource/jquery.min.js"></script>

<script src="https://tinaxiawuhao.github.io/resource/highlight.min.js"></script>

<link rel="stylesheet" href="https://tinaxiawuhao.github.io/resource/live2d.css">

<script>hljs.initHighlightingOnLoad();</script>

  <meta property="og:description" content="k8s1.24.1安装（基于Centos 7）" />
  <meta property="og:url" content="https://tinaxiawuhao.github.io/post/oqAZb-4wU/" />
  <meta property="og:locale" content="zh-CN" />
  <meta property="og:type" content="website" />
  <meta property="og:site_name" content="tianxia" />
  <!-- <script src="../assets/styles/scripts/tocScript.js"></script> -->
</head>

<body>
  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand" href="https://tinaxiawuhao.github.io">tianxia</a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
      data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
      aria-label="Toggle navigation">
      Menu
      <i class="fas fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item">
          
          <a class="nav-link" href="/">首页</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/archives">归档</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/tags">标签</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/post/about">关于</a>
          
        </li>
        
        <li class="nav-item">
          <div class="gridea-search-container">
            <form id="gridea-search-form" style="position: relative" data-update="1656598658594"
              action="/search/index.html">
              <input class="search-input" autocomplete="off" spellcheck="false" name="q" placeholder="搜索文章" />
              <i class="fas fa-search gt-c-content-color-first" style="position: absolute; top: 9px; left: 10px;"></i>
            </form>
          </div>
        </li>
      </ul>
    </div>
  </div>
</nav>
  <!-- Page Header -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand" href="https://tinaxiawuhao.github.io">tianxia</a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
      data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
      aria-label="Toggle navigation">
      Menu
      <i class="fas fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item">
          
          <a class="nav-link" href="/">首页</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/archives">归档</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/tags">标签</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/post/about">关于</a>
          
        </li>
        
        <li class="nav-item">
          <div class="gridea-search-container">
            <form id="gridea-search-form" style="position: relative" data-update="1656598658594"
              action="/search/index.html">
              <input class="search-input" autocomplete="off" spellcheck="false" name="q" placeholder="搜索文章" />
              <i class="fas fa-search gt-c-content-color-first" style="position: absolute; top: 9px; left: 10px;"></i>
            </form>
          </div>
        </li>
      </ul>
    </div>
  </div>
</nav>
<header class="masthead" style="background-image: url('https://tinaxiawuhao.github.io/media/images/home-bg.jpg')">
  <div class="overlay"></div>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">
        
          <!-- 没Title为其他页面Header -->
          
            <!-- 没Title并且有headerType为Post：文章Header -->
            <div class="post-heading">
              <span class="tags">
                
                <a href="https://tinaxiawuhao.github.io/tag/f6exx1YLd/" class="tag">k8s</a>
                
              </span>
              <h1>k8s1.24.1安装（基于Centos 7）</h1>
              <span class="meta">
                Posted on
                2022-05-28，35 min read
              </span>
            </div>
          
        
      </div>
    </div>
  </div>
</header>
  <!-- Post Content -->
  <article id="post-content-article">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto post-content-container">
          
          <img class="post-feature-header-image" src="https://tinaxiawuhao.github.io/post-images/oqAZb-4wU.png" alt="封面图">
          </img>
          
          <p><strong>注意：除了Master节点初始化及Node节点添加分别在Master节点和Node节点执行外，其余所有命令均在所有节点执行</strong></p>
<h2 id="整体环境">整体环境</h2>
<p>一台master节点，2台<a href="https://so.csdn.net/so/search?q=node&amp;spm=1001.2101.3001.7020">node</a>节点。采用了Centos 7，有网络，互相可以ping通。</p>
<h3 id="1内核升级可忽略">1.内核升级（可忽略）</h3>
<p>为避免出现不可预知的问题，提升centos 7内核到最新版本</p>
<h4 id="联网升级内核">联网升级内核</h4>
<h5 id="1-查看内核版本">1. 查看内核版本</h5>
<pre><code class="language-shell">uname -r
</code></pre>
<h5 id="2-导入elrepo软件仓库的公共秘钥">2. 导入ELRepo软件仓库的公共秘钥</h5>
<pre><code class="language-shell">rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
</code></pre>
<h5 id="3-安装elrepo软件仓库的yum源">3. 安装ELRepo软件仓库的yum源</h5>
<pre><code class="language-shell">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm
</code></pre>
<h5 id="4-启用-elrepo-软件源并下载安装最新稳定版内核">4. 启用 elrepo 软件源并下载安装最新稳定版内核</h5>
<pre><code class="language-shell">yum --enablerepo=elrepo-kernel install kernel-ml -y
</code></pre>
<h5 id="5-查看系统可用内核并设置内核启动顺序">5. 查看系统可用内核，并设置内核启动顺序</h5>
<pre><code class="language-shell">sudo awk -F\' '$1==&quot;menuentry &quot; {print i++ &quot; : &quot; $2}' /etc/grub2.cfg
</code></pre>
<h5 id="6-生成-grub-配置文件">6. 生成 grub 配置文件</h5>
<p>机器上存在多个内核，我们要使用最新版本，可以通过 grub2-set-default 0 命令生成 grub 配置文件</p>
<pre><code class="language-shell">grub2-set-default 0 　　#初始化页面的第一个内核将作为默认内核
grub2-mkconfig -o /boot/grub2/grub.cfg　　#重新创建内核配置
</code></pre>
<h5 id="7-重启系统并验证">7. 重启系统并验证</h5>
<pre><code class="language-shell">yum update
reboot
uname -r
</code></pre>
<h5 id="8-删除旧内核">8. 删除旧内核</h5>
<pre><code class="language-shell">yum -y remove kernel kernel-tools
</code></pre>
<h3 id="2centos网络配置文件">2.centos网络配置文件</h3>
<p>网络配置文件名可能会有不同，在输入到ifcfg时，可以连续按两下tab键，获取提示，比如我的机器 为 ifcfg-ens33</p>
<pre><code class="language-shell">vi /etc/sysconfig/network-scripts/ifcfg-ens33 
</code></pre>
<h4 id="1内容替换如下">1.内容替换如下：</h4>
<pre><code class="language-shell">BOOTPROTO=static #静态连接
ONBOOT=yes #网络设备开机启动
IPADDR=192.168.40.131 #192.168.40.132,192.168.40.133.
NETMASK=255.255.255.0 #子网掩码
GATEWAY=192.168.40.2 #网关
DNS1=114.114.114.114 #DNS解析
</code></pre>
<h4 id="2网络服务重启">2.网络服务重启</h4>
<pre><code class="language-shell">service network restart
</code></pre>
<h4 id="3查看ip地址">3.查看IP地址</h4>
<figure data-type="image" tabindex="1"><img src="https://tinaxiawuhao.github.io/post-images/1653818770568.png" alt="" loading="lazy"></figure>
<h3 id="3安装依赖包">3.安装依赖包</h3>
<pre><code class="language-shell">yum install -y  wget 
</code></pre>
<h3 id="4修改yum源视网络情况操作">4.修改yum源（视网络情况操作）</h3>
<h4 id="1备份">1.备份</h4>
<pre><code class="language-shell">cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup
</code></pre>
<h4 id="2下载">2.下载</h4>
<pre><code class="language-shell">wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
</code></pre>
<p>或者使用清华大学站</p>
<pre><code class="language-shell">sudo sed -e 's|^mirrorlist=|#mirrorlist=|g' -e 's|^#baseurl=http://mirror.centos.org/$contentdir|baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos|g' -i.bak /etc/yum.repos.d/CentOS-Base.repo
</code></pre>
<h4 id="3清除生成缓存">3.清除生成缓存</h4>
<pre><code class="language-shell">yum clean all     # 清除系统所有的yum缓存
yum makecache     # 生成yum缓存
</code></pre>
<h3 id="5修改主机名">5.修改主机名</h3>
<pre><code class="language-shell">hostnamectl set-hostname k8s-master
hostnamectl set-hostname k8s-node1
hostnamectl set-hostname k8s-node2
</code></pre>
<h4 id="查看主机名称">查看主机名称</h4>
<figure data-type="image" tabindex="2"><img src="https://tinaxiawuhao.github.io/post-images/1653818810010.png" alt="" loading="lazy"></figure>
<h3 id="6添加hosts解析">6.添加hosts解析</h3>
<pre><code class="language-shell">echo -e &quot;192.168.40.131 k8s-master\n192.168.40.132 k8s-node1\n192.168.40.133 k8s-node2&quot; &gt;&gt; /etc/hosts
</code></pre>
<h3 id="7关闭防火墙firewalld">7.关闭防火墙firewalld</h3>
<pre><code class="language-shell">systemctl stop firewalld &amp;&amp; systemctl disable firewalld
</code></pre>
<h3 id="8关闭selinux">8.关闭selinux</h3>
<pre><code class="language-shell">setenforce 0 &amp;&amp; sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config
</code></pre>
<h3 id="9关闭swap分区交换">9.关闭swap分区交换</h3>
<pre><code class="language-shell">swapoff -a &amp;&amp; sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
</code></pre>
<h3 id="10配置内核参数">10.配置内核参数</h3>
<p><strong>将桥接的IPv4流量传递倒iptables的链</strong></p>
<h4 id="1设置内核参数">1.设置内核参数</h4>
<pre><code class="language-shell">cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt;EOF
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
net.ipv4.tcp_tw_recycle=0
vm.swappiness=0 # 禁止使用swap空间，只有当系统00M时才允许使用它
vm.overcommit_memory=1 # 不检查物理内存是否够用
vm.panic_on_oom=0 # 开启oom
fs.inotify.max_user_instances=8192
fs.inotify.max_user_watches=1048576
fs.file-max=52786963
fs.nr_open=52706963
net.ipv6.conf.all.disable_ipv6=1
net.netfilter.nf_conntrack_max=2310720
EOF
</code></pre>
<h4 id="2加载内核模块">2.加载内核模块</h4>
<pre><code class="language-shell">modprobe br_netfilter &amp;&amp; echo &quot;modprobe br_netfilter&quot; &gt;&gt; /etc/rc.local
</code></pre>
<h4 id="3使内核参数生效">3.使内核参数生效</h4>
<pre><code class="language-shell">sysctl -p /etc/sysctl.d/k8s.conf
</code></pre>
<h3 id="11时间同步">11.时间同步</h3>
<pre><code class="language-shell">timedatectl set-timezone Asia/Shanghai &amp;&amp; timedatectl set-local-rtc 0
#重启依赖于系统时间的服务 
systemctl restart rsyslog &amp;&amp; systemctl restart crond
</code></pre>
<h3 id="12安装iptables设置空表">12.安装iptables，设置空表</h3>
<pre><code class="language-shell">yum -y install iptables-services &amp;&amp; systemctl start iptables &amp;&amp; systemctl enable iptables &amp;&amp; iptables -F &amp;&amp; service iptables save
</code></pre>
<p>检查服务的的规则：<code>iptables -L -n</code></p>
<h3 id="13开启ipvs">13.开启IPVS</h3>
<p><strong>由于ipvs已经加入到了内核的主干，所以为kube-proxy开启ipvs的前提需要加载以下的内核模块</strong></p>
<pre><code class="language-shell">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF
#！/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
</code></pre>
<h4 id="授权启动">授权启动</h4>
<pre><code class="language-shell">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod 
</code></pre>
<p><strong>接下来还需要确保各个节点上已经安装了ipset软件包。 为了便于查看ipvs的代理规则，最好安装一下管理工具ipvsadm。</strong></p>
<pre><code class="language-shell">yum install ipset ipvsadm -y
</code></pre>
<blockquote>
<p>service底层实现主要由两个网络模式组成：iptables与IPVS。他们都是有kube-proxy维护</p>
<p><strong>Iptables VS IPVS</strong></p>
<p><strong>Iptables：</strong></p>
<p><strong>• 灵活，功能强大</strong></p>
<p><strong>• 规则遍历匹配和更新，呈线性时延</strong></p>
<p><strong>IPVS：</strong></p>
<p><strong>• 工作在内核态，有更好的性能</strong></p>
<p><strong>• 调度算法丰富：rr，wrr，lc，wlc，ip hash</strong></p>
</blockquote>
<p>等集群部署成功，mode由空值修改成ipvs模式</p>
<pre><code>kubectl edit configmap kube-proxy -n kube-system configmap/kube-proxy edited
</code></pre>
<figure data-type="image" tabindex="3"><img src="https://tinaxiawuhao.github.io/post-images/1654316987021.png" alt="" loading="lazy"></figure>
<p><strong>删除所有kube-proxy的pod,等待重启</strong></p>
<pre><code>kubectl delete pod/kube-proxy-84p9n -n kube-system
# 查看ipvs相关规则
ipvsadm
</code></pre>
<h3 id="14安装docker软件">14.安装docker软件</h3>
<blockquote>
<p>自1.20版本被弃用之后，dockershim组件终于在1.24的kubelet中被删除。从1.24开始，大家需要使用其他受到支持的运行时选项（例如containerd或CRI-O）；如果您选择Docker Engine作为运行时，则需要使用cri-dockerd。</p>
</blockquote>
<h4 id="1删除自带的docker">1.删除自带的docker</h4>
<pre><code class="language-shell">yum remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-selinux \
                  docker-engine-selinux \
                  docker-engine
</code></pre>
<h4 id="2安装依赖包">2.安装依赖包</h4>
<pre><code class="language-shell">yum install -y yum-utils device-mapper-persistent-data lvm2
</code></pre>
<h4 id="3安装yum源">3.安装yum源</h4>
<pre><code class="language-shell">yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
</code></pre>
<h4 id="4安装docker-ce">4.安装docker-ce</h4>
<pre><code class="language-shell">yum -y install docker-ce
</code></pre>
<h4 id="5设置docker">5.设置docker</h4>
<pre><code class="language-shell">cat &gt; /etc/docker/daemon.conf &lt;&lt;EOF
{
  &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;,&quot;https://heusyzko.mirror.aliyuncs.com&quot;],
  &quot;insecure-registries&quot;: [&quot;https://hub.test.com&quot;],
  &quot;exec-opts&quot;:[&quot;native.cgroupdriver=systemd&quot;],
  &quot;log-driver&quot;:&quot;json-file&quot;,
  &quot;log-opts&quot;:{
    &quot;max-size&quot;: &quot;100m&quot;
    },
    &quot;storage-driver&quot;: &quot;overlay2&quot;
}
EOF

mkdir -p /etc/systemd/system/docker.service.d 
</code></pre>
<h4 id="6重启docker服务">6.重启docker服务</h4>
<pre><code class="language-shell">systemctl daemon-reload &amp;&amp; systemctl start docker &amp;&amp; systemctl enable docker
systemctl daemon-reload &amp;&amp; systemctl restart docker 
</code></pre>
<p>以下的NO_PROXY表示对这些网段的服务器不使用代理，如果不需要用到代理服务器，以下的配置可以不写，注意，以下的代理是不通的。不建议使用代理，因为国内有资源可以访问到gcr.io需要的镜像，下文会介绍</p>
<pre><code class="language-shell">#放在Type=notify下面
vi /usr/lib/systemd/system/docker.service
Environment=&quot;HTTPS_PROXY=http://www.ik8s.io:10080&quot;
Environment=&quot;HTTP_PROXY=http://www.ik8s.io:10080&quot;
Environment=&quot;NO_PROXY=127.0.0.0/8,172.20.0.0/16&quot;
#保存退出后，执行
systemctl  daemon-reload
#确保如下两个参数值为1，默认为1。
 cat /proc/sys/net/bridge/bridge-nf-call-ip6tables
 cat /proc/sys/net/bridge/bridge-nf-call-iptables
#启动docker-ce
systemctl restart docker
#设置开机启动
systemctl enable docker.service
</code></pre>
<pre><code class="language-shell">#想要删除容器，则要先停止所有容器（当然，也可以加-f强制删除，但是不推荐）：
docker stop $(docker ps -a -q)
#删除所有容器
docker  rm $(docker ps -a -q)
#.删除所有镜像（慎重）
docker rmi $(docker images -q)
</code></pre>
<h3 id="142cri-dockerd安装">14.2.cri-dockerd安装</h3>
<p><strong>CRI-Dockerd 其实就是从被移除的 Docker Shim 中，独立出来的一个项目，用于解决历史遗留的节点升级 Kubernetes 的问题。</strong></p>
<blockquote>
<p>kubelet并没有直接和dockerd交互，而是通过了一个dockershim的组件间接操作dockerd。dockershim提供了一个标准的接口，让kubelet能够专注于容器调度逻辑本身，而不用去适配dockerd的接口变动。而其他实现了相同标准接口的容器技术也可以被kubelet集成使用，这个接口称作CRI。dockershim和CRI的出现也是容器生态系统演化的历史产物。在k8s最早期的版本中是不存在dockershim的，kubelet直接和dockerd交互。但为了支持更多不同的容器技术（避免完全被docker控制容器技术市场），kubelet在之后的版本开始支持另一种容器技术rkt。这给kubelet的维护工作造成了巨大的挑战，因为两种容器技术没有统一的接口和使用逻辑，kubelet同时支持两种技术的使用还要保证一致的容器功能表现，对代码逻辑和功能可靠性都有很大的影响。为了解决这个问题，k8s提出了一个统一接口CRI，kubelet统一通过这个接口来调用容器功能。但是dockerd并不支持CRI，k8s就自己实现了配套的dockershim将CRI接口调用转换成dockerd接口调用来支持CRI。因此，dockershim并不是docker技术的一部分，而是k8s系统的一部分</p>
</blockquote>
<p><strong>使用 CRI-Dockerd 项目</strong></p>
<pre><code class="language-html">项目地址：https://github.com/Mirantis/cri-dockerd
</code></pre>
<h4 id="1下载cri-dockerd二进制包或者源码自己编译">1.下载cri-dockerd二进制包或者源码自己编译</h4>
<pre><code class="language-shell"># 下载文件
wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.2.0/cri-dockerd-v0.2.0-linux-amd64.tar.gz
# 解压文件
tar -xvf cri-dockerd-v0.2.0-linux-amd64.tar.gz
# 复制二进制文件到指定目录
cp cri-dockerd /usr/bin/
</code></pre>
<h4 id="2配置启动文件">2.配置启动文件</h4>
<pre><code class="language-shell"># 配置启动文件
cat &lt;&lt;&quot;EOF&quot; &gt; /usr/lib/systemd/system/cri-docker.service
[Unit]
Description=CRI Interface for Docker Application Container Engine
Documentation=https://docs.mirantis.com
After=network-online.target firewalld.service docker.service
Wants=network-online.target
Requires=cri-docker.socket

[Service]
Type=notify

ExecStart=/usr/bin/cri-dockerd --container-runtime-endpoint=unix:///var/run/cri-docker.sock --network-plugin=cni --cni-bin-dir=/opt/cni/bin \
          --cni-conf-dir=/etc/cni/net.d --image-pull-progress-deadline=30s --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.7 \
          --docker-endpoint=unix:///var/run/docker.sock --cri-dockerd-root-directory=/var/lib/docker
ExecReload=/bin/kill -s HUP $MAINPID
TimeoutSec=0
RestartSec=2
Restart=always

# Note that StartLimit* options were moved from &quot;Service&quot; to &quot;Unit&quot; in systemd 229.
# Both the old, and new location are accepted by systemd 229 and up, so using the old location
# to make them work for either version of systemd.
StartLimitBurst=3

# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
# this option work for either version of systemd.
StartLimitInterval=60s

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Comment TasksMax if your systemd version does not support it.
# Only systemd 226 and above support this option.
TasksMax=infinity
Delegate=yes
KillMode=process

[Install]
WantedBy=multi-user.target

EOF
</code></pre>
<pre><code class="language-shell"># 生成socket 文件
cat &lt;&lt;&quot;EOF&quot; &gt; /usr/lib/systemd/system/cri-docker.socket
[Unit]
Description=CRI Docker Socket for the API
PartOf=cri-docker.service

[Socket]
ListenStream=/var/run/cri-dockerd.sock
SocketMode=0660
SocketUser=root
SocketGroup=docker

[Install]
WantedBy=sockets.target

EOF
</code></pre>
<pre><code class="language-shell"># 启动 cri-dockerd
systemctl daemon-reload
systemctl start cri-docker
#设置开机启动
systemctl enable cri-docker
# 查看启动状态
systemctl status cri-docker
</code></pre>
<h4 id="3下载cri-tools验证cri-docker-是否正常">3.下载cri-tools验证cri-docker 是否正常</h4>
<pre><code class="language-shell"># 下载二进制文件
wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.24.0/crictl-v1.24.0-linux-amd64.tar.gz
# 解压
tar -xvf crictl-v1.24.0-linux-amd64.tar.gz
# 复制二进制文件到指定目录
cp crictl /usr/bin/
# 创建配置文件
vim /etc/crictl.yaml
runtime-endpoint: &quot;unix:///var/run/cri-docker.sock&quot;
image-endpoint: &quot;unix:///var/run/cri-docker.sock&quot;
timeout: 10
debug: false
pull-image-on-create: true
disable-pull-on-run: false
# 测试能否访问docker
# 查看运行的容器
crictl ps
# 查看拉取的镜像
 crictl images
 # 拉取镜像
 crictl pull busybox
 [root@k8s-node-4 ~]# crictl pull busybox
Image is up to date for busybox@sha256:5ecba83a746c7608ed511dc1533b87c737a0b0fb730301639a0179f9344b13448
返回一切正常cri-dockerd接入docker完整
</code></pre>
<h3 id="15部署-containerdk8s-124版本以上">15.部署 containerd(k8s-1.24版本以上)</h3>
<p><strong>服务版本</strong></p>
<table>
<thead>
<tr>
<th>服务名称</th>
<th>版本号</th>
</tr>
</thead>
<tbody>
<tr>
<td>内核</td>
<td>5.14.3-1.el7.elrepo.x86_64</td>
</tr>
<tr>
<td>containerd</td>
<td>v1.6.4（加入）</td>
</tr>
<tr>
<td>ctr</td>
<td>v1.6.4</td>
</tr>
<tr>
<td>k8s</td>
<td>1.24</td>
</tr>
</tbody>
</table>
<h4 id="1安装containerd">1.安装containerd</h4>
<p><strong>创建配置文件</strong></p>
<pre><code class="language-shell">mkdir /etc/modules-load.d/containerd.conf 
</code></pre>
<p><strong>创建完配置文件执行以下命令</strong></p>
<pre><code class="language-shell">modprobe overlay &amp;&amp; modprobe br_netfilter
</code></pre>
<p><strong>立即生效</strong></p>
<pre><code class="language-shell">sysctl --system
</code></pre>
<p><strong>下载 docker-ce 源</strong></p>
<pre><code class="language-shell">wget http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
或者
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
</code></pre>
<p><strong>安装 containerd 服务并加入开机启动</strong></p>
<pre><code class="language-shell">yum install -y containerd.io
systemctl enable containerd &amp;&amp; systemctl start containerd
</code></pre>
<h4 id="2配置-containerd">2.配置 containerd</h4>
<p><strong>创建路径</strong></p>
<pre><code class="language-shell">mkdir -p /etc/containerd
</code></pre>
<p><strong>获取默认配置文件</strong></p>
<pre><code class="language-shell">containerd config default | sudo tee /etc/containerd/config.toml
</code></pre>
<p><strong>修改配置文件，新增 &quot;SystemdCgroup = true&quot;，使用 systemd 作为 cgroup 驱动程序</strong></p>
<pre><code class="language-shell">[root@master1 ~]# vi /etc/containerd/config.toml 
#修改以下内容
[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options]
     SystemdCgroup = true                               ## 修改为true
</code></pre>
<p><strong>替换默认pause镜像地址</strong></p>
<p>默认情况下k8s.gcr.io无法访问，所以使用阿里云镜像仓库地址即可</p>
<pre><code class="language-shell"># 所有节点更换默认镜像地址
sed -i 's/k8s.gcr.io/registry.cn-beijing.aliyuncs.com\/abcdocker/' /etc/containerd/config.toml 
</code></pre>
<p><strong>重启 containerd</strong></p>
<pre><code class="language-shell">systemctl restart containerd
</code></pre>
<p><strong>查看 containerd 运行状态(以下状态视为正常)</strong></p>
<pre><code class="language-shell">[root@master1 ~]# systemctl status containerd
● containerd.service - containerd container runtime
   Loaded: loaded (/usr/lib/systemd/system/containerd.service; enabled; vendor preset: disabled)
   Active: active (running) since Sun 2022-03-06 08:09:00 CST; 1h 43min ago
     Docs: https://containerd.io
  Process: 931 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS)
 Main PID: 941 (containerd)
    Tasks: 11
   Memory: 61.4M
   CGroup: /system.slice/containerd.service
           └─941 /usr/bin/containerd
</code></pre>
<h3 id="16安装kubeadm-kubelet-kubectl">16.安装kubeadm、kubelet、kubectl</h3>
<h4 id="1配置文件修改">1.配置文件修改</h4>
<pre><code class="language-shell">cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
</code></pre>
<h4 id="2安装启用">2.安装启用</h4>
<pre><code class="language-shell">sudo yum install -y kubelet-1.24.1 kubeadm-1.24.1 kubectl-1.24.1 --disableexcludes=kubernetes 
sudo systemctl enable kubelet &amp;&amp; systemctl start kubelet
</code></pre>
<h4 id="3修改kubelet的配置文件">3.修改kubelet的配置文件</h4>
<p>先查看配置文件位置</p>
<pre><code class="language-shell">systemctl status kubelet
</code></pre>
<figure data-type="image" tabindex="4"><img src="https://tinaxiawuhao.github.io/post-images/1653818855680.png" alt="" loading="lazy"></figure>
<pre><code class="language-shell">vi /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf
</code></pre>
<p>并添加以下内容(使用和docker相同的cgroup-driver)。</p>
<pre><code class="language-shell">Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --cgroup-driver=systemd&quot;
</code></pre>
<h4 id="4重启kubelet">4.重启kubelet</h4>
<pre><code class="language-shell">systemctl daemon-reload &amp;&amp; systemctl restart kubelet
</code></pre>
<h3 id="17获取k8s镜像可忽略">17.获取K8S镜像（可忽略）</h3>
<h4 id="1获取镜像列表">1.获取镜像列表</h4>
<p><strong>使用阿里云镜像仓库下载（国内环境该命令可不执行，下步骤kubeadm init --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers已经默认为国内环境）</strong></p>
<p>由于官方镜像地址被墙，所以我们需要首先获取所需镜像以及它们的版本。然后从国内镜像站获取。</p>
<pre><code class="language-bash">kubeadm config images list
</code></pre>
<p>获取镜像列表后可以通过下面的脚本从阿里云获取：</p>
<pre><code class="language-shell">vi /usr/local/k8s/k8s-images.sh
</code></pre>
<blockquote>
<p>下面的镜像应该去除&quot;k8s.gcr.io/&quot;的前缀，版本换成上面获取到的版本</p>
</blockquote>
<pre><code class="language-bash">images=(  
    kube-apiserver:v1.24.1
    kube-controller-manager:v1.24.1
    kube-scheduler:v1.24.1
    kube-proxy:v1.24.1
    pause:3.7
    etcd:3.5.3-0
    coredns:v1.8.6
)

for imageName in ${images[@]} ; do
    docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
    docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName
    docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
done
</code></pre>
<h4 id="2赋权执行">2.赋权执行</h4>
<pre><code class="language-shell">chmod +x k8s-images.sh &amp;&amp; ./k8s-images.sh
</code></pre>
<p><strong>以上操作在所有机器执行</strong></p>
<h3 id="18初始化环境master操作">18.初始化环境（master操作）</h3>
<h4 id="1安装镜像">1.安装镜像</h4>
<p><strong>采用模板配置文件加载</strong></p>
<pre><code class="language-shell">kubeadm config print init-defaults  &gt; kubeadm-config.yaml
</code></pre>
<pre><code class="language-yaml">
[root@master1 ~]# cat kubeadm-config.yaml 
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.40.131    # 本机IP
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/cri-docker.sock  # 此处千万不要忘记修改，如果不修改等于没有替换。(此处已经更改完了)
  #criSocket: unix:///run/containerd/containerd.sock      # 此处千万不要忘记修改，如果不修改等于没有替换。(此处已经更改完了)
  name: master1        # 本主机名
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: &quot;192.168.40.151:16443&quot;      # 虚拟IP和haproxy端口
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers    # 镜像仓库源要根据自己实际情况修改
kind: ClusterConfiguration
kubernetesVersion: v1.24.1     # k8s版本
networking:
  dnsDomain: cluster.local
  podSubnet: &quot;10.244.0.0/16&quot;   #设置网段，和下面网络插件对应
  serviceSubnet: 10.96.0.0/12
scheduler: {}
 
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs
</code></pre>
<h4 id="2查看kubeadm版本修改命令参数">2.查看kubeadm版本，修改命令参数</h4>
<pre><code class="language-shell">kubeadm version
</code></pre>
<p>这个就很简单了，只需要简单的一个命令：</p>
<pre><code class="language-bash">#直接使用已经下载好的镜像
kubeadm init --kubernetes-version=v1.24.1 --apiserver-advertise-address=192.168.40.131 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap  --cri-socket unix:///var/run/cri-docker.sock | tee kubeadm-init.log
#或者采用aliyuncs镜像下载
kubeadm init --kubernetes-version=v1.24.1 --apiserver-advertise-address=192.168.40.131 --image-repository  registry.aliyuncs.com/google_containers  --service-cidr=10.1.0.0/16 --pod-network-cidr=10.244.0.0/16 --cri-socket unix:///var/run/cri-docker.sock| tee kubeadm-init.log
#使用上面系统生成配置文件加载
kubeadm init --config kubeadm-config.yaml
</code></pre>
<h4 id="3初始化命令说明">3.初始化命令说明：</h4>
<blockquote>
<p>指明用 Master 的哪个 interface 与 Cluster 的其他节点通信。如果 Master 有多个 interface，建议明确指定，如果不指定，kubeadm 会自动选择有默认网关的 interface。</p>
</blockquote>
<pre><code class="language-bash">--apiserver-advertise-address
</code></pre>
<blockquote>
<p>指定 Pod 网络的范围。Kubernetes 支持多种网络方案，而且不同网络方案对 --pod-network-cidr 有自己的要求，这里设置为 10.244.0.0/16 是因为我们将使用 flannel 网络方案，必须设置成这个 CIDR。</p>
</blockquote>
<pre><code class="language-bash">--pod-network-cidr
</code></pre>
<blockquote>
<p>Kubenetes默认Registries地址是 k8s.gcr.io，在国内并不能访问 gcr.io，在1.19.3版本中我们可以增加–image-repository参数，默认值是 k8s.gcr.io，将其指定为阿里云镜像地址：registry.aliyuncs.com/google_containers。</p>
</blockquote>
<pre><code class="language-bash">--image-repository
</code></pre>
<blockquote>
<p>关闭版本探测，因为它的默认值是stable-1，会导致从https://dl.k8s.io/release/stable-1.txt下载最新的版本号，我们可以将其指定为固定版本（最新版：v1.24.1）来跳过网络请求。</p>
</blockquote>
<pre><code class="language-bash">--kubernetes-version=v1.24.1 
</code></pre>
<blockquote>
<p>指定启动时使用cri-docker调用docker</p>
</blockquote>
<pre><code class="language-shell">--cri-socket unix:///var/run/cri-docker.sock 
</code></pre>
<h4 id="4错误启动重置">4.错误启动重置</h4>
<pre><code class="language-shell"># 重置 如果有需要
kubeadm reset --cri-socket unix:///var/run/cri-docker.sock
</code></pre>
<h4 id="5初始化成功后为顺利使用kubectl执行以下命令">5.初始化成功后，为顺利使用kubectl，执行以下命令：</h4>
<pre><code class="language-shell">mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>
<h4 id="6添加节点">6.添加节点</h4>
<pre><code class="language-shell">kubeadm join 192.168.40.131:6443 --token eyr8v6.j84sxak8aptse8j9 --discovery-token-ca-cert-hash sha256:c082f3c546bdbac02d0d0a3b696de4004b0d449e37838fa38d4752b39682676b --cri-socket unix:///var/run/cri-docker.sock 
</code></pre>
<h4 id="7执行kubectl-get-nodes查看master节点状态">7.执行kubectl get nodes，查看master节点状态：</h4>
<pre><code class="language-shell">kubectl get node
</code></pre>
<h4 id="8通过如下命令查看kubelet状态">8.通过如下命令查看kubelet状态：</h4>
<pre><code class="language-bash">journalctl -xef -u kubelet -n 20
</code></pre>
<p>提示未安装cni 网络插件。</p>
<h3 id="191安装flannel网络插件cni">19.1安装flannel网络插件(CNI)</h3>
<p>master执行以下命令安装flannel即可：</p>
<pre><code class="language-shell">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
</code></pre>
<p>kube-flannel.yaml文件中的net-conf.json-&gt;Network地址默认为命令中–pod-network-cidr=值相同</p>
<p><strong>输入命令kubectl get pods -n kube-system,等待所有插件为running状态</strong>。</p>
<p><strong>待所有pod status为Running的时候，再次执行kubectl get nodes：</strong></p>
<pre><code class="language-shell">[root@k8s-master ~]# kubectl get node
NAME         STATUS   ROLES    AGE   VERSION
k8s-master   Ready    master   16m   v1.19.3
</code></pre>
<p><strong>如上所示，master状态变为，表明Master节点部署成功！</strong></p>
<h3 id="192安装calico网络功能更完善">19.2安装calico网络(功能更完善)</h3>
<h4 id="1在master上下载配置calico网络的yaml">1.在master上下载配置calico网络的yaml。</h4>
<pre><code class="language-shell">kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
</code></pre>
<h4 id="2提前下载所需要的镜像">2.提前下载所需要的镜像。</h4>
<pre><code class="language-shell"># 查看此文件用哪些镜像：
[root@k8s-master ~]# grep image calico.yaml
image: docker.io/calico/cni:v3.23.1
image: docker.io/calico/node:v3.23.1
image: docker.io/calico/kube-controllers:v3.23.1
</code></pre>
<h4 id="3安装calico网络">3.安装calico网络。</h4>
<p>在master上执行如下命令：</p>
<pre><code class="language-csharp">kubectl apply -f calico.yaml
</code></pre>
<h4 id="5验证结果">5.验证结果。</h4>
<p>再次在master上运行命令 kubectl get nodes查看运行结果：</p>
<pre><code class="language-css">[root@k8s-master ~]# kubectl get nodes
NAME       STATUS   ROLES                  AGE   VERSION
master01   Ready    control-plane,master   21h   v1.23.4
worker01   Ready    &lt;none&gt;                 16h   v1.23.4
worker02   Ready    &lt;none&gt;                 16h   v1.23.4
</code></pre>
<h3 id="20部署k8s-node1-k8s-node2集群">20.部署k8s-node1、k8s-node2集群</h3>
<p><strong>1、在k8s-node1、k8s-node2等两台虚拟机中重复执行上面的步骤，安装好docker、kubelet、kubectl、kubeadm。</strong></p>
<h4 id="1node节点加入集群">1.node节点加入集群</h4>
<p>在上面第初始化master节点成功后，输出了下面的kubeadm join命令：</p>
<pre><code class="language-shell">kubeadm join 192.168.40.131:6443 --token zj0u08.ge77y7uv76flqgdk --discovery-token-ca-cert-hash sha256:7cd23cec6afb192b2d34c5c719b378082a6315a9d91a22d91b83066c870d4db5 --cri-socket unix:///var/run/cri-docker.sock 
</code></pre>
<p>该命令就是node加入集群的命令，分别在k8s-node1、k8s-node2上执行该命令加入集群。</p>
<p>如果忘记该命令，可以通过以下命令重新生成：</p>
<pre><code class="language-shell">kubeadm token create --print-join-command
</code></pre>
<h4 id="2在master节点执行下面命令查看集群状态">2.在master节点执行下面命令查看集群状态：</h4>
<pre><code class="language-shell">kubectl get nodes
</code></pre>
<pre><code class="language-shell">[root@k8s-master ~]# kubectl get node
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   24m     v1.19.3
k8s-node1    Ready    &lt;none&gt;   5m50s   v1.19.3
k8s-node2    Ready    &lt;none&gt;   5m21s   v1.19.3

</code></pre>
<p>如上所示，所有节点都为ready，集群搭建成功。</p>
<h3 id="21安装ingress-nginx">21.安装ingress-nginx</h3>
<pre><code class="language-yaml">vi ingress-nginx-deploy.yaml
apiVersion: v1
kind: Namespace
metadata:
  labels:
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  name: ingress-nginx
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.2.0
  name: ingress-nginx
  namespace: ingress-nginx
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.2.0
  name: ingress-nginx-admission
  namespace: ingress-nginx
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.2.0
  name: ingress-nginx
  namespace: ingress-nginx
rules:
- apiGroups:
  - &quot;&quot;
  resources:
  - namespaces
  verbs:
  - get
- apiGroups:
  - &quot;&quot;
  resources:
  - configmaps
  - pods
  - secrets
  - endpoints
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - &quot;&quot;
  resources:
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses/status
  verbs:
  - update
- apiGroups:
  - networking.k8s.io
  resources:
  - ingressclasses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - &quot;&quot;
  resourceNames:
  - ingress-controller-leader
  resources:
  - configmaps
  verbs:
  - get
  - update
- apiGroups:
  - &quot;&quot;
  resources:
  - configmaps
  verbs:
  - create
- apiGroups:
  - &quot;&quot;
  resources:
  - events
  verbs:
  - create
  - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.2.0
  name: ingress-nginx-admission
  namespace: ingress-nginx
rules:
- apiGroups:
  - &quot;&quot;
  resources:
  - secrets
  verbs:
  - get
  - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.2.0
  name: ingress-nginx
rules:
- apiGroups:
  - &quot;&quot;
  resources:
  - configmaps
  - endpoints
  - nodes
  - pods
  - secrets
  - namespaces
  verbs:
  - list
  - watch
- apiGroups:
  - &quot;&quot;
  resources:
  - nodes
  verbs:
  - get
- apiGroups:
  - &quot;&quot;
  resources:
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - &quot;&quot;
  resources:
  - events
  verbs:
  - create
  - patch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses/status
  verbs:
  - update
- apiGroups:
  - networking.k8s.io
  resources:
  - ingressclasses
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.2.0
  name: ingress-nginx-admission
rules:
- apiGroups:
  - admissionregistration.k8s.io
  resources:
  - validatingwebhookconfigurations
  verbs:
  - get
  - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.2.0
  name: ingress-nginx
  namespace: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ingress-nginx
subjects:
- kind: ServiceAccount
  name: ingress-nginx
  namespace: ingress-nginx
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.2.0
  name: ingress-nginx-admission
  namespace: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ingress-nginx-admission
subjects:
- kind: ServiceAccount
  name: ingress-nginx-admission
  namespace: ingress-nginx
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.2.0
  name: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: ingress-nginx
subjects:
- kind: ServiceAccount
  name: ingress-nginx
  namespace: ingress-nginx
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.2.0
  name: ingress-nginx-admission
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: ingress-nginx-admission
subjects:
- kind: ServiceAccount
  name: ingress-nginx-admission
  namespace: ingress-nginx
---
apiVersion: v1
data:
  allow-snippet-annotations: &quot;true&quot;
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.2.0
  name: ingress-nginx-controller
  namespace: ingress-nginx
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.2.0
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  ports:
  - appProtocol: http
    name: http
    port: 80
    protocol: TCP
    targetPort: http
  - appProtocol: https
    name: https
    port: 443
    protocol: TCP
    targetPort: https
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: NodePort
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.2.0
  name: ingress-nginx-controller-admission
  namespace: ingress-nginx
spec:
  ports:
  - appProtocol: https
    name: https-webhook
    port: 443
    targetPort: webhook
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.2.0
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --election-id=ingress-controller-leader
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: LD_PRELOAD
          value: /usr/local/lib/libmimalloc.so
        image: registry.aliyuncs.com/google_containers/nginx-ingress-controller:v1.2.0
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /wait-shutdown
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: controller
        ports:
        - containerPort: 80
          name: http
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8443
          name: webhook
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
        securityContext:
          allowPrivilegeEscalation: true
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - ALL
          runAsUser: 101
        volumeMounts:
        - mountPath: /usr/local/certificates/
          name: webhook-cert
          readOnly: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
      - name: webhook-cert
        secret:
          secretName: ingress-nginx-admission
---
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.2.0
  name: ingress-nginx-admission-create
  namespace: ingress-nginx
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/component: admission-webhook
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
        app.kubernetes.io/version: 1.2.0
      name: ingress-nginx-admission-create
    spec:
      containers:
      - args:
        - create
        - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
        - --namespace=$(POD_NAMESPACE)
        - --secret-name=ingress-nginx-admission
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        image: registry.aliyuncs.com/google_containers/kube-webhook-certgen:v1.1.1
        imagePullPolicy: IfNotPresent
        name: create
        securityContext:
          allowPrivilegeEscalation: false
      nodeSelector:
        kubernetes.io/os: linux
      restartPolicy: OnFailure
      securityContext:
        fsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
      serviceAccountName: ingress-nginx-admission
---
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.2.0
  name: ingress-nginx-admission-patch
  namespace: ingress-nginx
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/component: admission-webhook
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
        app.kubernetes.io/version: 1.2.0
      name: ingress-nginx-admission-patch
    spec:
      containers:
      - args:
        - patch
        - --webhook-name=ingress-nginx-admission
        - --namespace=$(POD_NAMESPACE)
        - --patch-mutating=false
        - --secret-name=ingress-nginx-admission
        - --patch-failure-policy=Fail
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        image: registry.aliyuncs.com/google_containers/kube-webhook-certgen:v1.1.1
        imagePullPolicy: IfNotPresent
        name: patch
        securityContext:
          allowPrivilegeEscalation: false
      nodeSelector:
        kubernetes.io/os: linux
      restartPolicy: OnFailure
      securityContext:
        fsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
      serviceAccountName: ingress-nginx-admission
---
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.2.0
  name: nginx
spec:
  controller: k8s.io/ingress-nginx
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.2.0
  name: ingress-nginx-admission
webhooks:
- admissionReviewVersions:
  - v1
  clientConfig:
    service:
      name: ingress-nginx-controller-admission
      namespace: ingress-nginx
      path: /networking/v1/ingresses
  failurePolicy: Fail
  matchPolicy: Equivalent
  name: validate.nginx.ingress.kubernetes.io
  rules:
  - apiGroups:
    - networking.k8s.io
    apiVersions:
    - v1
    operations:
    - CREATE
    - UPDATE
    resources:
    - ingresses
  sideEffects: None
  
kubectl create -f ingress-nginx-deploy.yaml
</code></pre>
<h2 id="卸载集群命令">卸载集群命令</h2>
<pre><code class="language-shell">#建议所有服务器都执行
#!/bin/bash
kubeadm reset -f
modprobe -r ipip
lsmod
rm -rf ~/.kube/
rm -rf /etc/kubernetes/
rm -rf /etc/systemd/system/kubelet.service.d
rm -rf /etc/systemd/system/kubelet.service
rm -rf /usr/bin/kube*
rm -rf /etc/cni
rm -rf /opt/cni
rm -rf /var/lib/etcd
rm -rf /var/etcd
yum -y remove kubeadm* kubectl* kubelet* docker*
reboot
</code></pre>

          <div class="toc-container"><ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#%E6%95%B4%E4%BD%93%E7%8E%AF%E5%A2%83">整体环境</a>
<ul>
<li><a href="#1%E5%86%85%E6%A0%B8%E5%8D%87%E7%BA%A7%E5%8F%AF%E5%BF%BD%E7%95%A5">1.内核升级（可忽略）</a>
<ul>
<li><a href="#%E8%81%94%E7%BD%91%E5%8D%87%E7%BA%A7%E5%86%85%E6%A0%B8">联网升级内核</a>
<ul>
<li><a href="#1-%E6%9F%A5%E7%9C%8B%E5%86%85%E6%A0%B8%E7%89%88%E6%9C%AC">1. 查看内核版本</a></li>
<li><a href="#2-%E5%AF%BC%E5%85%A5elrepo%E8%BD%AF%E4%BB%B6%E4%BB%93%E5%BA%93%E7%9A%84%E5%85%AC%E5%85%B1%E7%A7%98%E9%92%A5">2. 导入ELRepo软件仓库的公共秘钥</a></li>
<li><a href="#3-%E5%AE%89%E8%A3%85elrepo%E8%BD%AF%E4%BB%B6%E4%BB%93%E5%BA%93%E7%9A%84yum%E6%BA%90">3. 安装ELRepo软件仓库的yum源</a></li>
<li><a href="#4-%E5%90%AF%E7%94%A8-elrepo-%E8%BD%AF%E4%BB%B6%E6%BA%90%E5%B9%B6%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85%E6%9C%80%E6%96%B0%E7%A8%B3%E5%AE%9A%E7%89%88%E5%86%85%E6%A0%B8">4. 启用 elrepo 软件源并下载安装最新稳定版内核</a></li>
<li><a href="#5-%E6%9F%A5%E7%9C%8B%E7%B3%BB%E7%BB%9F%E5%8F%AF%E7%94%A8%E5%86%85%E6%A0%B8%E5%B9%B6%E8%AE%BE%E7%BD%AE%E5%86%85%E6%A0%B8%E5%90%AF%E5%8A%A8%E9%A1%BA%E5%BA%8F">5. 查看系统可用内核，并设置内核启动顺序</a></li>
<li><a href="#6-%E7%94%9F%E6%88%90-grub-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">6. 生成 grub 配置文件</a></li>
<li><a href="#7-%E9%87%8D%E5%90%AF%E7%B3%BB%E7%BB%9F%E5%B9%B6%E9%AA%8C%E8%AF%81">7. 重启系统并验证</a></li>
<li><a href="#8-%E5%88%A0%E9%99%A4%E6%97%A7%E5%86%85%E6%A0%B8">8. 删除旧内核</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#2centos%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">2.centos网络配置文件</a>
<ul>
<li><a href="#1%E5%86%85%E5%AE%B9%E6%9B%BF%E6%8D%A2%E5%A6%82%E4%B8%8B">1.内容替换如下：</a></li>
<li><a href="#2%E7%BD%91%E7%BB%9C%E6%9C%8D%E5%8A%A1%E9%87%8D%E5%90%AF">2.网络服务重启</a></li>
<li><a href="#3%E6%9F%A5%E7%9C%8Bip%E5%9C%B0%E5%9D%80">3.查看IP地址</a></li>
</ul>
</li>
<li><a href="#3%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96%E5%8C%85">3.安装依赖包</a></li>
<li><a href="#4%E4%BF%AE%E6%94%B9yum%E6%BA%90%E8%A7%86%E7%BD%91%E7%BB%9C%E6%83%85%E5%86%B5%E6%93%8D%E4%BD%9C">4.修改yum源（视网络情况操作）</a>
<ul>
<li><a href="#1%E5%A4%87%E4%BB%BD">1.备份</a></li>
<li><a href="#2%E4%B8%8B%E8%BD%BD">2.下载</a></li>
<li><a href="#3%E6%B8%85%E9%99%A4%E7%94%9F%E6%88%90%E7%BC%93%E5%AD%98">3.清除生成缓存</a></li>
</ul>
</li>
<li><a href="#5%E4%BF%AE%E6%94%B9%E4%B8%BB%E6%9C%BA%E5%90%8D">5.修改主机名</a>
<ul>
<li><a href="#%E6%9F%A5%E7%9C%8B%E4%B8%BB%E6%9C%BA%E5%90%8D%E7%A7%B0">查看主机名称</a></li>
</ul>
</li>
<li><a href="#6%E6%B7%BB%E5%8A%A0hosts%E8%A7%A3%E6%9E%90">6.添加hosts解析</a></li>
<li><a href="#7%E5%85%B3%E9%97%AD%E9%98%B2%E7%81%AB%E5%A2%99firewalld">7.关闭防火墙firewalld</a></li>
<li><a href="#8%E5%85%B3%E9%97%ADselinux">8.关闭selinux</a></li>
<li><a href="#9%E5%85%B3%E9%97%ADswap%E5%88%86%E5%8C%BA%E4%BA%A4%E6%8D%A2">9.关闭swap分区交换</a></li>
<li><a href="#10%E9%85%8D%E7%BD%AE%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0">10.配置内核参数</a>
<ul>
<li><a href="#1%E8%AE%BE%E7%BD%AE%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0">1.设置内核参数</a></li>
<li><a href="#2%E5%8A%A0%E8%BD%BD%E5%86%85%E6%A0%B8%E6%A8%A1%E5%9D%97">2.加载内核模块</a></li>
<li><a href="#3%E4%BD%BF%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0%E7%94%9F%E6%95%88">3.使内核参数生效</a></li>
</ul>
</li>
<li><a href="#11%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5">11.时间同步</a></li>
<li><a href="#12%E5%AE%89%E8%A3%85iptables%E8%AE%BE%E7%BD%AE%E7%A9%BA%E8%A1%A8">12.安装iptables，设置空表</a></li>
<li><a href="#13%E5%BC%80%E5%90%AFipvs">13.开启IPVS</a>
<ul>
<li><a href="#%E6%8E%88%E6%9D%83%E5%90%AF%E5%8A%A8">授权启动</a></li>
</ul>
</li>
<li><a href="#14%E5%AE%89%E8%A3%85docker%E8%BD%AF%E4%BB%B6">14.安装docker软件</a>
<ul>
<li><a href="#1%E5%88%A0%E9%99%A4%E8%87%AA%E5%B8%A6%E7%9A%84docker">1.删除自带的docker</a></li>
<li><a href="#2%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96%E5%8C%85">2.安装依赖包</a></li>
<li><a href="#3%E5%AE%89%E8%A3%85yum%E6%BA%90">3.安装yum源</a></li>
<li><a href="#4%E5%AE%89%E8%A3%85docker-ce">4.安装docker-ce</a></li>
<li><a href="#5%E8%AE%BE%E7%BD%AEdocker">5.设置docker</a></li>
<li><a href="#6%E9%87%8D%E5%90%AFdocker%E6%9C%8D%E5%8A%A1">6.重启docker服务</a></li>
</ul>
</li>
<li><a href="#142cri-dockerd%E5%AE%89%E8%A3%85">14.2.cri-dockerd安装</a>
<ul>
<li><a href="#1%E4%B8%8B%E8%BD%BDcri-dockerd%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%88%96%E8%80%85%E6%BA%90%E7%A0%81%E8%87%AA%E5%B7%B1%E7%BC%96%E8%AF%91">1.下载cri-dockerd二进制包或者源码自己编译</a></li>
<li><a href="#2%E9%85%8D%E7%BD%AE%E5%90%AF%E5%8A%A8%E6%96%87%E4%BB%B6">2.配置启动文件</a></li>
<li><a href="#3%E4%B8%8B%E8%BD%BDcri-tools%E9%AA%8C%E8%AF%81cri-docker-%E6%98%AF%E5%90%A6%E6%AD%A3%E5%B8%B8">3.下载cri-tools验证cri-docker 是否正常</a></li>
</ul>
</li>
<li><a href="#15%E9%83%A8%E7%BD%B2-containerdk8s-124%E7%89%88%E6%9C%AC%E4%BB%A5%E4%B8%8A">15.部署 containerd(k8s-1.24版本以上)</a>
<ul>
<li><a href="#1%E5%AE%89%E8%A3%85containerd">1.安装containerd</a></li>
<li><a href="#2%E9%85%8D%E7%BD%AE-containerd">2.配置 containerd</a></li>
</ul>
</li>
<li><a href="#16%E5%AE%89%E8%A3%85kubeadm-kubelet-kubectl">16.安装kubeadm、kubelet、kubectl</a>
<ul>
<li><a href="#1%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E4%BF%AE%E6%94%B9">1.配置文件修改</a></li>
<li><a href="#2%E5%AE%89%E8%A3%85%E5%90%AF%E7%94%A8">2.安装启用</a></li>
<li><a href="#3%E4%BF%AE%E6%94%B9kubelet%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">3.修改kubelet的配置文件</a></li>
<li><a href="#4%E9%87%8D%E5%90%AFkubelet">4.重启kubelet</a></li>
</ul>
</li>
<li><a href="#17%E8%8E%B7%E5%8F%96k8s%E9%95%9C%E5%83%8F%E5%8F%AF%E5%BF%BD%E7%95%A5">17.获取K8S镜像（可忽略）</a>
<ul>
<li><a href="#1%E8%8E%B7%E5%8F%96%E9%95%9C%E5%83%8F%E5%88%97%E8%A1%A8">1.获取镜像列表</a></li>
<li><a href="#2%E8%B5%8B%E6%9D%83%E6%89%A7%E8%A1%8C">2.赋权执行</a></li>
</ul>
</li>
<li><a href="#18%E5%88%9D%E5%A7%8B%E5%8C%96%E7%8E%AF%E5%A2%83master%E6%93%8D%E4%BD%9C">18.初始化环境（master操作）</a>
<ul>
<li><a href="#1%E5%AE%89%E8%A3%85%E9%95%9C%E5%83%8F">1.安装镜像</a></li>
<li><a href="#2%E6%9F%A5%E7%9C%8Bkubeadm%E7%89%88%E6%9C%AC%E4%BF%AE%E6%94%B9%E5%91%BD%E4%BB%A4%E5%8F%82%E6%95%B0">2.查看kubeadm版本，修改命令参数</a></li>
<li><a href="#3%E5%88%9D%E5%A7%8B%E5%8C%96%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E">3.初始化命令说明：</a></li>
<li><a href="#4%E9%94%99%E8%AF%AF%E5%90%AF%E5%8A%A8%E9%87%8D%E7%BD%AE">4.错误启动重置</a></li>
<li><a href="#5%E5%88%9D%E5%A7%8B%E5%8C%96%E6%88%90%E5%8A%9F%E5%90%8E%E4%B8%BA%E9%A1%BA%E5%88%A9%E4%BD%BF%E7%94%A8kubectl%E6%89%A7%E8%A1%8C%E4%BB%A5%E4%B8%8B%E5%91%BD%E4%BB%A4">5.初始化成功后，为顺利使用kubectl，执行以下命令：</a></li>
<li><a href="#6%E6%B7%BB%E5%8A%A0%E8%8A%82%E7%82%B9">6.添加节点</a></li>
<li><a href="#7%E6%89%A7%E8%A1%8Ckubectl-get-nodes%E6%9F%A5%E7%9C%8Bmaster%E8%8A%82%E7%82%B9%E7%8A%B6%E6%80%81">7.执行kubectl get nodes，查看master节点状态：</a></li>
<li><a href="#8%E9%80%9A%E8%BF%87%E5%A6%82%E4%B8%8B%E5%91%BD%E4%BB%A4%E6%9F%A5%E7%9C%8Bkubelet%E7%8A%B6%E6%80%81">8.通过如下命令查看kubelet状态：</a></li>
</ul>
</li>
<li><a href="#191%E5%AE%89%E8%A3%85flannel%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6cni">19.1安装flannel网络插件(CNI)</a></li>
<li><a href="#192%E5%AE%89%E8%A3%85calico%E7%BD%91%E7%BB%9C%E5%8A%9F%E8%83%BD%E6%9B%B4%E5%AE%8C%E5%96%84">19.2安装calico网络(功能更完善)</a>
<ul>
<li><a href="#1%E5%9C%A8master%E4%B8%8A%E4%B8%8B%E8%BD%BD%E9%85%8D%E7%BD%AEcalico%E7%BD%91%E7%BB%9C%E7%9A%84yaml">1.在master上下载配置calico网络的yaml。</a></li>
<li><a href="#2%E6%8F%90%E5%89%8D%E4%B8%8B%E8%BD%BD%E6%89%80%E9%9C%80%E8%A6%81%E7%9A%84%E9%95%9C%E5%83%8F">2.提前下载所需要的镜像。</a></li>
<li><a href="#3%E5%AE%89%E8%A3%85calico%E7%BD%91%E7%BB%9C">3.安装calico网络。</a></li>
<li><a href="#5%E9%AA%8C%E8%AF%81%E7%BB%93%E6%9E%9C">5.验证结果。</a></li>
</ul>
</li>
<li><a href="#20%E9%83%A8%E7%BD%B2k8s-node1-k8s-node2%E9%9B%86%E7%BE%A4">20.部署k8s-node1、k8s-node2集群</a>
<ul>
<li><a href="#1node%E8%8A%82%E7%82%B9%E5%8A%A0%E5%85%A5%E9%9B%86%E7%BE%A4">1.node节点加入集群</a></li>
<li><a href="#2%E5%9C%A8master%E8%8A%82%E7%82%B9%E6%89%A7%E8%A1%8C%E4%B8%8B%E9%9D%A2%E5%91%BD%E4%BB%A4%E6%9F%A5%E7%9C%8B%E9%9B%86%E7%BE%A4%E7%8A%B6%E6%80%81">2.在master节点执行下面命令查看集群状态：</a></li>
</ul>
</li>
<li><a href="#21%E5%AE%89%E8%A3%85ingress-nginx">21.安装ingress-nginx</a></li>
</ul>
</li>
<li><a href="#%E5%8D%B8%E8%BD%BD%E9%9B%86%E7%BE%A4%E5%91%BD%E4%BB%A4">卸载集群命令</a></li>
</ul>
</li>
</ul>
</div>
          
              <hr />
            
          
            
            <p class="next-post">上一篇：
                <a href="https://tinaxiawuhao.github.io/post/4kG3rvwit/">
                  <span class="post-title">
                    centos7安装Docker详细步骤&rarr;
                  </span>
                </a>
              </p>
            
          
            
           <p class="prev-post">下一篇：
                 <a href="https://tinaxiawuhao.github.io/post/WOO39eqh2/">
                   <span class="post-title">
                     harbor自建镜像仓库&rarr;
                   </span>
                 </a>
               </p>
          
          <div class="comment" style="text-align: center;">
            

            
            
          </div>
        </div>
      </div>
  </article>
  <!-- Footer -->
  <footer>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <ul class="list-inline text-center">
            
            
            <li class="list-inline-item">
              <a href="https://github.com/tinaxiawuhao" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
            
              
            
              
            
              
            
              
            
              
            
              
            
              
              <!-- <li class="list-inline-item">
              <a href="https://tinaxiawuhao.github.io/atom.xml" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
                </span>
              </a>
              </li> -->
          </ul>
          <p class="copyright text-muted">Copyright &copy;<span>tianxia</span><br><a href="https://github.com/getgridea/gridea" class="Themeinfo">Powered by Gridea</a></p>
        </div>
      </div>
    </div>
   </footer>
  <!-- Bootstrap core JavaScript -->
  <script src="https://cdn.bootcss.com/twitter-bootstrap/4.3.1/js/bootstrap.bundle.min.js"></script>
  <!-- <script src="https://tinaxiawuhao.github.io/media/scripts/bootstrap.bundle.min.js"></script> -->
  <!-- Bootstrap core JavaScript -->
  <script src="https://cdn.jsdelivr.net/gh/Alanrk/clean-cdn@1.0/scripts/clean-blog.min.js"></script>
  <!-- <script src="https://tinaxiawuhao.github.io/media/scripts/clean-blog.min.js"></script> -->
  <style type="text/css">a.back_to_top{text-decoration:none;position:fixed;bottom:40px;right:30px;background:#f0f0f0;height:40px;width:40px;border-radius:50%;line-height:36px;font-size:18px;text-align:center;transition-duration:.5s;transition-propety:background-color;display:none}a.back_to_top span{color:#888}a.back_to_top:hover{cursor:pointer;background:#dfdfdf}a.back_to_top:hover span{color:#555}@media print,screen and(max-width:580px){.back_to_top{display:none!important}}</style>
<a id="back_to_top" href="#" class="back_to_top">
  <span>▲</span></a>
<script>$(document).ready((function(_this) {
    return function() {
      var bt;
      bt = $('#back_to_top');
      if ($(document).width() > 480) {
        $(window).scroll(function() {
          var st;
          st = $(window).scrollTop();
          if (st > 30) {
            return bt.css('display', 'block')
          } else {
            return bt.css('display', 'none')
          }
        });
        return bt.click(function() {
          $('body,html').animate({
            scrollTop: 0
          },
          800);
          return false
        })
      }
    }
  })(this));</script>
  
  <div id="landlord-parent">
    <div id="landlord">
        <div class="message" style="opacity:0"></div>
        <canvas id="live2d" width="240" height="250" class="live2d"></canvas>
    </div>
</div>

<script type="text/javascript">
    if (/(iPhone|iPad|iPod|iOS|Android)/i.test(navigator.userAgent)) {
        //移动端
        console.log("------ 移动端");
    } else {
        console.log("------ PC端 " + navigator.userAgent);

        addScript("https://cdn.jsdelivr.net/gh/850552586/ericamcdn@0.1/js/live2d.js", () => {
            // 加载完成后再loadlive2d
            loadlive2d("live2d", "https://tinaxiawuhao.github.io/media/live2d/assets/tororo.model.json");
        });

        var home_Path = "https://tinaxiawuhao.github.io/";
        addScript("https://tinaxiawuhao.github.io/media/live2d/js/message.js", () => { });
    }

    // 插入js文件，完成后callback
    function addScript(jsfile, callback) {
        var landlord_parent = document.getElementById("landlord-parent");
        var script = document.createElement("script");
        script.type = "text/javascript";
        script.src = jsfile;
        landlord_parent.appendChild(script);
        script.onload = script.onreadystatechange = function () {
            if (!this.readyState || this.readyState === "loaded" || this.readyState === "complete") {
                script.onload = script.onreadystatechange = null;
                if (callback && typeof callback == "function") {
                    callback(); //window[callback]();如果传递字符串过来 调用window['函数名']() 调用方法
                }
            }
        };
    }
</script>
  
  <script src="https://tinaxiawuhao.github.io/media/scripts/tocScript.js"></script>
</body>

</html>