<!DOCTYPE html>
<html>

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta content="yes" name="apple-mobile-web-app-capable" />
<meta content="black" name="apple-mobile-web-app-status-bar-style" />
<meta name="referrer" content="never">
<meta name="keywords" content="">
<meta name="description" content="欢迎访问[tianxia]的个人博客">
<meta name="author" content="kveln">
<title>第五章 Flink 流处理 API | tianxia</title>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.0/dist/css/bootstrap.min.css">
<link href="https://cdn.bootcss.com/font-awesome/5.11.2/css/all.min.css" rel="stylesheet">
<link rel="alternate" type="application/rss+xml" title="第五章 Flink 流处理 API | tianxia » Feed"
  href="https://tinaxiawuhao.github.io/atom.xml">
<link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.15.10/build/styles/androidstudio.min.css">
<link href="https://tinaxiawuhao.github.io/styles/main.css" rel="stylesheet">
<script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script>

<script src="https://cdn.jsdelivr.net/npm/@highlightjs/cdn-assets/highlight.min.js"></script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/850552586/ericamcdn@0.1/css/live2d.css">

<script>hljs.initHighlightingOnLoad();</script>

  <meta property="og:description" content="第五章 Flink 流处理 API" />
  <meta property="og:url" content="https://tinaxiawuhao.github.io/post/lvf921hnA/" />
  <meta property="og:locale" content="zh-CN" />
  <meta property="og:type" content="website" />
  <meta property="og:site_name" content="tianxia" />
  <!-- <script src="../assets/styles/scripts/tocScript.js"></script> -->
</head>

<body>
  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand" href="https://tinaxiawuhao.github.io">tianxia</a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
      data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
      aria-label="Toggle navigation">
      Menu
      <i class="fas fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item">
          
          <a class="nav-link" href="/">首页</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/archives">归档</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/tags">标签</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/post/about">关于</a>
          
        </li>
        
        <li class="nav-item">
          <div class="gridea-search-container">
            <form id="gridea-search-form" style="position: relative" data-update="1653818895998"
              action="/search/index.html">
              <input class="search-input" autocomplete="off" spellcheck="false" name="q" placeholder="搜索文章" />
              <i class="fas fa-search gt-c-content-color-first" style="position: absolute; top: 9px; left: 10px;"></i>
            </form>
          </div>
        </li>
      </ul>
    </div>
  </div>
</nav>
  <!-- Page Header -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand" href="https://tinaxiawuhao.github.io">tianxia</a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
      data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
      aria-label="Toggle navigation">
      Menu
      <i class="fas fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item">
          
          <a class="nav-link" href="/">首页</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/archives">归档</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/tags">标签</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/post/about">关于</a>
          
        </li>
        
        <li class="nav-item">
          <div class="gridea-search-container">
            <form id="gridea-search-form" style="position: relative" data-update="1653818895998"
              action="/search/index.html">
              <input class="search-input" autocomplete="off" spellcheck="false" name="q" placeholder="搜索文章" />
              <i class="fas fa-search gt-c-content-color-first" style="position: absolute; top: 9px; left: 10px;"></i>
            </form>
          </div>
        </li>
      </ul>
    </div>
  </div>
</nav>
<header class="masthead" style="background-image: url('https://tinaxiawuhao.github.io/media/images/home-bg.jpg')">
  <div class="overlay"></div>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">
        
          <!-- 没Title为其他页面Header -->
          
            <!-- 没Title并且有headerType为Post：文章Header -->
            <div class="post-heading">
              <span class="tags">
                
                <a href="https://tinaxiawuhao.github.io/tag/1WJzfk7Hi/" class="tag">flink</a>
                
              </span>
              <h1>第五章 Flink 流处理 API</h1>
              <span class="meta">
                Posted on
                2021-04-11，20 min read
              </span>
            </div>
          
        
      </div>
    </div>
  </div>
</header>
  <!-- Post Content -->
  <article id="post-content-article">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto post-content-container">
          
          <img class="post-feature-header-image" src="https://tinaxiawuhao.github.io/post-images/lvf921hnA.png" alt="封面图">
          </img>
          
          <h2 id="environment">Environment</h2>
<h3 id="getexecutionenvironment">getExecutionEnvironment</h3>
<p style="text-indent:2em">创建一个执行环境， 表示当前执行程序的上下文。 如果程序是独立调用的， 则
此方法返回本地执行环境； 如果从命令行客户端调用程序以提交到集群， 则此方法
返回此集群的执行环境， 也就是说， getExecutionEnvironment 会根据查询运行的方
式决定返回什么样的运行环境， 是最常用的一种创建执行环境的方式。</p>
<pre><code class="language-java">// 批处理环境
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment()
// 流式数据处理环境
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()
</code></pre>
<p>如果没有设置并行度， 会以 flink-conf.yaml 中的配置为准， 默认是 1。<br>
<img src="https://tinaxiawuhao.github.io/post-images/1618450455593.png" alt="" loading="lazy"></p>
<h3 id="createlocalenvironment">createLocalEnvironment</h3>
<p>返回本地执行环境， 需要在调用时指定默认的并行度。</p>
<pre><code class="language-java"> LocalStreamEnvironment localEnvironment = StreamExecutionEnvironment.createLocalEnvironment(1);
</code></pre>
<h3 id="createremoteenvironment">createRemoteEnvironment</h3>
<p style="text-indent:2em">返回集群执行环境， 将 Jar 提交到远程服务器。 需要在调用时指定 JobManager的 IP 和端口号， 并指定要在集群中运行的 Jar 包。</p>
<pre><code class="language-java"> final ExecutionEnvironment env = ExecutionEnvironment.createRemoteEnvironment(
            cluster.getHostname(),
            cluster.getPort(),
            config
    );
</code></pre>
<p>scala</p>
<pre><code class="language-java">val env = ExecutionEnvironment.createRemoteEnvironment(&quot;jobmanage-hostname&quot;,
6123,&quot;YOURPATH//wordcount.jar&quot;)
</code></pre>
<h3 id="setparallelism">setParallelism</h3>
<pre><code class="language-java">// 为了打印到控制台的结果不乱序，我们配置全局的并发为1，改变并发对结果正确性没有影响
env.setParallelism(1);
</code></pre>
<h2 id="source">Source</h2>
<h3 id="从集合读取数据">从集合读取数据</h3>
<pre><code class="language-java">package myflink;

import lombok.*;

//传感器温度读数的数据类型
@Data
@NoArgsConstructor
@AllArgsConstructor
@Builder
@ToString
public class SensorReading {
    //属性 id,时间戳,温度值
    private String id;
    private Long timestamp;
    private Double temperature;

}

public class SourceReading_Collection {
    public static void main(String[] args) throws Exception {
        //创建执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        //设置并行调度
        //env.setParallelism(1);
        //从集合中读取数据
        //属性 id,时间戳,温度值
        DataStream&lt;SensorReading&gt; dataStream = env.fromCollection(Arrays.asList(new SensorReading(&quot;sensor_1&quot;, 1537718199L, 35.8),
                new SensorReading(&quot;sensor_6&quot;, 1547718201L, 15.4),
                new SensorReading(&quot;sensor_7&quot;, 1547718202L, 6.7),
                new SensorReading(&quot;sensor_10&quot;, 1547718205L, 38.1)));
 
        DataStream&lt;Integer&gt; integerDataStream = env.fromElements(1, 2, 4, 67, 189);
        //打印输出
        dataStream.print(&quot;data&quot;);
        integerDataStream.print(&quot;int&quot;);
        //执行 Flink的jobName
        env.execute(&quot;SensorReading&quot;);
    }
}
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://tinaxiawuhao.github.io/post-images/1618820709627.png" alt="" loading="lazy"></figure>
<h3 id="从文件读取数据">从文件读取数据</h3>
<pre><code class="language-java">public class SourceReading_File {
    public static void main(String[] args) throws Exception {
        //创建执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        //读取文件
       DataStream&lt;String&gt; dataStream = env.readTextFile(filePath,charsetName);
       //打印输出
        dataStream.print(&quot;data&quot;);
        //执行 Flink的jobName
        env.execute(&quot;SensorReading&quot;);
    }
}
</code></pre>
<h3 id="以-kafka-消息队列的数据作为来源">以 kafka 消息队列的数据作为来源</h3>
<p>需要引入 kafka 连接器的依赖：<br>
<code>pom.xml</code></p>
<pre><code class="language-java">&lt;!--
https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka-0.11
--&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
    &lt;artifactId&gt;flink-connector-kafka-0.11_2.12&lt;/artifactId&gt;
    &lt;version&gt;1.10.1&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<pre><code class="language-sh">//1. 创建一个Topic名为“test20201217”的主题
kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test20201217
//2. 创建producer(生产者)，生产主题的消息
kafka-console-producer.bat --broker-list localhost:9092 --topic test20201217
//3. 创建consumer(消费者)，消费主题消息
kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic test20201217
</code></pre>
<p>具体代码如下：</p>
<pre><code class="language-java">/**
 * kafkaSource
 *
 *    從指定的offset出消费kafka
 */
public class StreamingKafkaSource {

    public static void main(String[] args) throws Exception {
         // 创建流处理的执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
 
        // 配置KafKa
        //配置KafKa和Zookeeper的ip和端口
        Properties properties = new Properties();
        properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);
        properties.setProperty(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;);
        properties.setProperty(&quot;group.id&quot;, &quot;consumer-group&quot;);
        //将kafka和zookeeper配置信息加载到Flink的执行环境当中StreamExecutionEnvironment
        FlinkKafkaConsumer011&lt;String&gt; myConsumer = new FlinkKafkaConsumer011&lt;String&gt;(&quot;test20201217&quot;, new SimpleStringSchema(),
                properties);
 
        //添加数据源，此处选用数据流的方式，将KafKa中的数据转换成Flink的DataStream类型
        DataStream&lt;String&gt; stream = env.addSource(myConsumer);
 
 
        //打印输出
        stream.print();
        //执行Job，Flink执行环境必须要有job的执行步骤，而以上的整个过程就是一个Job
        env.execute(&quot;kafka sink test&quot;);
    }
}
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://tinaxiawuhao.github.io/post-images/1618821668432.png" alt="" loading="lazy"></figure>
<h3 id="自定义-source">自定义 Source</h3>
<p style="text-indent:2em">除了以上的 source 数据来源， 我们还可以自定义 source。 需要做的， 只是传入一个 SourceFunction 就可以。 具体调用如下：</p>
<pre><code class="language-java"> env.addSource( new MySensorSource() )
</code></pre>
<p>我们希望可以随机生成传感器数据， MySensorSource 具体的代码实现如下：</p>
<pre><code class="language-java">import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.java.tuple.Tuple;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.datastream.WindowedStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.source.SourceFunction;
import org.apache.flink.streaming.api.functions.windowing.WindowFunction;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.api.windowing.windows.TimeWindow;
import org.apache.flink.util.Collector;
import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.lucene.analysis.CachingTokenFilter;
 
import java.util.Random;
 
public class MySelfSourceTest01 {
    public static void main(String[] args) {
        Logger.getLogger(&quot;org&quot;).setLevel(Level.OFF);
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        DataStreamSource&lt;String&gt; dataStreamSource = env.addSource(new SourceFunction&lt;String&gt;() {
            @Override
            public void run(SourceContext&lt;String&gt; ctx) throws Exception {
                Random random = new Random();
                // 循环可以不停的读取静态数据
                while (true) {
                    int nextInt = random.nextInt(100);
                    ctx.collect(&quot;random : &quot; + nextInt);
                    Thread.sleep(1000);
                }
            }
 
            @Override
            public void cancel() {
 
            }
        });
        WindowedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple, TimeWindow&gt; window = dataStreamSource.map(new MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() {
            @Override
            public Tuple2&lt;String, Integer&gt; map(String value) throws Exception {
                String[] sps = value.split(&quot;:&quot;);
                return new Tuple2&lt;&gt;(value, Integer.parseInt(sps[1].trim()));
            }
        }).keyBy(0).timeWindow(Time.seconds(5));
 
        SingleOutputStreamOperator&lt;String&gt; apply = window.apply(new WindowFunction&lt;Tuple2&lt;String, Integer&gt;, String, Tuple, TimeWindow&gt;() {
            @Override
            public void apply(Tuple tuple, TimeWindow window, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; input, Collector&lt;String&gt; out) throws Exception {
                input.forEach(x -&gt; {
                    System.out.println(&quot;apply function -&gt; &quot; + x.f0);
                    out.collect(x.f0);
                });
            }
        });
 
        apply.print();
 
        try {
            env.execute(&quot;myself_source_test01&quot;);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
</code></pre>
<h2 id="transform">Transform</h2>
<p>转换算子</p>
<h3 id="map">map</h3>
<figure data-type="image" tabindex="3"><img src="https://tinaxiawuhao.github.io/post-images/1618450860405.png" alt="" loading="lazy"></figure>
<pre><code class="language-java">stream.map { x =&gt; x * 2 }
// 1.map.把string转化成长度输出
//参数T  R
//T就是传的数据，什么类型都行
//R就是返回的类型
DataStream&lt;Integer&gt; mapStream = stringDataStream.map(new MapFunction&lt;String, Integer&gt;() {
    @Override
    public Integer map(String value) throws Exception {
        return value.length();
    }
});

</code></pre>
<h3 id="flatmap">flatMap</h3>
<p>flatMap 的函数签名： <code>def flatMap[A,B](as: List[A])(f: A ⇒ List[B]): List[B]</code><br>
例如: flatMap(List(1,2,3))(i ⇒ List(i,i))<br>
结果是 List(1,1,2,2,3,3),<br>
而 List(&quot;a b&quot;, &quot;c d&quot;).flatMap(line ⇒ line.split(&quot; &quot;))<br>
结果是 List(a, b, c, d)。</p>
<pre><code class="language-java">stream.flatMap{
    x =&gt; x.split(&quot; &quot;)
}
//2、flatmap,按照逗号分隔
DataStream&lt;String&gt; flatMapStream = stringDataStream.flatMap(new FlatMapFunction&lt;String, String&gt;() {
    @Override
    public void flatMap(String value, Collector&lt;String&gt; out) throws Exception {
        String[] fields = value.split(&quot;,&quot;);
        for(String field:fields)
            out.collect(field);
    }
});

</code></pre>
<h3 id="filter">Filter</h3>
<figure data-type="image" tabindex="4"><img src="https://tinaxiawuhao.github.io/post-images/1618450868563.png" alt="" loading="lazy"></figure>
<pre><code class="language-java">stream.filter{
    x =&gt; x == 1
}
 //3.filter,筛选sensor_1开头的的id对应的数据
DataStream&lt;String&gt; filterStream = stringDataStream.filter(new FilterFunction&lt;String&gt;() {
    @Override
    public boolean filter(String value) throws Exception {
        return value.startsWith(&quot;sensor_1&quot;);
    }
});
</code></pre>
<h3 id="keyby">KeyBy</h3>
<figure data-type="image" tabindex="5"><img src="https://tinaxiawuhao.github.io/post-images/1618450909734.png" alt="" loading="lazy"></figure>
<p><code>DataStream → KeyedStream</code>： 逻辑地将一个流拆分成不相交的分区， 每个分区包含具有相同 key 的元素， 在内部以 hash 的形式实现的。</p>
<pre><code class="language-java"> DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; windowCounts = text
                .flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() {
                    @Override
                    public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) {
                        for (String word : value.split(&quot;\\s&quot;)) {
                            out.collect(Tuple2.of(word, 1));
                        }
                    }
                })
                //按Tuple2的第一个属性进行分区
                .keyBy(0)

//或者根据对象属性进行分区
 KeyedStream&lt;SensorReading, String&gt; tempkeyedStream = mapDataStream.keyBy(mpdata -&gt; mpdata.getId());
</code></pre>
<h3 id="滚动聚合算子rolling-aggregation">滚动聚合算子（Rolling Aggregation）</h3>
<p>这些算子可以针对 KeyedStream 的每一个支流做聚合。</p>
<ol>
<li>sum()</li>
<li>min()</li>
<li>max()</li>
<li>minBy()</li>
<li>maxBy()</li>
</ol>
<h3 id="reduce">Reduce</h3>
<p><code>KeyedStream → DataStream</code>： 一个分组数据流的聚合操作， 合并当前的元素和上次聚合的结果， 产生一个新的值， 返回的流中包含每一次聚合的结果， 而不是只返回最后一次聚合的最终结果。</p>
<pre><code class="language-java">DataStream&lt;String&gt; stream2 = env.readTextFile(&quot;YOUR_PATH\\sensor.txt&quot;)
    .map( data =&gt; {
        String[] dataArray = data.split(&quot;,&quot;)
        SensorReading(dataArray(0).trim, dataArray(1).trim.toLong,
        dataArray(2).trim.toDouble)
    })
    .keyBy(&quot;id&quot;)
    .reduce( (x, y) =&gt; SensorReading(x.id, x.timestamp + 1, y.temperature) )
</code></pre>
<h3 id="split-和-select">Split 和 Select</h3>
<p>Split<br>
<img src="https://tinaxiawuhao.github.io/post-images/1618450986439.png" alt="" loading="lazy"><br>
图 Split<br>
<code>DataStream → SplitStream</code>： 根据某些特征把一个 DataStream 拆分成两个或者<br>
多个 DataStream。<br>
Select<br>
<img src="https://tinaxiawuhao.github.io/post-images/1618450993631.png" alt="" loading="lazy"><br>
图 Select<br>
<code>SplitStream→ DataStream</code>： 从一个 SplitStream 中获取一个或者多个<br>
DataStream。<br>
需求： 传感器数据按照温度高低（ 以 30 度为界） ， 拆分成两个流。</p>
<pre><code class="language-java">public static void main(String[] args) throws Exception {
        final StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment();
        DataStream&lt;Long&gt; input=env.generateSequence(0,10);
        SplitStream&lt;Long&gt; splitStream = input.split(new OutputSelector&lt;Long&gt;(){
            @Override
            public Iterable&lt;String&gt; select(Long value) {
                List&lt;String&gt; output = new ArrayList&lt;String&gt;();
                if (value % 2 == 0) {
                    output.add(&quot;even&quot;);
                }else {
                    output.add(&quot;odd&quot;);
                }
                return output;
            }
        });
        //splitStream.print();
        DataStream&lt;Long&gt; even = splitStream.select(&quot;even&quot;);
        DataStream&lt;Long&gt; odd = splitStream.select(&quot;odd&quot;);
        DataStream&lt;Long&gt; all = splitStream.select(&quot;even&quot;,&quot;odd&quot;);
        //even.print();
        odd.print();
        //all.print();
        env.execute();
    }
</code></pre>
<h3 id="connect-和-comap">Connect 和 CoMap</h3>
<p><img src="https://tinaxiawuhao.github.io/post-images/1618451022910.png" alt="" loading="lazy"><br>
图 Connect 算子<br>
<code>DataStream,DataStream → ConnectedStreams</code>： 连接两个保持他们类型的数<br>
据流， 两个数据流被 Connect 之后， 只是被放在了一个同一个流中， 内部依然保持<br>
各自的数据和形式不发生任何变化， 两个流相互独立。</p>
<p><code>CoMap</code>,<code>CoFlatMap</code><br>
<img src="https://tinaxiawuhao.github.io/post-images/1618451082833.png" alt="" loading="lazy"><br>
图 CoMap/CoFlatMap</p>
<p><code>ConnectedStreams → DataStream</code>： 作用于 ConnectedStreams 上， 功能与 map<br>
和 flatMap 一样， 对 ConnectedStreams 中的每一个 Stream 分别进行 map 和 flatMap<br>
处理。</p>
<pre><code class="language-java">DataStream  warning = high.map( sensorData =&gt; (sensorData.id,
sensorData.temperature) )
ConnectedStreams connected = warning.connect(low)
DataStream coMap = connected.map(
    warningData =&gt; (warningData._1, warningData._2, &quot;warning&quot;),
    lowData =&gt; (lowData.id, &quot;healthy&quot;)
)
</code></pre>
<h3 id="union">Union</h3>
<p><img src="https://tinaxiawuhao.github.io/post-images/1618451109858.png" alt="" loading="lazy"><br>
图 Union<br>
<code>DataStream → DataStream</code>： 对两个或者两个以上的 DataStream 进行 union 操<br>
作， 产生一个包含所有 DataStream 元素的新 DataStream。</p>
<pre><code class="language-java">//合并以后打印
 DataStream&lt;StartUpLog&gt; unionStream = appStoreStream.union(otherStream)
unionStream.print(&quot;union:::&quot;)
</code></pre>
<p><code>Connect</code> 与 <code>Union</code> 区别：</p>
<blockquote>
<ol>
<li>Union 之前两个流的类型必须是一样， Connect 可以不一样， 在之后的 coMap<br>
中再去调整成为一样的。</li>
<li>Connect 只能操作两个流， Union 可以操作多个。</li>
</ol>
</blockquote>
<h2 id="支持的数据类型">支持的数据类型</h2>
<p style="text-indent:2em">Flink 流应用程序处理的是以数据对象表示的事件流。 所以在 Flink 内部， 我们
需要能够处理这些对象。 它们需要被序列化和反序列化， 以便通过网络传送它们；
或者从状态后端、 检查点和保存点读取它们。 为了有效地做到这一点， Flink 需要明
确知道应用程序所处理的数据类型。 Flink 使用类型信息的概念来表示数据类型， 并
为每个数据类型生成特定的序列化器、 反序列化器和比较器。
Flink 还具有一个类型提取系统， 该系统分析函数的输入和返回类型， 以自动获
取类型信息， 从而获得序列化器和反序列化器。 但是， 在某些情况下， 例如 lambda
函数或泛型类型， 需要显式地提供类型信息， 才能使应用程序正常工作或提高其性
能。</p>
<p>Flink 支持 Java 和 Scala 中所有常见数据类型。 使用最广泛的类型有以下几种。</p>
<h3 id="基础数据类型">基础数据类型</h3>
<p>Flink 支持所有的 Java 和 Scala 基础数据类型， Int, Double, Long, String, …​</p>
<pre><code class="language-java">DataStream&lt;Long&gt; numbers = env.fromElements(1L, 2L, 3L, 4L)
numbers.map( n =&gt; n + 1 )
</code></pre>
<h3 id="java-和-scala-元组tuples">Java 和 Scala 元组（Tuples）</h3>
<pre><code class="language-java">DataStream&lt;String, Integer&gt; persons= env.fromElements(
    (&quot;Adam&quot;, 17),
    (&quot;Sarah&quot;, 23) 
)
persons.filter(p =&gt; p._2 &gt; 18)
</code></pre>
<h3 id="scala-样例类case-classes">Scala 样例类（case classes）</h3>
<pre><code class="language-java">case class Person(name: String, age: Int)
val persons: DataStream[Person] = env.fromElements(
Person(&quot;Adam&quot;, 17),
Person(&quot;Sarah&quot;, 23) )
persons.filter(p =&gt; p.age &gt; 18)
</code></pre>
<h3 id="java-简单对象pojos">Java 简单对象（POJOs）</h3>
<pre><code class="language-java">public class Person {
    public String name;
    public int age;
    public Person() {}
    public Person(String name, int age) {
        this.name = name;
        this.age = age;
    }
}
 DataStream&lt;Person&gt; persons = env.fromElements(
new Person(&quot;Alex&quot;, 42),
new Person(&quot;Wendy&quot;, 23));
</code></pre>
<h3 id="其它arrays-lists-maps-enums-等等">其它（Arrays, Lists, Maps, Enums, 等等）</h3>
<p>Flink 对 Java 和 Scala 中的一些特殊目的的类型也都是支持的， 比如 Java 的<br>
ArrayList， HashMap， Enum 等等。</p>
<h2 id="实现-udf-函数更细粒度的控制流">实现 UDF 函数——更细粒度的控制流</h2>
<h3 id="函数类function-classes">函数类（Function Classes）</h3>
<p>Flink 暴露了所有 udf 函数的接口(实现方式为接口或者抽象类)。 例如<br>
MapFunction, FilterFunction, ProcessFunction 等等。<br>
下面例子实现了 FilterFunction 接口：</p>
<pre><code class="language-java">public class CustomFilterFunction implements FilterFunction&lt;SensorReading&gt; {
    @Override
    public boolean filter(SensorReading sensorReading) throws Exception {
        return sensorReading.temperature&gt;30.0;
    }
}
public static void main(String[] args) throws Exception {
 // 创建流处理的执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
 
        // 从文件中读取数据
        String inputPath  = &quot;F:\\Projects\\BigData\\Flink\\FlinkTutorial\\src\\main\\resources\\sensor.txt&quot;;
        // 获取数据
        DataStreamSource&lt;String&gt; dataStream  = env.readTextFile(inputPath);
 
        // 1、先转换成SensorReading类型（简单转换操作）
        DataStream&lt;SensorReading&gt; stream =  dataStream.map(new MapFunction&lt;String, SensorReading&gt;() {
            @Override
            public SensorReading map(String data) throws Exception {
                String[] arr = data.split(&quot;,&quot;);
                return new SensorReading(arr[0], arr[1], Double.valueOf(arr[2].toString()));
            }
        });
 
       // 调用自定义CustomFilterFunction类的，实现过滤
       DataStream&lt;SensorReading&gt; dataStreamFilter = stream.filter(new CustomFilterFunction());
 
       dataStreamFilter .print(&quot;CustomFilterFunction&quot;);
      
       env.execute(&quot;Function test&quot;);
}
</code></pre>
<p>还可以将函数实现成匿名类</p>
<pre><code class="language-java"> DataStream&lt;SensorReading&gt; dataStreamFilter = stream.filter(new FilterFunction&lt;SensorReading&gt;() {
    @Override
    public boolean filter(SensorReading sensorReading) throws Exception {
        return sensorReading.temperature&gt;30.0;
    }
);
</code></pre>
<p>我们 filter 的字符串&quot;flink&quot;还可以当作参数传进去。</p>
<pre><code class="language-java">DataStream&lt;String&gt; tweets = ...
DataStream&lt;String&gt; flinkTweets = tweets.filter(new KeywordFilter(&quot;flink&quot;))

public class KeywordFilter(String keyWord) implements FilterFunction&lt;String&gt; {
    @Override
    public boolean filter(String value) throws Exception {
        return value.contains(keyWord)
    }
}
</code></pre>
<h3 id="匿名函数lambda-functions">匿名函数（Lambda Functions）</h3>
<pre><code class="language-java">DataStream&lt;String&gt; tweets = ...
DataStream&lt;String&gt; flinkTweets = tweets.filter((value) -&gt;value.contains(&quot;flink&quot;))
</code></pre>
<h3 id="富函数rich-functions">富函数（Rich Functions）</h3>
<p>“ 富函数” 是 DataStream API 提供的一个函数类的接口， 所有 Flink 函数类都<br>
有其 Rich 版本。 它与常规函数的不同在于， 可以获取运行环境的上下文， 并拥有一<br>
些生命周期方法， 所以可以实现更复杂的功能。</p>
<ol>
<li>RichMapFunction</li>
<li>RichFlatMapFunction</li>
<li>RichFilterFunction<br>
…​</li>
</ol>
<p>Rich Function 有一个生命周期的概念。 典型的生命周期方法有：</p>
<ol>
<li>open()方法是 rich function 的初始化方法， 当一个算子例如 map 或者 filter<br>
被调用之前 open()会被调用。</li>
<li>close()方法是生命周期中的最后一个调用的方法， 做一些清理工作。</li>
<li>getRuntimeContext()方法提供了函数的 RuntimeContext 的一些信息， 例如函<br>
数执行的并行度， 任务的名字， 以及 state 状态</li>
</ol>
<pre><code class="language-java">    public class MyFlatMap extends RichFlatMapFunction&lt;Tuple2&lt;Integer, Integer&gt;, Tuple2&lt;Integer, Integer&gt;&gt; {
    Integer subTaskIndex = 0
    StreamingRuntimeContext context = (StreamingRuntimeContext) getRuntimeContext()
     public void open(Configuration parameters) throws Exception {
         subTaskIndex = context.getIndexOfThisSubtask
        // 以下可以做一些初始化工作， 例如建立一个和 HDFS 的连接
    }
    @Override
    public void flatMap(Tuple2&lt;Integer, Integer&gt; integerIntegerTuple2, Collector&lt;Tuple2&lt;Integer, Integer&gt;&gt; collector) throws Exception {
            if (in % 2 == subTaskIndex) {
                out.collect((subTaskIndex, in))
            }
        }
    public void close() throws Exception {
        // 以下做一些清理工作， 例如断开和 HDFS 的连接。
    }
}
</code></pre>
<h2 id="sink">Sink</h2>
<p style="text-indent:2em">Flink 没有类似于 spark 中 foreach 方法， 让用户进行迭代的操作。 虽有对外的
输出操作都要利用 Sink 完成。 最后通过类似如下方式完成整个任务最终输出操作。
stream.addSink(new MySink(xxxx))
官方提供了一部分的框架的 sink。 除此以外， 需要用户自定义实现 sink。</p>
<figure data-type="image" tabindex="6"><img src="https://tinaxiawuhao.github.io/post-images/1618451579569.png" alt="" loading="lazy"></figure>
<h3 id="kafka">Kafka</h3>
<pre><code class="language-java">pom.xml
&lt;!--
https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka-0.11
--&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
    &lt;artifactId&gt;flink-connector-kafka-0.11_2.12&lt;/artifactId&gt;
    &lt;version&gt;1.10.1&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>主函数中添加 sink：</p>
<pre><code class="language-java">DataStream&lt;String&gt; union = high.union(low).map(item-&gt;item.temperature.toString)
union.addSink(new FlinkKafkaProducer&lt;String&gt;(&quot;localhost:9092&quot;,
&quot;test&quot;, new SimpleStringSchema()))
</code></pre>
<h3 id="redis">Redis</h3>
<p>pom.xml</p>
<pre><code class="language-java">&lt;!-- https://mvnrepository.com/artifact/org.apache.bahir/flink-connector-redis
--&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.bahir&lt;/groupId&gt;
    &lt;artifactId&gt;flink-connector-redis_2.11&lt;/artifactId&gt;
    &lt;version&gt;1.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>定义一个 redis 的 mapper 类， 用于定义保存到 redis 时调用的命令：</p>
<pre><code class="language-java">public class MyRedisMapper extends RedisMapper&lt;SensorReading&gt;{
    public RedisCommandDescription getCommandDescription{
        new RedisCommandDescription(RedisCommand.HSET, &quot;sensor_temperature&quot;)
    } 
    public String  getValueFromData(SensorReading t){
        t.temperature.toString
        String getKeyFromData(SensorReading t) {
            return t.id
        }
    }
}
</code></pre>
<p>在主函数中调用：</p>
<pre><code class="language-java">dataStream.addSink( new RedisSink&lt;SensorReading&gt;( new
FlinkJedisPoolConfig.Builder().setHost(&quot;localhost&quot;).setPort(6379).build(), new MyRedisMapper) )
</code></pre>
<h3 id="elasticsearch">Elasticsearch</h3>
<p>pom.xml</p>
<pre><code class="language-java">&lt;dependency&gt;
    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
    &lt;artifactId&gt;flink-connector-elasticsearch6_2.12&lt;/artifactId&gt;
    &lt;version&gt;1.10.1&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>在主函数中调用：</p>
<pre><code class="language-java">List httpHosts = new ArrayList&lt;HttpHost&gt;()
httpHosts.add(new HttpHost(&quot;localhost&quot;, 9200))
DataStream esSinkBuilder = new ElasticsearchSink.Builder&lt;SensorReading&gt;( httpHosts,new ElasticsearchSinkFunction&lt;SensorReading&gt; {
    public  Unit process(SensorReading t, RuntimeContext runtimeContext,
    RequestIndexer requestIndexer ) {
        println(&quot;saving data: &quot; + t)
        map json = new util.HashMap&lt;String, String&gt;()
        json.put(&quot;data&quot;, t.toString)
        IndexRequest  indexRequest =
        Requests.indexRequest().index(&quot;sensor&quot;).`type`(&quot;readingData&quot;).source(json)
        requestIndexer.add(indexRequest)
        println(&quot;saved successfully&quot;)
    }
} )
dataStream.addSink( esSinkBuilder.build() )
</code></pre>
<h3 id="jdbc-自定义-sink">JDBC 自定义 sink</h3>
<pre><code class="language-java">&lt;!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;mysql&lt;/groupId&gt;
    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
    &lt;version&gt;5.1.44&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>添加 MyJdbcSink</p>
<pre><code class="language-java">public class MyJdbcSink() extends RichSinkFunction&lt;SensorReading&gt;{
    Connection conn;
    PreparedStatement insertStmt;
    PreparedStatement updateStmt;
    // open 主要是创建连接
    public Unit open(Configuration parameters)  = {
        super.open(parameters)
        conn = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/test&quot;,
        &quot;root&quot;, &quot;123456&quot;)
        insertStmt = conn.prepareStatement(&quot;INSERT INTO temperatures (sensor,
        temp) VALUES (?, ?)&quot;)
        updateStmt = conn.prepareStatement(&quot;UPDATE temperatures SET temp = ? WHERE
        sensor = ?&quot;)
    } 
    //调用连接， 执行 sql
    public Unit invoke(SensorReading value , SinkFunction.Context[] context）{
        updateStmt.setDouble(1, value.temperature)
        updateStmt.setString(2, value.id)
        updateStmt.execute()
        if (updateStmt.getUpdateCount == 0) {
            insertStmt.setString(1, value.id)
            insertStmt.setDouble(2, value.temperature)
            insertStmt.execute()
        }
    } 
    public Unit close() {
        insertStmt.close()
        updateStmt.close()
        conn.close()
    }
}
</code></pre>
<p>在 main 方法中增加， 把明细保存到 mysql 中</p>
<pre><code class="language-java">dataStream.addSink(new MyJdbcSink())
</code></pre>

          <div class="toc-container"><ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#environment">Environment</a>
<ul>
<li><a href="#getexecutionenvironment">getExecutionEnvironment</a></li>
<li><a href="#createlocalenvironment">createLocalEnvironment</a></li>
<li><a href="#createremoteenvironment">createRemoteEnvironment</a></li>
<li><a href="#setparallelism">setParallelism</a></li>
</ul>
</li>
<li><a href="#source">Source</a>
<ul>
<li><a href="#%E4%BB%8E%E9%9B%86%E5%90%88%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE">从集合读取数据</a></li>
<li><a href="#%E4%BB%8E%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE">从文件读取数据</a></li>
<li><a href="#%E4%BB%A5-kafka-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E6%95%B0%E6%8D%AE%E4%BD%9C%E4%B8%BA%E6%9D%A5%E6%BA%90">以 kafka 消息队列的数据作为来源</a></li>
<li><a href="#%E8%87%AA%E5%AE%9A%E4%B9%89-source">自定义 Source</a></li>
</ul>
</li>
<li><a href="#transform">Transform</a>
<ul>
<li><a href="#map">map</a></li>
<li><a href="#flatmap">flatMap</a></li>
<li><a href="#filter">Filter</a></li>
<li><a href="#keyby">KeyBy</a></li>
<li><a href="#%E6%BB%9A%E5%8A%A8%E8%81%9A%E5%90%88%E7%AE%97%E5%AD%90rolling-aggregation">滚动聚合算子（Rolling Aggregation）</a></li>
<li><a href="#reduce">Reduce</a></li>
<li><a href="#split-%E5%92%8C-select">Split 和 Select</a></li>
<li><a href="#connect-%E5%92%8C-comap">Connect 和 CoMap</a></li>
<li><a href="#union">Union</a></li>
</ul>
</li>
<li><a href="#%E6%94%AF%E6%8C%81%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B">支持的数据类型</a>
<ul>
<li><a href="#%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B">基础数据类型</a></li>
<li><a href="#java-%E5%92%8C-scala-%E5%85%83%E7%BB%84tuples">Java 和 Scala 元组（Tuples）</a></li>
<li><a href="#scala-%E6%A0%B7%E4%BE%8B%E7%B1%BBcase-classes">Scala 样例类（case classes）</a></li>
<li><a href="#java-%E7%AE%80%E5%8D%95%E5%AF%B9%E8%B1%A1pojos">Java 简单对象（POJOs）</a></li>
<li><a href="#%E5%85%B6%E5%AE%83arrays-lists-maps-enums-%E7%AD%89%E7%AD%89">其它（Arrays, Lists, Maps, Enums, 等等）</a></li>
</ul>
</li>
<li><a href="#%E5%AE%9E%E7%8E%B0-udf-%E5%87%BD%E6%95%B0%E6%9B%B4%E7%BB%86%E7%B2%92%E5%BA%A6%E7%9A%84%E6%8E%A7%E5%88%B6%E6%B5%81">实现 UDF 函数——更细粒度的控制流</a>
<ul>
<li><a href="#%E5%87%BD%E6%95%B0%E7%B1%BBfunction-classes">函数类（Function Classes）</a></li>
<li><a href="#%E5%8C%BF%E5%90%8D%E5%87%BD%E6%95%B0lambda-functions">匿名函数（Lambda Functions）</a></li>
<li><a href="#%E5%AF%8C%E5%87%BD%E6%95%B0rich-functions">富函数（Rich Functions）</a></li>
</ul>
</li>
<li><a href="#sink">Sink</a>
<ul>
<li><a href="#kafka">Kafka</a></li>
<li><a href="#redis">Redis</a></li>
<li><a href="#elasticsearch">Elasticsearch</a></li>
<li><a href="#jdbc-%E8%87%AA%E5%AE%9A%E4%B9%89-sink">JDBC 自定义 sink</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
          
              <hr />
            
          
            
            <p class="next-post">上一篇：
                <a href="https://tinaxiawuhao.github.io/post/4_EhGqVjY/">
                  <span class="post-title">
                    第四章 Flink 运行架构&rarr;
                  </span>
                </a>
              </p>
            
          
            
           <p class="prev-post">下一篇：
                 <a href="https://tinaxiawuhao.github.io/post/FnbZ5kiHQ/">
                   <span class="post-title">
                     第六章 Flink 中的 Window&rarr;
                   </span>
                 </a>
               </p>
          
          <div class="comment" style="text-align: center;">
            

            
            
          </div>
        </div>
      </div>
  </article>
  <!-- Footer -->
  <footer>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <ul class="list-inline text-center">
            
            
            <li class="list-inline-item">
              <a href="https://github.com/tinaxiawuhao" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
            
              
            
              
            
              
            
              
            
              
            
              
            
              
              <!-- <li class="list-inline-item">
              <a href="https://tinaxiawuhao.github.io/atom.xml" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
                </span>
              </a>
              </li> -->
          </ul>
          <p class="copyright text-muted">Copyright &copy;<span>tianxia</span><br><a href="https://github.com/getgridea/gridea" class="Themeinfo">Powered by Gridea</a></p>
        </div>
      </div>
    </div>
   </footer>
  <!-- Bootstrap core JavaScript -->
  <script src="https://cdn.bootcss.com/twitter-bootstrap/4.3.1/js/bootstrap.bundle.min.js"></script>
  <!-- <script src="https://tinaxiawuhao.github.io/media/scripts/bootstrap.bundle.min.js"></script> -->
  <!-- Bootstrap core JavaScript -->
  <script src="https://cdn.jsdelivr.net/gh/Alanrk/clean-cdn@1.0/scripts/clean-blog.min.js"></script>
  <!-- <script src="https://tinaxiawuhao.github.io/media/scripts/clean-blog.min.js"></script> -->
  <style type="text/css">a.back_to_top{text-decoration:none;position:fixed;bottom:40px;right:30px;background:#f0f0f0;height:40px;width:40px;border-radius:50%;line-height:36px;font-size:18px;text-align:center;transition-duration:.5s;transition-propety:background-color;display:none}a.back_to_top span{color:#888}a.back_to_top:hover{cursor:pointer;background:#dfdfdf}a.back_to_top:hover span{color:#555}@media print,screen and(max-width:580px){.back_to_top{display:none!important}}</style>
<a id="back_to_top" href="#" class="back_to_top">
  <span>▲</span></a>
<script>$(document).ready((function(_this) {
    return function() {
      var bt;
      bt = $('#back_to_top');
      if ($(document).width() > 480) {
        $(window).scroll(function() {
          var st;
          st = $(window).scrollTop();
          if (st > 30) {
            return bt.css('display', 'block')
          } else {
            return bt.css('display', 'none')
          }
        });
        return bt.click(function() {
          $('body,html').animate({
            scrollTop: 0
          },
          800);
          return false
        })
      }
    }
  })(this));</script>
  
  <div id="landlord-parent">
    <div id="landlord">
        <div class="message" style="opacity:0"></div>
        <canvas id="live2d" width="240" height="250" class="live2d"></canvas>
    </div>
</div>

<script type="text/javascript">
    if (/(iPhone|iPad|iPod|iOS|Android)/i.test(navigator.userAgent)) {
        //移动端
        console.log("------ 移动端");
    } else {
        console.log("------ PC端 " + navigator.userAgent);

        addScript("https://cdn.jsdelivr.net/gh/850552586/ericamcdn@0.1/js/live2d.js", () => {
            // 加载完成后再loadlive2d
            loadlive2d("live2d", "https://tinaxiawuhao.github.io/media/live2d/assets/tororo.model.json");
        });

        var home_Path = "https://tinaxiawuhao.github.io/";
        addScript("https://tinaxiawuhao.github.io/media/live2d/js/message.js", () => { });
    }

    // 插入js文件，完成后callback
    function addScript(jsfile, callback) {
        var landlord_parent = document.getElementById("landlord-parent");
        var script = document.createElement("script");
        script.type = "text/javascript";
        script.src = jsfile;
        landlord_parent.appendChild(script);
        script.onload = script.onreadystatechange = function () {
            if (!this.readyState || this.readyState === "loaded" || this.readyState === "complete") {
                script.onload = script.onreadystatechange = null;
                if (callback && typeof callback == "function") {
                    callback(); //window[callback]();如果传递字符串过来 调用window['函数名']() 调用方法
                }
            }
        };
    }
</script>
  
  <script src="https://tinaxiawuhao.github.io/media/scripts/tocScript.js"></script>
</body>

</html>