---
title: 'elasticSearch基础概念汇总'
date: 2021-05-29 15:17:08
tags: [ELK]
published: true
hideInList: false
feature: /post-images/w_tIMVx00.png
isTop: false
---


### 1.什么是ElasticSearch？

Elasticsearch是一个基于Lucene的搜索引擎。它提供了具有HTTP Web界面和无架构JSON文档的分布式，多租户能力的全文搜索引擎。Elasticsearch是用Java开发的，根据Apache许可条款作为开源发布。它可以用于全文搜索，结构化搜索以及分析，当然你也可以将这三者进行组合。

### 2.为什么要使用Elasticsearch?

用数据库，也可以实现搜索的功能，为什么还需要搜索引擎呢？

就像 [Stackoverflow](https://link.zhihu.com/?target=https%3A//stackoverflow.com/questions/51639166/elasticsearch-vs-relational-database) 的网友说的：

> A relational database can store data and also index it. A search engine can index data but also store it.

数据库（理论上来讲，ES 也是数据库，这里的数据库，指的是关系型数据库），首先是存储，搜索只是顺便提供的功能，

而搜索引擎，首先是搜索，但是不把数据存下来就搜不了，所以只好存一存。

术业有专攻，专攻搜索的搜索引擎，自然会提供更强大的搜索能力。。

1. Elasticsearch是分布式的。不需要其他组件，分发是实时的，被叫做”Push replication”。
2. Elasticsearch 完全支持 Apache Lucene 的接近实时的搜索。
3. 处理多租户（[multitenancy](http://en.wikipedia.org/wiki/Multitenancy)）不需要特殊配置，而Solr则需要更多的高级设置。
4. Elasticsearch 采用 Gateway 的概念，使得完备份更加简单。
5. 各节点组成对等的网络结构，某些节点出现故障时会自动分配其他节点代替其进行工作。

### 3.Elasticsearch是如何实现Master选举的？

- Elasticsearch的选主是ZenDiscovery模块负责的，主要包含Ping（节点之间通过这个RPC来发现彼此）和Unicast（单播模块包含一个主机列表以控制哪些节点需要ping通）这两部分；
- 对所有可以成为master的节点（**node.master: true**）根据nodeId字典排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个（第0位）节点，暂且认为它是master节点。
- 如果对某个节点的投票数达到一定的值（可以成为master节点数n/2+1）并且该节点自己也选举自己，那这个节点就是master。否则重新选举一直到满足上述条件。
- *补充：master节点的职责主要包括集群、节点和索引的管理，不负责文档级别的管理；data节点可以关闭http功能*。

### 4.Elasticsearch中如何避免脑裂？

　　为了避免产生脑裂，ES采用了常见的分布式系统思路，保证选举出的master被多数派(quorum)的master-eligible node认可，以此来保证只有一个master。这个quorum通过以下配置进行配置：

```java
conf/elasticsearch.yml:
    discovery.zen.minimum_master_nodes: 2
```

### 5.Elasticsearch中的倒排索引

#### 为什么叫倒排索引

在没有搜索引擎时，我们是直接输入一个网址，然后获取网站内容，这时我们的行为是：`document -> to -> words` 通过文章，获取里面的单词，此谓「正向索引」，forward index.后来，我们希望能够输入一个单词，找到含有这个单词，或者和这个单词有关系的文章：`word -> to -> documents`于是我们把这种索引，成为inverted index，直译过来，应该叫「反向索引」，国内翻译成「倒排索引」。

#### 倒排索引的内部结构

首先，在数据生成的时候，比如爬虫爬到一篇文章，这时我们需要对这篇文章进行分析，将文本拆解成一个个单词。

这个过程很复杂，比如“生存还是死亡”，你要如何让分词器自动将它分解为“生存”、“还是”、“死亡”三个词语，然后把“还是”这个无意义的词语干掉。这里不展开，感兴趣的同学可以查看关于「分析器」的内容。

接着，把这两个词语以及它对应的文档id存下来：

| word | documentId |
| ---- | ---------- |
| 生存 | 1          |
| 死亡 | 1          |

接着爬虫继续爬，又爬到一个含有“生存”的文档，于是索引变成：

| word | documentId |
| ---- | ---------- |
| 生存 | 1，2       |
| 死亡 | 1          |

下次搜索“生存”，就会返回文档ID是 1、2两份文档。然而上面这套索引的实现，给小孩子当玩具玩还行，要上生产环境，那还远着。想想看，这个世界上那么多单词，中文、英文、日文、韩文 … 你每次搜索一个单词，我都要全局遍历一遍，很明显不行。于是有了排序，我们需要对单词进行排序，像 B+ 树一样，可以在页里实现二分查找。光排序还不行，你单词都放在磁盘呢，磁盘 IO 慢的不得了，所以 Mysql 特意把索引缓存到了内存。你说好，我也学 Mysql 的，放内存，3，2，1，放，哐当，内存爆了。哪本字典，会把所有单词都贴在目录里的？

所以，上图：

![](https://tinaxiawuhao.github.io/post-images/1622014331550.png)

Lucene 的倒排索，增加了最左边的一层「字典树」`term index`，它不存储所有的单词，只存储单词前缀，通过字典树找到单词所在的块，也就是单词的大概位置，再在块里二分查找，找到对应的单词，再找到单词对应的文档列表。

当然，内存寸土寸金，能省则省，所以 Lucene 还用了 `FST（Finite State Transducers）`对它进一步压缩。

最右边的 Posting List ，别看它只是存一个文档 ID 数组，但是它在设计时，遇到的问题可不少。

**Frame Of Reference**

原生的 Posting List 有两个痛点：

- **如何压缩以节省磁盘空间**
- **如何快速求交并集（intersections and unions）**

先来聊聊压缩。我们来简化下 Lucene 要面对的问题，假设有这样一个数组：**[73, 300, 302, 332, 343, 372]**

Lucene 里，数据是按 Segment 存储的，每个 Segment 最多存 65536 个文档 ID， 所以文档 ID 的范围，从 0 到 2^16-1，所以如果不进行任何处理，那么每个元素都会占用 2 bytes ，对应上面的数组，就是 6 * 2 = 12 bytes.

怎么压缩呢？

**压缩，就是尽可能降低每个数据占用的空间，同时又能让信息不失真，能够还原回来。**

**Step 1：Delta-encode —— 增量编码**

我们只记录元素与元素之间的增量，于是数组变成了：[73, 227, 2, 30, 11, 29]

**Step 2：Split into blocks —— 分割成块**

Lucene里每个块是 256 个文档 ID，这样可以保证每个块，增量编码后，每个元素都不会超过 256（1 byte）.为了方便演示，我们假设每个块是 3 个文档 ID：

**[73, 227, 2], [30, 11, 29]**

**Step 3：Bit packing —— 按需分配空间**

对于第一个块，[73, 227, 2]，最大元素是227，需要 8 bits，好，那我给你这个块的每个元素，都分配 8 bits的空间。但是对于第二个块，[30, 11, 29]，最大的元素才30，只需要 5 bits，那我就给你每个元素，只分配 5 bits 的空间，足矣。这一步，可以说是把吝啬发挥到极致，精打细算，按需分配。

以上三个步骤，共同组成了一项编码技术，Frame Of Reference（FOR）：

![](https://tinaxiawuhao.github.io/post-images/1622014347604.png)

 **Roaring bitmaps**

接着来聊聊 Posting List 的第二个痛点 —— 如何快速求交并集（intersections and unions）。

在 Lucene 中查询，通常不只有一个查询条件，比如我们想搜索：

- 含有“生存”相关词语的文档
- 文档发布时间在最近一个月
- 文档发布者是平台的特约作者

这样就需要根据三个字段，去三棵倒排索引里去查，当然，磁盘里的数据，上一节提到过，用了 FOR 进行压缩，所以我们要把数据进行反向处理，即解压，才能还原成原始的文档 ID，然后把这三个文档 ID 数组在内存中做一个交集。

> 即使没有多条件查询， Lucene 也需要频繁求并集，因为 Lucene 是分片存储的。

同样，我们把 Lucene 遇到的问题，简化成一道算法题。

假设有下面三个数组：
```java
[64, 300, 303, 343]

[73, 300, 302, 303, 343, 372]

[303, 311, 333, 343]
```
求它们的交集。

**Option 1: Integer 数组**

直接用原始的文档 ID ，可能你会说，那就逐个数组遍历一遍吧，遍历完就知道交集是什么了。

其实对于有序的数组，用跳表（skip table）可以更高效，这里就不展开了，因为不管是从性能，还是空间上考虑，Integer 数组都不靠谱，假设有100M 个文档 ID，每个文档 ID 占 2 bytes，那已经是 200 MB，而这些数据是要放到内存中进行处理的，把这么大量的数据，从磁盘解压后丢到内存，内存肯定撑不住。

**Option 2: Bitmap**

假设有这样一个数组：**[3,6,7,10]** 那么我们可以这样来表示：**[0,0,1,0,0,1,1,0,0,1]** 看出来了么，对，**我们用 0 表示角标对应的数字不存在，用 1 表示存在。**

这样带来了两个好处：

- 节省空间：既然我们只需要0和1，那每个文档 ID 就只需要 1 bit，还是假设有 100M 个文档，那只需要 100M bits = 100M * 1/8 bytes = 12.5 MB，比之前用 Integer 数组 的 200 MB，优秀太多
- 运算更快：0 和 1，天然就适合进行位运算，求交集，「与」一下，求并集，「或」一下，一切都回归到计算机的起点

**Option 3: Roaring Bitmaps**

细心的你可能发现了，bitmap 有个硬伤，就是不管你有多少个文档，你占用的空间都是一样的，之前说过，Lucene Posting List 的每个 Segement 最多放 65536 个文档ID，举一个极端的例子，有一个数组，里面只有两个文档 ID：**[0, 65535]**用 Bitmap，要怎么表示？**[1,0,0,0,….(超级多个0),…,0,0,1]**

你需要 65536 个 bit，也就是 65536/8 = 8192 bytes，而用 Integer 数组，你只需要 2 * 2 bytes = 4 bytes

呵呵，死板的 bitmap。可见在文档数量不多的时候，使用 Integer 数组更加节省内存。

我们来算一下临界值，很简单，无论文档数量多少，bitmap都需要 8192 bytes，而 Integer 数组则和文档数量成线性相关，每个文档 ID 占 2 bytes，所以：`8192 / 2 = 4096`当文档数量少于 4096 时，用 Integer 数组，否则，用 bitmap.

![](https://tinaxiawuhao.github.io/post-images/1622014361997.png)

> 这里补充一下 Roaring bitmaps 和 之前讲的 Frame Of Reference 的关系。
> Frame Of Reference 是压缩数据，减少磁盘占用空间，所以当我们从磁盘取数据时，也需要一个反向的过程，即解压，解压后才有我们上面看到的这样子的文档ID数组：[73, 300, 302, 303, 343, 372] ，接着我们需要对数据进行处理，求交集或者并集，这时候数据是需要放到内存进行处理的，我们有三个这样的数组，这些数组可能很大，而内存空间比磁盘还宝贵，于是需要更强有力的压缩算法，同时还要有利于快速的求交并集，于是有了Roaring Bitmaps 算法。
> 另外，Lucene 还会把从磁盘取出来的数据，通过 Roaring bitmaps 处理后，缓存到内存中，Lucene 称之为 filter cache.

### 6.ElasticSearch中的集群、节点、索引、文档、类型是什么？

　　**`群集`** 是一个或多个节点（服务器）的集合，它们共同保存您的整个数据，并提供跨所有节点的联合索引和搜索功能。群集由唯一名称标识，默认情况下为“elasticsearch”。此名称很重要，因为如果节点设置为按名称加入群集，则该节点只能是群集的一部分。

　　**`节点`** 是属于集群一部分的单个服务器。它存储数据并参与群集索引和搜索功能。

　　**`索引`** 就像关系数据库中的“数据库”。它有一个定义多种类型的映射。索引是逻辑名称空间，映射到一个或多个主分片，并且可以有零个或多个副本分片。 MySQL =>数据库 　　 ElasticSearch =>索引

　　**`文档`** 类似于关系数据库中的一行。不同之处在于索引中的每个文档可以具有不同的结构（字段），但是对于通用字段应该具有相同的数据类型。 MySQL => Databases =>Tables => Columns / Rows  ElasticSearch => Indices => Types =>具有属性的文档

　　**`类型`** 是索引的逻辑类别/分区，其语义完全取决于用户。

### 7.ElasticSearch中的分片是什么?

　　在大多数环境中，每个节点都在单独的盒子或虚拟机上运行。

　　**`索引`**  - 在Elasticsearch中，索引是文档的集合。

　　**`分片`**  -因为Elasticsearch是一个分布式搜索引擎，所以索引通常被分割成分布在多个节点上的被称为分片的元素。

**`Segment`** -每个shard（分片）包含多个segment（段），每一个segment都是一个倒排索引
在查询的时，会把所有的segment查询结果汇总归并后最为最终的分片查询结果返回
1.segment是不可变的，物理上你并不能从中删除信息，所以在删除文档的时候，是在文档上面打上一个删除的标记，然后在执行段合并的时候，进行删除
2.索引segment段的个数越多，搜索性能越低且消耗内存更多

​		**`副本`**  -一个索引被分解成碎片以便于分发和扩展。副本是分片的副本。

​		**`分析器`**  -在ElasticSearch中索引数据时，数据由为索引定义的Analyzer在内部进行转换。 分析器由一个Tokenizer和零个或多个TokenFilter组成。编译器可以在一个或多个CharFilter之前。分析模块允许您在逻辑名称下注册分析器，然后可以在映射定义或某些API中引用它们。Elasticsearch附带了许多可以随时使用的预建分析器。或者，您可以组合内置的字符过滤器，编译器和过滤器器来创建自定义分析器。

​		**`编译器`**  -编译器用于将字符串分解为术语或标记流。一个简单的编译器可能会将字符串拆分为任何遇到空格或标点的地方。Elasticsearch有许多内置标记器，可用于构建自定义分析器。

### 8.详细描述一下Elasticsearch索引文档的过程。

- 协调节点默认使用文档ID参与计算（也支持通过routing），以便为路由提供合适的分片。

```
shard = hash(document_id) % (num_of_primary_shards)
```

- 当分片所在的节点接收到来自协调节点的请求后，会将请求写入到Memory Buffer，然后定时（默认是每隔1秒）写入到Filesystem Cache，这个从Momery Buffer到Filesystem Cache的过程就叫做refresh；
- 当然在某些情况下，存在Momery Buffer和Filesystem Cache的数据可能会丢失，ES是通过translog的机制来保证数据的可靠性的。其实现机制是接收到请求后，同时也会写入到translog中，当Filesystem cache中的数据写入到磁盘中时，才会清除掉，这个过程叫做flush；
- 在flush过程中，内存中的缓冲将被清除，内容被写入一个新段，段的fsync将创建一个新的提交点，并将内容刷新到磁盘，旧的translog将被删除并开始一个新的translog。
- flush触发的时机是定时触发（默认30分钟）或者translog变得太大（默认为512M）时；

![](https://tinaxiawuhao.github.io/post-images/1622018263120.jpeg)

*补充：关于Lucene的Segement：*

- Lucene索引是由多个段组成，段本身是一个功能齐全的倒排索引。
- 段是不可变的，允许Lucene将新的文档增量地添加到索引中，而不用从头重建索引。
- 对于每一个搜索请求而言，索引中的所有段都会被搜索，并且每个段会消耗CPU的时钟周、文件句柄和内存。这意味着段的数量越多，搜索性能会越低。
- 为了解决这个问题，Elasticsearch会合并小段到一个较大的段，提交新的合并段到磁盘，并删除那些旧的小段。

### 9.详细描述一下Elasticsearch更新和删除文档的过程

- 删除和更新也都是写操作，但是Elasticsearch中的文档是不可变的，因此不能被删除或者改动以展示其变更；
- 磁盘上的每个段都有一个相应的.del文件。当删除请求发送后，文档并没有真的被删除，而是在.del文件中被标记为删除。该文档依然能匹配查询，但是会在结果中被过滤掉。当段合并时，在.del文件中被标记为删除的文档将不会被写入新段。
- 在新的文档被创建时，Elasticsearch会为该文档指定一个版本号，当执行更新时，旧版本的文档在.del文件中被标记为删除，新版本的文档被索引到一个新段。旧版本的文档依然能匹配查询，但是会在结果中被过滤掉。

### 10.详细描述一下Elasticsearch搜索的过程

- 搜索被执行成一个两阶段过程，我们称之为 Query Then Fetch；
- 在初始*查询阶段*时，查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的大小为 from + size 的优先队列。*PS：在搜索的时候是会查询Filesystem Cache的，但是有部分数据还在Memory Buffer，所以搜索是近实时的。*
- 每个分片返回各自优先队列中 **所有文档的 ID 和排序值** 给协调节点，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。
- 接下来就是 *取回阶段*，协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。每个分片加载并 *丰富* 文档，如果有需要的话，接着返回文档给协调节点。一旦所有的文档都被取回了，协调节点返回结果给客户端。
- *补充：Query Then Fetch的搜索类型在文档相关性打分的时候参考的是本分片的数据，这样在文档数量较少的时候可能不够准确，DFS Query Then Fetch增加了一个预查询的处理，询问Term和Document frequency，这个评分更准确，但是性能会变差。*

![](https://tinaxiawuhao.github.io/post-images/1622014432949.jpeg)

### 11.Elasticsearch对于大数据量（上亿量级）的聚合如何实现？

- Elasticsearch 提供的首个近似聚合是cardinality 度量。它提供一个字段的基数，即该字段的*distinct*或者*unique*值的数目。它是基于HLL算法的。HLL 会先对我们的输入作哈希运算，然后根据哈希运算的结果中的 bits 做概率估算从而得到基数。其特点是：可配置的精度，用来控制内存的使用（更精确 ＝ 更多内存）；小的数据集精度是非常高的；我们可以通过配置参数，来设置去重需要的固定内存使用量。无论数千还是数十亿的唯一值，内存使用量只与你配置的精确度相关。

### 12.在并发情况下，Elasticsearch如果保证读写一致？

- 可以通过版本号使用乐观并发控制，以确保新版本不会被旧版本覆盖，由应用层来处理具体的冲突；
- 另外对于写操作，一致性级别支持quorum/one/all，默认为quorum，即只有当大多数分片可用时才允许写操作。但即使大多数可用，也可能存在因为网络等原因导致写入副本失败，这样该副本被认为故障，分片将会在一个不同的节点上重建。
- 对于读操作，可以设置replication为sync(默认)，这使得操作在主分片和副本分片都完成后才会返回；如果设置replication为async时，也可以通过设置搜索请求参数_preference为primary来查询主分片，确保文档是最新版本。


