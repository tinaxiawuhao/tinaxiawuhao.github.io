{"posts":[{"title":"消息推送的7种方案","content":" 什么是消息推送（push） 推送的场景比较多，比如有人关注我的公众号，这时我就会收到一条推送消息，以此来吸引我点击打开应用。 消息推送(push)通常是指网站的运营工作等人员，通过某种工具对用户当前网页或移动设备APP进行的主动消息推送。 消息推送一般又分为web端消息推送和移动端消息推送。 web端消息推送常见的诸如站内信、未读邮件数量、监控报警数量等，应用的也非常广泛。 在具体实现之前，咱们再来分析一下前边的需求，其实功能很简单，只要触发某个事件（主动分享了资源或者后台主动推送消息），web页面的通知小红点就会实时的+1就可以了。 通常在服务端会有若干张消息推送表，用来记录用户触发不同事件所推送不同类型的消息，前端主动查询（拉）或者被动接收（推）用户所有未读的消息数。 消息推送无非是推（push）和拉（pull）两种形式，下边我们逐个了解下。 短轮询 轮询(polling)应该是实现消息推送方案中最简单的一种，这里我们暂且将轮询分为短轮询和长轮询。 短轮询很好理解，指定的时间间隔，由浏览器向服务器发出HTTP请求，服务器实时返回未读消息数据给客户端，浏览器再做渲染显示。 一个简单的JS定时器就可以搞定，每秒钟请求一次未读消息数接口，返回的数据展示即可。 setInterval(() =&gt; { // 方法请求 messageCount().then((res) =&gt; { if (res.code === 200) { this.messageCount = res.data } }) }, 1000); 效果还是可以的，短轮询实现固然简单，缺点也是显而易见，由于推送数据并不会频繁变更，无论后端此时是否有新的消息产生，客户端都会进行请求，势必会对服务端造成很大压力，浪费带宽和服务器资源。 长轮询 长轮询是对上边短轮询的一种改进版本，在尽可能减少对服务器资源浪费的同时，保证消息的相对实时性。长轮询在中间件中应用的很广泛，比如Nacos和apollo配置中心，消息队列kafka、RocketMQ中都有用到长轮询。 [nocas长轮询](nocas长轮询 | tianxia (tinaxiawuhao.github.io))一文中我详细介绍过Nacos长轮询的实现原理，感兴趣的小伙伴可以瞅瞅。 这次我使用apollo配置中心实现长轮询的方式，应用了一个类DeferredResult，它是在servelet3.0后经过Spring封装提供的一种异步请求机制，直意就是延迟结果。 DeferredResult可以允许容器线程快速释放占用的资源，不阻塞请求线程，以此接受更多的请求提升系统的吞吐量，然后启动异步工作线程处理真正的业务逻辑，处理完成调用DeferredResult.setResult(200)提交响应结果。 下边我们用长轮询来实现消息推送。 因为一个ID可能会被多个长轮询请求监听，所以我采用了guava包提供的Multimap结构存放长轮询，一个key可以对应多个value。一旦监听到key发生变化，对应的所有长轮询都会响应。前端得到非请求超时的状态码，知晓数据变更，主动查询未读消息数接口，更新页面数据。 @Controller @RequestMapping(&quot;/polling&quot;) public class PollingController { // 存放监听某个Id的长轮询集合 // 线程同步结构 public static Multimap&lt;String, DeferredResult&lt;String&gt;&gt; watchRequests = Multimaps.synchronizedMultimap(HashMultimap.create()); /** * 设置监听 */ @GetMapping(path = &quot;watch/{id}&quot;) @ResponseBody public DeferredResult&lt;String&gt; watch(@PathVariable String id) { // 延迟对象设置超时时间 DeferredResult&lt;String&gt; deferredResult = new DeferredResult&lt;&gt;(TIME_OUT); // 异步请求完成时移除 key，防止内存溢出 deferredResult.onCompletion(() -&gt; { watchRequests.remove(id, deferredResult); }); // 注册长轮询请求 watchRequests.put(id, deferredResult); return deferredResult; } /** * 变更数据 */ @GetMapping(path = &quot;publish/{id}&quot;) @ResponseBody public String publish(@PathVariable String id) { // 数据变更 取出监听ID的所有长轮询请求，并一一响应处理 if (watchRequests.containsKey(id)) { Collection&lt;DeferredResult&lt;String&gt;&gt; deferredResults = watchRequests.get(id); for (DeferredResult&lt;String&gt; deferredResult : deferredResults) { deferredResult.setResult(&quot;我更新了&quot; + new Date()); } } return &quot;success&quot;; } 当请求超过设置的超时时间，会抛出AsyncRequestTimeoutException异常，这里直接用@ControllerAdvice全局捕获统一返回即可，前端获取约定好的状态码后再次发起长轮询请求，如此往复调用。 @ControllerAdvice public class AsyncRequestTimeoutHandler { @ResponseStatus(HttpStatus.NOT_MODIFIED) @ResponseBody @ExceptionHandler(AsyncRequestTimeoutException.class) public String asyncRequestTimeoutHandler(AsyncRequestTimeoutException e) { System.out.println(&quot;异步请求超时&quot;); return &quot;304&quot;; } } 我们来测试一下，首先页面发起长轮询请求/polling/watch/10086监听消息更变，请求被挂起，不变更数据直至超时，再次发起了长轮询请求；紧接着手动变更数据/polling/publish/10086，长轮询得到响应，前端处理业务逻辑完成后再次发起请求，如此循环往复。 长轮询相比于短轮询在性能上提升了很多，但依然会产生较多的请求，这是它的一点不完美的地方。 iframe流 iframe流就是在页面中插入一个隐藏的&lt;iframe&gt;标签，通过在src中请求消息数量API接口，由此在服务端和客户端之间创建一条长连接，服务端持续向iframe传输数据。 传输的数据通常是HTML、或是内嵌的javascript脚本，来达到实时更新页面的效果。 这种方式实现简单，前端只要一个&lt;iframe&gt;标签搞定了 &lt;iframe src=&quot;/iframe/message&quot; style=&quot;display:none&quot;&gt;&lt;/iframe&gt; 服务端直接组装html、js脚本数据向response写入就行了 @Controller @RequestMapping(&quot;/iframe&quot;) public class IframeController { @GetMapping(path = &quot;message&quot;) public void message(HttpServletResponse response) throws IOException, InterruptedException { while (true) { response.setHeader(&quot;Pragma&quot;, &quot;no-cache&quot;); response.setDateHeader(&quot;Expires&quot;, 0); response.setHeader(&quot;Cache-Control&quot;, &quot;no-cache,no-store&quot;); response.setStatus(HttpServletResponse.SC_OK); response.getWriter().print(&quot; &lt;script type=\\&quot;text/javascript\\&quot;&gt;\\n&quot; + &quot;parent.document.getElementById('clock').innerHTML = \\&quot;&quot; + count.get() + &quot;\\&quot;;&quot; + &quot;parent.document.getElementById('count').innerHTML = \\&quot;&quot; + count.get() + &quot;\\&quot;;&quot; + &quot;&lt;/script&gt;&quot;); } } } 但我个人不推荐，因为它在浏览器上会显示请求未加载完，图标会不停旋转，简直是强迫症杀手。 SSE (我的方式) 很多人可能不知道，服务端向客户端推送消息，其实除了可以用WebSocket这种耳熟能详的机制外，还有一种服务器发送事件(Server-sent events)，简称SSE。 SSE它是基于HTTP协议的，我们知道一般意义上的HTTP协议是无法做到服务端主动向客户端推送消息的，但SSE是个例外，它变换了一种思路。 SSE在服务器和客户端之间打开一个单向通道，服务端响应的不再是一次性的数据包而是text/event-stream类型的数据流信息，在有数据变更时从服务器流式传输到客户端。 整体的实现思路有点类似于在线视频播放，视频流会连续不断的推送到浏览器，你也可以理解成，客户端在完成一次用时很长（网络不畅）的下载。 SSE与WebSocket作用相似，都可以建立服务端与浏览器之间的通信，实现服务端向客户端推送消息，但还是有些许不同： SSE 是基于HTTP协议的，它们不需要特殊的协议或服务器实现即可工作；WebSocket需单独服务器来处理协议。 SSE 单向通信，只能由服务端向客户端单向通信；webSocket全双工通信，即通信的双方可以同时发送和接受信息。 SSE 实现简单开发成本低，无需引入其他组件；WebSocket传输数据需做二次解析，开发门槛高一些。 SSE 默认支持断线重连；WebSocket则需要自己实现。 SSE 只能传送文本消息，二进制数据需要经过编码后传送；WebSocket默认支持传送二进制数据。 SSE 与 WebSocket 该如何选择？ 技术并没有好坏之分，只有哪个更合适 SSE好像一直不被大家所熟知，一部分原因是出现了WebSockets，这个提供了更丰富的协议来执行双向、全双工通信。对于游戏、即时通信以及需要双向近乎实时更新的场景，拥有双向通道更具吸引力。 但是，在某些情况下，不需要从客户端发送数据。而你只需要一些服务器操作的更新。比如：站内信、未读消息数、状态更新、股票行情、监控数量等场景，SEE不管是从实现的难易和成本上都更加有优势。此外，SSE 具有WebSockets在设计上缺乏的多种功能，例如：自动重新连接、事件ID和发送任意事件的能力。 前端只需进行一次HTTP请求，带上唯一ID，打开事件流，监听服务端推送的事件就可以了 &lt;script&gt; let source = null; let userId = 7777 if (window.EventSource) { // 建立连接 source = new EventSource('http://localhost:7777/sse/sub/'+userId); setMessageInnerHTML(&quot;连接用户=&quot; + userId); /** * 连接一旦建立，就会触发open事件 * 另一种写法：source.onopen = function (event) {} */ source.addEventListener('open', function (e) { setMessageInnerHTML(&quot;建立连接。。。&quot;); }, false); /** * 客户端收到服务器发来的数据 * 另一种写法：source.onmessage = function (event) {} */ source.addEventListener('message', function (e) { setMessageInnerHTML(e.data); }); } else { setMessageInnerHTML(&quot;你的浏览器不支持SSE&quot;); } &lt;/script&gt; 服务端的实现更简单，创建一个SseEmitter对象放入sseEmitterMap进行管理 private static Map&lt;String, SseEmitter&gt; sseEmitterMap = new ConcurrentHashMap&lt;&gt;(); /** * 创建连接 * * @date: 2022/7/12 14:51 */ public static SseEmitter connect(String userId) { try { // 设置超时时间，0表示不过期。默认30秒 SseEmitter sseEmitter = new SseEmitter(0L); // 注册回调 sseEmitter.onCompletion(completionCallBack(userId)); sseEmitter.onError(errorCallBack(userId)); sseEmitter.onTimeout(timeoutCallBack(userId)); sseEmitterMap.put(userId, sseEmitter); count.getAndIncrement(); return sseEmitter; } catch (Exception e) { log.info(&quot;创建新的sse连接异常，当前用户：{}&quot;, userId); } return null; } /** * 给指定用户发送消息 * * @date: 2022/7/12 14:51 */ public static void sendMessage(String userId, String message) { if (sseEmitterMap.containsKey(userId)) { try { sseEmitterMap.get(userId).send(message); } catch (IOException e) { log.error(&quot;用户[{}]推送异常:{}&quot;, userId, e.getMessage()); removeUser(userId); } } } 我们模拟服务端推送消息，看下客户端收到了消息，和我们预期的效果一致。 注意： SSE不支持IE浏览器，对其他主流浏览器兼容性做的还不错。 MQTT 什么是 MQTT协议？ MQTT 全称(Message Queue Telemetry Transport)：一种基于发布/订阅（publish/subscribe）模式的轻量级通讯协议，通过订阅相应的主题来获取消息，是物联网（Internet of Thing）中的一个标准传输协议。 该协议将消息的发布者（publisher）与订阅者（subscriber）进行分离，因此可以在不可靠的网络环境中，为远程连接的设备提供可靠的消息服务，使用方式与传统的MQ有点类似。 TCP协议位于传输层，MQTT 协议位于应用层，MQTT 协议构建于TCP/IP协议上，也就是说只要支持TCP/IP协议栈的地方，都可以使用MQTT协议。 为什么要用 MQTT协议？ MQTT协议为什么在物联网（IOT）中如此受偏爱？而不是其它协议，比如我们更为熟悉的 HTTP协议呢？ 首先HTTP协议它是一种同步协议，客户端请求后需要等待服务器的响应。而在物联网（IOT）环境中，设备会很受制于环境的影响，比如带宽低、网络延迟高、网络通信不稳定等，显然异步消息协议更为适合IOT应用程序。 HTTP是单向的，如果要获取消息客户端必须发起连接，而在物联网（IOT）应用程序中，设备或传感器往往都是客户端，这意味着它们无法被动地接收来自网络的命令。 通常需要将一条命令或者消息，发送到网络上的所有设备上。HTTP要实现这样的功能不但很困难，而且成本极高。 Websocket websocket应该是大家都比较熟悉的一种实现消息推送的方式，上边我们在讲SSE的时候也和websocket进行过比较。 WebSocket是一种在TCP连接上进行全双工通信的协议，建立客户端和服务器之间的通信渠道。浏览器和服务器仅需一次握手，两者之间就直接可以创建持久性的连接，并进行双向数据传输。 springboot整合websocket，先引入websocket相关的工具包，和SSE相比额外的开发成本。 &lt;!-- 引入websocket --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-websocket&lt;/artifactId&gt; &lt;/dependency&gt; 服务端使用@ServerEndpoint注解标注当前类为一个websocket服务器，客户端可以通过ws://localhost:7777/webSocket/10086来连接到WebSocket服务器端。 @Component @Slf4j @ServerEndpoint(&quot;/websocket/{userId}&quot;) public class WebSocketServer { //与某个客户端的连接会话，需要通过它来给客户端发送数据 private Session session; private static final CopyOnWriteArraySet&lt;WebSocketServer&gt; webSockets = new CopyOnWriteArraySet&lt;&gt;(); // 用来存在线连接数 private static final Map&lt;String, Session&gt; sessionPool = new HashMap&lt;String, Session&gt;(); /** * 链接成功调用的方法 */ @OnOpen public void onOpen(Session session, @PathParam(value = &quot;userId&quot;) String userId) { try { this.session = session; webSockets.add(this); sessionPool.put(userId, session); log.info(&quot;websocket消息: 有新的连接，总数为:&quot; + webSockets.size()); } catch (Exception e) { } } /** * 收到客户端消息后调用的方法 */ @OnMessage public void onMessage(String message) { log.info(&quot;websocket消息: 收到客户端消息:&quot; + message); } /** * 此为单点消息 */ public void sendOneMessage(String userId, String message) { Session session = sessionPool.get(userId); if (session != null &amp;&amp; session.isOpen()) { try { log.info(&quot;websocket消: 单点消息:&quot; + message); session.getAsyncRemote().sendText(message); } catch (Exception e) { e.printStackTrace(); } } } } 前端初始化打开WebSocket连接，并监听连接状态，接收服务端数据或向服务端发送数据。 &lt;script&gt; var ws = new WebSocket('ws://localhost:7777/webSocket/10086'); // 获取连接状态 console.log('ws连接状态：' + ws.readyState); //监听是否连接成功 ws.onopen = function () { console.log('ws连接状态：' + ws.readyState); //连接成功则发送一个数据 ws.send('test1'); } // 接听服务器发回的信息并处理展示 ws.onmessage = function (data) { console.log('接收到来自服务器的消息：'); console.log(data); //完成通信后关闭WebSocket连接 ws.close(); } // 监听连接关闭事件 ws.onclose = function () { // 监听整个过程中websocket的状态 console.log('ws连接状态：' + ws.readyState); } // 监听并处理error事件 ws.onerror = function (error) { console.log(error); } function sendMessage() { var content = $(&quot;#message&quot;).val(); $.ajax({ url: '/socket/publish?userId=10086&amp;message=' + content, type: 'GET', data: { &quot;id&quot;: &quot;7777&quot;, &quot;content&quot;: content }, success: function (data) { console.log(data) } }) } &lt;/script&gt; 页面初始化建立websocket连接，之后就可以进行双向通信了，效果还不错 自定义推送 上边我们给我出了6种方案的原理和代码实现，但在实际业务开发过程中，不能盲目的直接拿过来用，还是要结合自身系统业务的特点和实际场景来选择合适的方案。 推送最直接的方式就是使用第三推送平台，毕竟钱能解决的需求都不是问题，无需复杂的开发运维，直接可以使用，省时、省力、省心，像goEasy、极光推送都是很不错的三方服务商。 一般大型公司都有自研的消息推送平台，像我们本次实现的web站内信只是平台上的一个触点而已，短信、邮件、微信公众号、小程序凡是可以触达到用户的渠道都可以接入进来。 消息推送系统内部是相当复杂的，诸如消息内容的维护审核、圈定推送人群、触达过滤拦截（推送的规则频次、时段、数量、黑白名单、关键词等等）、推送失败补偿非常多的模块，技术上涉及到大数据量、高并发的场景也很多。所以我们今天的实现方式在这个庞大的系统面前只是小打小闹。 ","link":"https://tinaxiawuhao.github.io/post/GdjsuDFQV/"},{"title":"MQ 消息丢失、重复、积压问题，如何解决","content":"面试官在面试候选人时，如果发现候选人的简历中写了在项目中使用了 MQ 技术（如 Kafka、RabbitMQ、RocketMQ），基本都会抛出一个问题：在使用 MQ 的时候，怎么确保消息 100% 不丢失？ 这个问题在实际工作中很常见，既能考察候选者对于 MQ 中间件技术的掌握程度，又能很好地区分候选人的能力水平。接下来，我们就从这个问题出发，探讨你应该掌握的基础知识和答题思路，以及延伸的面试考点。 案例背景 以京东系统为例，用户在购买商品时，通常会选择用京豆抵扣一部分的金额，在这个过程中，交易服务和京豆服务通过 MQ 消息队列进行通信。在下单时，交易服务发送“扣减账户 X 100 个京豆”的消息给 MQ 消息队列，而京豆服务则在消费端消费这条命令，实现真正的扣减操作。 那在这个过程中你会遇到什么问题呢？ 案例分析 要知道，在互联网面试中，引入 MQ 消息中间件最直接的目的是：做系统解耦合流量控制，追其根源还是为了解决互联网系统的高可用和高性能问题。 系统解耦：用 MQ 消息队列，可以隔离系统上下游环境变化带来的不稳定因素，比如京豆服务的系统需求无论如何变化，交易服务不用做任何改变，即使当京豆服务出现故障，主交易流程也可以将京豆服务降级，实现交易服务和京豆服务的解耦，做到了系统的高可用。 流量控制：遇到秒杀等流量突增的场景，通过 MQ 还可以实现流量的“削峰填谷”的作用，可以根据下游的处理能力自动调节流量。 不过引入 MQ 虽然实现了系统解耦合流量控制，也会带来其他问题。 引入 MQ 消息中间件实现系统解耦，会影响系统之间数据传输的一致性。 在分布式系统中，如果两个节点之间存在数据同步，就会带来数据一致性的问题。同理，在这一讲你要解决的就是：消息生产端和消息消费端的消息数据一致性问题（也就是如何确保消息不丢失）。 而引入 MQ 消息中间件解决流量控制， 会使消费端处理能力不足从而导致消息积压，这也是你要解决的问题。 所以你能发现，问题与问题之间往往是环环相扣的，面试官会借机考察你解决问题思路的连贯性和知识体系的掌握程度。 那面对“在使用 MQ 消息队列时，如何确保消息不丢失”这个问题时，你要怎么回答呢？首先，你要分析其中有几个考点，比如： 如何知道有消息丢失？ 哪些环节可能丢消息？ 如何确保消息不丢失？ 候选人在回答时，要先让面试官知道你的分析思路，然后再提供解决方案：网络中的数据传输不可靠，想要解决如何不丢消息的问题，首先要知道哪些环节可能丢消息，以及我们如何知道消息是否丢失了，最后才是解决方案（而不是上来就直接说自己的解决方案）。 就好比“架构设计”“架构”体现了架构师的思考过程，而“设计”才是最后的解决方案，两者缺一不可。 案例解答 我们首先来看消息丢失的环节，一条消息从生产到消费完成这个过程，可以划分三个阶段，分别为消息生产阶段，消息存储阶段和消息消费阶段。 消息生产阶段： 从消息被生产出来，然后提交给 MQ 的过程中，只要能正常收到 MQ Broker 的 ack 确认响应，就表示发送成功，所以只要处理好返回值和异常，这个阶段是不会出现消息丢失的。 消息存储阶段： 这个阶段一般会直接交给 MQ 消息中间件来保证，但是你要了解它的原理，比如 Broker 会做副本，保证一条消息至少同步两个节点再返回 ack。 消息消费阶段： 消费端从 Broker 上拉取消息，只要消费端在收到消息后，不立即发送消费确认给 Broker，而是等到执行完业务逻辑后，再发送消费确认，也能保证消息的不丢失。 方案看似万无一失，每个阶段都能保证消息的不丢失，但在分布式系统中，故障不可避免，作为消息生产端，你并不能保证 MQ 是不是弄丢了你的消息，消费者是否消费了你的消息，所以，本着 Design for Failure 的设计原则，你还是需要一种机制，来 Check 消息是否丢失了。 紧接着，你还可以向面试官阐述怎么进行消息检测？ 总体方案解决思路为：在消息生产端，给每个发出的消息都指定一个全局唯一 ID，或者附加一个连续递增的版本号，然后在消费端做对应的版本校验。 具体怎么落地实现呢？你可以利用拦截器机制。 在生产端发送消息之前，通过拦截器将消息版本号注入消息中（版本号可以采用连续递增的 ID 生成，也可以通过分布式全局唯一 ID生成）。然后在消费端收到消息后，再通过拦截器检测版本号的连续性或消费状态，这样实现的好处是消息检测的代码不会侵入到业务代码中，可以通过单独的任务来定位丢失的消息，做进一步的排查。 这里需要你注意：如果同时存在多个消息生产端和消息消费端，通过版本号递增的方式就很难实现了，因为不能保证版本号的唯一性，此时只能通过全局唯一 ID 的方案来进行消息检测，具体的实现原理和版本号递增的方式一致。 现在，你已经知道了哪些环节（消息存储阶段、消息消费阶段）可能会出问题，并有了如何检测消息丢失的方案，然后就要给出解决防止消息丢失的设计方案。 回答完“如何确保消息不会丢失？” 之后，面试官通常会追问“怎么解决消息被重复消费的问题？ ” 比如：在消息消费的过程中，如果出现失败的情况，通过补偿的机制发送方会执行重试，重试的过程就有可能产生重复的消息，那么如何解决这个问题？ 这个问题其实可以换一种说法，就是如何解决消费端幂等性问题（幂等性，就是一条命令，任意多次执行所产生的影响均与一次执行的影响相同），只要消费端具备了幂等性，那么重复消费消息的问题也就解决了。 我们还是来看扣减京豆的例子，将账户 X 的金豆个数扣减 100 个，在这个例子中，我们可以通过改造业务逻辑，让它具备幂等性。 最简单的实现方案，就是在数据库中建一张消息日志表， 这个表有两个字段：消息 ID 和消息执行状态。这样，我们消费消息的逻辑可以变为：在消息日志表中增加一条消息记录，然后再根据消息记录，异步操作更新用户京豆余额。 因为我们每次都会在插入之前检查是否消息已存在，所以就不会出现一条消息被执行多次的情况，这样就实现了一个幂等的操作。当然，基于这个思路，不仅可以使用关系型数据库，也可以通过 Redis 来代替数据库实现唯一约束的方案。 在这里我多说一句，想要解决“消息丢失”和“消息重复消费”的问题，有一个前提条件就是要实现一个全局唯一 ID 生成的技术方案。这也是面试官喜欢考察的问题，你也要掌握。 在分布式系统中，全局唯一 ID 生成的实现方法有数据库自增主键、UUID、Redis，Twitter-Snowflake 算法，我总结了几种方案的特点，你可以参考下。 我提醒你注意，无论哪种方法，如果你想同时满足简单、高可用和高性能，就要有取舍，所以你要站在实际的业务中，说明你的选型所考虑的平衡点是什么。 我个人在业务中比较倾向于选择 Snowflake 算法，在项目中也进行了一定的改造，主要是让算法中的 ID 生成规则更加符合业务特点，以及优化诸如时钟回拨等问题。 当然，除了“怎么解决消息被重复消费的问题？”之外，面试官还会问到你“消息积压”。 原因在于消息积压反映的是性能问题，解决消息积压问题，可以说明候选者有能力处理高并发场景下的消费能力问题。 你在解答这个问题时，依旧要传递给面试官一个这样的思考过程： 如果出现积压，那一定是性能问题，想要解决消息从生产到消费上的性能问题，就首先要知道哪些环节可能出现消息积压，然后在考虑如何解决。 因为消息发送之后才会出现积压的问题，所以和消息生产端没有关系，又因为绝大部分的消息队列单节点都能达到每秒钟几万的处理能力，相对于业务逻辑来说，性能不会出现在中间件的消息存储上面。毫无疑问，出问题的肯定是消息消费阶段，那么从消费端入手，如何回答呢？ 如果是线上突发问题，要临时扩容，增加消费端的数量，与此同时，降级一些非核心的业务。通过扩容和降级承担流量，这是为了表明你对应急问题的处理能力。 其次，才是排查解决异常问题，如通过监控，日志等手段分析是否消费端的业务逻辑代码出现了问题，优化消费端的业务处理逻辑。 最后，如果是消费端的处理能力不足，可以通过水平扩容来提供消费端的并发处理能力，但这里有一个考点需要特别注意， 那就是在扩容消费者的实例数的同时，必须同步扩容主题 Topic 的分区数量，确保消费者的实例数和分区数相等。如果消费者的实例数超过了分区数，由于分区是单线程消费，所以这样的扩容就没有效果。 比如在 Kafka 中，一个 Topic 可以配置多个 Partition（分区），数据会被写入到多个分区中，但在消费的时候，Kafka 约定一个分区只能被一个消费者消费，Topic 的分区数量决定了消费的能力，所以，可以通过增加分区来提高消费者的处理能力。 总结 至此，我们讲解了 MQ 消息队列的热门问题的解决方案，无论是初中级还是高级研发工程师，本篇文章的内容都是你需要掌握的，你都可以从这几点出发，与面试官进行友好的交流。我来总结一下今天的重点内容。 如何确保消息不会丢失？ 你要知道一条消息从发送到消费的每个阶段，是否存在丢消息，以及如何监控消息是否丢失，最后才是如何解决问题，方案可以基于“ MQ 的可靠消息投递 ”的方式。 如何保证消息不被重复消费？ 在进行消息补偿的时候，一定会存在重复消息的情况，那么如何实现消费端的幂等性就这道题的考点。 如何处理消息积压问题？ 这道题的考点就是如何通过 MQ 实现真正的高性能，回答的思路是，本着解决线上异常为最高优先级，然后通过监控和日志进行排查并优化业务逻辑，最后是扩容消费端和分片的数量。 在回答问题的时候，你需要特别注意的是，让面试官了解到你的思维过程，这种解决问题的能力是面试官更为看中的，比你直接回答一道面试题更有价值。 另外，如果你应聘的部门是基础架构部，那么除了要掌握本讲中的常见问题的主线知识以外，还要掌握消息中间件的其他知识体系，如： 如何选型消息中间件？ 消息中间件中的队列模型与发布订阅模型的区别？ 为什么消息队列能实现高吞吐？ 序列化、传输协议，以及内存管理等问题 ","link":"https://tinaxiawuhao.github.io/post/-GeRQ4Ed-/"},{"title":"消息服务：MQ使用场景与选型对比","content":"MQ的使用场景，引入MQ后的注意事项以及MQ的选型对比。 MQ的使用场景 MQ的英文全称是Message Queue，翻译成中文就是消息队列，队列实现了先进先出（FIFO）的消息模型。通过消息队列，我们可以实现多个进程之间的通信，例如，可以实现多个微服务之间的消息通信。MQ的最简模型就是生产者生产消息，将消息发送到MQ，消息消费者订阅MQ，消费消息。 MQ的使用场景通常包含：异步解耦、流量削峰。 异步解耦 关于异步的场景，我们这里举一个用户下单成功后，向用户发送通知消息，为用户增加积分和优惠券的场景。 同步耦合场景分析 如果是同步调用的场景，则具体业务为：当用户提交订单成功后，订单系统会调用通知系统为用户发送消息通知，告知用户下单成功，订单系统调用积分系统为用户增加积分，订单系统调用优惠券系统为用户增加优惠券。整个调用流程如下所示。 通过上图的分析，可以看到，用户调用订单系统下单时，总共会经过8个步骤。并且每个步骤都是紧密耦合在一起串行执行，如下所示。 此时，订单系统、通知系统、积分系统和优惠券系统是紧密耦合在一起的，订单系统下单、通知系统发通知、积分系统发积分和优惠券系统发优惠券，四个任务全部完成后，才会给用户返回提交订单的结果信息。 用户提交订单花费的总时间为调用订单系统下单的时间+订单系统调用通知系统发送通知的时间+订单系统调用积分系统发放积分的时间+订单系统调用优惠券系统发放优惠券的时间。 注意：这里为了更好的说明系统之间串行执行的问题，忽略了网络的延迟时间。 这种串行化的系统执行方式，在高并发、大流量场景下是不可取的。另外，如果其中一个系统异常或者宕机，势必会影响到订单系统的可用性。在系统维护上，只要任意一个系统的接口发生变动，订单系统的逻辑也要跟着发生变动。 异步解耦场景分析 既然在高并发、大流量场景下使用订单系统直接串行调用通知系统、积分系统和优惠券系统的方式不可取。那我们是否能够使用异步解耦的方式呢。 其实，在用户提交订单的场景中，用户最关心的是自己的订单是否提交成功，由于下单时，订单系统会直接返回是否下单成功的提示。 对于通知、积分和优惠券可以以异步的方式延后一小段时间执行。并且通知系统、积分系统和优惠券系统之间不存在必然的业务关联逻辑，它们之间可以以并行的方式执行。 所以，可以使用MQ将订单系统与通知系统、积分系统和优惠券系统进行解耦，用户调用订单系统的接口下单时，订单系统向数据库写入订单数据后，向MQ写入消息，就可以直接返回给用户下单成功的提示，此时通知系统、积分系统和优惠券系统都订阅MQ中的消息，收到消息后各自执行自身的业务逻辑即可。 当引入MQ进行异步解耦之后，用户调用订单系统的接口下单，订单系统执行完业务逻辑将订单数据入口，会向MQ发送一条消息，随后便直接返回用户下单成功的提示。通知系统、积分系统和优惠券系统会同时订阅MQ中的消息，当收到消息时，它们各自会执行自身的业务逻辑，并且它们是以并行的方式执行各自的业务逻辑。 从执行的时间线上可以看出，当引入MQ进行异步解耦之后，通知系统、积分系统、优惠券系统和订单系统回复响应都是并行执行的，大大提高系统的执行性能。 并且解耦后，任意一个系统异常或者宕机，都不会影响到订单系统的可用性。只要订单系统与其他系统提前约定好发送的消息格式和消息内容，后续任意一个系统的业务逻辑变动，几乎都不会影响到订单系统的逻辑。 流量削峰 MQ在高并发、大流量的场景下可以用作削峰填谷的利器，例如，12306的春运抢票场景、高并发秒杀场景、双十一和618的大促场景等。 在高并发、大流量业务场景下，瞬间会有大量用户的请求涌入系统，如果不对这些流量做处理的话，直接让这些流量进入下游系统，则很可能由于下游系统无法支撑如此高的并发而导致系统崩溃或宕机。为了解决这些问题，可以引入MQ进行流量的削峰填谷。 将流量发送到MQ中后，下游系统根据自身的处理能力进行消费即可。保证了下游系统的高可用性。 引入MQ后的注意事项 引入MQ最大的优点就是异步解耦和流量削峰，但是引入MQ后也有很多需要注意的事项和问题，主要包括：系统的整体可用性降低、系统的复杂度变高、引入了消息一致性的问题。 系统的整体可用性降低 在对一个系统进行架构设计时，引入的外部依赖越多，系统的稳定性和可用性就会降低。系统中引入了MQ，部分业务就会出现强依赖MQ的现象，此时，如果MQ宕机，则部分业务就会变得不可用。所以，引入MQ时，我们就要考虑如何实现MQ的高可用。 系统的复杂度变高 引入MQ后，会使之前的同步接口调用变成通过MQ的异步调用，在实际的开发过程中，异步调用会比同步调用复杂的多。并且异步调用出现问题后，重现问题，定位问题和解决问题都会比同步调用复杂的多。 并且引入MQ后，还要考虑如何保证消息的顺序等问题。 消息一致性问题 引入MQ后，不得不考虑的一个问题就是消息的一致性问题。这期间就要考虑如何保证消息不丢失，消息幂等和消息数据处理的幂等性问题。 MQ选型对比 目前，在行业内使用的比较多的MQ包含RabbitMQ、Kafka和RocketMQ。这里，我将三者的对比简单整理了个表格，如下所示。 消息中间件(MQ) 优点 缺点 使用场景 RabbitMQ 功能全面、消息的可靠性比较高 吞吐量低，消息大量积累会影响性能，使用的开发语言是erlang，不好定制功能。 规模不大的场景 Kafka 吞吐量最高，性能最好，集群模式下高可用 功能上比较单一，会丢失部分数据 日志分析，大数据场景 RocketMQ 吞吐量高，性能高，可用性高，功能全面。使用Java语言开发，容易定制功能。 开源版不如阿里云上版，文档比较简单。 几乎支持所有场景，包含大数据场景和业务场景。 ","link":"https://tinaxiawuhao.github.io/post/8FPjAXq14/"},{"title":"搭建并整合Seata","content":"搭建并整合Seata 接下来，我们就正式在项目中整合Seata来实现分布式事务。这里，我们主要整合Seata的AT模式。 搭建Seata基础环境 （1）到https://github.com/seata/seata/releases/tag/v1.4.2链接下载Seata的安装包和源码，这里，下载的是1.4.2版本，如下所示。 这里我下载的都是zip压缩文件。 （2）进入Nacos，选择的命名空间，如下所示。 点击新建命名空间，并填写Seata相关的信息，如下所示。 可以看到，这里我填写的信息如下所示。 命名空间ID：seata_namespace_001，如果不填的话Nacos会自动生成命名空间的ID。 命名空间名：seata。 描述：seata的命名空间。 「这里，需要记录下命名空间的ID：seata_namespace_001，在后面的配置中会使用到。」 点击确定后如下所示。 可以看到，这里为Seata在Nacos中创建了命名空间。 （3）解压Seata安装文件，进入解压后的seata/seata-server-1.4.2/conf目录，修改registry.conf注册文件，修改后的部分文件内容如下所示。 registry { # file 、nacos 、eureka、redis、zk、consul、etcd3、sofa type = &quot;nacos&quot; nacos { application = &quot;seata-server&quot; serverAddr = &quot;127.0.0.1:8848&quot; group = &quot;SEATA_GROUP&quot; namespace = &quot;seata_namespace_001&quot; cluster = &quot;default&quot; username = &quot;nacos&quot; password = &quot;nacos&quot; } } config { # file、nacos 、apollo、zk、consul、etcd3 type = &quot;nacos&quot; nacos { serverAddr = &quot;127.0.0.1:8848&quot; namespace = &quot;seata_namespace_001&quot; group = &quot;SEATA_GROUP&quot; username = &quot;nacos&quot; password = &quot;nacos&quot; dataId = &quot;seataServer.properties&quot; } } 其中，namespace的值就是在Nacos中配置的Seata的命名空间ID：seata_namespace_001。 「注意：这里只列出了修改的部分内容，完整的registry.conf文件可以到项目的doc/nacos/config/chapter25目录下获取。」 （4）修改Seata安装文件的seata/seata-server-1.4.2/conf目录下的file.conf文件，修改后的部分配置如下所示。 store { mode = &quot;db&quot; publicKey = &quot;&quot; db { datasource = &quot;druid&quot; dbType = &quot;mysql&quot; driverClassName = &quot;com.mysql.jdbc.Driver&quot; url = &quot;jdbc:mysql://127.0.0.1:3306/seata?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;allowMultiQueries=true&amp;serverTimezone=Asia/Shanghai&quot; user = &quot;root&quot; password = &quot;root&quot; minConn = 5 maxConn = 100 globalTable = &quot;global_table&quot; branchTable = &quot;branch_table&quot; lockTable = &quot;lock_table&quot; queryLimit = 100 maxWait = 5000 } } 「注意：这里只列出了修改的部分内容，完整的file.conf文件可以到项目的doc/nacos/config/chapter25目录下获取。」 （5）在下载的Seata源码的seata-1.4.2/script/config-center目录下找到config.txt文件，如下所示。 将其复制到Seata安装包解压的根目录下，如下所示。 接下来，修改Seata安装包解压的根目录下的config.txt文件，这里还是只列出修改的部分，如下所示。 service.vgroupMapping.server-order-tx_group=default service.vgroupMapping.server-product-tx_group=default store.mode=db store.publicKey=&quot;&quot; store.db.datasource=druid store.db.dbType=mysql store.db.driverClassName=com.mysql.jdbc.Driver store.db.url=jdbc:mysql://127.0.0.1:3306/seata?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;allowMultiQueries=true&amp;serverTimezone=Asia/Shanghai store.db.user=root store.db.password=root store.redis.sentinel.masterName=&quot;&quot; store.redis.sentinel.sentinelHosts=&quot;&quot; store.redis.password=&quot;&quot; 「注意：在config.txt中，部分配置的等号“=”后面为空，需要在等号“=“后面添加空字符串&quot;&quot;。同样的，小伙伴们可以到项目的doc/nacos/config/chapter25目录下获取完整的config.txt文件。」 （6）在下载的Seata源码的seata-1.4.2/script/config-center/nacos目录下找到nacos-config.sh文件，如下所示。 将nacos-config.sh文件复制到Seata安装文件解压目录的seata/seata-server-1.4.2/scripts目录下，其中scripts目录需要手动创建，如下所示。 （7）.sh文件是Linux操作系统上的脚本文件，如果想在Windows操作系统上运行.sh文件，可以在Windows操作系统上安装Git后在运行.sh文件。 接下来，在Git的Bash命令行进入Seata安装文件中nacos-config.sh文件所在的目录，执行如下命令。 sh nacos-config.sh -h 127.0.0.1 -p 8848 -g SEATA_GROUP -t seata_namespace_001 -u nacos -w nacos 其中，命令中的每个参数含义如下所示。 -h：Nacos所在的IP地址。 -p：Nacos的端口号。 -g：分组。 -t：命名空间的ID，这里我们填写在Nacos中创建的命名空间的ID：seata_namespace_001。如果不填，默认是public命名空间。 -u：Nacos的用户名。 -w：Nacos的密码。 执行命令后的结果信息如下所示。 ========================================================================= Complete initialization parameters, total-count:89 , failure-count:0 ========================================================================= Init nacos config finished, please start seata-server. 可以看到，整个配置执行成功。 （8）打开Nacos的配置管理-配置列表界面，切换到seata命名空间，可以看到有关Seata的配置都注册到Nacos中了，如下所示。 （9）在MySQL数据库中创建seata数据库，如下所示。 create database if not exists seata; 接下来，在seata数据库中执行Seata源码包seata-1.4.2/script/server/db目录下的mysql.sql脚本文件，mysql.sql脚本的内容如下所示。 -- -------------------------------- The script used when storeMode is 'db' -------------------------------- -- the table to store GlobalSession data CREATE TABLE IF NOT EXISTS `global_table` ( `xid` VARCHAR(128) NOT NULL, `transaction_id` BIGINT, `status` TINYINT NOT NULL, `application_id` VARCHAR(32), `transaction_service_group` VARCHAR(32), `transaction_name` VARCHAR(128), `timeout` INT, `begin_time` BIGINT, `application_data` VARCHAR(2000), `gmt_create` DATETIME, `gmt_modified` DATETIME, PRIMARY KEY (`xid`), KEY `idx_gmt_modified_status` (`gmt_modified`, `status`), KEY `idx_transaction_id` (`transaction_id`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8; -- the table to store BranchSession data CREATE TABLE IF NOT EXISTS `branch_table` ( `branch_id` BIGINT NOT NULL, `xid` VARCHAR(128) NOT NULL, `transaction_id` BIGINT, `resource_group_id` VARCHAR(32), `resource_id` VARCHAR(256), `branch_type` VARCHAR(8), `status` TINYINT, `client_id` VARCHAR(64), `application_data` VARCHAR(2000), `gmt_create` DATETIME(6), `gmt_modified` DATETIME(6), PRIMARY KEY (`branch_id`), KEY `idx_xid` (`xid`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8; -- the table to store lock data CREATE TABLE IF NOT EXISTS `lock_table` ( `row_key` VARCHAR(128) NOT NULL, `xid` VARCHAR(128), `transaction_id` BIGINT, `branch_id` BIGINT NOT NULL, `resource_id` VARCHAR(256), `table_name` VARCHAR(32), `pk` VARCHAR(36), `gmt_create` DATETIME, `gmt_modified` DATETIME, PRIMARY KEY (`row_key`), KEY `idx_branch_id` (`branch_id`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8; 这里，也将mysql.sql文件放在了项目的doc/nacos/config/chapter25目录下。 （10）启动Seata服务，进入在命令行进入Seata安装文件的seata/seata-server-1.4.2/bin目录，执行如下命令。 seata-server.bat -p 8091 -h 127.0.0.1 -m db 可以看到，在启动Seata的命令行输出了如下信息。 i.s.core.rpc.netty.NettyServerBootstrap : Server started, listen port: 8091 说明Seata已经启动成功。 至此，Seata的基础环境搭建完毕。 项目整合Seata 在我们开发的微服务程序中，订单微服务下单成功后会调用库存微服务扣减商品的库存信息，而用户微服务只提供了查询用户信息的接口。这里，我们在商品微服务和订单微服务中整合Seata。 导入unlog表 我们使用的是Seata的AT模式，需要我们在涉及到使用Seata解决分布式事务问题的每个业务库中创建一个Seata的undo_log数据表，Seata中本身提供了创建数据表的SQL文件，这些SQL文件位于Seata源码包下的seata-1.4.2/script/client/at/db目录中，如下所示。 这里，我们使用mysql.sql脚本。mysql.sql脚本的内容如下所示。 -- for AT mode you must to init this sql for you business database. the seata server not need it. CREATE TABLE IF NOT EXISTS `undo_log` ( `branch_id` BIGINT NOT NULL COMMENT 'branch transaction id', `xid` VARCHAR(128) NOT NULL COMMENT 'global transaction id', `context` VARCHAR(128) NOT NULL COMMENT 'undo_log context,such as serialization', `rollback_info` LONGBLOB NOT NULL COMMENT 'rollback info', `log_status` INT(11) NOT NULL COMMENT '0:normal status,1:defense status', `log_created` DATETIME(6) NOT NULL COMMENT 'create datetime', `log_modified` DATETIME(6) NOT NULL COMMENT 'modify datetime', UNIQUE KEY `ux_undo_log` (`xid`, `branch_id`) ) ENGINE = InnoDB AUTO_INCREMENT = 1 DEFAULT CHARSET = utf8 COMMENT ='AT transaction mode undo table'; 注意，这里要在shop数据库中执行mysql.sql脚本，同样的，我会将这里的mysql.sql文件放到项目的doc/nacos/config/chapter25目录下，并重命名为mysql_client.sql。 商品微服务整合Seata （1）在商品微服务shop-product的pom.xml文件中引入Seata依赖，如下所示。 &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-seata&lt;/artifactId&gt; &lt;/dependency&gt; （2）修改商品微服务shop-product的bootstrap.yml，修改后的文件如下所示。 spring: application: name: server-product cloud: nacos: config: server-addr: 127.0.0.1:8848 file-extension: yaml group: product_group shared-configs[0]: data_id: server-all.yaml group: all_group refresh: true discovery: server-addr: 127.0.0.1:8848 alibaba: seata: tx-service-group: ${spring.application.name}-tx_group profiles: active: dev seata: application-id: ${spring.application.name} service: vgroup-mapping: server-product-tx_group: default registry: nacos: server-addr: ${spring.cloud.nacos.discovery.server-addr} username: nacos password: nacos group: SEATA_GROUP namespace: seata_namespace_001 application: seata-server config: type: nacos nacos: server-addr: ${spring.cloud.nacos.discovery.server-addr} username: nacos password: nacos group: SEATA_GROUP namespace: seata_namespace_001 其中，配置的Nacos的namespace与group与registry.conf文件中的一致。 订单微服务整合Seata （1）在订单微服务shop-product的pom.xml文件中引入Seata依赖，如下所示。 &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-seata&lt;/artifactId&gt; &lt;/dependency&gt; （2）修改订单微服务shop-order的bootstrap.yml，修改后的文件如下所示。 spring: application: name: server-order cloud: nacos: config: server-addr: 127.0.0.1:8848 file-extension: yaml group: order_group shared-configs[0]: data_id: server-all.yaml group: all_group refresh: true discovery: server-addr: 127.0.0.1:8848 alibaba: seata: tx-service-group: ${spring.application.name}-tx_group profiles: active: dev seata: application-id: ${spring.application.name} service: vgroup-mapping: server-order-tx_group: default registry: nacos: server-addr: ${spring.cloud.nacos.discovery.server-addr} username: nacos password: nacos group: SEATA_GROUP namespace: seata_namespace_001 application: seata-server config: type: nacos nacos: server-addr: ${spring.cloud.nacos.discovery.server-addr} username: nacos password: nacos group: SEATA_GROUP namespace: seata_namespace_001 （3）修改订单微服务的io.binghe.shop.order.service.impl.OrderServiceV8Impl类的saveOrder()方法，在saveOrder()方法上添加Seata的@GlobalTransactional注解，如下所示。 @Override @GlobalTransactional public void saveOrder(OrderParams orderParams) { //省略具体方法代码 } 至此，搭建并整合Seata完毕，就是这么简单。 验证Seata事务 重置数据库数据 这里，首先将商品数据表t_product中id为1001的数据的库存信息重置为100，如下所示。 update t_product set t_pro_stock = 100 where id = 1001; 查询数据表数据 （1）打开cmd终端，进入MySQL命令行，并进入shop商城数据库，如下所示。 C:\\Users\\binghe&gt;mysql -uroot -p Enter password: **** Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 15 Server version: 5.7.35 MySQL Community Server (GPL) Copyright (c) 2000, 2021, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql&gt; use shop; Database changed （2）查看商品数据表，如下所示。 mysql&gt; select * from t_product; +------+------------+-------------+-------------+ | id | t_pro_name | t_pro_price | t_pro_stock | +------+------------+-------------+-------------+ | 1001 | 华为 | 2399.00 | 100 | | 1002 | 小米 | 1999.00 | 100 | | 1003 | iphone | 4999.00 | 100 | +------+------------+-------------+-------------+ 3 rows in set (0.00 sec) 这里，我们以id为1001的商品为例，此时发现商品的库存为100。 （3）查询订单数据表，如下所示。 mysql&gt; select * from t_order; Empty set (0.00 sec) 可以发现订单数据表为空。 （4）查询订单条目数据表，如下所示。 mysql&gt; select * from t_order_item; Empty set (0.00 sec) 可以看到，订单条目数据表为空。 验证Seata事务 （1）分别启动Nacos、Sentinel、ZinKin、RocketMQ，Seata，并启动用户微服务，商品微服务，订单微服务和服务网关。打开浏览器访问http://localhost:10001/server-order/order/submit_order?userId=1001&amp;productId=1001&amp;count=1，如下所示。 返回的原始数据如下所示。 {&quot;code&quot;:500,&quot;codeMsg&quot;:&quot;执行失败&quot;,&quot;data&quot;:&quot;/ by zero&quot;} （2）查看各个微服务和网关输出的日志信息，分别如下所示。 用户微服务输出的日志如下所示。 获取到的用户信息为：{&quot;address&quot;:&quot;北京&quot;,&quot;id&quot;:1001,&quot;password&quot;:&quot;c26be8aaf53b15054896983b43eb6a65&quot;,&quot;phone&quot;:&quot;13212345678&quot;,&quot;username&quot;:&quot;binghe&quot;} 说明用户微服务无异常信息。 商品微服务输出的日志如下所示。 获取到的商品信息为：{&quot;id&quot;:1001,&quot;proName&quot;:&quot;华为&quot;,&quot;proPrice&quot;:2399.00,&quot;proStock&quot;:100} 更新商品库存传递的参数为: 商品id:1001, 购买数量:1 说明商品微服务无异常信息。 值得注意的是，整合Seata后，商品微服务同时输出了如下日志。 rm handle branch rollback process:xid=192.168.0.111:8091:6638572304823066625,branchId=6638572304823066634,branchType=AT,resourceId=jdbc:mysql://localhost:3306/shop,applicationData=null Branch Rollbacking: 192.168.0.111:8091:6638572304823066625 6638572304823066634 jdbc:mysql://localhost:3306/shop xid 192.168.0.111:8091:6638572304823066625 branch 6638572304823066634, undo_log deleted with GlobalFinished Branch Rollbacked result: PhaseTwo_Rollbacked 看上去应该是有事务回滚了。 订单微服务输出的日志如下所示。 提交订单时传递的参数:{&quot;count&quot;:1,&quot;empty&quot;:false,&quot;productId&quot;:1001,&quot;userId&quot;:1001} 库存扣减成功 服务器抛出了异常：{} java.lang.ArithmeticException: / by zero 说明订单微服务抛出了ArithmeticException异常。 同时，商品微服务会输出如下日志。 Branch Rollbacked result: PhaseTwo_Rollbacked [192.168.0.111:8091:6638572304823066625] rollback status: Rollbacked 看上去应该是有事务回滚了。 网关服务输出的日志如下所示。 执行前置过滤器逻辑 执行后置过滤器逻辑 访问接口主机: localhost 访问接口端口: 10001 访问接口URL: /server-order/order/submit_order 访问接口URL参数: userId=1001&amp;productId=1001&amp;count=1 访问接口时长: 1632ms 可以看到，网关服务无异常信息。 通过微服务打印出的日志信息，可以看到，有事务回滚了。 查询数据表数据 （1）打开cmd终端，进入MySQL命令行，并进入shop商城数据库，如下所示。 C:\\Users\\binghe&gt;mysql -uroot -p Enter password: **** Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 15 Server version: 5.7.35 MySQL Community Server (GPL) Copyright (c) 2000, 2021, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql&gt; use shop; Database changed （2）查看商品数据表，如下所示。 mysql&gt; select * from t_product; +------+------------+-------------+-------------+ | id | t_pro_name | t_pro_price | t_pro_stock | +------+------------+-------------+-------------+ | 1001 | 华为 | 2399.00 | 100 | | 1002 | 小米 | 1999.00 | 100 | | 1003 | iphone | 4999.00 | 100 | +------+------------+-------------+-------------+ 3 rows in set (0.00 sec) 可以看到，此时商品数据表中，id为1001的商品库存数量仍然为100。 （3）查看订单数据表，如下所示。 mysql&gt; select * from t_order; Empty set (0.00 sec) 可以看到，订单数据表为空。 （4）查看订单条目数据表，如下所示。 mysql&gt; select * from t_order_item; Empty set (0.00 sec) 可以看到，订单条目数据表为空。 至此，我们成功在项目中整合了Seata解决了分布式事务的问题 ","link":"https://tinaxiawuhao.github.io/post/IMU3BaL0b/"},{"title":"Seata","content":"Seata 是什么? Seata 是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式，为用户打造一站式的分布式解决方案。 AT 模式 前提 基于支持本地 ACID 事务的关系型数据库。 Java 应用，通过 JDBC 访问数据库。 整体机制 两阶段提交协议的演变： 一阶段：业务数据和回滚日志记录在同一个本地事务中提交，释放本地锁和连接资源。 二阶段： 提交异步化，非常快速地完成。 回滚通过一阶段的回滚日志进行反向补偿。 写隔离 一阶段本地事务提交前，需要确保先拿到 「全局锁」 。 拿不到 「全局锁」 ，不能提交本地事务。 拿 「全局锁」 的尝试被限制在一定范围内，超出范围将放弃，并回滚本地事务，释放本地锁。 以一个示例来说明： 两个全局事务 tx1 和 tx2，分别对 a 表的 m 字段进行更新操作，m 的初始值 1000。 tx1 先开始，开启本地事务，拿到本地锁，更新操作 m = 1000 - 100 = 900。本地事务提交前，先拿到该记录的 「全局锁」 ，本地提交释放本地锁。tx2 后开始，开启本地事务，拿到本地锁，更新操作 m = 900 - 100 = 800。本地事务提交前，尝试拿该记录的 「全局锁」 ，tx1 全局提交前，该记录的全局锁被 tx1 持有，tx2 需要重试等待 「全局锁」 。 tx1 二阶段全局提交，释放 「全局锁」 。tx2 拿到 「全局锁」 提交本地事务。 如果 tx1 的二阶段全局回滚，则 tx1 需要重新获取该数据的本地锁，进行反向补偿的更新操作，实现分支的回滚。 此时，如果 tx2 仍在等待该数据的 「全局锁」，同时持有本地锁，则 tx1 的分支回滚会失败。分支的回滚会一直重试，直到 tx2 的 「全局锁」 等锁超时，放弃 「全局锁」 并回滚本地事务释放本地锁，tx1 的分支回滚最终成功。 因为整个过程 「全局锁」 在 tx1 结束前一直是被 tx1 持有的，所以不会发生 「脏写」 的问题。 读隔离 在数据库本地事务隔离级别 「读已提交（Read Committed）」 或以上的基础上，Seata（AT 模式）的默认全局隔离级别是 「读未提交（Read Uncommitted）」 。 如果应用在特定场景下，必需要求全局的 「读已提交」 ，目前 Seata 的方式是通过 SELECT FOR UPDATE 语句的代理。 SELECT FOR UPDATE 语句的执行会申请 「全局锁」 ，如果 「全局锁」 被其他事务持有，则释放本地锁（回滚 SELECT FOR UPDATE 语句的本地执行）并重试。这个过程中，查询是被 block 住的，直到 「全局锁」 拿到，即读取的相关数据是 「已提交」 的，才返回。 出于总体性能上的考虑，Seata 目前的方案并没有对所有 SELECT 语句都进行代理，仅针对 FOR UPDATE 的 SELECT 语句。 工作机制 以一个示例来说明整个 AT 分支的工作过程。 业务表：product Field Type Key id bigint(20) PRI name varchar(100) since varchar(100) AT 分支事务的业务逻辑： update product set name = 'GTS' where name = 'TXC'; 「一阶段」 过程： 解析 SQL：得到 SQL 的类型（UPDATE），表（product），条件（where name = 'TXC'）等相关的信息。 查询前镜像：根据解析得到的条件信息，生成查询语句，定位数据。 select id, name, since from product where name = 'TXC'; 得到前镜像： id name since 1 TXC 2014 执行业务 SQL：更新这条记录的 name 为 'GTS'。 查询后镜像：根据前镜像的结果，通过 「主键」 定位数据。 select id, name, since from product where id = 1; 得到后镜像： id name since 1 GTS 2014 插入回滚日志：把前后镜像数据以及业务 SQL 相关的信息组成一条回滚日志记录，插入到 UNDO_LOG 表中。 { &quot;branchId&quot;: 641789253, &quot;undoItems&quot;: [{ &quot;afterImage&quot;: { &quot;rows&quot;: [{ &quot;fields&quot;: [{ &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: 4, &quot;value&quot;: 1 }, { &quot;name&quot;: &quot;name&quot;, &quot;type&quot;: 12, &quot;value&quot;: &quot;GTS&quot; }, { &quot;name&quot;: &quot;since&quot;, &quot;type&quot;: 12, &quot;value&quot;: &quot;2014&quot; }] }], &quot;tableName&quot;: &quot;product&quot; }, &quot;beforeImage&quot;: { &quot;rows&quot;: [{ &quot;fields&quot;: [{ &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: 4, &quot;value&quot;: 1 }, { &quot;name&quot;: &quot;name&quot;, &quot;type&quot;: 12, &quot;value&quot;: &quot;TXC&quot; }, { &quot;name&quot;: &quot;since&quot;, &quot;type&quot;: 12, &quot;value&quot;: &quot;2014&quot; }] }], &quot;tableName&quot;: &quot;product&quot; }, &quot;sqlType&quot;: &quot;UPDATE&quot; }], &quot;xid&quot;: &quot;xid:xxx&quot; } 提交前，向 TC 注册分支：申请 product 表中，主键值等于 1 的记录的 「全局锁」 。 本地事务提交：业务数据的更新和前面步骤中生成的 UNDO LOG 一并提交。 将本地事务提交的结果上报给 TC。 「二阶段-回滚」 收到 TC 的分支回滚请求，开启一个本地事务，执行如下操作。 通过 XID 和 Branch ID 查找到相应的 UNDO LOG 记录。 数据校验：拿 UNDO LOG 中的后镜与当前数据进行比较，如果有不同，说明数据被当前全局事务之外的动作做了修改。这种情况，需要根据配置策略来做处理，详细的说明在另外的文档中介绍。 根据 UNDO LOG 中的前镜像和业务 SQL 的相关信息生成并执行回滚的语句： update product set name = 'TXC' where id = 1; 提交本地事务。并把本地事务的执行结果（即分支事务回滚的结果）上报给 TC。 「二阶段-提交」 收到 TC 的分支提交请求，把请求放入一个异步任务的队列中，马上返回提交成功的结果给 TC。 异步任务阶段的分支提交请求将异步和批量地删除相应 UNDO LOG 记录。 附录 「回滚日志表」 UNDO_LOG Table：不同数据库在类型上会略有差别。 以 MySQL 为例： Field Type branch_id bigint PK xid varchar(100) context varchar(128) rollback_info longblob log_status tinyint log_created datetime log_modified datetime -- 注意此处0.7.0+ 增加字段 context CREATE TABLE `undo_log` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `branch_id` bigint(20) NOT NULL, `xid` varchar(100) NOT NULL, `context` varchar(128) NOT NULL, `rollback_info` longblob NOT NULL, `log_status` int(11) NOT NULL, `log_created` datetime NOT NULL, `log_modified` datetime NOT NULL, PRIMARY KEY (`id`), UNIQUE KEY `ux_undo_log` (`xid`,`branch_id`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; TCC 模式 回顾总览中的描述：一个分布式的全局事务，整体是 「两阶段提交」 的模型。全局事务是由若干分支事务组成的，分支事务要满足 「两阶段提交」 的模型要求，即需要每个分支事务都具备自己的： 一阶段 prepare 行为 二阶段 commit 或 rollback 行为 根据两阶段行为模式的不同，我们将分支事务划分为 「Automatic (Branch) Transaction Mode」 和 「Manual (Branch) Transaction Mode」. AT 模式（参考链接 TBD）基于 「支持本地 ACID 事务」 的 「关系型数据库」： 一阶段 prepare 行为：在本地事务中，一并提交业务数据更新和相应回滚日志记录。 二阶段 commit 行为：马上成功结束，「自动」 异步批量清理回滚日志。 二阶段 rollback 行为：通过回滚日志，「自动」 生成补偿操作，完成数据回滚。 相应的，TCC 模式，不依赖于底层数据资源的事务支持： 一阶段 prepare 行为：调用 「自定义」 的 prepare 逻辑。 二阶段 commit 行为：调用 「自定义」 的 commit 逻辑。 二阶段 rollback 行为：调用 「自定义」 的 rollback 逻辑。 所谓 TCC 模式，是指支持把 「自定义」 的分支事务纳入到全局事务的管理中。 Saga 模式 Saga模式是SEATA提供的长事务解决方案，在Saga模式中，业务流程中每个参与者都提交本地事务，当出现某一个参与者失败则补偿前面已经成功的参与者，一阶段正向服务和二阶段补偿服务都由业务开发实现。 理论基础：Hector &amp; Kenneth 发表论⽂ Sagas （1987） 适用场景 业务流程长、业务流程多 参与者包含其它公司或遗留系统服务，无法提供 TCC 模式要求的三个接口 优势 一阶段提交本地事务，无锁，高性能 事件驱动架构，参与者可异步执行，高吞吐 补偿服务易于实现 缺点 不保证隔离性 ","link":"https://tinaxiawuhao.github.io/post/XFKzbOO8H/"},{"title":"服务容错：服务雪崩与容错方案","content":"本文主要内容如下所示。 并发对系统的影响 当一个系统的架构设计采用微服务架构模式时，会将庞大而复杂的业务拆分成一个个小的微服务，各个微服务之间以接口或者RPC的形式进行互相调用。在调用的过程中，就会涉及到网路的问题，再加上微服务自身的原因，例如很难做到100%的高可用等。如果众多微服务当中的某个或某些微服务出现问题，不可用或者宕机了，那么其他微服务调用这些微服务的接口时就会出现延迟。如果此时有大量请求进入系统，就会造成请求任务的大量堆积，甚至会造成整体服务的瘫痪。 压测说明 为了更加直观的说明当系统没有容错能力时，高并发、大流量场景对于系统的影响，我们在这里模拟一个并发的场景。在订单微服务shop-order的io.binghe.shop.order.controller.OrderController类中新增一个concurrentRequest()方法，源码如下所示。 @GetMapping(value = &quot;/concurrent_request&quot;) public String concurrentRequest(){ log.info(&quot;测试业务在高并发场景下是否存在问题&quot;); return &quot;binghe&quot;; } 接下来，为了更好的演示效果，我们限制下Tomcat处理请求的最大并发数，在订单微服务shop-order的resources目录下的application.yml文件中添加如下配置。 server: port: 8080 tomcat: max-threads: 20 限制Tomcat一次最多只能处理20个请求。接下来，我们就使用JMeter对 http://localhost:8080/order/submit_order 接口进行压测，由于订单微服务中没有做任何的容错处理，当对 http://localhost:8080/order/submit_order 接口的请求压力过大时，我们再访问http://localhost:8080/order/concurrent_request 接口时，会发现http://localhost:8080/order/concurrent_request 接口会受到并发请求的影响，访问很慢甚至根本访问不到。 压测实战 使用JMeter对 http://localhost:8080/order/submit_order 接口进行压测，JMeter的配置过程如下所示。 （1）打开JMeter的主界面，如下所示。 （2）在JMeter中右键测试计划添加线程组，如下所示。 （3）在JMeter中的线程组中配置并发线程数，如下所示。 如上图所示，将线程数配置成50，Ramp-Up时间配置成0，循环次数为100。表示JMeter每次会在同一时刻向系统发送50个请求，发送100次为止。 （4）在JMeter中右键线程组添加HTTP请求，如下所示。 （5）在JMeter中配置HTTP请求，如下所示。 具体配置如下所示。 协议：http 服务器名称或IP：localhost 端口号：8080 方法：GET 路径：/order/submit_order?userId=1001&amp;productId=1001&amp;count=1 内容编码：UTF-8 （6）配置好JMeter后，点击JMeter上的绿色小三角开始压测，如下所示。 点击后会弹出需要保存JMeter脚本的弹出框，根据实际需要点击保存即可。 点击保存后，开始对 http://localhost:8080/order/submit_order 接口进行压测，在压测的过程中会发现订单微服务打印日志时，会比较卡顿，同时在浏览器或其他工具中访问http://localhost:8080/order/concurrent_request 接口会卡顿，甚至根本访问不到。 说明订单微服务中的某个接口一旦访问的并发量过高，其他接口也会受到影响，进而导致订单微服务整体不可用。为了说明这个问题，我们再来看看服务雪崩是个什么鬼。 服务雪崩 系统采用分布式或微服务的架构模式后，由于网络或者服务自身的问题，一般服务是很难做到100%高可用的。如果一个服务出现问题，就可能会导致其他的服务级联出现问题，这种故障性问题会在整个系统中不断扩散，进而导致服务不可用，甚至宕机，最终会对整个系统造成灾难性后果。 为了最大程度的预防服务雪崩，组成整体系统的各个微服务需要支持服务容错的功能。 服务容错方案 服务容错在一定程度上就是尽最大努力来兼容错误情况的发生，因为在分布式和微服务环境中，不可避免的会出现一些异常情况，我们在设计分布式和微服务系统时，就要考虑到这些异常情况的发生，使得系统具备服务容错能力。 常见的服务错误方案包含：服务限流、服务隔离、服务超时、服务熔断和服务降级等。 服务限流 服务限流就是限制进入系统的流量，以防止进入系统的流量过大而压垮系统。其主要的作用就是保护服务节点或者集群后面的数据节点，防止瞬时流量过大使服务和数据崩溃（如前端缓存大量实效），造成不可用；还可用于平滑请求。 限流算法有两种，一种就是简单的请求总量计数，一种就是时间窗口限流（一般为1s），如令牌桶算法和漏牌桶算法就是时间窗口的限流算法。 服务隔离 服务隔离有点类似于系统的垂直拆分，就按照一定的规则将系统划分成多个服务模块，并且每个服务模块之间是互相独立的，不会存在强依赖的关系。如果某个拆分后的服务发生故障后，能够将故障产生的影响限制在某个具体的服务内，不会向其他服务扩散，自然也就不会对整体服务产生致命的影响。 互联网行业常用的服务隔离方式有：线程池隔离和信号量隔离。 服务超时 整个系统采用分布式和微服务架构后，系统被拆分成一个个小服务，就会存在服务与服务之间互相调用的现象，从而形成一个个调用链。形成调用链关系的两个服务中，主动调用其他服务接口的服务处于调用链的上游，提供接口供其他服务调用的服务处于调用链的下游。 服务超时就是在上游服务调用下游服务时，设置一个最大响应时间，如果超过这个最大响应时间下游服务还未返回结果，则断开上游服务与下游服务之间的请求连接，释放资源。 服务熔断 在分布式与微服务系统中，如果下游服务因为访问压力过大导致响应很慢或者一直调用失败时，上游服务为了保证系统的整体可用性，会暂时断开与下游服务的调用连接。这种方式就是熔断。 服务熔断一般情况下会有三种状态：关闭、开启和半熔断。 关闭状态：服务一切正常，没有故障时，上游服务调用下游服务时，不会有任何限制。 开启状态：上游服务不再调用下游服务的接口，会直接返回上游服务中预定的方法。 半熔断状态：处于开启状态时，上游服务会根据一定的规则，尝试恢复对下游服务的调用。此时，上游服务会以有限的流量来调用下游服务，同时，会监控调用的成功率。如果成功率达到预期，则进入关闭状态。如果未达到预期，会重新进入开启状态。 服务降级 服务降级，说白了就是一种服务托底方案，如果服务无法完成正常的调用流程，就使用默认的托底方案来返回数据。例如，在商品详情页一般都会展示商品的介绍信息，一旦商品详情页系统出现故障无法调用时，会直接获取缓存中的商品介绍信息返回给前端页面。 ","link":"https://tinaxiawuhao.github.io/post/WLM-Ra1r4/"},{"title":"nocas长轮询","content":"前言 传统的静态配置方式想要修改某个配置时，必须重新启动一次应用，如果是数据库连接串的变更，那可能还容易接受一些，但如果变更的是一些运行时实时感知的配置，如某个功能项的开关，重启应用就显得有点大动干戈了。 配置中心正是为了解决此类问题应运而生的，特别是在微服务架构体系中，更倾向于使用配置中心来统一管理配置。 配置中心最核心的能力就是配置的动态推送，常见的配置中心如 Nacos、Apollo 等都实现了这样的能力。 在早期接触配置中心时，我就很好奇，配置中心是如何做到服务端感知配置变化实时推送给客户端的？ 在没有研究过配置中心的实现原理之前，我一度认为配置中心是通过长连接来做到配置推送的。 事实上，目前比较流行的两款配置中心：Nacos 和 Apollo 恰恰都没有使用长连接，而是使用的长轮询。 本文便是介绍一下长轮询这种听起来好像已经是上个世纪的技术，老戏新唱，看看能不能品出别样的韵味。 文中会有代码示例，呈现一个简易的配置监听流程。 数据交互模式 众所周知，数据交互有两种模式：Push（推模式）和 Pull（拉模式）。 推模式指的是客户端与服务端建立好网络长连接，服务方有相关数据，直接通过长连接通道推送到客户端。 其优点是及时，一旦有数据变更，客户端立马能感知到；另外对客户端来说逻辑简单，不需要关心有无数据这些逻辑处理。缺点是不知道客户端的数据消费能力，可能导致数据积压在客户端，来不及处理。 拉模式指的是客户端主动向服务端发出请求，拉取相关数据。 其优点是此过程由客户端发起请求，故不存在推模式中数据积压的问题。缺点是可能不够及时，对客户端来说需要考虑数据拉取相关逻辑，何时去拉，拉的频率怎么控制等等。 长轮询与轮询 在开头，重点介绍一下长轮询（Long Polling）和轮询（Polling）的区别，两者都是拉模式的实现。 「轮询」是指不管服务端数据有无更新，客户端每隔定长时间请求拉取一次数据，可能有更新数据返回，也可能什么都没有。 配置中心如果使用「轮询」实现动态推送，会有以下问题： 推送延迟。客户端每隔 5s 拉取一次配置，若配置变更发生在第 6s，则配置推送的延迟会达到 4s。 服务端压力。配置一般不会发生变化，频繁的轮询会给服务端造成很大的压力。 推送延迟和服务端压力无法中和。降低轮询的间隔，延迟降低，压力增加；增加轮询的间隔，压力降低，延迟增高。 「长轮询」则不存在上述的问题。 客户端发起长轮询，如果服务端的数据没有发生变更，会 hold 住请求，直到服务端的数据发生变化，或者等待一定时间超时才会返回。返回后，客户端又会立即再次发起下一次长轮询。 配置中心使用「长轮询」如何解决「轮询」遇到的问题也就显而易见了： 推送延迟。服务端数据发生变更后，长轮询结束，立刻返回响应给客户端。 服务端压力。长轮询的间隔期一般很长，例如 30s、60s，并且服务端 hold 住连接不会消耗太多服务端资源。 以 Nacos 为例的长轮询流程如下： 可能有人会有疑问，为什么一次长轮询需要等待一定时间超时，超时后又发起长轮询，为什么不让服务端一直 hold 住？ 主要有两个层面的考虑，一是连接稳定性的考虑，长轮询在传输层本质上还是走的 TCP 协议，如果服务端假死、fullgc 等异常问题，或者是重启等常规操作，长轮询没有应用层的心跳机制，仅仅依靠 TCP 层的心跳保活很难确保可用性，所以一次长轮询设置一定的超时时间也是在确保可用性。 除此之外，在配置中心场景，还有一定的业务需求需要这么设计。 在配置中心的使用过程中，用户可能随时新增配置监听，而在此之前，长轮询可能已经发出，新增的配置监听无法包含在旧的长轮询中，所以在配置中心的设计中，一般会在一次长轮询结束后，将新增的配置监听给捎带上，而如果长轮询没有超时时间，只要配置一直不发生变化，响应就无法返回，新增的配置也就没法设置监听了。 配置中心长轮询设计 上文的图中，介绍了长轮询的流程，本节会详解配置中心长轮询的设计细节。 客户端发起长轮询 客户端发起一个 HTTP 请求，请求信息包含配置中心的地址，以及监听的 dataId（本文出于简化说明的考虑，认为 dataId 是定位配置的唯一键）。若配置没有发生变化，客户端与服务端之间一直处于连接状态。 服务端监听数据变化 服务端会维护 dataId 和长轮询的映射关系，如果配置发生变化，服务端会找到对应的连接，为响应写入更新后的配置内容。如果超时内配置未发生变化，服务端找到对应的超时长轮询连接，写入 304 响应。 304 在 HTTP 响应码中代表“未改变”，并不代表错误。比较契合长轮询时，配置未发生变更的场景。 客户端接收长轮询响应 首先查看响应码是 200 还是 304，以判断配置是否变更，做出相应的回调。之后再次发起下一次长轮询。 服务端设置配置写入的接入点 主要用配置控制台和 client 发布配置，触发配置变更 这几点便是配置中心实现长轮询的核心步骤，也是指导下面章节代码实现的关键。但在编码之前，仍有一些其他的注意点需要实现阐明。 配置中心往往是为分布式的集群提供服务的，而每个机器上部署的应用，又会有多个 dataId 需要监听，实例级别 * 配置数是一个不小的数字，配置中心服务端维护这些 dataId 的长轮询连接显然不能用线程一一对应，否则会导致服务端线程数爆炸式增长。 一个 Tomcat 默认也就 200 个线程，长轮询也不应该阻塞 Tomcat 的业务线程，所以需要配置中心在实现长轮询时，往往采用异步响应的方式来实现。 而比较方便实现异步 HTTP 的常见手段便是 Servlet3.0 提供的 AsyncContext 机制。 Servlet3.0 并不是一个特别新的规范，它跟 Java 6 是同一时期的产物。例如 SpringBoot 内嵌的 Tomcat 很早就支持了 Servlet3.0，你无需担心 AsyncContext 机制不起作用。 SpringMVC 实现了 DeferredResult 和 Servlet3.0 提供的 AsyncContext 其实没有多大区别，我并没有深入研究过两个实现背后的源码，但从使用层面上来看，AsyncContext 更加的灵活，例如其可以自定义响应码，而 DeferredResult 在上层做了封装，可以快速的帮助开发者实现一个异步响应，但没法细粒度地控制响应。 所以下文的示例中，我选择了 AsyncContext。 配置中心长轮询实现 &lt;!--注解工具包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/com.google.guava/guava --&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;22.0&lt;/version&gt; &lt;/dependency&gt; 客户端实现 @Slf4j public class ConfigClient { private CloseableHttpClient httpClient; private RequestConfig requestConfig; public ConfigClient() { this.httpClient = HttpClientBuilder.create().build(); // ① httpClient 客户端超时时间要大于长轮询约定的超时时间 this.requestConfig = RequestConfig.custom().setSocketTimeout(40000).build(); } @SneakyThrows public void longPolling(String url, String dataId) { String endpoint = url + &quot;?dataId=&quot; + dataId; HttpGet request = new HttpGet(endpoint); CloseableHttpResponse response = httpClient.execute(request); switch (response.getStatusLine().getStatusCode()) { case 200: { BufferedReader rd = new BufferedReader(new InputStreamReader(response.getEntity() .getContent())); StringBuilder result = new StringBuilder(); String line; while ((line = rd.readLine()) != null) { result.append(line); } response.close(); String configInfo = result.toString(); log.info(&quot;dataId: [{}] changed, receive configInfo: {}&quot;, dataId, configInfo); longPolling(url, dataId); break; } // ② 304 响应码标记配置未变更 case 304: { log.info(&quot;longPolling dataId: [{}] once finished, configInfo is unchanged, longPolling again&quot;, dataId); longPolling(url, dataId); break; } default: { throw new RuntimeException(&quot;unExcepted HTTP status code&quot;); } } } public static void main(String[] args) { // httpClient 会打印很多 debug 日志，关闭掉 Logger logger = (Logger)LoggerFactory.getLogger(&quot;org.apache.http&quot;); logger.setLevel(Level.INFO); logger.setAdditive(false); ConfigClient configClient = new ConfigClient(); // ③ 对 dataId: user 进行配置监听 configClient.longPolling(&quot;http://127.0.0.1:8080/listener&quot;, &quot;user&quot;); } } 主要有三个注意点： RequestConfig.custom().setSocketTimeout(40000).build() 。httpClient 客户端超时时间要大于长轮询约定的超时时间。很好理解，不然还没等服务端返回，客户端会自行断开 HTTP 连接。 response.getStatusLine().getStatusCode() == 304 。前文介绍过，约定使用 304 响应码来标识配置未发生变更，客户端继续发起长轮询。 configClient.longPolling(&quot;http://127.0.0.1:8080/listener&quot;, &quot;user&quot;)。在示例中，我们处于简单考虑，仅仅启动一个客户端，对单一的 dataId：user 进行监听（注意，需要先启动 server 端）。 服务端实现 @RestController @Slf4j @SpringBootApplication public class ConfigServer { @Data private static class AsyncTask { // 长轮询请求的上下文，包含请求和响应体 private AsyncContext asyncContext; // 超时标记 private boolean timeout; public AsyncTask(AsyncContext asyncContext, boolean timeout) { this.asyncContext = asyncContext; this.timeout = timeout; } } // guava 提供的多值 Map，一个 key 可以对应多个 value private Multimap&lt;String, AsyncTask&gt; dataIdContext = Multimaps.synchronizedSetMultimap(HashMultimap.create()); private ThreadFactory threadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;longPolling-timeout-checker-%d&quot;) .build(); private ScheduledExecutorService timeoutChecker = new ScheduledThreadPoolExecutor(1, threadFactory); // 配置监听接入点 @RequestMapping(&quot;/listener&quot;) public void addListener(HttpServletRequest request, HttpServletResponse response) { String dataId = request.getParameter(&quot;dataId&quot;); // 开启异步 AsyncContext asyncContext = request.startAsync(request, response); AsyncTask asyncTask = new AsyncTask(asyncContext, true); // 维护 dataId 和异步请求上下文的关联 dataIdContext.put(dataId, asyncTask); // 启动定时器，30s 后写入 304 响应 timeoutChecker.schedule(() -&gt; { if (asyncTask.isTimeout()) { dataIdContext.remove(dataId, asyncTask); response.setStatus(HttpServletResponse.SC_NOT_MODIFIED); asyncContext.complete(); } }, 30000, TimeUnit.MILLISECONDS); } // 配置发布接入点 @RequestMapping(&quot;/publishConfig&quot;) @SneakyThrows public String publishConfig(String dataId, String configInfo) { log.info(&quot;publish configInfo dataId: [{}], configInfo: {}&quot;, dataId, configInfo); Collection&lt;AsyncTask&gt; asyncTasks = dataIdContext.removeAll(dataId); for (AsyncTask asyncTask : asyncTasks) { asyncTask.setTimeout(false); HttpServletResponse response = (HttpServletResponse)asyncTask.getAsyncContext().getResponse(); response.setStatus(HttpServletResponse.SC_OK); response.getWriter().println(configInfo); asyncTask.getAsyncContext().complete(); } return &quot;success&quot;; } public static void main(String[] args) { SpringApplication.run(ConfigServer.class, args); } } 对上述实现的一些说明： @RequestMapping(&quot;/listener&quot;) ，配置监听接入点，也是长轮询的入口。在获取 dataId 之后，使用 request.startAsync 将请求设置为异步，这样在方法结束后，不会占用 Tomcat 的线程池。 接着 dataIdContext.put(dataId, asyncTask) 会将 dataId 和异步请求上下文给关联起来，方便配置发布时，拿到对应的上下文。 注意这里使用了一个 guava 提供的数据结构 Multimap&lt;String, AsyncTask&gt; dataIdContext ，它是一个多值 Map，一个 key 可以对应多个 value，你也可以理解为 Map&lt;String,List&gt; ，但使用 Multimap 维护起来可以更方便地处理一些并发逻辑。 至于为什么会有多值，很好理解，因为配置中心的 Server 端会接受来自多个客户端对同一个 dataId 的监听。 timeoutChecker.schedule() 启动定时器，30s 后写入 304 响应。再结合之前客户端的逻辑，接收到 304 之后，会重新发起长轮询，形成一个循环。 @RequestMapping(&quot;/publishConfig&quot;) ，配置发布的入口。配置变更后，根据 dataId 一次拿出所有的长轮询，为之写入变更的响应，同时不要忘记取消定时任务。至此，完成了一个配置变更后推送的流程。 启动配置监听 先启动 ConfigServer，再启动 ConfigClient。客户端打印长轮询的日志如下： 22:18:09.185 [main] INFO moe.cnkirito.demo.ConfigClient - longPolling dataId: [user] once finished, configInfo is unchanged, longPolling again 22:18:39.197 [main] INFO moe.cnkirito.demo.ConfigClient - longPolling dataId: [user] once finished, configInfo is unchanged, longPolling again 发布一条配置，curl -X GET &quot;localhost:8080/publishConfig?dataId=user&amp;configInfo=helloworld&quot; 服务端打印日志如下： 22:18:50.801 INFO 73301 --- [nio-8080-exec-6] moe.cnkirito.demo.ConfigServer : publish configInfo dataId: [user], configInfo: helloworld 客户端接受配置推送： 22:18:50.806 [main] INFO moe.cnkirito.demo.ConfigClient - dataId: [user] changed, receive configInfo: helloworld 实现细节思考 为什么需要定时器返回 304 上述的实现中，服务端采用了一个定时器，在配置未发生变更时，定时返回 304，客户端接收到 304 之后，重新发起长轮询。 在前文，已经解释过了为什么需要超时后重新发起长轮询，而不是由服务端一直 hold，直到配置变更再返回。 但可能有读者还会有疑问，为什么不由客户端控制超时，服务端去除掉定时器，这样客户端超时后重新发起下一次长轮询，这样的设计不是更简单吗？ 无论是 Nacos 还是 Apollo 都有这样的定时器，而不是靠客户端控制超时，这样做主要有两点考虑： 和真正的客户端超时区分开。 仅仅使用异常（Exception）来表达异常流，而不应该用异常来表达正常的业务流。304 不是超时异常，而是长轮询中配置未变更的一种正常流程，不应该使用超时异常来表达。 客户端超时需要单独配置，且需要比服务端长轮询的超时要长。 正如上述的 demo 中客户端超时设置的是 40s，服务端判断一次长轮询超时是 30s。 这两个值在 Nacos 中默认是 30s 和 29.5s，在 Apollo 中默认是是 90s 和 60s。 长轮询包含多组 dataId 在上述的 demo 中，一个 dataId 会发起一次长轮询，在实际配置中心的设计中肯定不能这样设计，一般的优化方式是，一批 dataId 组成一个组批量包含在一个长轮询任务中。 在 Nacos 中，按照 3000 个 dataId 为一组包装成一个长轮询任务。 长轮询和长连接 讲完实现细节，本文最核心的部分已经介绍完了。 再回到最前面提到的数据交互模式上提到的推模型和拉模型，其实在写这篇文章时，我曾经问过交流群中的小伙伴们“配置中心实现动态推送的原理”，他们中绝大多数人认为是长连接的推模型。 然而事实上，主流的配置中心几乎都是使用了本文介绍的长轮询方案，这又是为什么呢？ 我也翻阅了不少博客，显然他们给出的理由并不能说服我，我尝试着从自己的角度分析了一下这个既定的事实。 长轮询实现起来比较容易，完全依赖于 HTTP 便可以实现全部逻辑，而 HTTP 是最能够被大众接受的通信方式。 长轮询使用 HTTP，便于多语言客户端的编写，大多数语言都有 HTTP 的客户端。 那么长连接是不是真的就不适合用于配置中心场景呢？ 有人可能会认为维护一条长连接会消耗大量资源，而长轮询可以提升系统的吞吐量，而在配置中心场景，这一假设并没有实际的压测数据能够论证，benchmark everything！please~ 另外，翻阅了一下 Nacos 2.0 的 milestone，我发现了一个有意思的规划，Nacos 的注册中心（目前是短轮询 + udp 推送）和配置中心（目前是长轮询）都有计划改造为长连接模式。 再回过头来看，长轮询实现已经将配置中心这个组件支撑的足够好了，替换成长连接，一定需要找到合适的理由才行。 总结 本文介绍了长轮询、轮询、长连接这几种数据交互模型的差异性。 分析了 Nacos 和 Apollo 等主流配置中心均是通过长轮询的方式实现配置的实时推送的。实时感知建立在客户端拉的基础上，因为本质上还是通过 HTTP 进行的数据交互，之所以有“推”的感觉，是因为服务端 hold 住了客户端的响应体，并且在配置变更后主动写入了返回 response 对象再进行返回。 通过一个简单的 demo，实现了长轮询配置实时推送的过程演示 ","link":"https://tinaxiawuhao.github.io/post/t9Kci8xf6/"},{"title":"ZipKin核心架构","content":"ZipKin核心架构 Zipkin 是 Twitter 的一个开源项目，它基于Google Dapper论文实现，可以收集微服务运行过程中的实时链路数据，并进行展示。 ZipKin概述 Zipkin是一种分布式链路跟踪系统，能够收集微服务运行过程中的实时调用链路信息，并能够将这些调用链路信息展示到Web界面上供开发人员分析，开发人员能够从ZipKin中分析出调用链路中的性能瓶颈，识别出存在问题的应用程序，进而定位问题和解决问题。 ZipKin核心架构 ZipKin的核心架构图如下所示。 注：图片来源：zipkin.io/pages/architecture.html 其中，ZipKin核心组件的功能如下所示。 Reporter：ZipKin中上报链路数据的模块，主要配置在具体的微服务应用中。 Transport：ZipKin中传输链路数据的模块，此模块可以配置为Kafka，RocketMQ、RabbitMQ等。 Collector：ZipKin中收集并消费链路数据的模块，默认是通过http协议收集，可以配置为Kafka消费。 Storage：ZipKin中存储链路数据的模块，此模块的具体可以配置为ElasticSearch、Cassandra或者MySQL，目前ZipKin支持这三种数据持久化方式。 API：ZipKin中的API 组件，主要用来提供外部访问接口。比如给客户端展示跟踪信息，或是开放给外部系统实现监控等。 UI：ZipKin中的UI 组件，基于API组件实现的上层应用。通过UI组件用户可以方便并且很直观地查询和分析跟踪信息。 Zipkin在总体上会分为两个端，一个是Zipkin服务端，一个是Zipkin客户端，客户端主要是配置在微服务应用中，收集微服务中的调用链路信息，将数据发送给ZipKin服务端。 项目整合ZipKin Zipkin总体上分为服务端和客户端，我们需要下载并启动ZipKin服务端的Jar包，在微服务中集成ZipKin的客户端。 下载安装ZipKin服务端 （1）下载ZipKin服务端Jar文件，可以直接在浏览器中输入如下链接进行下载。 https://search.maven.org/remote_content?g=io.zipkin.java&amp;a=zipkin-server&amp;v=LATEST&amp;c=exec 如果大家使用的是Linux操作系统，也可以在命令行输入如下命令进行下载。 wget https://search.maven.org/remote_content?g=io.zipkin.java&amp;a=zipkin-server&amp;v=LATEST&amp;c=exec 这里，我通过浏览器下载的ZipKin服务端Jar文件为：zipkin-server-2.12.9-exec.jar。 （2）在命令行输入如下命令启动ZipKin服务端。 java -jar zipkin-server-2.12.9-exec.jar （3）由于ZipKin服务端启动时，默认监听的端口号为9411，所以，在浏览器中输入http://localhost:9411链接就可以打开ZipKin的界面，如下所示。 在浏览器中输入http://localhost:9411链接能够打开上述页面就说明ZipKin服务端已经准备好啦。 项目整合ZipKin客户端 （1）在每个微服务（用户微服务shop-user，商品微服务shop-product，订单微服务shop-order，网关服务shop-gateway）中添加ZipKin依赖，如下所示。 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt; &lt;/dependency&gt; （2）在网关服务shop-gateway的application.yml文件中添加如下配置。 spring: sleuth: sampler: probability: 1.0 zipkin: base-url: http://127.0.0.1:9411 discovery-client-enabled: false 其中各配置的说明如下所示。 spring.sleuth.sampler.probability：表示Sleuth的采样百分比。 spring.zipkin.base-url：ZipKin服务端的地址。 spring.zipkin.discovery-client-enabled：配置成false，使Nacos将其当成一个URL，不要按服务名处理。 （3）分别启动用户微服务，商品微服务，订单微服务和服务网关，在浏览器中访问链接http://localhost:10001/server-order/order/submit_order?userId=1001&amp;productId=1001&amp;count=1，如下所示。 （4）点击Zipkin界面上的查找按钮，如下所示。 点击后的界面如下所示。 可以看到，点击查找按钮后，会出现一个请求链路，包含：网关服务server-gateway耗时63.190毫秒，订单微服务server-order耗时53.101毫秒，用户微服务server-user耗时14.640毫秒，商品微服务server-product耗时10.941毫秒。 （5）点开ZipKin界面上显示的调用链路，如下所示。 点开后的界面如下所示。 可以非常清晰的看到整个调用的访问链路。 我们还可以点击具体的节点来查看具体的调用信息。 例如我们点击网关微服务查看网关的具体链路，如下所示。 点开后的效果如下所示。 接下来，查看下订单微服务的调用链路具体信息，如下所示。 点开后的效果如下所示。 可以看到，通过ZipKin能够查看服务的调用链路，并且能够查看具体微服务的调用情况。我们可以基于ZipKin来分析系统的调用链路情况，找出系统的瓶颈点，进而进行针对性的优化。 另外，ZipKin中也支持下载系统调用链路的Json数据，如下所示。 点击JSON按钮后，效果如下所示。 其中，显示的Json数据如下所示。 [ [ { &quot;traceId&quot;: &quot;9d244edbc1668d92&quot;, &quot;parentId&quot;: &quot;3f01ba499fac4ce9&quot;, &quot;id&quot;: &quot;5f0932b5d06fe757&quot;, &quot;kind&quot;: &quot;SERVER&quot;, &quot;name&quot;: &quot;get /get/{pid}&quot;, &quot;timestamp&quot;: 1652413758790051, &quot;duration&quot;: 10941, &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;server-product&quot;, &quot;ipv4&quot;: &quot;192.168.0.111&quot; }, &quot;remoteEndpoint&quot;: { &quot;ipv4&quot;: &quot;192.168.0.111&quot;, &quot;port&quot;: 54140 }, &quot;tags&quot;: { &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/product/get/1001&quot;, &quot;mvc.controller.class&quot;: &quot;ProductController&quot;, &quot;mvc.controller.method&quot;: &quot;getProduct&quot; }, &quot;shared&quot;: true }, { &quot;traceId&quot;: &quot;9d244edbc1668d92&quot;, &quot;parentId&quot;: &quot;3f01ba499fac4ce9&quot;, &quot;id&quot;: &quot;c020c7f6e0fa1604&quot;, &quot;kind&quot;: &quot;SERVER&quot;, &quot;name&quot;: &quot;get /update_count/{pid}/{count}&quot;, &quot;timestamp&quot;: 1652413758808052, &quot;duration&quot;: 5614, &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;server-product&quot;, &quot;ipv4&quot;: &quot;192.168.0.111&quot; }, &quot;remoteEndpoint&quot;: { &quot;ipv4&quot;: &quot;192.168.0.111&quot;, &quot;port&quot;: 54140 }, &quot;tags&quot;: { &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/product/update_count/1001/1&quot;, &quot;mvc.controller.class&quot;: &quot;ProductController&quot;, &quot;mvc.controller.method&quot;: &quot;updateCount&quot; }, &quot;shared&quot;: true }, { &quot;traceId&quot;: &quot;9d244edbc1668d92&quot;, &quot;parentId&quot;: &quot;9d244edbc1668d92&quot;, &quot;id&quot;: &quot;3f01ba499fac4ce9&quot;, &quot;kind&quot;: &quot;CLIENT&quot;, &quot;name&quot;: &quot;get&quot;, &quot;timestamp&quot;: 1652413758763816, &quot;duration&quot;: 54556, &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;server-gateway&quot;, &quot;ipv4&quot;: &quot;192.168.0.111&quot; }, &quot;remoteEndpoint&quot;: { &quot;ipv4&quot;: &quot;192.168.0.111&quot;, &quot;port&quot;: 8080 }, &quot;tags&quot;: { &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/order/submit_order&quot; } }, { &quot;traceId&quot;: &quot;9d244edbc1668d92&quot;, &quot;parentId&quot;: &quot;9d244edbc1668d92&quot;, &quot;id&quot;: &quot;475ff483fb0973b1&quot;, &quot;kind&quot;: &quot;CLIENT&quot;, &quot;name&quot;: &quot;get&quot;, &quot;timestamp&quot;: 1652413758759023, &quot;duration&quot;: 59621, &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;server-gateway&quot;, &quot;ipv4&quot;: &quot;192.168.0.111&quot; }, &quot;tags&quot;: { &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/order/submit_order&quot; } }, { &quot;traceId&quot;: &quot;9d244edbc1668d92&quot;, &quot;id&quot;: &quot;9d244edbc1668d92&quot;, &quot;kind&quot;: &quot;SERVER&quot;, &quot;name&quot;: &quot;get&quot;, &quot;timestamp&quot;: 1652413758757034, &quot;duration&quot;: 63190, &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;server-gateway&quot;, &quot;ipv4&quot;: &quot;192.168.0.111&quot; }, &quot;remoteEndpoint&quot;: { &quot;ipv4&quot;: &quot;127.0.0.1&quot;, &quot;port&quot;: 54137 }, &quot;tags&quot;: { &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/server-order/order/submit_order&quot; } }, { &quot;traceId&quot;: &quot;9d244edbc1668d92&quot;, &quot;parentId&quot;: &quot;3f01ba499fac4ce9&quot;, &quot;id&quot;: &quot;a048eda8d5fd3dc9&quot;, &quot;kind&quot;: &quot;CLIENT&quot;, &quot;name&quot;: &quot;get&quot;, &quot;timestamp&quot;: 1652413758774201, &quot;duration&quot;: 12054, &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;server-order&quot;, &quot;ipv4&quot;: &quot;192.168.0.111&quot; }, &quot;tags&quot;: { &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/user/get/1001&quot; } }, { &quot;traceId&quot;: &quot;9d244edbc1668d92&quot;, &quot;parentId&quot;: &quot;3f01ba499fac4ce9&quot;, &quot;id&quot;: &quot;5f0932b5d06fe757&quot;, &quot;kind&quot;: &quot;CLIENT&quot;, &quot;name&quot;: &quot;get&quot;, &quot;timestamp&quot;: 1652413758787924, &quot;duration&quot;: 12557, &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;server-order&quot;, &quot;ipv4&quot;: &quot;192.168.0.111&quot; }, &quot;tags&quot;: { &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/product/get/1001&quot; } }, { &quot;traceId&quot;: &quot;9d244edbc1668d92&quot;, &quot;parentId&quot;: &quot;3f01ba499fac4ce9&quot;, &quot;id&quot;: &quot;c020c7f6e0fa1604&quot;, &quot;kind&quot;: &quot;CLIENT&quot;, &quot;name&quot;: &quot;get&quot;, &quot;timestamp&quot;: 1652413758805787, &quot;duration&quot;: 7031, &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;server-order&quot;, &quot;ipv4&quot;: &quot;192.168.0.111&quot; }, &quot;tags&quot;: { &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/product/update_count/1001/1&quot; } }, { &quot;traceId&quot;: &quot;9d244edbc1668d92&quot;, &quot;parentId&quot;: &quot;9d244edbc1668d92&quot;, &quot;id&quot;: &quot;3f01ba499fac4ce9&quot;, &quot;kind&quot;: &quot;SERVER&quot;, &quot;name&quot;: &quot;get /submit_order&quot;, &quot;timestamp&quot;: 1652413758765048, &quot;duration&quot;: 53101, &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;server-order&quot;, &quot;ipv4&quot;: &quot;192.168.0.111&quot; }, &quot;remoteEndpoint&quot;: { &quot;ipv4&quot;: &quot;127.0.0.1&quot; }, &quot;tags&quot;: { &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/order/submit_order&quot;, &quot;mvc.controller.class&quot;: &quot;OrderController&quot;, &quot;mvc.controller.method&quot;: &quot;submitOrder&quot; }, &quot;shared&quot;: true }, { &quot;traceId&quot;: &quot;9d244edbc1668d92&quot;, &quot;parentId&quot;: &quot;3f01ba499fac4ce9&quot;, &quot;id&quot;: &quot;a048eda8d5fd3dc9&quot;, &quot;kind&quot;: &quot;SERVER&quot;, &quot;name&quot;: &quot;get /get/{uid}&quot;, &quot;timestamp&quot;: 1652413758777073, &quot;duration&quot;: 14640, &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;server-user&quot;, &quot;ipv4&quot;: &quot;192.168.0.111&quot; }, &quot;remoteEndpoint&quot;: { &quot;ipv4&quot;: &quot;192.168.0.111&quot;, &quot;port&quot;: 54139 }, &quot;tags&quot;: { &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/user/get/1001&quot;, &quot;mvc.controller.class&quot;: &quot;UserController&quot;, &quot;mvc.controller.method&quot;: &quot;getUser&quot; }, &quot;shared&quot;: true } ] ] 小伙伴们也可以根据Json数据分析下系统的调用链路。 ZipKin数据持久化 我们实现了在项目中集成ZipKin，但是此时我们集成ZipKin后，ZipKin中的数据是保存在系统内存中的，如果我们重启了ZipKin，则保存在系统内存中的数据就会丢失，那我如何避免数据丢失呢？ZipKin支持将数据进行持久化来防止数据丢失，可以将数据保存到ElasticSearch、Cassandra或者MySQL中。这里，我们重点介绍下如何将数据保存到MySQL和ElasticSearch中。 ZipKin数据持久化到MySQL （1）将Zipkin数据持久化到MySQL，我们需要知道MySQL的数据表结构，好在ZipKin提供了MySQL脚本，小伙伴们可以在链接：https://github.com/openzipkin/zipkin/tree/master/zipkin-storage里面下载。 当然，我将下载后的MySQL脚本放到了网关服务shop-gateway的resources目录下的scripts目录下。 （2）在MySQL数据库中新建zipkin数据库，如下所示。 create database if not exists zipkin; （3）在新建的数据库zipkin中运行mysql.sql脚本，运行脚本后的效果如下所示。 可以看到，在zipkin数据库中新建了zipkin_annotations、zipkin_dependencies和zipkin_spans三张数据表。 （4）启动ZipKin时指定MySQL数据源，如下所示。 java -jar zipkin-server-2.12.9-exec.jar --STORAGE_TYPE=mysql --MYSQL_HOST=127.0.0.1 --MYSQL_TCP_PORT=3306 --MYSQL_DB=zipkin --MYSQL_USER=root --MYSQL_PASS=root （5）启动ZipKin后，在浏览器中访问链接http://localhost:10001/server-order/order/submit_order?userId=1001&amp;productId=1001&amp;count=1，如下所示。 （6）查看zipkin数据库中的数据，发现zipkin_annotations数据表与zipkin_spans数据表已经存在系统的调用链路数据。 zipkin_annotations数据表部分数据如下所示。 zipkin_spans数据表部分数据如下所示。 可以看到，ZipKin已经将数据持久化到MySQL中，重启ZipKin后就会从MySQL中读取数据，数据也不会丢失了。 ZipKin数据持久化到ElasticSearch （1）到ElasticSearch官网下载ElasticSearch，链接为： https://www.elastic.co/cn/downloads/elasticsearch。 这里下载的安装包是：elasticsearch-8.2.0-windows-x86_64.zip。 （2）解压elasticsearch-8.2.0-windows-x86_64.zip，在解压后的bin目录下找到elasticsearch.bat脚本，双击运行ElasticSearch。 （3）启动ZipKin服务端时，指定ElasticSearch，如下所示。 java -jar zipkin-server-2.12.9-exec.jar --STORAGE_TYPE=elasticsearch --ESHOST=localhost:9200 （4）启动ZipKin服务端后，在浏览器中访问链接http://localhost:10001/server-order/order/submit_order?userId=1001&amp;productId=1001&amp;count=1，如下所示。 ZipKin就会将请求的链路信息保存到ElasticSearch中进行持久化。 ","link":"https://tinaxiawuhao.github.io/post/OTKk-Bytc/"},{"title":"Sleuth概述","content":"Sleuth概述 Sleuth是SpringCloud中提供的一个分布式链路追踪组件，在设计上大量参考并借用了Google Dapper的设计。 Span简介 Span在Sleuth中代表一组基本的工作单元，当请求到达各个微服务时，Sleuth会通过一个唯一的标识，也就是SpanId来标记开始通过这个微服务，在当前微服务中执行的具体过程和执行结束。 此时，通过SpanId标记的开始时间戳和结束时间戳，就能够统计到当前Span的调用时间，也就是当前微服务的执行时间。另外，也可以用过Span获取到事件的名称，请求的信息等数据。 总结：远程调用和Span是一对一的关系，是分布式链路追踪中最基本的工作单元，每次发送一个远程调用服务就会产生一个 Span。Span Id 是一个 64 位的唯一 ID，通过计算 Span 的开始和结束时间，就可以统计每个服务调用所耗费的时间。 Trace简介 Trace的粒度比Span的粒度大，Trace主要是由具有一组相同的Trace ID的Span组成的，从请求进入分布式系统入口经过调用各个微服务直到返回的整个过程，都是一个Trace。 也就是说，当请求到达分布式系统的入口时，Sleuth会为请求创建一个唯一标识，这个唯一标识就是Trace Id，不管这个请求在分布式系统中如何流转，也不管这个请求在分布式系统中调用了多少个微服务，这个Trace Id都是不变的，直到整个请求返回。 总结：一个 Trace 可以对应多个 Span，Trace和Span是一对多的关系。Trace Id是一个64 位的唯一ID。Trace Id可以将进入分布式系统入口经过调用各个微服务，再到返回的整个过程的请求串联起来，内部每通过一次微服务时，都会生成一个新的SpanId。Trace串联了整个请求链路，而Span在请求链路中区分了每个微服务。 Annotation简介 Annotation记录了一段时间内的事件，内部使用的重要注解如下所示。 cs（Client Send）客户端发出请求，标记整个请求的开始时间。 sr（Server Received）服务端收到请求开始进行处理，通过sr与cs可以计算网络的延迟时间，例如：sr－ cs = 网络延迟（服务调用的时间）。 ss（Server Send）服务端处理完毕准备将结果返回给客户端， 通过ss与sr可以计算服务器上的请求处理时间，例如：ss - sr = 服务器上的请求处理时间。 cr（Client Reveived）客户端收到服务端的响应，请求结束。通过cr与cs可以计算请求的总时间，例如：cr - cs = 请求的总时间。 总结：链路追踪系统内部定义了少量核心注解，用来定义一个请求的开始和结束，通过这些注解，我们可以计算出请求的每个阶段的时间。需要注解的是，这里说的请求，是在系统内部流转的请求，而不是从浏览器、APP、H5、小程序等发出的请求。 项目整合Sleuth Sleuth提供了分布式链路追踪能力，如果需要使用Sleuth的链路追踪功能，需要在项目中集成Sleuth。 最简使用 （1）在每个微服务（用户微服务shop-user、商品微服务shop-product、订单微服务shop-order、网关服务shop-gateway）下的pom.xml文件中添加如下Sleuth的依赖。 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt; &lt;/dependency&gt; （2）将项目的application.yml文件备份成application-pre-filter.yml，并将application.yml文件的内容替换为application-sentinel.yml文件的内容，这一步是为了让整个项目集成Sentinel、SpringCloud Gateway和Nacos。application.yml替换后的文件内容如下所示。 server: port: 10002 spring: application: name: server-gateway cloud: nacos: discovery: server-addr: 127.0.0.1:8848 sentinel: transport: port: 7777 dashboard: 127.0.0.1:8888 web-context-unify: false eager: true gateway: globalcors: cors-configurations: '[/**]': allowedOrigins: &quot;*&quot; allowedMethods: &quot;*&quot; allowCredentials: true allowedHeaders: &quot;*&quot; discovery: locator: enabled: true route-id-prefix: gateway- （3）分别启动Nacos、Sentinel、用户微服务shop-user，商品微服务shop-product，订单微服务shop-order和网关服务shop-gateway，在浏览器中输入链接localhost:10001/server-order/order/submit_order?userId=1001&amp;productId=1001&amp;count=1，如下所示。 （4）分别查看用户微服务shop-user，商品微服务shop-product，订单微服务shop-order和网关服务shop-gateway的控制台输出，每个服务的控制台都输出了如下格式所示的信息。 [微服务名称,TraceID,SpanID,是否将结果输出到第三方平台] 具体如下所示。 用户微服务shop-user [server-user,03fef3d312450828,76b298dba54ec579,true] 商品微服务shop-product [server-product,03fef3d312450828,41ac8836d2df4eec,true] [server-product,03fef3d312450828,6b7b3662d63372bf,true] 订单微服务shop-order [server-order,03fef3d312450828,cbd935d57cae84f9,true] 网关服务shop-gateway [server-gateway,03fef3d312450828,03fef3d312450828,true] 可以看到，每个服务都打印出了链路追踪的日志信息，说明引入Sleuth的依赖后，就可以在命令行查看链路追踪情况。 抽样采集数据 Sleuth支持抽样采集数据。尤其是在高并发场景下，如果采集所有的数据，那么采集的数据量就太大了，非常耗费系统的性能。通常的做法是可以减少一部分数据量，特别是对于采用Http方式去发送采集数据，能够提升很大的性能。 Sleuth可以采用如下方式配置抽样采集数据。 spring: sleuth: sampler: probability: 1.0 追踪自定义线程池 Sleuth支持对异步任务的链路追踪，在项目中使用@Async注解开启一个异步任务后，Sleuth会为异步任务重新生成一个Span。但是如果使用了自定义的异步任务线程池，则会导致Sleuth无法新创建一个Span，而是会重新生成Trace和Span。此时，需要使用Sleuth提供的LazyTraceExecutor类来包装下异步任务线程池，才能在异步任务调用链路中重新创建Span。 在服务中开启异步线程池任务，需要使用@EnableAsync。所以，在演示示例前，先在用户微服务shop-user的io.binghe.shop.UserStarter启动类上添加@EnableAsync注解，如下所示。 /** * @author binghe * @version 1.0.0 * @description 启动用户服的类 */ @SpringBootApplication @EnableTransactionManagement(proxyTargetClass = true) @MapperScan(value = { &quot;io.binghe.shop.user.mapper&quot; }) @EnableDiscoveryClient @EnableAsync public class UserStarter { public static void main(String[] args){ SpringApplication.run(UserStarter.class, args); } } 演示使用@Async注解开启任务 （1）在用户微服务shop-user的io.binghe.shop.user.service.UserService接口中定义一个asyncMethod()方法，如下所示。 void asyncMethod(); （2）在用户微服务shop-user的io.binghe.shop.user.service.impl.UserServiceImpl类中实现asyncMethod()方法，并在asyncMethod()方法上添加@Async注解，如下所示。 @Async @Override public void asyncMethod() { log.info(&quot;执行了异步任务...&quot;); } （3）在用户微服务shop-user的io.binghe.shop.user.controller.UserController类中新增asyncApi()方法，如下所示。 @GetMapping(value = &quot;/async/api&quot;) public String asyncApi() { log.info(&quot;执行异步任务开始...&quot;); userService.asyncMethod(); log.info(&quot;异步任务执行结束...&quot;); return &quot;asyncApi&quot;; } （4）分别启动用户微服务和网关服务，在浏览器中输入链接http://localhost:10001/server-user/user/async/api （5）查看用户微服务与网关服务的控制台日志，分别存在如下日志。 用户微服务 [server-user,499d6c7128399ed0,a81bd920de0b07de,true]执行异步任务开始... [server-user,499d6c7128399ed0,a81bd920de0b07de,true]异步任务执行结束... [server-user,499d6c7128399ed0,e2f297d512f40bb8,true]执行了异步任务... 网关服务 [server-gateway,499d6c7128399ed0,499d6c7128399ed0,true] 可以看到Sleuth为异步任务重新生成了Span。 演示自定义任务线程池 在演示使用@Async注解开启任务的基础上继续演示自定义任务线程池，验证Sleuth是否为自定义线程池新创建了Span。 （1）在用户微服务shop-user中新建io.binghe.shop.user.config包，在包下创建ThreadPoolTaskExecutorConfig类，继承org.springframework.scheduling.annotation.AsyncConfigurerSupport类，用来自定义异步任务线程池，代码如下所示。 /** * @author binghe * @version 1.0.0 * @description Sleuth异步线程池配置 */ @Configuration @EnableAutoConfiguration public class ThreadPoolTaskExecutorConfig extends AsyncConfigurerSupport { @Override public Executor getAsyncExecutor() { ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(2); executor.setMaxPoolSize(5); executor.setQueueCapacity(10); executor.setThreadNamePrefix(&quot;trace-thread-&quot;); executor.initialize(); return executor; } } （2）以debug的形式启动用户微服务和网关服务，并在io.binghe.shop.user.config.ThreadPoolTaskExecutorConfig#getAsyncExecutor()方法中打上断点，如下所示。 可以看到，项目启动后并没有进入io.binghe.shop.user.config.ThreadPoolTaskExecutorConfig#getAsyncExecutor()方法，说明项目启动时，并不会创建异步任务线程池。 （3）在浏览器中输入链接http://localhost:10001/server-user/user/async/api，此时可以看到程序已经执行到io.binghe.shop.user.config.ThreadPoolTaskExecutorConfig#getAsyncExecutor()方法的断点位置。 说明异步任务线程池是在调用了异步任务的时候创建的。 接下来，按F8跳过断点继续运行程序，可以看到浏览器上的显示结果如下。 （4）查看用户微服务与网关服务的控制台日志，分别存在如下日志。 用户微服务 [server-user,f89f2355ec3f9df1,4d679555674e96a4,true]执行异步任务开始... [server-user,f89f2355ec3f9df1,4d679555674e96a4,true]异步任务执行结束... [server-user,0ee48d47e58e2a42,0ee48d47e58e2a42,true]执行了异步任务... 网关服务 [server-gateway,f89f2355ec3f9df1,f89f2355ec3f9df1,true] 可以看到，使用自定义异步任务线程池时，在用户微服务中在执行异步任务时，重新生成了Trace和Span。 注意对比用户微服务中输出的三条日志信息，最后一条日志信息的TraceID和SpanID与前两条日志都不同。 演示包装自定义线程池 在自定义任务线程池的基础上继续演示包装自定义线程池，验证Sleuth是否为包装后的自定义线程池新创建了Span。 （1）在用户微服务shop-user的io.binghe.shop.user.config.ThreadPoolTaskExecutorConfig类中注入BeanFactory，并在getAsyncExecutor()方法中使用org.springframework.cloud.sleuth.instrument.async.LazyTraceExecutor()来包装返回的异步任务线程池，修改后的io.binghe.shop.user.config.ThreadPoolTaskExecutorConfig类的代码如下所示。 /** * @author binghe * @version 1.0.0 * @description Sleuth异步线程池配置 */ @Configuration @EnableAutoConfiguration public class ThreadPoolTaskExecutorConfig extends AsyncConfigurerSupport { @Autowired private BeanFactory beanFactory; @Override public Executor getAsyncExecutor() { ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(2); executor.setMaxPoolSize(5); executor.setQueueCapacity(10); executor.setThreadNamePrefix(&quot;trace-thread-&quot;); executor.initialize(); return new LazyTraceExecutor(this.beanFactory, executor); } } （2）分别启动用户微服务和网关服务，在浏览器中输入链接http://localhost:10001/server-user/user/async/api （3）查看用户微服务与网关服务的控制台日志，分别存在如下日志。 用户微服务 [server-user,157891cb90fddb65,0a278842776b1f01,true]执行异步任务开始... [server-user,157891cb90fddb65,0a278842776b1f01,true]异步任务执行结束... [server-user,157891cb90fddb65,1ba55fd3432b77ae,true]执行了异步任务... 网关服务 [server-gateway,157891cb90fddb65,157891cb90fddb65,true] 可以看到Sleuth为异步任务重新生成了Span。 综上说明：Sleuth支持对异步任务的链路追踪，在项目中使用@Async注解开启一个异步任务后，Sleuth会为异步任务重新生成一个Span。但是如果使用了自定义的异步任务线程池，则会导致Sleuth无法新创建一个Span，而是会重新生成Trace和Span。此时，需要使用Sleuth提供的LazyTraceExecutor类来包装下异步任务线程池，才能在异步任务调用链路中重新创建Span。 自定义链路过滤器 在Sleuth中存在链路过滤器，并且还支持自定义链路过滤器。 自定义链路过滤器概述 TracingFilter是Sleuth中负责处理请求和响应的组件，可以通过注册自定义的TracingFilter实例来实现一些扩展性的需求。 演示自定义链路过滤器 本案例演示通过过滤器验证只有HTTP或者HTTPS请求才能访问接口，并且在访问的链接不是静态文件时，将traceId放入HttpRequest中在服务端获取，并在响应结果中添加自定义Header，名称为SLEUTH-HEADER，值为traceId。 （1）在用户微服务shop-user中新建io.binghe.shop.user.filter包，并创建MyGenericFilter类，继承org.springframework.web.filter.GenericFilterBean类，代码如下所示。 /** * @author binghe * @version 1.0.0 * @description 链路过滤器 */ @Component @Order( Ordered.HIGHEST_PRECEDENCE + 6) public class MyGenericFilter extends GenericFilterBean{ private Pattern skipPattern = Pattern.compile(SleuthWebProperties.DEFAULT_SKIP_PATTERN); private final Tracer tracer; public MyGenericFilter(Tracer tracer){ this.tracer = tracer; } @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { if (!(request instanceof HttpServletRequest) || !(response instanceof HttpServletResponse)){ throw new ServletException(&quot;只支持HTTP访问&quot;); } Span currentSpan = this.tracer.currentSpan(); if (currentSpan == null) { chain.doFilter(request, response); return; } HttpServletRequest httpServletRequest = (HttpServletRequest) request; HttpServletResponse httpServletResponse = ((HttpServletResponse) response); boolean skipFlag = skipPattern.matcher(httpServletRequest.getRequestURI()).matches(); if (!skipFlag){ String traceId = currentSpan.context().traceIdString(); httpServletRequest.setAttribute(&quot;traceId&quot;, traceId); httpServletResponse.addHeader(&quot;SLEUTH-HEADER&quot;, traceId); } chain.doFilter(httpServletRequest, httpServletResponse); } } （2）在用户微服务shop-user的io.binghe.shop.user.controller.UserController类中新建sleuthFilter()方法，在sleuthFilter()方法中获取并打印traceId，如下所示。 @GetMapping(value = &quot;/sleuth/filter/api&quot;) public String sleuthFilter(HttpServletRequest request) { Object traceIdObj = request.getAttribute(&quot;traceId&quot;); String traceId = traceIdObj == null ? &quot;&quot; : traceIdObj.toString(); log.info(&quot;获取到的traceId为: &quot; + traceId); return &quot;sleuthFilter&quot;; } （3）分别启动用户微服务和网关服务，在浏览器中输入http://localhost:10001/server-user/user/sleuth/filter/api，如下所示。 查看用户微服务的控制台会输出如下信息。 获取到的traceId为: f63ae7702f6f4bba 查看浏览器的控制台，看到在响应的结果信息中新增了一个名称为SLEUTH-HEADER，值为f63ae7702f6f4bba的Header，如下所示。 说明使用Sleuth的过滤器可以处理请求和响应信息，并且可以在Sleuth的过滤器中获取到TraceID。 ","link":"https://tinaxiawuhao.github.io/post/ZbHW6UCYN/"},{"title":"分布式链路追踪","content":"随着互联网的不断发展，企业的业务系统变得越来越复杂，原本单一的单体应用系统已经无法满足企业业务发展的需要。于是，很多企业开始了对项目的分布式与微服务改造，新项目也在开始的时候就会采用分布式与微服务的架构模式。 一个系统采用分布式与微服务架构后，会被拆分成许多服务模块，这些服务模块之间的调用关系错综复杂，对于客户端请求的分析与处理就会显得异常复杂。此时，就需要一种技术来解决这些问题，而这种技术就是分布式链路追踪技术。 分布式链路追踪 随着互联网业务快速扩展，企业的业务系统变得越来越复杂，不少企业开始向分布式、微服务方向发展，将原本的单体应用拆分成分布式、微服务。这也使得当客户端请求系统的接口时，原本在同一个系统内部的请求逻辑变成了需要在多个微服务之间流转的请求。 单体架构中可以使用AOP在调用具体的业务逻辑前后分别打印一下时间即可计算出整体的调用时间，使用 AOP捕获异常也可知道是哪里的调用导致的异常。 但是在分布式微服务场景下，使用AOP技术是无法追踪到各个微服务的调用情况的，也就无法知道系统中处理一次请求的整体调用链路。 另外，在分布式与微服务场景下，我们需要解决如下问题： 如何快速发现并定位到分布式系统中的问题。 如何尽可能精确的判断故障对系统的影响范围与影响程度。 如何尽可能精确的梳理出服务之间的依赖关系，并判断出服务之间的依赖关系是否合理。 如何尽可能精确的分析整个系统调用链路的性能与瓶颈点。 如何尽可能精确的分析系统的存储瓶颈与容量规划。 如何实时观测系统的整体调用链路情况。 上述问题就是分布式链路追踪技术要解决的问题。所谓的分布式链路追踪，就是将对分布式系统的一次请求转化成一个完整的调用链路。这个完整的调用链路从请求进入分布式系统的入口开始，直到整个请求返回为止。并在请求调用微服务的过程中，记录相应的调用日志，监控系统调用的性能，并且可以按照某种方式显示请求调用的情况。 在分布式链路追踪中，可以统计调用每个微服务的耗时，请求会经过哪些微服务的流转，每个微服务的运行状况等信息。 核心原理 假定三个微服务调用的链路如下图所示：Service 1 调用 Service 2，Service 2 调用 Service 3 和 Service 4。 那么链路追踪会在每个服务调用的时候加上 Trace ID 和 Span ID。如下图所示： 小伙伴注意上面的颜色，相同颜色的代表是同一个 Span ID，说明是链路追踪中的一个节点。 第一步：用户端调用 Service 1，生成一个 Request，Trace ID 和 Span ID 为空，那个时候请求还没有到 Service 1。 第二步：请求到达 Service 1，记录了 Trace ID = X，Span ID 等于 A。 第三步：Service 1 发送请求给 Service 2，Span ID 等于 B，被称作 Client Sent，即用户端发送一个请求。 第四步：请求到达 Service 2，Span ID 等于 B，Trace ID 不会改变，被称作 Server Received，即服务端取得请求并准备开始解决它。 第五步：Service 2 开始解决这个请求，解决完之后，Trace ID 不变，Span ID = C。 第六步：Service 2 开始发送这个请求给 Service 3，Trace ID 不变，Span ID = D，被称作 Client Sent，即用户端发送一个请求。 第七步：Service 3 接收到这个请求，Span ID = D，被称作 Server Received。 第八步：Service 3 开始解决这个请求，解决完之后，Span ID = E。 第九步：Service 3 开始发送响应给 Service 2，Span ID = D，被称作 Server Sent，即服务端发送响应。 第十步：Service 3 收到 Service 2 的响应，Span ID = D，被称作 Client Received，即用户端接收响应。 第十一步：Service 2 开始返回 响应给 Service 1，Span ID = B，和第三步的 Span ID 相同，被称作 Client Received，即用户端接收响应。 第十二步：Service 1 解决完响应，Span ID = A，和第二步的 Span ID 相同。 第十三步：Service 1 开始向用户端返回响应，Span ID = A、 Service 3 向 Service 4 发送请求和 Service 3 相似，对应的 Span ID 是 F 和 G。可以参照上面前面的第六步到第十步。 把以上的相同颜色的步骤简化为下面的链路追踪图： 第一个节点：Span ID = A，Parent ID = null，Service 1 接收到请求。 第二个节点：Span ID = B，Parent ID= A，Service 1 发送请求到 Service 2 返回响应给Service 1 的过程。 第三个节点：Span ID = C，Parent ID= B，Service 2 的 中间解决过程。 第四个节点：Span ID = D，Parent ID= C，Service 2 发送请求到 Service 3 返回响应给Service 2 的过程。 第五个节点：Span ID = E，Parent ID= D，Service 3 的中间解决过程。 第六个节点：Span ID = F，Parent ID= C，Service 3 发送请求到 Service 4 返回响应给 Service 3 的过程。 第七个节点：Span ID = G，Parent ID= F，Service 4 的中间解决过程。 通过 Parent ID 就可找到父节点，整个链路即可以进行跟踪追溯了。 备注：核心原理部分内容来源：cnblogs.com/jackson0714/p/sleuth_zipkin.html 解决方案 目前，行业内比较成熟的分布式链路追踪技术解决方案如下所示。 技术 说明 Cat 由大众点评开源，基于Java开发的实时应用监控平台，包括实时应用监控，业务监控 。集成方案是通过代码埋点的方式来实现监控，比如：拦截器，过滤器等。对代码的侵入性很大，集成成本较高。风险较大。 ZipKin 由Twitter公司开源，开放源代码分布式的跟踪系统，用于收集服务的定时数据，以解决微服务架构中的延迟问题，包括：数据的收集、存储、查找和展现。结合spring-cloud-sleuth使用较为简单， 集成方便， 但是功能较简单。 Pinpoint Pinpoint是一款开源的基于字节码注入的调用链分析，以及应用监控分析工具。特点是支持多种插件， UI功能强大，接入端无代码侵入。 Skywalking SkyWalking是国人开源的基于字节码注入的调用链分析，以及应用监控分析工具。特点是支持多种插件， UI功能较强，接入端无代码侵入。 Sleuth Sleuth是SpringCloud中的一个组件，为Spring Cloud实现了分布式跟踪解决方案。 注意：我们后续会使用 Sleuth+ZipKin的方案实现分布式链路追踪。 ","link":"https://tinaxiawuhao.github.io/post/Lq63Gr95h/"},{"title":"网关断言","content":"网关断言 断言的英文是Predicate，也可以翻译成谓词。主要的作用就是进行条件判断，可以在网关中实现多种条件判断，只有所有的判断结果都通过时，也就是所有的条件判断都返回true，才会真正的执行路由功能。 SpringCloud Gateway内置断言 SpringCloud Gateway包括许多内置的断言工厂，所有这些断言都与HTTP请求的不同属性匹配。 基于日期时间类型的断言 基于日期时间类型的断言根据时间做判断，主要有三个： AfterRoutePredicateFactory：接收一个日期时间参数，判断当前请求的日期时间是否晚于指定的日期时间。 BeforeRoutePredicateFactory：接收一个日期时间参数，判断当前请求的日期时间是否早于指定的日期时间。 BetweenRoutePredicateFactory：接收两个日期时间参数，判断当前请求的日期时间是否在指定的时间时间段内。 使用示例 - After=2022-05-10T23:59:59.256+08:00[Asia/Shanghai] 基于远程地址的断言 RemoteAddrRoutePredicateFactory：接收一个IP地址段，判断发出请求的客户端的IP地址是否在指定的IP地址段内。 使用示例 - RemoteAddr=192.168.0.1/24 基于Cookie的断言 CookieRoutePredicateFactory：接收两个参数， Cookie的名称和一个正则表达式。判断请求的Cookie是否具有给定名称且值与正则表达式匹配。 使用示例 - Cookie=name, binghe. 基于Header的断言 HeaderRoutePredicateFactory：接收两个参数，请求Header的名称和正则表达式。判断请求Header中是否具有给定的名称且值与正则表达式匹配。 使用示例 - Header=X-Request-Id, \\d+ 基于Host的断言 HostRoutePredicateFactory：接收一个参数，这个参数通常是主机名或者域名的模式，例如**.binghe.com这种格式。判断发出请求的主机是否满足匹配规则。 使用示例 - Host=**.binghe.com 基于Method请求方法的断言 MethodRoutePredicateFactory：接收一个参数，判断请求的类型是否跟指定的类型匹配，通常指的是请求方式。例如，POST、GET、PUT等请求方式。 使用示例 - Method=GET 基于Path请求路径的断言 PathRoutePredicateFactory：接收一个参数，判断请求的链接地址是否满足路径规则，通常指的是请求的URI部分。 使用示例 - Path=/binghe/{segment} 基于Query请求参数的断言 QueryRoutePredicateFactory ：接收两个参数，请求参数和正则表达式， 判断请求的参数是否具有给定的名称并且参数值是否与正则表达式匹配。 使用示例 - Query=name, binghe. 基于路由权重的断言 WeightRoutePredicateFactory：接收一个[组名,权重]格式的数组，然后对于同一个组内的路由按照权重转发。 使用示例 - id: weight1 uri: http://localhost:8080 predicates: - Path=/api/** - Weight=group1,2 filters: - StripPrefix=1 - id: weight2 uri: http://localhost:8081 predicates: - Path=/api/** - Weight=group1,8 filters: - StripPrefix=1 演示内置断言 在演示的示例中，我们基于Path请求路径的断言判断请求路径是否符合规则，基于远程地址的断言判断请求主机地址是否在地址段中，并且限制请求的方式为GET方式。整个演示的过程以访问用户微服务的接口为例。 （1）由于在开发项目时，所有的服务都是在我本地启动的，首先查看下我本机的IP地址，如下所示。 可以看到，我本机的IP地址为192.168.0.27，属于192.168.0.1/24网段。 （2）在服务网关模块shop-gateway中，将application.yml文件备份成application-sentinel.yml文件，并将application.yml文件中的内容修改成application-simple.yml文件中的内容。接下来，在application.yml文件中的spring.cloud.gateway.routes节点下的- id: user-gateway下面进行断言配置，配置后的结果如下所示。 spring: cloud: gateway: routes: - id: user-gateway uri: http://localhost:8060 order: 1 predicates: - Path=/server-user/** - RemoteAddr=192.168.0.1/24 - Method=GET filters: - StripPrefix=1 注意：完整的配置参见案例完整源代码。 （3）配置完成后启动用户微服务和网关服务，通过网关服务访问用户微服务，在浏览器中输入http://localhost:10001/server-user/user/get/1001，如下所示。 可以看到通过http://localhost:10001/server-user/user/get/1001链接不能正确访问到用户信息。 接下来，在浏览器中输入http://192.168.0.27:10001/server-user/user/get/1001，能够正确获取到用户的信息。 （4）停止网关微服务，将基于远程地址的断言配置成- RemoteAddr=192.168.1.1/24，也就是将基于远程地址的断言配置成与我本机IP地址不在同一个网段，这样就能演示请求主机地址不在地址段中的情况，修改后的基于远程地址的断言配置如下所示。 - RemoteAddr=192.168.1.1/24 （5）重启网关服务，再次在浏览器中输入http://localhost:10001/server-user/user/get/1001，如下所示。 可以看到通过http://localhost:10001/server-user/user/get/1001链接不能正确访问到用户信息。 接下来，在浏览器中输入http://192.168.0.27:10001/server-user/user/get/1001，也不能正确获取到用户的信息了。 自定义断言 SpringCloud Gateway支持自定义断言功能，我们可以在具体业务中，基于SpringCloud Gateway自定义特定的断言功能。 自定义断言概述 SpringCloud Gateway虽然提供了多种内置的断言功能，但是在某些场景下无法满足业务的需要，此时，我们就可以基于SpringCloud Gateway自定义断言功能，以此来满足我们的业务场景。 实现自定义断言 这里，我们基于SpringCloud Gateway实现断言功能，实现后的效果是在服务网关的application.yml文件中的spring.cloud.gateway.routes节点下的- id: user-gateway下面进行如下配置。 spring: cloud: gateway: routes: - id: user-gateway uri: http://localhost:8060 order: 1 predicates: - Path=/server-user/** - Name=binghe filters: - StripPrefix=1 通过服务网关访问用户微服务时，只有在访问的链接后面添加?name=binghe参数时才能正确访问用户微服务。 （1）在网关服务shop-gateway中新建io.binghe.shop.predicate包，在包下新建NameRoutePredicateConfig类，主要定义一个Spring类型的name成员变量，用来接收配置文件中的参数，源码如下所示。 /** * @author binghe * @version 1.0.0 * @description 接收配置文件中的参数 */ @Data public class NameRoutePredicateConfig implements Serializable { private static final long serialVersionUID = -3289515863427972825L; private String name; } （2）实现自定义断言时，需要新建类继承org.springframework.cloud.gateway.handler.predicate.AbstractRoutePredicateFactory类，在io.binghe.shop.predicate包下新建NameRoutePredicateFactory类，继承org.springframework.cloud.gateway.handler.predicate.AbstractRoutePredicateFactory类，并覆写相关的方法，源码如下所示。 /** * @author binghe * @version 1.0.0 * @description 自定义断言功能 */ @Component public class NameRoutePredicateFactory extends AbstractRoutePredicateFactory&lt;NameRoutePredicateConfig&gt; { public NameRoutePredicateFactory() { super(NameRoutePredicateConfig.class); } @Override public Predicate&lt;ServerWebExchange&gt; apply(NameRoutePredicateConfig config) { return (serverWebExchange)-&gt;{ String name = serverWebExchange.getRequest().getQueryParams().getFirst(&quot;name&quot;); if (StringUtils.isEmpty(name)){ name = &quot;&quot;; } return name.equals(config.getName()); }; } @Override public List&lt;String&gt; shortcutFieldOrder() { return Arrays.asList(&quot;name&quot;); } } （3）在服务网关的application.yml文件中的spring.cloud.gateway.routes节点下的- id: user-gateway下面进行如下配置。 spring: cloud: gateway: routes: - id: user-gateway uri: http://localhost:8060 order: 1 predicates: - Path=/server-user/** - Name=binghe filters: - StripPrefix=1 （4）分别启动用户微服务与网关服务，在浏览器中输入http://localhost:10001/server-user/user/get/1001，如下所示。 可以看到，在浏览器中输入http://localhost:10001/server-user/user/get/1001，无法获取到用户信息。 （5）在浏览器中输入http://localhost:10001/server-user/user/get/1001?name=binghe，如下所示。 可以看到，在访问链接后添加?name=binghe参数后，能够正确获取到用户信息。 至此，我们实现了自定义断言功能。 网关过滤器 过滤器可以在请求过程中，修改请求的参数和响应的结果等信息。在生命周期的角度总体上可以分为前置过滤器（Pre）和后置过滤器(Post)。在实现的过滤范围角度可以分为局部过滤器（GatewayFilter）和全局过滤器（GlobalFilter）。局部过滤器作用的范围是某一个路由，全局过滤器作用的范围是全部路由。 Pre前置过滤器：在请求被网关路由之前调用，可以利用这种过滤器实现认证、鉴权、路由等功能，也可以记录访问时间等信息。 Post后置过滤器：在请求被网关路由到微服务之后执行。可以利用这种过滤器修改HTTP的响应Header信息，修改返回的结果数据（例如对于一些敏感的数据，可以在此过滤器中统一处理后返回），收集一些统计信息等。 局部过滤器（GatewayFilter）：也可以称为网关过滤器，这种过滤器主要是作用于单一路由或者某个路由分组。 全局过滤器（GlobalFilter）：这种过滤器主要作用于所有的路由。 局部过滤器 局部过滤器又称为网关过滤器，这种过滤器主要是作用于单一路由或者某个路由分组。 局部过滤器概述 在SpringCloud Gateway中内置了很多不同类型的局部过滤器，主要如下所示。 演示内部过滤器 演示内部过滤器时，我们为原始请求添加一个名称为IP的Header，值为localhost，并添加一个名称为name的参数，参数值为binghe。同时修改响应的结果状态，将结果状态修改为1001。 （1）在服务网关的application.yml文件中的spring.cloud.gateway.routes节点下的- id: user-gateway下面进行如下配置。 spring: cloud: gateway: routes: - id: user-gateway uri: http://localhost:8060 order: 1 predicates: - Path=/server-user/** filters: - StripPrefix=1 - AddRequestHeader=IP,localhost - AddRequestParameter=name,binghe - SetStatus=1001 （2）在用户微服务的io.binghe.shop.user.controller.UserController类中新增apiFilter1()方法，如下所示。 @GetMapping(value = &quot;/api/filter1&quot;) public String apiFilter1(HttpServletRequest request, HttpServletResponse response){ log.info(&quot;访问了apiFilter1接口&quot;); String ip = request.getHeader(&quot;IP&quot;); String name = request.getParameter(&quot;name&quot;); log.info(&quot;ip = &quot; + ip + &quot;, name = &quot; + name); return &quot;apiFilter1&quot;; } 可以看到，在新增加的apiFilter1()方法中，获取到新增加的Header与参数，并将获取出来的参数与Header打印出来。并且方法返回的是字符串apiFilter1。 （3）分别启动用户微服务与网关服务，在浏览器中输入http://localhost:10001/server-user/user/api/filter1，如下所示。 此时，查看浏览器中的响应状态码，如下所示。 可以看到，此时的状态码已经被修改为1001。 接下来，查看下用户微服务的控制台输出的信息，发现在输出的信息中存在如下数据。 访问了apiFilter1接口 ip = localhost, name = binghe 说明使用SpringCloud Gateway的内置过滤器成功为原始请求添加了一个名称为IP的Header，值为localhost，并添加了一个名称为name的参数，参数值为binghe。同时修改了响应的结果状态，将结果状态修改为1001，符合预期效果。 自定义局部过滤器 这里，我们基于SpringCloud Gateway自定义局部过滤器实现是否开启灰度发布的功能，整个实现过程如下所示。 （1）在服务网关的application.yml文件中的spring.cloud.gateway.routes节点下的- id: user-gateway下面进行如下配置。 spring: cloud: gateway: routes: - id: user-gateway uri: http://localhost:8060 order: 1 predicates: - Path=/server-user/** filters: - StripPrefix=1 - Grayscale=true （2）在网关服务模块shop-gateway中新建io.binghe.shop.filter包，在包下新建GrayscaleGatewayFilterConfig类，用于接收配置中的参数，如下所示。 /** * @author binghe * @version 1.0.0 * @description 接收配置参数 */ @Data public class GrayscaleGatewayFilterConfig implements Serializable { private static final long serialVersionUID = 983019309000445082L; private boolean grayscale; } （3）在io.binghe.shop.filter包下GrayscaleGatewayFilterFactory类，继承org.springframework.cloud.gateway.filter.factory.AbstractGatewayFilterFactory类，主要是实现自定义过滤器，模拟实现灰度发布。代码如下所示。 /** * @author binghe * @version 1.0.0 * @description 自定义过滤器模拟实现灰度发布 */ @Component public class GrayscaleGatewayFilterFactory extends AbstractGatewayFilterFactory&lt;GrayscaleGatewayFilterConfig&gt; { public GrayscaleGatewayFilterFactory(){ super(GrayscaleGatewayFilterConfig.class); } @Override public GatewayFilter apply(GrayscaleGatewayFilterConfig config) { return (exchange, chain) -&gt; { if (config.isGrayscale()){ System.out.println(&quot;开启了灰度发布功能...&quot;); }else{ System.out.println(&quot;关闭了灰度发布功能...&quot;); } return chain.filter(exchange); }; } @Override public List&lt;String&gt; shortcutFieldOrder() { return Arrays.asList(&quot;grayscale&quot;); } } （4）分别启动用户微服务和服务网关，在浏览器中输入http://localhost:10001/server-user/user/get/1001，如下所示。 可以看到，通过服务网关正确访问到了用户微服务，并正确获取到了用户信息。 接下来，查看下服务网关的终端，发现已经成功输出了如下信息。 开启了灰度发布功能... 说明正确实现了自定义的局部过滤器。 全局过滤器 全局过滤器是一系列特殊的过滤器，会根据条件应用到所有路由中。 全局过滤器概述 在SpringCloud Gateway中内置了多种不同的全局过滤器，如下所示。 演示全局过滤器 （1）在服务网关模块shop-gateway模块下的io.binghe.shop.config包下新建GatewayFilterConfig类，并在类中配置几个全局过滤器，如下所示。 /** * @author binghe * @version 1.0.0 * @description 网关过滤器配置 */ @Configuration @Slf4j public class GatewayFilterConfig { @Bean @Order(-1) public GlobalFilter globalFilter() { return (exchange, chain) -&gt; { log.info(&quot;执行前置过滤器逻辑&quot;); return chain.filter(exchange).then(Mono.fromRunnable(() -&gt; { log.info(&quot;执行后置过滤器逻辑&quot;); })); }; } } 注意：@Order注解中的数字越小，执行的优先级越高。 （2）启动用户微服务与服务网关，在浏览器中访问http://localhost:10001/server-user/user/get/1001，如下所示。 在服务网关终端输出如下信息。 执行前置过滤器逻辑 执行后置过滤器逻辑 说明我们演示的全局过滤器生效了。 自定义全局过滤器 SpringCloud Gateway内置了很多全局过滤器，一般情况下能够满足实际开发需要，但是对于某些特殊的业务场景，还是需要我们自己实现自定义全局过滤器。 这里，我们就模拟实现一个获取客户端访问信息，并统计访问接口时长的全局过滤器。 （1）在网关服务模块shop-order的io.binghe.shop.filter包下，新建GlobalGatewayLogFilter类，实现org.springframework.cloud.gateway.filter.GlobalFilter接口和org.springframework.core.Ordered接口，代码如下所示。 /** * @author binghe * @version 1.0.0 * @description 自定义全局过滤器，模拟实现获取客户端信息并统计接口访问时长 */ @Slf4j @Component public class GlobalGatewayLogFilter implements GlobalFilter, Ordered { /** * 开始访问时间 */ private static final String BEGIN_VISIT_TIME = &quot;begin_visit_time&quot;; @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) { //先记录下访问接口的开始时间 exchange.getAttributes().put(BEGIN_VISIT_TIME, System.currentTimeMillis()); return chain.filter(exchange).then(Mono.fromRunnable(()-&gt;{ Long beginVisitTime = exchange.getAttribute(BEGIN_VISIT_TIME); if (beginVisitTime != null){ log.info(&quot;访问接口主机: &quot; + exchange.getRequest().getURI().getHost()); log.info(&quot;访问接口端口: &quot; + exchange.getRequest().getURI().getPort()); log.info(&quot;访问接口URL: &quot; + exchange.getRequest().getURI().getPath()); log.info(&quot;访问接口URL参数: &quot; + exchange.getRequest().getURI().getRawQuery()); log.info(&quot;访问接口时长: &quot; + (System.currentTimeMillis() - beginVisitTime) + &quot;ms&quot;); } })); } @Override public int getOrder() { return 0; } } 上述代码的实现逻辑还是比较简单的，这里就不再赘述了。 （2）启动用户微服务与网关服务，在浏览器中输入http://localhost:10001/server-user/user/api/filter1?name=binghe，如下所示。 接下来，查看服务网关的终端日志，可以发现已经输出了如下信息。 访问接口主机: localhost 访问接口端口: 10001 访问接口URL: /server-user/user/api/filter1 访问接口URL参数: name=binghe 访问接口时长: 126ms 说明我们自定义的全局过滤器生效了。 ","link":"https://tinaxiawuhao.github.io/post/ipsD-XbrM/"},{"title":"gateway项目整合网关","content":"项目整合网关 我们需要在项目中增加一个服务网关模块shop-gateway，在服务网关模块中实现网关的能力。此时，我们的项目中就会有用户微服务、商品微服务、订单微服务和服务网关。 新建网关模块 在项目中新建shop-gateway模块，新增网关模块后项目的结构如下图所示。 初步整合SpringCloud Gateway （1）在服务网关shop-gateway模块的pom.xml文件中添加如下依赖。 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; （2）在服务网关shop-gateway模块的resources目录下新建application.yml文件，并在文件中添加如下配置信息。 server: port: 10001 spring: application: name: server-gateway cloud: gateway: globalcors: cors-configurations: '[/**]': allowedOrigins: &quot;*&quot; allowedMethods: &quot;*&quot; allowCredentials: true allowedHeaders: &quot;*&quot; routes: - id: user-gateway uri: http://localhost:8060 order: 1 predicates: - Path=/server-user/** filters: - StripPrefix=1 - id: product-gateway uri: http://localhost:8070 order: 1 predicates: - Path=/server-product/** filters: - StripPrefix=1 - id: order-gateway uri: http://localhost:8080 order: 1 predicates: - Path=/server-order/** filters: - StripPrefix=1 我们重点来看下 spring.cloud.gateway 节点下的配置。 globalcors：此节点下的配置是为了解决SpringCloud Gateway跨域的问题。 routes：表示一个路由数组，可以在此节点下配置多个路由信息。 id：当前路由的唯一标识。 order：路由的优先级，数字越小表示优先级越高。 predicates：网关断言，也就是路由转发的条件，也是一个数组，可以配置多个路由转发条件。 Path：当客户端请求的路径满足Path的规则时，进行路由转发操作。 filters：网关过滤器，在过滤器中可以修改请求的参数和header信息，以及响应的结果和header信息，网关过滤器也是一个数组，可以配置多个过滤规则。 StripPrefix：网关在进行路由转发之前，会去掉1层访问路径。 （3）在服务网关shop-gateway模块的io.binghe.shop包下新建GatewayStarter类，表示服务网关的启动类，源码如下所示。 /** * @version 1.0.0 * @description 服务网关启动类 */ @SpringBootApplication public class GatewayStarter { public static void main(String[] args){ SpringApplication.run(GatewayStarter.class, args); } } （4）由于之前项目中整合了Nacos和Sentinel，所以，在启动项目前，要分别启动Nacos和Sentinel。 进入到Nacos的bin目录下，输入如下命令启动Nacos。 startup.cmd -m standalone 进入Sentinel Jar包所在的目录，输入如下命令启动Sentinel。 java -Dserver.port=8888 -Dcsp.sentinel.dashboard.server=localhost:8888 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard-1.8.4.jar （5）分别启动用户微服务、商品微服务、订单微服务和服务网关。 （6）通过服务网关访问用户微服务，在浏览器中输入http://localhost:10001/server-user/user/get/1001，如下所示。 用户微服务返回的原始数据如下所示。 { &quot;id&quot;: 1001, &quot;username&quot;: &quot;binghe&quot;, &quot;password&quot;: &quot;c26be8aaf53b15054896983b43eb6a65&quot;, &quot;phone&quot;: &quot;13212345678&quot;, &quot;address&quot;: &quot;北京&quot; } 可以看到，通过服务网关能够正确访问到用户微服务。 （7）通过服务网关访问商品微服务，在浏览器中输入http://localhost:10001/server-product/product/get/1001，如下所示。 商品微服务返回的原始数据如下所示。 { &quot;id&quot;: 1001, &quot;proName&quot;: &quot;华为&quot;, &quot;proPrice&quot;: 2399, &quot;proStock&quot;: 100 } 可以看到，通过服务网关能够正确访问到商品微服务。 （8）通过服务网关访问订单微服务，在浏览器中输入http://localhost:10001/server-order/order/test_sentinel，如下所示。 可以看到，通过服务网关能够正确访问到订单微服务。 网关整合Nacos 在初步整合SpringCloud Gateway中，我们在服务网关模块的application.yml文件中硬编码配置了服务转发的地址，如下所示。 硬编码用户微服务地址 uri: http://localhost:8060 硬编码商品微服务地址 uri: http://localhost:8070 硬编码订单微服务地址 uri: http://localhost:8080 这里，我们将网关整合Nacos实现从Nacos注册中心获取转发的服务地址。 （1）在服务网关shop-gateway模块的pom.xml文件中继续添加如下依赖。 &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; （2）在服务网关shop-gateway模块的启动类io.binghe.shop.GatewayStarter上添加@EnableDiscoveryClient注解，如下所示。 /** * @version 1.0.0 * @description 服务网关启动类 */ @SpringBootApplication @EnableDiscoveryClient public class GatewayStarter { public static void main(String[] args){ SpringApplication.run(GatewayStarter.class, args); } } （3）将application.yml备份一份，命名为application-simple.yml，并修改application.yml配置文件，修改后的文件如下所示。 server: port: 10001 spring: application: name: server-gateway cloud: nacos: discovery: server-addr: 127.0.0.1:8848 gateway: globalcors: cors-configurations: '[/**]': allowedOrigins: &quot;*&quot; allowedMethods: &quot;*&quot; allowCredentials: true allowedHeaders: &quot;*&quot; discovery: locator: enabled: true routes: - id: user-gateway uri: lb://server-user order: 1 predicates: - Path=/server-user/** filters: - StripPrefix=1 - id: product-gateway uri: lb://server-product order: 1 predicates: - Path=/server-product/** filters: - StripPrefix=1 - id: order-gateway uri: lb://server-order order: 1 predicates: - Path=/server-order/** filters: - StripPrefix=1 上述配置中增加了Nacos相关的配置，如下所示。 spring: cloud: nacos: discovery: server-addr: 127.0.0.1:8848 新增了让SpringCloud Gateway可以发现Nacos中的服务配置，如下所示。 Spring: cloud: gateway: discovery: locator: enabled: true 另外，将硬编码的服务转发地址修改成从Nacos中按照名称获取微服务地址，并按照负载均衡策略分发。 从Nacos中获取用户微服务 uri: lb://server-user 从Nacos中获取商品微服务 uri: lb://server-product 从Nacos中获取订单微服务 uri: lb://server-order 其中，lb指的是从Nacos中按照微服务的名称获取微服务地址，并按照负载均衡的策略分发。使用lb从Nacos中获取微服务时，遵循如下的格式。 lb://微服务名称 微服务的名称就是各个微服务在application.yml文件中配置的服务名称。 spring: application: name: 服务名称 （4）分别启动用户微服务、商品微服务、订单微服务和服务网关。 （5）通过服务网关访问用户微服务，在浏览器中输入http://localhost:10001/server-user/user/get/1001，如下所示。 用户微服务返回的原始数据如下所示。 { &quot;id&quot;: 1001, &quot;username&quot;: &quot;binghe&quot;, &quot;password&quot;: &quot;c26be8aaf53b15054896983b43eb6a65&quot;, &quot;phone&quot;: &quot;13212345678&quot;, &quot;address&quot;: &quot;北京&quot; } 可以看到，通过服务网关能够正确访问到用户微服务。 （6）通过服务网关访问商品微服务，在浏览器中输入http://localhost:10001/server-product/product/get/1001，如下所示。 商品微服务返回的原始数据如下所示。 { &quot;id&quot;: 1001, &quot;proName&quot;: &quot;华为&quot;, &quot;proPrice&quot;: 2399, &quot;proStock&quot;: 100 } 可以看到，通过服务网关能够正确访问到商品微服务。 （7）通过服务网关访问订单微服务，在浏览器中输入http://localhost:10001/server-order/order/test_sentinel，如下所示。 可以看到，通过服务网关能够正确访问到订单微服务。 网关整合Nacos最简配置 SpringCloud Gateway整合Nacos后，可以不用手动指定其他微服务的名称来从Nacos中获取微服务的地址。接下来，我们就来实现SpringCloud Gateway网关整合Nacos的最简配置。 （1）将application.yml备份一份，命名为application-nacos.yml，并修改application.yml配置文件，修改后的文件如下所示。 server: port: 10001 spring: application: name: server-gateway cloud: nacos: discovery: server-addr: 127.0.0.1:8848 gateway: globalcors: cors-configurations: '[/**]': allowedOrigins: &quot;*&quot; allowedMethods: &quot;*&quot; allowCredentials: true allowedHeaders: &quot;*&quot; discovery: locator: enabled: true 可以看到，在application.yml文件中，去掉了spring.cloud.gateway.routes 节点及其下面的所有配置。 （2）分别启动用户微服务、商品微服务、订单微服务和服务网关。 （3）通过服务网关访问用户微服务，在浏览器中输入http://localhost:10001/server-user/user/get/1001，如下所示。 用户微服务返回的原始数据如下所示。 { &quot;id&quot;: 1001, &quot;username&quot;: &quot;binghe&quot;, &quot;password&quot;: &quot;c26be8aaf53b15054896983b43eb6a65&quot;, &quot;phone&quot;: &quot;13212345678&quot;, &quot;address&quot;: &quot;北京&quot; } 可以看到，通过服务网关能够正确访问到用户微服务。 （4）通过服务网关访问商品微服务，在浏览器中输入http://localhost:10001/server-product/product/get/1001，如下所示。 商品微服务返回的原始数据如下所示。 { &quot;id&quot;: 1001, &quot;proName&quot;: &quot;华为&quot;, &quot;proPrice&quot;: 2399, &quot;proStock&quot;: 100 } 可以看到，通过服务网关能够正确访问到商品微服务。 （5）通过服务网关访问订单微服务，在浏览器中输入http://localhost:10001/server-order/order/test_sentinel，如下所示。 可以看到，通过服务网关能够正确访问到订单微服务。 注意：SpringCloud Gateway整合Nacos最简配置时，通过网关访问微服务的格式如下所示。 http(s)://网关IP:网关端口/访问的目标微服务名称/接口地址 网关整合Sentinel限流 Sentinel从1.6.0版本开始，提供了SpringCloud Gateway的适配模块，并且可以提供两种资源维度的限流，一种是route维度；另一种是自定义API分组维度。 route维度：对application.yml文件中配置的spring.cloud.gateway.routes.id限流，并且资源名为spring.cloud.gateway.routes.id对应的值。 自定义API分组维度：利用Sentinel提供的API接口来自定义API分组，并且对这些API分组进行限流。 实现route维度限流 （1）在服务网关shop-gateway模块的pom.xml文件中添加如下依赖。 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-sentinel-gateway&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-spring-cloud-gateway-adapter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; （2）在服务网关shop-gateway模块中新建io.binghe.shop.config包，并在包下新建GatewayConfig类。基于Sentinel 的Gateway限流是通过其提供的Filter来完成的，使用时只需注入对应的SentinelGatewayFilter实例以及 SentinelGatewayBlockExceptionHandler 实例即可。 GatewayConfig类的源代码如下所示。 /** * @version 1.0.0 * @description 网关配置类 */ @Configuration public class GatewayConfig { private final List&lt;ViewResolver&gt; viewResolvers; private final ServerCodecConfigurer serverCodecConfigurer; @Value(&quot;${spring.cloud.gateway.discovery.locator.route-id-prefix}&quot;) private String routeIdPrefix; public GatewayConfig(ObjectProvider&lt;List&lt;ViewResolver&gt;&gt; viewResolversProvider, ServerCodecConfigurer serverCodecConfigurer) { this.viewResolvers = viewResolversProvider.getIfAvailable(Collections::emptyList); this.serverCodecConfigurer = serverCodecConfigurer; } /** * 初始化一个限流的过滤器 */ @Bean @Order(Ordered.HIGHEST_PRECEDENCE) public GlobalFilter sentinelGatewayFilter() { return new SentinelGatewayFilter(); } @PostConstruct public void init() { this.initGatewayRules(); this.initBlockHandlers(); } /** * 配置初始化的限流参数 */ private void initGatewayRules() { Set&lt;GatewayFlowRule&gt; rules = new HashSet&lt;&gt;(); /** * Sentinel整合SpringCloud Gateway使用的API类型为Route ID类型，也就是基于route维度时， * 由于Sentinel为SpringCloud Gateway网关生成的API名称规则如下： * 生成的规则为：${spring.cloud.gateway.discovery.locator.route-id-prefix}后面直接加上目标微服务的名称，如下所示。 * ${spring.cloud.gateway.discovery.locator.route-id-prefix}目标微服务的名称 * 其中，${spring.cloud.gateway.discovery.locator.route-id-prefix}是在yml文件中配置的访问前缀 * * 为了让通过服务网关访问目标微服务链接后，请求链路中生成的API名称与流控规则中生成的API名称一致，以达到启动项目即可实现访问链接的限流效果， * 而无需登录Setinel管理界面手动配置限流规则，可以将 * resource参数设置为${spring.cloud.gateway.discovery.locator.route-id-prefix}目标微服务的名称 * * 当然，如果不按照上述配置，也可以在项目启动后，通过服务网关访问目标微服务链接后，在Sentinel管理界面的请求链路中找到对应的API名称所代表的请求链路， * 然后手动配置限流规则。 **/ // //用户微服务网关 // rules.add(this.getGatewayFlowRule(&quot;user-gateway&quot;)); // //商品微服务网关 // rules.add(this.getGatewayFlowRule(&quot;product-gateway&quot;)); // //订单微服务网关 // rules.add(this.getGatewayFlowRule(&quot;order-gateway&quot;)); //用户微服务网关 rules.add(this.getGatewayFlowRule(getResource(&quot;server-user&quot;))); //商品微服务网关 rules.add(this.getGatewayFlowRule(getResource(&quot;server-product&quot;))); //订单微服务网关 rules.add(this.getGatewayFlowRule(getResource(&quot;server-order&quot;))); //加载规则 GatewayRuleManager.loadRules(rules); } private String getResource(String targetServiceName){ if (routeIdPrefix == null){ routeIdPrefix = &quot;&quot;; } return routeIdPrefix.concat(targetServiceName); } private GatewayFlowRule getGatewayFlowRule(String resource){ //传入资源名称生成GatewayFlowRule GatewayFlowRule gatewayFlowRule = new GatewayFlowRule(resource); //限流阈值 gatewayFlowRule.setCount(1); //统计的时间窗口，单位为 gatewayFlowRule.setIntervalSec(1); return gatewayFlowRule; } /** * 配置限流的异常处理器 */ @Bean @Order(Ordered.HIGHEST_PRECEDENCE) public SentinelGatewayBlockExceptionHandler sentinelGatewayBlockExceptionHandler() { return new SentinelGatewayBlockExceptionHandler(viewResolvers, serverCodecConfigurer); } /** * 自定义限流异常页面 */ private void initBlockHandlers() { BlockRequestHandler blockRequestHandler = new BlockRequestHandler() { @Override public Mono&lt;ServerResponse&gt; handleRequest(ServerWebExchange serverWebExchange, Throwable throwable) { Map map = new HashMap&lt;&gt;(); map.put(&quot;code&quot;, 1001); map.put(&quot;codeMsg&quot;, &quot;接口被限流了&quot;); return ServerResponse.status(HttpStatus.OK). contentType(MediaType.APPLICATION_JSON_UTF8). body(BodyInserters.fromObject(map)); } }; GatewayCallbackManager.setBlockHandler(blockRequestHandler); } } GatewayConfig类的源代码看上去比较多，但是都是一些非常简单的方法，冰河在这里就不再赘述了。 这里有个需要特别注意的地方： Sentinel1.8.4整合SpringCloud Gateway使用的API类型为Route ID类型时，也就是基于route维度时，由于Sentinel为SpringCloud Gateway网关生成的API名称规则如下： 生成的规则为：****${spring.cloud.gateway.discovery.locator.route-id-prefix}后面直接加上目标微服务的名称，如下所示。 {spring.cloud.gateway.discovery.locator.route-id-prefix}目标微服务的名称。其中，${spring.cloud.gateway.discovery.locator.route-id-prefix}是在yml文件中配置的访问前缀。 为了让通过服务网关访问目标微服务链接后，请求链路中生成的API名称与流控规则中生成的API名称一致，以达到启动项目即可实现访问链接的限流效果，而无需登录Setinel管理界面手动配置限流规则，可以将生成GatewayFlowRule对象的resource参数设置为${spring.cloud.gateway.discovery.locator.route-id-prefix}目标微服务的名称 当然，如果不按照上述配置，也可以在项目启动后，通过服务网关访问目标微服务链接后，在Sentinel管理界面的请求链路中找到对应的API名称所代表的请求链路，然后手动配置限流规则。 （3）将服务网关shop-gateway模块的application.yml文件备份一份名称为application-nacos-simple.yml的文件，并将application.yml文件的内容修改成如下所示。 server: port: 10001 spring: application: name: server-gateway main: allow-bean-definition-overriding: true cloud: nacos: discovery: server-addr: 127.0.0.1:8848 sentinel: transport: port: 7777 dashboard: 127.0.0.1:8888 web-context-unify: false eager: true gateway: globalcors: cors-configurations: '[/**]': allowedOrigins: &quot;*&quot; allowedMethods: &quot;*&quot; allowCredentials: true allowedHeaders: &quot;*&quot; discovery: locator: enabled: true route-id-prefix: gateway- 其中： spring.cloud.sentinel.eager表示程序启动时，流控规则是否立即注册到Sentinel，配置为true表示立即注册到Sentinel。 spring.cloud.gateway.discovery.locator.route-id-prefix：生成流控规则API名称的前缀。 （4）在IDEA中配置启动服务网关shop-gateway模块的参数-Dcsp.sentinel.app.type=1，如下所示。 如果是在命令行启动网关服务的Jar包，则可以使用如下命令。 java -Dcsp.sentinel.app.type=1 shop-gateway.jar 或者在启动类io.binghe.shop.GatewayStarter的main()方法中添加一行System.setProperty(&quot;csp.sentinel.app.type&quot;, &quot;1&quot;);代码，如下所示。 /** * @version 1.0.0 * @description 服务网关启动类 */ @SpringBootApplication @EnableDiscoveryClient public class GatewayStarter { public static void main(String[] args){ System.setProperty(&quot;csp.sentinel.app.type&quot;, &quot;1&quot;); SpringApplication.run(GatewayStarter.class, args); } } （5）分别启动用户微服务、商品微服务、订单微服务和服务网关，启动后会在Sentinel管理界面左侧菜单栏中看到server-gateway菜单，如下所示。 在server-gateway菜单下的流控规则子菜单中可以看到网关的流控规则已经注册到Sentinel，如下所示。 （6）通过服务网关访问用户微服务，在浏览器中输入http://localhost:10001/server-user/user/get/1001，不断刷新页面，如下所示。 用户微服务返回的原始数据如下所示。 { &quot;code&quot;: 1001, &quot;codeMsg&quot;: &quot;接口被限流了&quot; } 可以看到，通过服务网关不断刷新用户微服务时，触发了服务限流，并返回了自定义的限流结果数据。 （7）通过服务网关访问商品微服务，在浏览器中输入http://localhost:10001/server-product/product/get/1001，不断刷新页面，如下所示。 商品微服务返回的原始数据如下所示。 { &quot;code&quot;: 1001, &quot;codeMsg&quot;: &quot;接口被限流了&quot; } 可以看到，通过服务网关不断刷新商品微服务时，触发了服务限流，并返回了自定义的限流结果数据。 （8）通过服务网关访问订单微服务，在浏览器中输入http://localhost:10001/server-order/order/test_sentinel，不断刷新页面，如下所示。 可以看到，通过服务网关不断刷新订单微服务时，触发了服务限流，并返回了自定义的限流结果数据。 实现自定义API分组维度限流 前面，我们实现了route维度的限流，接下来，我们再基于Sentinel与SpringCloud gateway实现自定义API分组维度的限流。 （1）在服务网关shop-gateway模块的io.binghe.shop.config.GatewayConfig配置类中新增initCustomizedApis()方法，初始化API管理的信息，源码如下所示。 private void initCustomizedApis() { Set&lt;ApiDefinition&gt; definitions = new HashSet&lt;&gt;(); ApiDefinition api1 = new ApiDefinition(&quot;user_api1&quot;) .setPredicateItems(new HashSet&lt;ApiPredicateItem&gt;() {{ // 以/server-user/user/api1 开头的请求 add(new ApiPathPredicateItem().setPattern(&quot;/server-user/user/api1/**&quot;). setMatchStrategy(SentinelGatewayConstants.URL_MATCH_STRATEGY_PREFIX)); }}); ApiDefinition api2 = new ApiDefinition(&quot;user_api2&quot;) .setPredicateItems(new HashSet&lt;ApiPredicateItem&gt;() {{ // 以/server-user/user/api2/demo1 完成的url路径匹配 add(new ApiPathPredicateItem().setPattern(&quot;/server-user/user/api2/demo1&quot;)); }}); definitions.add(api1); definitions.add(api2); GatewayApiDefinitionManager.loadApiDefinitions(definitions); } 上述代码中，配置了两个API分组，每个API分组的规则如下。 user_api1分组：匹配以/product-serv/product/api1开头的所有请求。 user_api2分组：精确匹配/server-user/user/api2/demo1。 （2）在服务网关shop-gateway模块的io.binghe.shop.config.GatewayConfig配置类中init()方法中调用initCustomizedApis()方法，为了避免route维度的限流对自定义API分组维度的限流产生影响，这里，同时在init()方法中注释掉调用initGatewayRules()方法，修改后的init()方法的代码如下所示。 @PostConstruct public void init() { //this.initGatewayRules(); this.initBlockHandlers(); this.initCustomizedApis(); } （3）在用户微服务shop-user的io.binghe.shop.user.controller.UserController类中新增四个测试接口，源码如下所示。 @GetMapping(value = &quot;/api1/demo1&quot;) public String api1Demo1(){ log.info(&quot;访问了api1Demo1接口&quot;); return &quot;api1Demo1&quot;; } @GetMapping(value = &quot;/api1/demo2&quot;) public String api1Demo2(){ log.info(&quot;访问了api1Demo2接口&quot;); return &quot;api1Demo2&quot;; } @GetMapping(value = &quot;/api2/demo1&quot;) public String api2Demo1(){ log.info(&quot;访问了api2Demo1接口&quot;); return &quot;api2Demo1&quot;; } @GetMapping(value = &quot;/api2/demo2&quot;) public String api2Demo2(){ log.info(&quot;访问了api2Demo2接口&quot;); return &quot;api2Demo2&quot;; } （4）分别启动用户微服务、商品微服务、订单微服务和服务网关，启动后会在Sentinel管理界面左侧菜单栏中看到server-gateway菜单，如下所示。 此时，由于我们注释了调用以route维度限流的方法，所以，在流控规则里的限流规则为空，如下所示。 在API管理里面会发现我们定义的API分组已经自动注册到Sentinel中了，如下所示。 （5）在Sentinel管理界面的流控规则中，新增网关流控规则，如下所示。 点击新增网关流控规则后，会弹出新增网关流控规则配置框，按照如下方式为user_api1分组配置限流规则。 点击新增按钮后，按照同样的方式为user_api2分组配置限流规则。 配置完毕后，在流控规则中的限流规则如下所示。 （6）预期的测试结果如下。 当频繁访问http://localhost:10001/server-user/user/api1/demo1时会被限流。 当频繁访问http://localhost:10001/server-user/user/api1/demo2时会被限流。 当频繁访问http://localhost:10001/server-user/user/api2/demo1时会被限流。 当频繁访问http://localhost:10001/server-user/user/api2/demo2时不会被限流。 注意：只有最后一个不会被限流。 （7）在浏览器上频繁访问http://localhost:10001/server-user/user/api1/demo1，如下所示。 返回的原始数据如下所示。 { &quot;code&quot;: 1001, &quot;codeMsg&quot;: &quot;接口被限流了&quot; } 说明触发了服务限流，并返回了自定义的限流结果数据。 （8）在浏览器上频繁访问http://localhost:10001/server-user/user/api1/demo2，如下所示。 返回的原始数据如下所示。 { &quot;code&quot;: 1001, &quot;codeMsg&quot;: &quot;接口被限流了&quot; } 说明触发了服务限流，并返回了自定义的限流结果数据。 （9）在浏览器上频繁访问http://localhost:10001/server-user/user/api2/demo1，如下所示。 返回的原始数据如下所示。 { &quot;code&quot;: 1001, &quot;codeMsg&quot;: &quot;接口被限流了&quot; } 说明触发了服务限流，并返回了自定义的限流结果数据。 （10）在浏览器上频繁访问http://localhost:10001/server-user/user/api2/demo2，如下所示。 可以看到，访问http://localhost:10001/server-user/user/api2/demo2时，无论访问的频率多频繁，都不会触发Sentinel限流。 至此，我们就成功在项目中整合了SpringCloud Gateway网关，并通过Sentinel整合SpringCloud Gateway实现了网关的限流操作。 ","link":"https://tinaxiawuhao.github.io/post/PN2nvPeue/"},{"title":"gateway网关概述","content":"网关概述 当采用分布式、微服务的架构模式开发系统中，服务网关是整个系统中必不可少的一部分。 没有网关的弊端 当一个系统使用分布式、微服务架构后，系统会被拆分为一个个小的微服务，每个微服务专注一个小的业务。那么，客户端如何调用这么多微服务的接口呢？如果不做任何处理，没有服务网关，就只能在客户端记录下每个微服务的每个接口地址，然后根据实际需要去调用相应的接口。 这种直接使用客户端记录并管理每个微服务的每个接口的方式，存在着太多的问题。比如，这里我列举几个常见的问题。 由客户端记录并管理所有的接口缺乏安全性。 由客户端直接请求不同的微服务，会增加客户端程序编写的复杂性。 涉及到服务认证与鉴权规则时，需要在每个微服务中实现这些逻辑，增加了代码的冗余性。 客户端调用多个微服务，由于每个微服务可能部署的服务器和域名不同，存在跨域的风险。 当客户端比较多时，每个客户端上都管理和配置所有的接口，维护起来相对比较复杂。 引入API网关 API网关，其实就是整个系统的统一入口。网关会封装微服务的内部结构，为客户端提供统一的入口服务，同时，一些与具体业务逻辑无关的通用逻辑可以在网关中实现，比如认证、授权、路由转发、限流、监控等。引入API网关后，如下所示。 可以看到，引入API网关后，客户端只需要连接API网关，由API网关根据实际情况进行路由转发，将请求转发到具体的微服务，同时，API网关会提供认证、授权、限流和监控等功能。 主流的API网关 当系统采用分布式、微服务的架构模式后，API网关就成了整个系统不可分割的一部分。业界通过不断的探索与创新，实现了多种API网关的解决方案。目前，比较主流的API网关有：Nginx+Lua、Kong官网、Zuul网关、Apache Shenyu网关、SpringCloud Gateway网关。 Nginx+Lua Nginx的一些插件本身就实现了限流、缓存、黑白名单和灰度发布，再加上Nginx的反向代理和负载均衡，能够实现对服务接口的负载均衡和高可用。而Lua语言可以实现一些简单的业务逻辑，Nginx又支持Lua语言。所以，可以基于Nginx+Lua脚本实现网关。 Kong网关 Kong网关基于Nginx与Lua脚本开发，性能高，比较稳定，提供多个限流、鉴权等插件，这些插件支持热插拔，开箱即用。Kong网关提供了管理页面，但是，目前基于Kong网关二次开发比较困难。 Zuul网关 Zuul网关是Netflix开源的网关，功能比较丰富，主要基于Java语言开发，便于在Zuul网关的基础上进行二次开发。但是Zuul网关无法实现动态配置规则，依赖的组件相对来说也比较多，在性能上不如Nginx。 Apache Shenyu网关 Dromara社区开发的网关框架，ShenYu 的前名是 soul，最近正式加入了 Apache 的孵化器，因此改名为 ShenYu。其是一个异步的，高性能的，跨语言的，响应式的API网关，并在此基础上提供了非常丰富的扩展功能： 支持各种语言(http协议)，支持Dubbo, Spring-Cloud, Grpc, Motan, Sofa, Tars等协议。 插件化设计思想，插件热插拔，易扩展。 灵活的流量筛选，能满足各种流量控制。 内置丰富的插件支持，鉴权，限流，熔断，防火墙等等。 流量配置动态化，性能极高。 支持集群部署，支持 A/B Test，蓝绿发布。 SpringCloud Gateway网关 Spring为了替换Zuul而开发的网关，SpringCloud Alibaba技术栈中，并没有单独实现网关的组件。在后续的案例实现中，我们会使用SpringCloud Gateway实现网关功能。 SpringCloud Gateway网关 Spring Cloud Gateway是Spring公司基于Spring 5.0， Spring Boot 2.0 和 Project Reactor 等技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的 API 路由管理方式。 它的目标是替代Netflix Zuul，其不仅提供统一的路由方式，并且基于 Filter 链的方式提供了网关基本的功能，例如：安全，监控和限流、重试等。 SpringCloud Gateway概述 Spring Cloud Gateway是Spring Cloud的一个全新项目，基于Spring 5.0 + Spring Boot 2.0和Project Reactor等技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的API路由管理方式。 Spring Cloud Geteway作为Spring Cloud生态系统中的网关，目标是替代Zuul，在Spring Cloud2.0以上版本中，没有对新版本的Zuul 2.0以上最新高性能版本进行集成，仍然还是使用的Zuul 1.x非Reactor模式的老版本。 为了提升网关性能，Spring Cloud Gateway是基于WebFlux框架实现的，而WebFlux框架底层则使用了高性能的Reactor模式通信框架Netty。 Spring Cloud Gateway的目标提供统一的路由方式且基于Filter链的方式提供了网关基本的功能，例如：安全，监控/指标，和限流。 总结一句话：Spring Cloud Gateway使用的Webflux中的reactor-netty响应式编程组件，底层使用Netty通讯框架。 SpringCloud Gateway核心架构 客户端请求到 Gateway 网关，会先经过 Gateway Handler Mapping 进行请求和路由匹配。匹配成功后再发送到 Gateway Web Handler 处理，然后会经过特定的过滤器链，经过所有前置过滤后，会发送代理请求。请求结果返回后，最后会执行所有的后置过滤器。 由上图可以看出，SpringCloud Gateway的主要流程为：客户端请求会先打到Gateway，具体的讲应该是DispacherHandler（因为Gateway引入了WebFlux，作用可以类比MVC的DispacherServlet）。 Gateway根据用户的请求找到相应的HandlerMapping，请求和具体的handler之间有一个映射关系，网关会对请求进行路由，handler会匹配到RoutePredicateHandlerMapping，匹配请求对应的Route，然后到达Web处理器。 WebHandler代理了一系列网关过滤器和全局过滤器的实例，这些过滤器可以对请求和响应进行修改，最后由代理服务完成用户请求，并将结果返回。 ","link":"https://tinaxiawuhao.github.io/post/8Wk_K0cjl/"},{"title":"Sentinel源码拓展之——限流的各种实现方式","content":"Sentinel源码拓展之——限流的各种实现方式 一 常见的限流功能实现有以下三种方式 滑动时间窗口、令牌桶、漏桶，这三种实现方式，有各自擅长的应用场景，而在 Sentinel 中这三种限流实现都有被用到，只不过使用在不同的限流场景下： **滑动时间窗口：**普通QPS限流下的快速失败、Warmup预热，使用场景最多；—— Sentinel的实现，是使用“环形时间窗口”来表示无边无际的时间； **令牌桶：**热点参数限流，需要为每一个请求参数，创建一个令牌桶；—— Sentinel的实现，实际不是真的创建很多个桶； **漏桶（流量整形）：**普通QPS限流下的排队等待，将请求暂存在一个队列中，按时间间隔拉取并处理；—— Sentinel的实现，实际不是真的放入一个队列中； 1 时间窗口（滑动时间窗口） 固定时间窗口，由于时间窗口粒度太粗，而时间本身其实是没有边界的，所以无法保证任意单位时间窗口中的QPS都不超限（如下图粉红色区域）； 为了解决这种问题，可以将我们设置的时间窗口做N等分，划分为更小粒度的窗口，这就是滑动时间窗口： 假设滑动时间跨度Internel为1秒，我们可以做N=2等分，那么最小时间窗口其实就是500ms； 在限流统计QPS时，窗口范围就是从( currentTime – interval) 之后的第一个时区开始，到 currentTime 所在时间窗口。 随着N的值越大，限流的精度就会越高。 2 令牌桶 以固定的速率生成令牌，存在令牌桶中，如果令牌桶满了以后，多余的令牌将会被丢弃； 请求进入后，必须先尝试从令牌桶中获取令牌，获取到令牌的请求才会被处理，没有获取到令牌的请求将会被拒绝，或者等待。 3 漏桶 将每一个“请求”视作“水滴”，放入“漏桶”中存储； “漏桶”以固定速率向外“漏出”请求进行处理，如果“漏桶”空了则代表没有待处理的请求； 如果“漏桶”满了，则多余的“请求”将被快速拒绝； 以上都是三种实现方案的理论知识，而Sentinel在实际实现时，是做了一定的优化的！ 二 Sentinel中三种限流方案的实现源码剖析之——滑动时间窗口算法 滑动时间窗口 —— QPS快速失败 + WarmUp预热： Sentinel中的时间窗口，其实使用的是如下图所示的环形时间窗口： 因为时间是没有边界的， 而且我们一般也只会关注一个Internel之内的时间，如果一个Internel被分成了N个小窗口，那么我们也只会关注N个小的时间窗口； 所以使用环形时间窗口，既能满足使用需求，又能解决内存空间。 通过上一篇关于 ProcessorSlotChain 插槽链的介绍，我们已经清楚了： QPS 的统计工作将会由 StatisticSlot 插槽完成； 限流判断的逻辑将会由 FlowSlot 插槽来完成； 而所有的 PrcessorSlot 的处理逻辑，都在他们的 entry() 方法中； 1 时间窗口请求量统计：StatisticSlot public class StatisticSlot extends AbstractLinkedProcessorSlot&lt;DefaultNode&gt; { @Override public void entry(Context context, ResourceWrapper resourceWrapper, DefaultNode node, int count, boolean prioritized, Object... args) throws Throwable { try { // 先去执行后面插槽和请求体的任务 fireEntry(context, resourceWrapper, node, count, prioritized, args); // 处理完之后才会回来做统计（如果后面失败了，肯定就不用统计了） node.increaseThreadNum(); // 线程数统计（服务隔离） node.addPassRequest(count); // QPS请求量统计（服务限流） ...... } catch (Exception e){ ...... } } } // com.alibaba.csp.sentinel.node.DefaultNode#addPassRequest public void addPassRequest(int count) { super.addPassRequest(count); // 为DefaultNode做统计 this.clusterNode.addPassRequest(count); // 为ClusterNode做统计 } 我们知道DefaultNode 和 ClusterNode 都是 StatisticNode 的子类，所以这里都会调用到StatisticNode的addPassRequest()方法： public class StatisticNode implements Node { // 秒级统计（看变量名就知道是一个环形计数器） private transient volatile Metric rollingCounterInSecond = new ArrayMetric(SampleCountProperty.SAMPLE_COUNT, IntervalProperty.INTERVAL); // 分钟级统计（看变量名就知道是一个环形计数器） private transient Metric rollingCounterInMinute = new ArrayMetric(60, 60 * 1000, false); // 统计QPS方法 public void addPassRequest(int count) { rollingCounterInSecond.addPass(count); // 秒级统计 rollingCounterInMinute.addPass(count); // 分钟级统计 } } // intervalInMs：滑动窗口的时间间隔，Sentinel默认为 1000ms // sampleCount：时间窗口的分割数量，Sentinel默认为 2 // 所以最小的时间窗口就为 500ms public class ArrayMetric implements Metric { private final LeapArray&lt;MetricBucket&gt; data; public ArrayMetric(int sampleCount, int intervalInMs) { this.data = new OccupiableBucketLeapArray(sampleCount, intervalInMs); } } 时间线铺平的效果如下图： 获取当前请求所在时间窗口，并增加计数 public class ArrayMetric implements Metric { private final LeapArray&lt;MetricBucket&gt; data; public ArrayMetric(int sampleCount, int intervalInMs) { this.data = new OccupiableBucketLeapArray(sampleCount, intervalInMs); } @Override public void addPass(int count) { // 获取当前请求所在时间窗口 WindowWrap&lt;MetricBucket&gt; wrap = data.currentWindow(); // 计数器 + count wrap.value().addPass(count); } } ArrayMetric 又是如何获取当前所在时间窗口的呢？ 首先我们得要弄清楚LeapArray保存了哪些数据？ public abstract class LeapArray&lt;T&gt; { // 小时间窗口的时间长度，默认为 500ms protected int windowLengthInMs; // 一个Interval时间被划分的数量，秒级统计默认为2，分钟级默认为60 protected int sampleCount; // 滑动时间窗口时间间隔，默认为 1000ms protected int intervalInMs; // 每一个Interval时间窗口内的小窗口数组，这里就是2 protected final AtomicReferenceArray&lt;WindowWrap&lt;T&gt;&gt; array; } 再次上图，LeapArray是一个环形数组： 我们直接看data.currentWindow() 方法： // com.alibaba.csp.sentinel.slots.statistic.base.LeapArray#currentWindow(当前时间戳) public WindowWrap&lt;T&gt; currentWindow(long timeMillis) { if (timeMillis &lt; 0) { return null; } // 计算当前时间对应的数组角标 = (当前时间/500ms)%16 int idx = calculateTimeIdx(timeMillis); // 计算当前时间所在窗口的开始时间 = 当前时间 - 当前时间 % 500ms long windowStart = calculateWindowStart(timeMillis); /* * 先根据角标获取数组中保存的 oldWindow 对象，可能是旧数据，需要判断. * * (1) oldWindow 不存在, 说明是第一次，创建新 window并存入，然后返回即可 * (2) oldWindow的 starTime = 本次请求的 windowStar, 说明正是要找的窗口，直接返回. * (3) oldWindow的 starTime &lt; 本次请求的 windowStar, 说明是旧数据，需要被覆盖，创建 * 新窗口，覆盖旧窗口 */ while (true) { WindowWrap&lt;T&gt; old = array.get(idx); if (old == null) { // 创建新 window WindowWrap&lt;T&gt; window = new WindowWrap&lt;T&gt;(windowLengthInMs, windowStart, newEmptyBucket(timeMillis)); // 基于CAS写入数组，避免线程安全问题 if (array.compareAndSet(idx, null, window)) { // 写入成功，返回新的 window return window; } else { // 写入失败，说明有并发更新，等待其它人更新完成即可 Thread.yield(); } } else if (windowStart == old.windowStart()) { return old; } else if (windowStart &gt; old.windowStart()) { if (updateLock.tryLock()) { try { // 获取并发锁，覆盖旧窗口并返回 return resetWindowTo(old, windowStart); } finally { updateLock.unlock(); } } else { // 获取锁失败，等待其它线程处理就可以了 Thread.yield(); } } else if (windowStart &lt; old.windowStart()) { // 这种情况不应该存在，写这里只是以防万一。 return new WindowWrap&lt;T&gt;(windowLengthInMs, windowStart, newEmptyBucket(timeMillis)); } } } 获取到时间窗口后，增加计数就比较简单了，使用“自旋 + CAS”的方式完成计数器安全增加即可。 2 滑动窗口QPS计算，并判断限流逻辑：FlowSlot public class FlowSlot extends AbstractLinkedProcessorSlot&lt;DefaultNode&gt; { @Override public void entry(Context context, ResourceWrapper resourceWrapper, DefaultNode node, int count, boolean prioritized, Object... args) throws Throwable { // 先做限流判断 checkFlow(resourceWrapper, context, node, count, prioritized); // 判断通过后，才会放行到下一个插槽 fireEntry(context, resourceWrapper, node, count, prioritized, args); } } FlowSlot的限流判断最终都由TrafficShapingController接口中的canPass方法来实现。该接口有三个实现类： DefaultController：快速失败，默认的方式，基于滑动时间窗口算法； WarmUpController：预热模式，基于滑动时间窗口算法，只不过阈值是动态的； RateLimiterController：排队等待模式，基于漏桶算法； 这里我们就以DefaultController.canPass() 方法为例： com.alibaba.csp.sentinel.slots.block.flow.controller.DefaultController#canPass() public boolean canPass(Node node, int acquireCount, boolean prioritized) { // 重点：计算目前为止滑动窗口内已经存在的请求量 int curCount = avgUsedTokens(node); // 判断：已使用请求量 + 需要的请求量（1） 是否大于 窗口的请求阈值 if (curCount + acquireCount &gt; count) { // 大于，说明超出阈值，返回false if (prioritized &amp;&amp; grade == RuleConstant.FLOW_GRADE_QPS) { long currentTime; long waitInMs; currentTime = TimeUtil.currentTimeMillis(); waitInMs = node.tryOccupyNext(currentTime, acquireCount, count); if (waitInMs &lt; OccupyTimeoutProperty.getOccupyTimeout()) { node.addWaitingRequest(currentTime + waitInMs, acquireCount); node.addOccupiedPass(acquireCount); sleep(waitInMs); // PriorityWaitException indicates that the request will pass after waiting for {@link @waitInMs}. throw new PriorityWaitException(waitInMs); } } return false; } // 小于等于，说明在阈值范围内，返回true return true; } 很显然，关键点就在于，如何计算滑动窗口内已经用掉的请求量 private int avgUsedTokens(Node node) { if (node == null) { return DEFAULT_AVG_USED_TOKENS; } return grade == RuleConstant.FLOW_GRADE_THREAD ? node.curThreadNum() : (int)(node.passQps()); } // 我们这里肯定是QPS限流 // com.alibaba.csp.sentinel.node.StatisticNode#passQps public double passQps() { // 请求量 ÷ 滑动窗口时间间隔(默认1秒) ，得到的就是QPS return rollingCounterInSecond.pass() / rollingCounterInSecond.getWindowIntervalInSec(); } 那么rollingCounterInSecond.pass() 又是如何计算当前时间窗口内的请求量的呢？ 以秒级统计为例，其实环形数组的长度只为2，只需要记录当前小时间窗口和前一个小时间窗口即可，一个都不多记录，节约内存！ // com.alibaba.csp.sentinel.slots.statistic.metric.ArrayMetric#pass public long pass() { // 获取当前窗口 data.currentWindow(); long pass = 0; // 获取 当前时间的 滑动窗口范围内 的所有小窗口 List&lt;MetricBucket&gt; list = data.values(); // 遍历 for (MetricBucket window : list) { // 累加求和 pass += window.pass(); } // 返回 return pass; } | | // data.values()如何获取 滑动窗口范围内 的所有小窗口的 // com.alibaba.csp.sentinel.slots.statistic.base.LeapArray#values(long) public List&lt;T&gt; values(long timeMillis) { if (timeMillis &lt; 0) { return new ArrayList&lt;T&gt;(); } // 创建空集合，大小等于 LeapArray长度（2） int size = array.length(); List&lt;T&gt; result = new ArrayList&lt;T&gt;(size); // 遍历 LeapArray for (int i = 0; i &lt; size; i++) { // 获取每一个小窗口 WindowWrap&lt;T&gt; windowWrap = array.get(i); // 判断这个小窗口是否在 滑动窗口时间范围内（1秒内） if (windowWrap == null || isWindowDeprecated(timeMillis, windowWrap)) { // 不在范围内，则跳过 continue; } // 在范围内，则添加到集合中 result.add(windowWrap.value()); } // 返回集合 return result; } | | // isWindowDeprecated(timeMillis, windowWrap)又是如何判断窗口是否符合要求呢？ public boolean isWindowDeprecated(long time, WindowWrap&lt;T&gt; windowWrap) { // 当前时间 - 窗口开始时间 是否大于 滑动窗口的最大间隔（1秒） // 也就是说，我们要统计的是 距离当前时间1秒内的 小窗口的 count之和 return time - windowWrap.windowStart() &gt; intervalInMs; } 到这里，我们就可以理清了： StatisticSlot会帮助我们记录每一次的request请求，统计每个小时间窗口内的请求数； 秒级统计的时间窗口环只有 2 格； 分钟级统计的时间窗口环有 60格； FlowSlot在需要的时候，会去除当前时间窗口内包含的所有小窗口，然后累加他们的请求量； 最后判断是否溢出限流阈值，允许通过，或直接拒绝！ 三 Sentinel中三种限流方案的实现源码剖析之——令牌桶算法 “热点参数”限流策略，不适合使用 StatisticSlot 中常规的 “滑动时间窗口算法”，因为StatisticSlot中统计的维度是Node级别； 很显然，“热点参数”并不适合使用上面的“环形时间窗口算法”来实现； 相比下来，“令牌桶算法”最适合用在“热点参数限流”场景下，只需要为每个不同的参数值创建一个令牌桶即可。 Controller中的方法资源是不可以进行热点参数限流的：通过Sentinel添加的springmvc拦截器实现，创建Entry时候没有传入params参数； 其它的我们通过@SentinelResource添加的资源才艺进行热点参数限流：通过AOP切面编程实现，创建Entry的时候，也将Params一并传入了； public class ParamFlowSlot extends AbstractLinkedProcessorSlot&lt;DefaultNode&gt; { public void entry(Context context, ResourceWrapper resourceWrapper, DefaultNode node, int count, boolean prioritized, Object... args) throws Throwable { if (!ParamFlowRuleManager.hasRules(resourceWrapper.getName())) { this.fireEntry(context, resourceWrapper, node, count, prioritized, args); } else { // 校验参数限流 this.checkFlow(resourceWrapper, count, args); // 放行到下一个插槽 this.fireEntry(context, resourceWrapper, node, count, prioritized, args); } } void checkFlow(ResourceWrapper resourceWrapper, int count, Object... args) throws BlockException { // args == null 情况有二：1、确实没有参数； 2、Controller方法无法进行参数限流 if (args != null) { if (ParamFlowRuleManager.hasRules(resourceWrapper.getName())) { List&lt;ParamFlowRule&gt; rules = ParamFlowRuleManager.getRulesOfResource(resourceWrapper.getName()); Iterator var5 = rules.iterator(); ParamFlowRule rule; // do-while循环，对每一条rule规则做判断 do { if (!var5.hasNext()) { return; } rule = (ParamFlowRule)var5.next(); this.applyRealParamIdx(rule, args.length); // 初始化“令牌桶”—— 加“”号，并非真的令牌桶 ParameterMetricStorage.initParamMetricsFor(resourceWrapper, rule); } while(ParamFlowChecker.passCheck(resourceWrapper, rule, count, args)); String triggeredParam = &quot;&quot;; if (args.length &gt; rule.getParamIdx()) { Object value = args[rule.getParamIdx()]; triggeredParam = String.valueOf(value); } throw new ParamFlowException(resourceWrapper.getName(), triggeredParam, rule); } } } } 1ParameterMetricStorage.initParamMetricsFor() 令牌桶初始化方法 public final class ParameterMetricStorage { // 以资源名区分不同资源下的令牌桶容器（因为这还不是真正的令牌桶） private static final Map&lt;String, ParameterMetric&gt; metricsMap = new ConcurrentHashMap(); private static final Object LOCK = new Object(); public static void initParamMetricsFor(ResourceWrapper resourceWrapper, ParamFlowRule rule) { if (resourceWrapper != null &amp;&amp; resourceWrapper.getName() != null) { String resourceName = resourceWrapper.getName(); ParameterMetric metric; if ((metric = (ParameterMetric)metricsMap.get(resourceName)) == null) { synchronized(LOCK) { // 如果当前资源还没创建过自己的令牌桶容器，那就创建一个 if ((metric = (ParameterMetric)metricsMap.get(resourceName)) == null) { metric = new ParameterMetric(); metricsMap.put(resourceWrapper.getName(), metric); RecordLog.info(&quot;[ParameterMetricStorage] Creating parameter metric for: &quot; + resourceWrapper.getName(), new Object[0]); } } } // 对令牌桶容器进行初始化 metric.initialize(rule); } } } 2 初始化令牌桶容器，真正的令牌桶即将出现 Sentinel中的令牌桶，其实是维护2个双层Map： 容器中剩余令牌数的Map：&lt;ParamFlowRule, &lt;参数值, 属于这个参数值得桶中的剩余可用令牌数&gt;&gt;：此时第二层Map还是空Map； 记录最近一次通过的请求时间戳的Map：&lt;ParamFlowRule, &lt;参数值, 最近一次请求的时间戳&gt;&gt;：此时第二层Map还是空Map； public class ParameterMetric { private static final int THREAD_COUNT_MAX_CAPACITY = 4000; private static final int BASE_PARAM_MAX_CAPACITY = 4000; private static final int TOTAL_MAX_CAPACITY = 200000; private final Object lock = new Object(); // 令牌桶实现之一：双层Map：&lt;ParamFlowRule, &lt;参数值, 上次请求的时间戳&gt;&gt; private final Map&lt;ParamFlowRule, CacheMap&lt;Object, AtomicLong&gt;&gt; ruleTimeCounters = new HashMap(); // 令牌桶之二：桶中剩余的令牌数 private final Map&lt;ParamFlowRule, CacheMap&lt;Object, AtomicLong&gt;&gt; ruleTokenCounter = new HashMap(); private final Map&lt;Integer, CacheMap&lt;Object, AtomicInteger&gt;&gt; threadCountMap = new HashMap(); // 初始化令牌桶容器 public void initialize(ParamFlowRule rule) { long size; if (!this.ruleTimeCounters.containsKey(rule)) { synchronized(this.lock) { // 初始化ParamRule对应的“最近请求时间戳Map” if (this.ruleTimeCounters.get(rule) == null) { size = Math.min(4000L * rule.getDurationInSec(), 200000L); this.ruleTimeCounters.put(rule, new ConcurrentLinkedHashMapWrapper(size)); } } } if (!this.ruleTokenCounter.containsKey(rule)) { synchronized(this.lock) { // 初始化ParamRule对应的“桶中可用令牌Map” if (this.ruleTokenCounter.get(rule) == null) { size = Math.min(4000L * rule.getDurationInSec(), 200000L); this.ruleTokenCounter.put(rule, new ConcurrentLinkedHashMapWrapper(size)); } } } if (!this.threadCountMap.containsKey(rule.getParamIdx())) { synchronized(this.lock) { if (this.threadCountMap.get(rule.getParamIdx()) == null) { this.threadCountMap.put(rule.getParamIdx(), new ConcurrentLinkedHashMapWrapper(4000L)); } } } } } 以上，只算是将令牌桶容器初始化好了，但是还没开始正式使用！ 3 正式使用上面创建的令牌桶容器，一步步调用至“热点参数限流”判断逻辑核心方法 ParamFlowChecker.passCheck(resourceWrapper, rule, count, args) com.alibaba.csp.sentinel.slots.block.flow.param.ParamFlowChecker#passLocalCheck com.alibaba.csp.sentinel.slots.block.flow.param.ParamFlowChecker#passSingleValueCheck com.alibaba.csp.sentinel.slots.block.flow.param.ParamFlowChecker#passDefaultLocalCheck 4ParamFlowChecker.passDefaultLocalCheck() 即为“热点限流逻辑”的核心方法 // com.alibaba.csp.sentinel.slots.block.flow.param.ParamFlowChecker#passDefaultLocalCheck static boolean passDefaultLocalCheck(ResourceWrapper resourceWrapper, ParamFlowRule rule, int acquireCount, Object value) { ParameterMetric metric = getParameterMetric(resourceWrapper); // 根据rule从上面初始化好的令牌桶容器中去除第二层Map // &lt;参数值, 上次请求的时间戳&gt; // &lt;参数值, 属于这个参数值得桶中的剩余可用令牌数&gt; CacheMap&lt;Object, AtomicLong&gt; tokenCounters = metric == null ? null : metric.getRuleTokenCounter(rule); CacheMap&lt;Object, AtomicLong&gt; timeCounters = metric == null ? null : metric.getRuleTimeCounter(rule); if (tokenCounters != null &amp;&amp; timeCounters != null) { Set&lt;Object&gt; exclusionItems = rule.getParsedHotItems().keySet(); long tokenCount = (long)rule.getCount(); if (exclusionItems.contains(value)) { tokenCount = (long)(Integer)rule.getParsedHotItems().get(value); } if (tokenCount == 0L) { return false; } else { // 允许的最大请求数，也是桶的最大值，也是我们rule中设置的单机阈值 // 后者是突发阈值，一般不配置，为0 long maxCount = tokenCount + (long)rule.getBurstCount(); if ((long)acquireCount &gt; maxCount) { return false; } else { while(true) { long currentTime = TimeUtil.currentTimeMillis(); // 从timeMap中获取最近一次请求通过的时间戳 AtomicLong lastAddTokenTime = (AtomicLong)timeCounters.putIfAbsent(value, new AtomicLong(currentTime)); if (lastAddTokenTime == null) { // 相同参数第一次，直接放行，并更新tokenMap = maxCount - 1 tokenCounters.putIfAbsent(value, new AtomicLong(maxCount - (long)acquireCount)); return true; } // 距离上一次请求通过的时间的差值 long passTime = currentTime - lastAddTokenTime.get(); AtomicLong oldQps; long restQps; // 距最近一次时间差 &gt; 一次统计窗口 if (passTime &gt; rule.getDurationInSec() * 1000L) { oldQps = (AtomicLong)tokenCounters.putIfAbsent(value, new AtomicLong(maxCount - (long)acquireCount)); if (oldQps == null) { lastAddTokenTime.set(currentTime); return true; } // tokenMap中剩余的令牌数 restQps = oldQps.get(); // 在距离上次请求的这段时间内，应该补充生成多少新的令牌 long toAddCount = passTime * tokenCount / (rule.getDurationInSec() * 1000L); // 上面2者相加，与maxCount允许的最大令牌数对比，取min值，并减去本次需要的令牌数 long newQps = toAddCount + restQps &gt; maxCount ? maxCount - (long)acquireCount : restQps + toAddCount - (long)acquireCount; if (newQps &lt; 0L) { return false; } // 通过CAS替换原来的tokenMap，并修改最新通过的请求时间戳 if (oldQps.compareAndSet(restQps, newQps)) { lastAddTokenTime.set(currentTime); return true; } Thread.yield(); } else { // 距最近一次时间差 &lt; 一次统计窗口 oldQps = (AtomicLong)tokenCounters.get(value); if (oldQps != null) { restQps = oldQps.get(); // tokenMap中剩余令牌不足，直接拒绝 if (restQps - (long)acquireCount &lt; 0L) { return false; } // 剩余令牌充足，CAS从tokenMap中减去当前需要的令牌数 if (oldQps.compareAndSet(restQps, restQps - (long)acquireCount)) { return true; } } Thread.yield(); } } } } } else { return true; } } 总结：对于热点参数限流： Sentinel 会为每个资源，维护两个双层数组： 容器中剩余令牌数的tokenMap：&lt;ParamFlowRule, &lt;参数值, 属于这个参数值得桶中的剩余可用令牌数&gt;&gt; 记录最近一次通过的请求时间戳的timeMap：&lt;ParamFlowRule, &lt;参数值, 最近一次请求的时间戳&gt;&gt; 以参数 x=100，限流为5为例： 第一次到达时，肯定不会超限，放行，同时往tokenMap中put入&lt;100, 4&gt; 过一会儿，x = 100 的请求再次到达，则判断，本次请求，距离上一次请求时间，是否超过一个时间统计窗口： 在同一个时间窗口，则在前一次的基础上继续减少token值，tokenMap中值变为 &lt;100, 3&gt; 如果不在一个时间窗口，那么计算距离上次请求的这段时间内，应该新生成的token数 + 现在桶中剩余的token数，与maxToken数做对比，取小值就相当于是此时桶中应该有的令牌数，然后减去自己本次需要的令牌数，之后再更新tokenMap； 四、Sentinel中三种限流方案的实现源码剖析之——漏桶算法 漏桶算法的核心思想是：将请求放在漏桶中，漏桶会按照固定时间间隔，向外“漏出”请求，进行处理，这样很明显的好处就是“流量整形”，当瞬间流量过大时，也可以先放在漏桶中，慢慢处理！ 漏桶算法的入口也是在FlowSlot中，上面有讲过： FlowSlot的限流判断最终都由TrafficShapingController接口中的canPass方法来实现。该接口有三个实现类： DefaultController：快速失败，默认的方式，基于滑动时间窗口算法； WarmUpController：预热模式，基于滑动时间窗口算法，只不过阈值是动态的； RateLimiterController：排队等待模式，基于漏桶算法； 直接进入RateLimiterController.canPass()方法逻辑： // com.alibaba.csp.sentinel.slots.block.flow.controller.RateLimiterController#canPass() // 最新一次的请求执行时间（不一定是已经通过的，而是队列中排在最尾端的请求的预期执行时间） private final AtomicLong latestPassedTime = new AtomicLong(-1); @Override public boolean canPass(Node node, int acquireCount, boolean prioritized) { // Pass when acquire count is less or equal than 0. if (acquireCount &lt;= 0) { return true; } // 阈值小于等于 0 ，阻止请求，不太可能 if (count &lt;= 0) { return false; } // 获取当前时间 long currentTime = TimeUtil.currentTimeMillis(); // 计算两次请求之间允许的最小时间间隔 // 正常时候acquireCount=1，那么costTime = 200ms long costTime = Math.round(1.0 * (acquireCount) / count * 1000); // 计算本次请求 允许执行的时间点 = 上一次请求的可能执行时间 + 两次请求的最小间隔 long expectedTime = costTime + latestPassedTime.get(); // 如果允许执行的时间点小于当前时间，说明可以立即执行 if (expectedTime &lt;= currentTime) { // 更新上一次的请求的执行时间 latestPassedTime.set(currentTime); // 这种情况说明该执行了，立即执行 return true; } else { // 不能立即执行，需要计算 预期等待时长 // 预期等待时长 = 两次请求的最小间隔 + 最近一次请求的可执行时间 - 当前时间 long waitTime = costTime + latestPassedTime.get() - TimeUtil.currentTimeMillis(); // 如果预期等待时间超出阈值，则拒绝请求 if (waitTime &gt; maxQueueingTimeMs) { return false; } else { // 预期等待时间小于阈值，更新最近一次请求的可执行时间，加上costTime long oldTime = latestPassedTime.addAndGet(costTime); try { // 保险起见，再判断一次预期等待时间，是否超过阈值 waitTime = oldTime - TimeUtil.currentTimeMillis(); if (waitTime &gt; maxQueueingTimeMs) { // 如果超过，则把刚才 加 的时间再 减回来 latestPassedTime.addAndGet(-costTime); // 拒绝 return false; } // in race condition waitTime may &lt;= 0 if (waitTime &gt; 0) { // 预期等待时间在阈值范围内，休眠要等待的时间，醒来后继续执行 Thread.sleep(waitTime); } return true; } catch (InterruptedException e) { } } } return false; } 总结： Sentinel的漏桶算法，不是真的维护了一个队列，而是通过计算各个请求的预计执行时间； 如果预计执行时间 &gt; 最大等待时间，那么久不用等了，直接拒绝； 如果预计执行时间 &lt; 最大等待时间，那么就等待吧，自己通过 Thread.sleep(waitTime) 实现等待！ ","link":"https://tinaxiawuhao.github.io/post/xUda2KT1A/"},{"title":"Sentinel核心源码——插槽机制（责任链模式）","content":"Sentinel核心源码——插槽机制（责任链模式） Sentinel的工作原理：https://github.com/alibaba/Sentinel/wiki Sentinel会为所有的资源，以资源名为区分，创建各自的DefaultProcessorSlotChain，放在缓存中； DefaultProcessorSlotChain的9个ProcessorSlot插槽都是通过SPI机制从 META/services/ 目录下加载的； 每一个ProcessorSlot 其实是一个 AbstractLinkedProcessorSlot 抽象链表处理器插槽， 有一个next属性，指向下一个Slot，当某一个Slot执行完后，会调用fireEntry()方法， 将请求转到下一个Slot继续执行。 最终完成责任链上所有ProcessorSlot的逻辑！ —————————————————————————————————————————————— Context链路上下文为request请求级别的，放在ThreadLocal中，请求结束即释放； entranceNode是应用级别的，创建完成后，会缓存起来（key为contextName），下一个请求可以继续使用； processorSlotChain也是应用级别的，创建完成后，会缓存起来（key为resourceName），下一个请求可以继续使用； 一 ProcessorSlotChain处理器插槽链和Node节点的引入 首先，根据官方Wiki中的图，我们可以很形象地看到，整个请求处理过程就像一个链条一样，一步步地向后执行，这是一种典型地“责任链模式”； 责任链模式 —— 为请求创建一个接收者对象的链，链上的每一个节点服务处理各自的业务逻辑，实现解耦，每一个处理者节点记录着下一个节点的引用，请求将沿着这条链被传递下去，以此处理对应的逻辑。 1 ProcessorSlotChain处理器插槽链的引入 ProcessorSlotChain是上图整个链的骨架，基于“责任链模式”设计，将“统计、授权、限流、降级等”处理逻辑封装成一个个的Slot插槽，串联起来。 处理链中的Slot插槽可粗分为上下两大类：数据统计部分 + 规则判断部分 数据统计： **NodeSelectorSlot：**负责构建簇点链路中的各个节点（DefaultNode），形成NodeTree **ClusterBuilderSlot：**负责构建某个资源的ClusterNode（具体的DefaultNode和ClusterNode的区别见下文） **StatisticSlot：**负责实时统计请求的各种调用信息，如来源信息、请求次数、运行信息等； 规则判断： AuthoritySlot：授权规则判断（来源控制） SystemSlot：系统保护规则判断，当系统资源使用量达到一定程度后，拒绝新的请求进入等； ParamFlowSlot：热点参数限流规则判断 FlowSolt：普通限流规则判断 DegradeSlot：降级规则判断 2 为什么要存在NodeSelectorSlot和ClusterBuilderSlot两个插槽？DefaultNode和ClusterNode有什么区别？ **DefaultNode： 同一份资源，经过不同的链路调用，会创建不同的DefaultNode，记录不同链路访问当前资源的统计元数据 **，因为整个Sentinel是支持“根据链路限流”的，所以肯定要分开统计； ClusterNode： 同一份资源，在整个系统中只会创建一个ClusterNode，记录所有入口访问当前资源的统计元数据，因为很多时候，我们只需要统计该资源的整体使用情况。 注意这里的用词，DefaultNode和ClusterNode都只是负责记录统计元数据，真正的统计工作由之后的StatisticSlot进行，另外ParmFlowSlot会负责热点参数限流这种特殊场景下的数据统计。（热点参数限流的统计为什么要单独出来，后面做限流算法实现的讲解时就清楚了）。 3 如何自定义一个Sentinel资源？@SentinelResource注解？ 我们知道，在实际使用过程中，当我们要自定义sentinel资源时，只需要使用@SentinelResource注解定义即可，很方便。 而且Sentinel默认就已经将 springmvc 的 controller 中的方法注册为sentinel资源了，但是这些方法并没有添加 @SentinelResource 注解呀！ 其实@SentinelResource底层也就是通过AOP + Entry 的方式来手动注册 Sentinel资源的： // 资源名可使用任意有业务语义的字符串，比如方法名、接口名或其它可唯一标识的字符串。 try (Entry entry = SphU.entry(&quot;resourceName&quot;)) { // 被保护的业务逻辑 // do something here... } catch (BlockException ex) { // 资源访问阻止，被限流或被降级 // 在此处进行相应的处理操作 } SentinelResourceAspect切面类： @Aspect public class SentinelResourceAspect extends AbstractSentinelAspectSupport { @Pointcut(&quot;@annotation(com.alibaba.csp.sentinel.annotation.SentinelResource)&quot;) public void sentinelResourceAnnotationPointcut() { } // 经典AOP实现 @Around(&quot;sentinelResourceAnnotationPointcut()&quot;) public Object invokeResourceWithSentinel(ProceedingJoinPoint pjp) throws Throwable { Method originMethod = this.resolveMethod(pjp); SentinelResource annotation = (SentinelResource)originMethod.getAnnotation(SentinelResource.class); if (annotation == null) { throw new IllegalStateException(&quot;Wrong state for SentinelResource annotation&quot;); } else { String resourceName = this.getResourceName(annotation.value(), originMethod); EntryType entryType = annotation.entryType(); int resourceType = annotation.resourceType(); Entry entry = null; try { Object var18; try { // 注册对应的资源 entry = SphU.entry(resourceName, resourceType, entryType, pjp.getArgs()); // 执行具体的业务逻辑 return pjp.proceed(); } catch (Exception e) { ...... } } finally { ...... } } } } 所以，通过Entry手动注册资源 和 通过@SentinelResource 注解自动注入资源，原理上时一样的，都是通过 SphU.entry(…) 方法实现。 4 链路上下文Context public class Context { private final String name; private DefaultNode entranceNode; private Entry curEntry; private String origin = &quot;&quot;; private final boolean async; } Context代表调用链路的上下文，贯穿一次链路调用中的所有资源（Entry），基于ThreadLocal实现； Context维护者入口节点（entranceNode）、当前资源节点（curEntry —&gt;curNode）、调用来源origin等信息； 后续所有的Slot插槽都可以通过context拿到DefaultNode 和 ClusterNode，从而完成统计或判断逻辑； Context创建过程中，会创建EntranceNode，contextName 就是entranceNode的名称； // 创建context，包含两个参数：context名称、 来源名称 ContextUtil.enter(&quot;contextName&quot;, &quot;originName&quot;); 默认情况下，Sentinel的entranceNode是sentinel_default_context，如果我们要想做链路限流，就必须关闭“统一入口配置”，从而让每一个Controller方法为Context的入口。 public final static String CONTEXT_DEFAULT_NAME = &quot;sentinel_default_context&quot;; spring: cloud: sentinel: web-context-unify: false # 关闭context统一入口配置 二 Sentinel源码剖析——Context的初始化 1 spring-cloud-starter-alibaba-sentinel 的 spring.factory 中有两个相关的自动装配类 由于，Context的初始化，涉及到了将Controller中的方法定义为entranceNode的过程，所以肯定是看 SentinelWebAutoConfiguration 这个自动装配类！ 2 向 springmvc 处理链中添加一个Sentinel的拦截器 @Configuration( proxyBeanMethods = false // Lite模式，关闭Full模式 ) @ConditionalOnWebApplication( type = Type.SERVLET ) @ConditionalOnProperty( name = {&quot;spring.cloud.sentinel.enabled&quot;}, matchIfMissing = true ) @ConditionalOnClass({SentinelWebInterceptor.class}) @EnableConfigurationProperties({SentinelProperties.class}) public class SentinelWebAutoConfiguration implements WebMvcConfigurer { // 通过实现 WebMvcConfigurer 接口，允许了手动向springmvc中添加拦截器 Interceptor ...... // 注入本类下文定义的SentinelWebInterceptor @Autowired private Optional&lt;SentinelWebInterceptor&gt; sentinelWebInterceptorOptional; public SentinelWebAutoConfiguration() { } // 添加 public void addInterceptors(InterceptorRegistry registry) { if (this.sentinelWebInterceptorOptional.isPresent()) { Filter filterConfig = this.properties.getFilter(); registry.addInterceptor((HandlerInterceptor)this.sentinelWebInterceptorOptional.get()) .order(filterConfig.getOrder()).addPathPatterns(filterConfig.getUrlPatterns()); } } // 向 IOC 容器中注入一个 SentinelWebInterceptor 拦截器 @Bean @ConditionalOnProperty( name = {&quot;spring.cloud.sentinel.filter.enabled&quot;}, matchIfMissing = true ) public SentinelWebInterceptor sentinelWebInterceptor(SentinelWebMvcConfig sentinelWebMvcConfig) { return new SentinelWebInterceptor(sentinelWebMvcConfig); } } 3 SentinelWebInterceptor拦截器的核心方法 SentinelWebInterceptor 中会对父类 AbstractSentinelInterceptor 中的抽象方法做实现（模板方法模式）： public class SentinelWebInterceptor extends AbstractSentinelInterceptor { // 获取resourceName： // controller中请求方法的路径（资源）：/order/{orderId} protected String getResourceName(HttpServletRequest request) { Object resourceNameObject = request.getAttribute(HandlerMapping.BEST_MATCHING_PATTERN_ATTRIBUTE); if (resourceNameObject != null &amp;&amp; resourceNameObject instanceof String) { String resourceName = (String)resourceNameObject; UrlCleaner urlCleaner = this.config.getUrlCleaner(); if (urlCleaner != null) { resourceName = urlCleaner.clean(resourceName); } if (StringUtil.isNotEmpty(resourceName) &amp;&amp; this.config.isHttpMethodSpecify()) { resourceName = request.getMethod().toUpperCase() + &quot;:&quot; + resourceName; } return resourceName; } else { return null; } } // 获取contextName： // 如果开启了统一入口配置，则contextName就是默认的统一入口：sentinel_spring_web_context // 如果关闭了统一入口配置，则contextName就是当前资源的名称； protected String getContextName(HttpServletRequest request) { return this.config.isWebContextUnify() ? super.getContextName(request) : this.getResourceName(request); } } 而作为一个拦截器，最重要的逻辑，肯定是在 prehandler() 中： public abstract class AbstractSentinelInterceptor implements HandlerInterceptor { public static final String SENTINEL_SPRING_WEB_CONTEXT_NAME = &quot;sentinel_spring_web_context&quot;; private static final String EMPTY_ORIGIN = &quot;&quot;; private final BaseWebMvcConfig baseWebMvcConfig; // 前置拦截的核心逻辑 public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { try { // 获取资源名称，一般是controller方法的@RequestMapping路径，例如/order/{orderId} String resourceName = this.getResourceName(request); if (StringUtil.isEmpty(resourceName)) { return true; } else if (this.increaseReferece(request, this.baseWebMvcConfig.getRequestRefName(), 1) != 1) { return true; } else { // 从request中获取请求来源，将来做 授权规则（来源控制） 判断时会用 String origin = this.parseOrigin(request); // 获取 contextName，默认是sentinel_spring_web_context； // 如果关闭统一入口，那就是当前resourceName String contextName = this.getContextName(request); // 创建Context核心方法 ContextUtil.enter(contextName, origin); // 构建ProcessorSlotChain处理器插槽链的核心逻辑 Entry entry = SphU.entry(resourceName, 1, EntryType.IN); request.setAttribute(this.baseWebMvcConfig.getRequestAttributeName(), entry); return true; } } catch (BlockException var12) { BlockException e = var12; try { this.handleBlockException(request, response, e); } finally { ContextUtil.exit(); } return false; } } // 当请求体业务处理完成后，关闭所有的资源 public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { if (this.increaseReferece(request, this.baseWebMvcConfig.getRequestRefName(), -1) == 0) { Entry entry = this.getEntryInRequest(request, this.baseWebMvcConfig.getRequestAttributeName()); if (entry == null) { ...Log... } else { this.traceExceptionAndExit(entry, ex); // entry.exit()退出 this.removeEntryInRequest(request); ContextUtil.exit(); // contextHolder.set(null); } } } } // private static ThreadLocal&lt;Context&gt; contextHolder = new ThreadLocal&lt;&gt;(); // 保存context的threadLocal 4、ContextUtil.enter(contextName, origin) 创建Context核心方法： // com.alibaba.csp.sentinel.context.ContextUtil#enter public static Context enter(String name, String origin) { // &quot;sentinel_default_context&quot;是不允许被创建的 if (Constants.CONTEXT_DEFAULT_NAME.equals(name)) { throw new ContextNameDefineException( &quot;The &quot; + Constants.CONTEXT_DEFAULT_NAME + &quot; can't be permit to defined!&quot;); } return trueEnter(name, origin); } | | // com.alibaba.csp.sentinel.context.ContextUtil#trueEnter protected static Context trueEnter(String name, String origin) { // 尝试获取context，一般一个新的请求到达后，获取context肯定为null Context context = contextHolder.get(); // 判空 if (context == null) { // 如果为空，开始初始化 Map&lt;String, DefaultNode&gt; localCacheNameMap = contextNameNodeMap; // 尝试获取入口节点 DefaultNode node = localCacheNameMap.get(name); if (node == null) { LOCK.lock(); try { node = contextNameNodeMap.get(name); if (node == null) { // 双重检测锁 // 入口节点为空，初始化入口节点 EntranceNode node = new EntranceNode(new StringResourceWrapper(name, EntryType.IN), null); // 添加入口节点到 ROOT，所有的节点共用一个ROOT根节点 Constants.ROOT.addChild(node); // 将入口节点放入缓存（下次请求时候，根据contextName获取，可直接使用） Map&lt;String, DefaultNode&gt; newMap = new HashMap&lt;&gt;(contextNameNodeMap.size() + 1); newMap.putAll(contextNameNodeMap); newMap.put(name, node); contextNameNodeMap = newMap; // CopyOnWrite } } finally { LOCK.unlock(); } } // 创建Context，参数为：入口节点 和 contextName context = new Context(node, name); // 设置请求来源 origin context.setOrigin(origin); // 将context放入ThreadLocal contextHolder.set(context); } // 返回 return context; } 由此我们可以得出重要结论： 在每一个请求到达时，Sentinel的拦截器都会为本次请求封装一个“链路上下文context”，然后放入到ThreadLocal中，便于请求在后面的处理过程中取用； 默认情况下，“统一入口配置开启”，“链路上下文context”以sentinel-spring-web-context 命名； 如果关闭了“统一入口配置”，“链路上下文context”将以本次请求对应的controller方法的 @RequestMapping() 的值命名，如“/order/{orderId}”； 由于context是放在Thread中的，所以当本次请求结束后，context就会被释放，下次请求需要重新创建；（context生命周期为request） 但是入口 entranceNode 却是放在缓存HashMap中的，所以下一次新的请求到达时，就没有必要再重新创建了；(entranceNode生命周期为应用级) 创建入口方法 entranceNode 时，使用了 双重检测锁 + CopyOnWrite ，因为存在多个请求线程并发情况； 创建 context 过程不需要考虑多线程安全 ，原因也是因为 context时线程内的，单线程 。 三 Sentinel核心源码之ProcessorSlotChain的构建 1 入口方法，正式上文拦截器中创建Context之后的方法 Entry entry = SphU.entry(resourceName, 1, EntryType.IN); 该方法，将“一脉单传”调用到以下方法 entryWithPriority() ： private Entry entryWithPriority(ResourceWrapper resourceWrapper, int count, boolean prioritized, Object... args) throws BlockException { // 获取 Context Context context = ContextUtil.getContext(); if (context == null) { // Using default context. context = InternalContextUtil.internalEnter(Constants.CONTEXT_DEFAULT_NAME); } // 获取 Slot执行链，同一个资源（如：/order/{orderId}），会创建一个执行链，放入缓存 ProcessorSlot&lt;Object&gt; chain = lookProcessChain(resourceWrapper); // 创建 Entry，并将 resource、chain、context 记录在 Entry中 Entry e = new CtEntry(resourceWrapper, chain, context); try { // 执行 slotChain chain.entry(context, resourceWrapper, null, count, prioritized, args); } catch (BlockException e1) { // 如果执行 slotChain 过程中发生异常，也直接将对应的资源释放 e.exit(count, args); ...... } return e; } 2 lookProcessChain() 创建或获取资源对应的ProcessorSlotChain的方法 // com.alibaba.csp.sentinel.CtSph#lookProcessChain ProcessorSlot&lt;Object&gt; lookProcessChain(ResourceWrapper resourceWrapper) { // 从缓存chainMap中获取 ProcessorSlotChain chain = chainMap.get(resourceWrapper); if (chain == null) { synchronized (LOCK) { chain = chainMap.get(resourceWrapper); if (chain == null) { // 又是双重检测锁 // Entry size limit. if (chainMap.size() &gt;= Constants.MAX_SLOT_CHAIN_SIZE) { return null; } // 入口本资源对应的chain不存在，则创建一个新的 chain = SlotChainProvider.newSlotChain(); Map&lt;ResourceWrapper, ProcessorSlotChain&gt; newMap = new HashMap&lt;ResourceWrapper, ProcessorSlotChain&gt;( chainMap.size() + 1); newMap.putAll(chainMap); newMap.put(resourceWrapper, chain); chainMap = newMap; // 又是CopyOnWrite } } } return chain; } 虽然每一次请求的ResourceWrapper都是新new的，但是由于它的hashCode() 和 equals() 方法，只会对比 name; public abstract class ResourceWrapper { protected final String name; protected final EntryType entryType; protected final int resourceType; @Override public int hashCode() { return getName().hashCode(); } @Override public boolean equals(Object obj) { if (obj instanceof ResourceWrapper) { ResourceWrapper rw = (ResourceWrapper)obj; return rw.getName().equals(getName()); } return false; } } 所以，得出结论： Sentinel会为所有的资源，以资源名为区分，创建对应的ProcessorSlotChain ，并缓存到chainMap中； ProcessorSlotChain应用级有效，创建后，下次相同名称的Resource请求进入时，将不需要再次创建chain； 3 SlotChainProvider.newSlotChain() 处理器插槽链的构建过程 // com.alibaba.csp.sentinel.slotchain.SlotChainProvider#newSlotChain public static ProcessorSlotChain newSlotChain() { if (slotChainBuilder != null) { return slotChainBuilder.build(); } // 默认肯定是得到一个 DefaultSlotChainBuilder slotChainBuilder = SpiLoader.loadFirstInstanceOrDefault(SlotChainBuilder.class, DefaultSlotChainBuilder.class); if (slotChainBuilder == null) { slotChainBuilder = new DefaultSlotChainBuilder(); } else { ...... } return slotChainBuilder.build(); } public class DefaultSlotChainBuilder implements SlotChainBuilder { @Override public ProcessorSlotChain build() { // 创建一个 DefaultProcessorSlotChain ProcessorSlotChain chain = new DefaultProcessorSlotChain(); // 该方法会通过spi机制从 \\META-INF\\services\\目录下，加载所有的ProcessorSlot类 List&lt;ProcessorSlot&gt; sortedSlotList = SpiLoader.loadPrototypeInstanceListSorted(ProcessorSlot.class); for (ProcessorSlot slot : sortedSlotList) { if (!(slot instanceof AbstractLinkedProcessorSlot)) { continue; } // 最终创建的 chain.addLast((AbstractLinkedProcessorSlot&lt;?&gt;) slot); } return chain; } } 4 SpiLoader.loadPrototypeInstanceListSorted(ProcessorSlot.class)通过SPI机制加载所有的ProcessorSlot插槽类 // com.alibaba.csp.sentinel.util.SpiLoader#loadPrototypeInstanceListSorted public static &lt;T&gt; List&lt;T&gt; loadPrototypeInstanceListSorted(Class&lt;T&gt; clazz) { try { // Not use SERVICE_LOADER_MAP, to make sure the instances loaded are different. ServiceLoader&lt;T&gt; serviceLoader = ServiceLoaderUtil.getServiceLoader(clazz); List&lt;SpiOrderWrapper&lt;T&gt;&gt; orderWrappers = new ArrayList&lt;&gt;(); // SPI机制会从本地的 META-INF/services/ 目录下加载 ProcessorSlot 列表； for (T spi : serviceLoader) { int order = SpiOrderResolver.resolveOrder(spi); // Since SPI is lazy initialized in ServiceLoader, we use online sort algorithm here. SpiOrderResolver.insertSorted(orderWrappers, spi, order); } List&lt;T&gt; list = new ArrayList&lt;&gt;(orderWrappers.size()); for (int i = 0; i &lt; orderWrappers.size(); i++) { list.add(orderWrappers.get(i).spi); } return list; } catch (Throwable t) { t.printStackTrace(); return new ArrayList&lt;&gt;(); } } 本地 META/services/ 目录下的 ProcessorSlot文件定义了9个插槽！ 5 最终构建成的ProcessorSlotChain的结构 首先，所有的9大ProcessorSlot都继承于一个AbstractLinkedProcessorSlot类： public abstract class AbstractLinkedProcessorSlot&lt;T&gt; implements ProcessorSlot&lt;T&gt; { private AbstractLinkedProcessorSlot&lt;?&gt; next = null; // fireEntry的作用主要就是让请求流转到下一个ProcessorSlot(如果存在的话) @Override public void fireEntry(Context context, ResourceWrapper resourceWrapper, Object obj, int count, boolean prioritized, Object... args) throws Throwable { if (next != null) { next.transformEntry(context, resourceWrapper, obj, count, prioritized, args); } } // 所有ProcessorSlot的入口方法，其中会通过模板方法模式，调用各自的entry处理逻辑 // 而再所有的处理逻辑的最后，都会再调一次 fireEntry() 方法 void transformEntry(Context context, ResourceWrapper resourceWrapper, Object o, int count, boolean prioritized, Object... args) throws Throwable { T t = (T)o; entry(context, resourceWrapper, t, count, prioritized, args); } } 经过 next 指向，最终构建出来的 DefaultProcessorSlotChain 如下： 综上总结： Sentinel会 为所有的资源，以资源名为区分，创建各自的DefaultProcessorSlotChain，放在缓存 中； DefaultProcessorSlotChain的 每一个ProcessorSlot插槽都是通过SPI机制从 META/services/ 目录下加载的 ； 每一个ProcessorSlot 其实是一个 AbstractLinkedProcessorSlot 抽象链表处理器插槽，有一个next属性，指向下一个Slot ，当某一个Slot执行完后，会调用 fireEntry() 方法，将请求转到下一个Slot继续执行。 最终完成责任链上所有ProcessorSlot的逻辑！ 四 九大ProcessorSlot处理器插槽的工作原理 LogSlot插槽是一个边缘插槽，做一些日志记录，所以不算重要，排除在外后，就剩8大插槽，也就是&lt;第一章节&gt;列出的八大插槽： 数据统计部分 + 规则判断部分 数据统计： NodeSelectorSlot：负责构建簇点链路中的各个节点（DefaultNode），形成NodeTree ClusterBuilderSlot：负责构建某个资源的ClusterNode（具体的DefaultNode和ClusterNode的区别见下文） StatisticSlot：负责实时统计请求的各种调用信息，如来源信息、请求次数、运行信息等； 规则判断： AuthoritySlot：授权规则判断（来源控制） SystemSlot：系统保护规则判断，当系统资源使用量达到一定程度后，拒绝新的请求进入等； ParamFlowSlot：热点参数限流规则判断 FlowSolt：普通限流规则判断 DegradeSlot：降级规则判断 其实，当Sentinel的整体架构，和调用逻辑梳理清楚后，每一个责任链节点的处理逻辑就很简单了 ","link":"https://tinaxiawuhao.github.io/post/xdN8U8BuX/"},{"title":"Sentinel限流熔断降级——知识点总结","content":" Sentinei官网地址：https://sentinelguard.io/zh-cn/docs/quick-start.html Sentinel Github地址：https://github.com/alibaba/Sentinel/wiki Sentinel生产级使用：https://github.com/alibaba/Sentinel/wiki/在生产环境中使用-Sentinel 一 Sentinel是什么,为什么要引入Sentinel 1 什么是“服务雪崩”？ 在微服务架构中，存在着大量的服务间调用，有时候，由于某个服务发生了故障，而导致调用它的服务的资源得不到正常释放，从而也发生故障，故障不断传播，最终导致整个微服务无法对外提供服务，这就发生了“服务雪崩”！ 2 “服务雪崩”如何解决？ 解决“服务雪崩”的核心点有2个：尽量控制流量，不要让服务因为过载还出现故障；当一个服务已经出现故障时，不要让故障在微服务整个调用链路中进行传播！ 针对上面两点，“服务雪崩”就有了常见的2大类解决方案： 流量控制，避免出现故障： 流量控制：在微服务调用链路的各个核心资源点，进行流量控制，超出的流量直接不要放行，以对目标资源进行直接保护！ 出现故障时，防止故障传播： 超时处理：设置超时时间，超过设定时间就返回报错信息，做对应处理，防止“无休止”等待！ 服务隔离：限定每个业务能使用的线程数，避免耗尽整个服务的所有线程资源，保护其他业务！ 熔断降级：通过断路器统计服务调用的“异常数”或“异常比例”，如果超出设定阈值，则直接熔断，后续一定时间内的请求到达时，直接返回降级处理的结果，而不真正去调用目标服务！ 而上面所说的这一系列解决方案，Sentinel都为我们提供了实现，所以我们选择Sentinel！ 3 阿里的Sentinel和网飞的Hystrix的对比？ 其实很明显，Sentinel比Hystrix强大很多，而且Hystrix已经进入维护期不再更新了，以后的微服务项目中肯定是选择Sentinel！ 二 Sentinel的简单使用和 首先虽然Sentinel是作为SpringCloud Alibaba的重要组件而出名，但是并不是强依赖于SpringCloud Alibaba！ 独立使用Sentinel文档：https://sentinelguard.io/zh-cn/docs/quick-start.html 在spring-cloud-alibaba框架中使用：https://github.com/alibaba/spring-cloud-alibaba/wiki/Sentinel 1 在spring-cloud-alibaba框架中的简单使用Sentinel 引入依赖starter： &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; 启动Sentinel Dashboard控制台： https://sentinelguard.io/zh-cn/docs/dashboard.html # Sentinel的启动非常简单，java -jar sentinel-dashboard.jar 运行即可： java -Dserver.port=8080 -Dcsp.sentinel.dashboard.server=localhost:8080 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard.jar 在我们的项目文件中配置sentinel-dashboard控制台的地址： spring: cloud: sentinel: transport: dashboard: localhost:8080 2 开启Sentinel对Feign调用的支持 除了上面的依赖和配置外，我们还需要增加一下配置： 如果我们引入的Feign是通过 spring-cloud-starter-openfeign 引入Feign的，那么 Sentinel starter 中的自动配置类就会生效！ 此时，我们只需要在 application.yml 配置文件中开启 Sentinel 对 Feign 的支持即可： feign: sentinel: enabled: true # 开启feign对sentinel的支持 3 Sentinel Dashboard 的配置持久化到 Nacos 配置中心 https://github.com/alibaba/Sentinel/wiki/在生产环境中使用-Sentinel Sentinel Dashboard中的所有配置在默认情况下，刷新后就会丢失，那么实际生产中肯定是可允许的，所以我们必须要想办法将Sentinel的配置持久化； 而正好Nacos就提供了配置的“持久化”与“监听机制”，所以我们就选择通过 Nacos 完成 Sentinel 的配置持久化！（push模式，也是生产中最常用的） 在项目的pom.xml中增加 sentinel-datasource-nacos 的依赖，开启对nacos的配置中心的监听： &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-datasource-nacos&lt;/artifactId&gt; &lt;/dependency&gt; 在 application.yml 中增加 sentinel 的数据源配置： spring: application: name: orderservice cloud: sentinel: transport: dashboard: localhost:8080 # sentinel控制台地址 web-context-unify: false # 关闭context整合 datasource: flow: nacos: server-addr: localhost:8848 # nacos地址 dataId: orderservice-flow-rules groupId: SENTINEL_GROUP rule-type: flow # 还可以是：degrade、authority、param-flow feign: sentinel: enabled: true # 开启feign对sentinel的支持 4 Sentinel的三种规则配置管理模式 原始模式：如果我们简单使用Dashboard，不做任何修改，就是采用的这种模式： ​ 客户端在启动时，会同时启动一个ServerSocket，默认端口为：8719（如果被占用，尝试3次后+1，继续尝试），告诉Dashboard，当Dashboard中的配置有变化时，通过API接口告诉我们的服务中的Sentinel客户端，Sentinel将这些配置存到内存中，服务重启即丢失，该模式只能用于简单测试，一定不能用于生产环境！ 可从Env类一路向下研究，就可以知道逻辑： public class Env { public static final Sph sph = new CtSph(); static { // If init fails, the process will exit. InitExecutor.doInit(); } } Pull模式：Sentinel客户端（我们的应用服务）向远端的配置管理中心主动定期轮询拉取规则，更新到内存缓存，同时写入到本地磁盘文件，这样的话也可以实现持久化，但是数据一致性、实时性不太好，而且大量的轮询对服务性能又有影响！ // 需要在客户端注册数据源： // —将对应的读数据源注册至对应的 RuleManager， // ——将写数据源注册至 transport 的 WritableDataSourceRegistry 中。 public class FileDataSourceInit implements InitFunc { @Override public void init() throws Exception { String flowRulePath = &quot;xxx&quot;; ReadableDataSource&lt;String, List&lt;FlowRule&gt;&gt; ds = new FileRefreshableDataSource&lt;&gt;( flowRulePath, source -&gt; JSON.parseObject(source, new TypeReference&lt;List&lt;FlowRule&gt;&gt;() {}) ); // 将可读数据源注册至 FlowRuleManager. FlowRuleManager.register2Property(ds.getProperty()); WritableDataSource&lt;List&lt;FlowRule&gt;&gt; wds = new FileWritableDataSource&lt;&gt;(flowRulePath, this::encodeJson); // 将可写数据源注册至 transport 模块的 WritableDataSourceRegistry 中. // 这样收到控制台推送的规则时，Sentinel 会先更新到内存，然后将规则写入到文件中. WritableDataSourceRegistry.registerFlowDataSource(wds); } private &lt;T&gt; String encodeJson(T t) { return JSON.toJSONString(t); } } Push模式：生产环境最推荐的一种模式，需要依赖于外部的注册中心，nacos、zookeeper等 我们在Sentinel Dashboard中修改的规则配置，首先会先持久化到我们的配置中心中（配置中心已经实现了持久化）； sentinel客户端（我们的应用程序）是从配置中心获取数据。 具体的Push模式（nacos）的实现已经在“第3段”有讲述； 但是暂时默认开源的Sentinel Dashboard中并没有直接提供对nacos等注册中心的直接支持，得自己改造，或者直接去Nacos中盲配！ 三 Sentinel Dashboard中得各种规则配置与解读 首先，当我们第一次打开Sentinel Dashboard时，我们会发现控制台空空如也，这是由于Sentinel使用的是懒加载，所以，我们需要先调用一次服务的接口，然后才可以看到“簇点链路”： 之后我们就可以开始愉快的配置啦！ 本章节，不做具体配置和调试，主要是对所有的配置进行解释，加深印象！ 1 流控规则：见名知意，流量控制 2种阈值类型： QPS：对单位时间内的请求数量进行统计，控制流量； 线程数：属于服务隔离（线程隔离），Sentinel默认使用的是“信号量隔离”，而Hystrix默认采用的是“线程池隔离” 信号量隔离：通过“信号量计数器”实现，开销小，但是隔离性一般；适合“扇出”大的场景，如gateway就是扇出比较大的场景； 线程池隔离：基于线程池实现，额外开销大，但是隔离性更强；适合于“扇出”小的场景； 3种流控模式： 直接：流量统计对象 和 限流对象 都是当前资源本身； 关联：流量统计的对象是其它资源，限流的对象却是自己； 链路：只统计从指定链路访问本资源的请求，触发阈值时，也只对指定的链路进行限流； 注意，默认情况下，所有的请求的入口链路都是默认的： sentinel_default_context ，链路模式是不好使用的，所以，如果要使用链路模式，需要关闭“context统一入口配置” spring: application: name: orderservice cloud: sentinel: transport: dashboard: localhost:8080 # sentinel控制台地址 web-context-unify: false # 关闭context统一入口 3种流控效果： 快速失败：达到限流阈值后，直接抛出FlowException异常； WarmUp预热：与“快速失败”类似，达到阈值后，也是直接抛出FlowException异常，但是该模式下的阈值是变化的，默认从一个最大值的1/3，逐渐增加到最大值；常用于服务的预热启动阶段，防止服务流量一下子打到最大，导致一些非常态问题发生； 排队等待（流量整形）：请求到达后，放入一个队列中，按照 1/QPS 的速度进行消费处理，在队列中的请求等待时间，最大不可以超过“设定的超时时间”！ 2 热点规则（热点限流规则）—— 特殊的流控规则 由于Sentinel默认知乎将Springmvc的注解如@RequestMapping等注册为资源，所以当我们需要定义其它资源时，需要手动使用@SentinelResource注解去定义： @SentinelResource(&quot;hot&quot;) public Order queryOrderById(Long orderId) { // 1.查询订单 Order order = orderMapper.findById(orderId); // 2.用Feign远程调用 User user = userClient.findById(order.getUserId()); // 3.封装user到Order order.setUser(user); // 4.返回 return order; } 之后，当我们调用过一次后，就可以看到“hot”这个资源了，而后对它进行流控配置： 上图的热点流控规则解读为：对于hot这个资源，对于它的第0个请求参数，允许它1秒内相同值的最大请求数为5，同时，包含一个特殊情况，对于value=101，允许的QPS阈值为10。 3 降级规则（熔断降级） 熔断降级通常配置 Feign 服务间调用使用，我们通常会定义两个类 UserClient代理接口 + UserClientFallbackFactory降级工厂类： UserClient： @FeignClient(value = &quot;userservice&quot;, fallbackFactory = UserClientFallbackFactory.class) public interface UserClient { @GetMapping(&quot;/user/{id}&quot;) User findById(@PathVariable(&quot;id&quot;) Long id); } UserClientFallbackFactory： @Slf4j @Component public class UserClientFallbackFactory implements FallbackFactory&lt;UserClient&gt; { @Override public UserClient create(Throwable throwable) { return new UserClient() { @Override public User findById(Long id) { log.info(&quot;请求用户数据失败&quot;); return new User().setId(100L).setUsername(&quot;default&quot;).setAddress(&quot;defaultAddress&quot;); } }; } } 上图的熔断降级规则解读为： 当ResponseTime时间超过400ms时为慢调用，统计最近10000ms内的请求，如果请求总数超过10次，且慢调用的比例达到0.6，则触发熔断OPEN； 熔断时常为5秒； 5秒后，进入HALF-OPEN状态，放行一次请求做测试，如果OK，则断路器重新CLOSE闭合，开始正常工作！ 4、授权规则（对请求的来源做控制） 有时候，我们担心由于服务暴露，导致有些请求会越过Gateway，而直接访问我们的服务，这时候，我们就可以通过Sentinel授权只允许从Gateway过来的请求访问我们的服务。 在gateway中通过filter为经过的请求增加一个header值 origin = gateway： spring: application: name: gateway cloud: nacos: server-addr: localhost:8848 # nacos地址 gateway: routes: - id: user-service # 路由标示，必须唯一 uri: lb://userservice # 路由的目标地址 predicates: # 路由断言，判断请求是否符合规则 - Path=/user/** # 路径断言，判断路径是否是以/user开头，如果是则符合 - id: order-service uri: lb://orderservice predicates: - Path=/order/** default-filters: - AddRequestHeader=origin,gateway 在项目中注入一个RequestOriginParser： @Component public class HeaderOriginParser implements RequestOriginParser { @Override public String parseOrigin(HttpServletRequest request) { // 1.获取请求头 String origin = request.getHeader(&quot;origin&quot;); // 2.非空判断 if (StringUtils.isEmpty(origin)) { origin = &quot;blank&quot;; } return origin; } } 最后，配置一条授权规则白名单： 上图授权规则解读：只允许请求头中，origin = gateway 的请求通过！ 四 补充其他知识点 1 默认情况下，发生限流、授权拦截时，会直接抛出异常到调用方，很不友好，最好要自定义异常返回结果 需要为 BlockExceptionHandler 写一个实现： @Component public class SentinelExceptionHandler implements BlockExceptionHandler { @Override public void handle(HttpServletRequest request, HttpServletResponse response, BlockException e) throws Exception { String msg = &quot;未知异常&quot;; int status = 429; if (e instanceof FlowException) { msg = &quot;请求被限流了&quot;; } else if (e instanceof ParamFlowException) { msg = &quot;请求被热点参数限流&quot;; } else if (e instanceof DegradeException) { msg = &quot;请求被降级了&quot;; } else if (e instanceof AuthorityException) { msg = &quot;没有权限访问&quot;; status = 401; } response.setContentType(&quot;application/json;charset=utf-8&quot;); response.setStatus(status); response.getWriter().println(&quot;{\\&quot;msg\\&quot;: &quot; + msg + &quot;, \\&quot;status\\&quot;: &quot; + status + &quot;}&quot;); } } 2 不同的流控规则，使用的技术实现方案不一样 对于限流的需求，常见的实现方案有多种，滑动时间窗口、令牌桶、漏桶，这三种有各自擅长的业务场景，而Sentinel支持的业务场景很多，在不同的场景下，就选择了不同的限流实现。 快速失败、WarmUp预热：滑动时间窗口 热点限流：令牌桶 排队等待：漏桶 具体的限流实现，请参阅另一篇文章：Sentinel源码拓展之——限流的各种实现方式 ","link":"https://tinaxiawuhao.github.io/post/XPUt_T_-A/"},{"title":"openfeign-ribbon核心源码剖析","content":"一、总结前置： 1 ribbon,feign,openfeign三者的对比 我们现在工作中现在几乎都是直接使用openfeign，而我们很有必要了解一下，ribbon、feign、openfeign三者之间的关系！ ribbon：ribbon是netflix开源的客户端负载均衡组件，可以调用注册中心获取服务列表，并通过负载均衡算法挑选一台Server； **feign：**feign也是netflix开源的，是对ribbon的一层封装，通过接口的方式使用，更加优雅，不需要每次都去写restTemplate，只需要定义一个接口来使用；但是Feign不支持springmvc的注解； openfeign：是对feign的进一步封装，使其能够支持springmvc注解，如@GetMapping等，使用起来更加方便和优雅！ 2 工作中都是如何使用openfeign // 想要使用openfeign，必须要在启动类增加@EnableFeignClient注解，否则启动报错 @FeignClient(&quot;feign-provider&quot;) public interface OrderService { @GetMapping(&quot;/order/get/{id}&quot;) public String getById(@PathVariable(&quot;id&quot;) String id); } 3 openfeign与ribbon如何分工 openfeign通过动态代理的方式，对feignclient注解修饰的类进行动态代理，拼接成临时URL：http://feign-provider/order/get/100，交给ribbon ribbon通过自己的拦截器，截取出serviceName在服务注册表中找到对应的serverList，并通过负载均衡策略挑选一台Server，拼接成最终的URL：http://10.206.73.156:1111/order/get/100，交还给feign； 最后，feign通过自己封装的Client对目标地址发起调用，并获得返回结果；（Client是对原生 java.net 中的URL类的封装，实现远程调用） 二 核心代码——Openfeign生成的动态代理类是啥 1 启动类必须增加@EnableFeignClient，那我们就从这里入口 @EnableFeignClients @Import(FeignClientsRegistrar.class) // 遇到import(Registrar)，我们就去看它的registerBeanDefinitions()方法： public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) { registerDefaultConfiguration(metadata, registry); // 注册FeignClient，其实FeignClient就是我们需要的动态代理类 registerFeignClients(metadata, registry); } public void registerFeignClients(AnnotationMetadata metadata,BeanDefinitionRegistry registry) { ...... // 定义过滤器，把加了@FeignClient注解的接口都过滤出来 AnnotationTypeFilter annotationTypeFilter = new AnnotationTypeFilter(FeignClient.class); ...过滤逻辑... for (String basePackage : basePackages) { Set&lt;BeanDefinition&gt; candidateComponents = scanner .findCandidateComponents(basePackage); for (BeanDefinition candidateComponent : candidateComponents) { if (candidateComponent instanceof AnnotatedBeanDefinition) { // verify annotated class is an interface AnnotatedBeanDefinition beanDefinition = (AnnotatedBeanDefinition) candidateComponent; AnnotationMetadata annotationMetadata = beanDefinition.getMetadata(); Assert.isTrue(annotationMetadata.isInterface(), &quot;@FeignClient can only be specified on an interface&quot;); Map&lt;String, Object&gt; attributes = annotationMetadata .getAnnotationAttributes( FeignClient.class.getCanonicalName()); String name = getClientName(attributes); registerClientConfiguration(registry, name, attributes.get(&quot;configuration&quot;)); // 将为每一个过滤出来的接口，注册FeignClient registerFeignClient(registry, annotationMetadata, attributes); } } } } private void registerFeignClient(BeanDefinitionRegistry registry, AnnotationMetadata annotationMetadata, Map&lt;String, Object&gt; attributes) { String className = annotationMetadata.getClassName(); BeanDefinitionBuilder definition = BeanDefinitionBuilder .genericBeanDefinition(FeignClientFactoryBean.class); ...目标FactoryBean为FeignClientFactoryBean... ...FactoryBean一般用于定制化一些特殊的Bean，spring会调用它的getObject接口... BeanDefinitionHolder holder = new BeanDefinitionHolder(beanDefinition, className, new String[] { alias }); BeanDefinitionReaderUtils.registerBeanDefinition(holder, registry); } 2 见名知意FeignClient的工厂bean：FeignClientFactoryBean.getObject()方法 class FeignClientFactoryBean{ @Override public Object getObject() throws Exception { return getTarget(); } &lt;T&gt; T getTarget() { FeignContext context = this.applicationContext.getBean(FeignContext.class); Feign.Builder builder = feign(context); // 当我们不为@FeignClient()注解配置url属性时 // 这里同时会根据注解，拼接出url前缀，如：http://feign-provider if (!StringUtils.hasText(this.url)) { if (!this.name.startsWith(&quot;http&quot;)) { this.url = &quot;http://&quot; + this.name; } else { this.url = this.name; } this.url += cleanPath(); return (T) loadBalance(builder, context, new HardCodedTarget&lt;&gt;(this.type, this.name, this.url)); } ...如果我们为@FeignClient()注解配置url属性之后的逻辑... } } protected &lt;T&gt; T loadBalance(Feign.Builder builder, FeignContext context, HardCodedTarget&lt;T&gt; target) { // 从容器中获取一个类型为Client的bean，那么IOC容器中肯定有一个这样的Bean Client client = getOptional(context, Client.class); if (client != null) { builder.client(client); Targeter targeter = get(context, Targeter.class); return targeter.target(this, builder, context, target); } // 如果找不到类型为Client的Bean就是有问题的啦！ throw new IllegalStateException( &quot;No Feign Client for loadBalancing defined. Did you forget to include spring-cloud-starter-netflix-ribbon?&quot;); } 3 容器中类型为Client的Bean是谁 此时还是启动阶段，我们去看看有哪些bean会被注入，在 spring.factories 中，我们会找到两个自动配置类： 这两个类，很关键，都会往容器中注入负载均衡的Client Bean，但是只会有一个成功， FeignRibbonClientAutoCOnfiguration类： @ConditionalOnClass({ ILoadBalancer.class, Feign.class }) @ConditionalOnProperty(value = &quot;spring.cloud.loadbalancer.ribbon.enabled&quot;,matchIfMissing = true) @Configuration(proxyBeanMethods = false) // 重点1：在FeignAutoConfiguration之前装载配置 @AutoConfigureBefore(FeignAutoConfiguration.class) @EnableConfigurationProperties({ FeignHttpClientProperties.class }) @Import({ HttpClientFeignLoadBalancedConfiguration.class, OkHttpFeignLoadBalancedConfiguration.class, DefaultFeignLoadBalancedConfiguration.class }) // 重点2： public class FeignRibbonClientAutoConfiguration { ...非重点... } @Configuration(proxyBeanMethods = false) class DefaultFeignLoadBalancedConfiguration { @Bean @ConditionalOnMissingBean // 如果Client类型的bean已经存在，则不执行 public Client feignClient(CachingSpringLoadBalancerFactory cachingFactory, SpringClientFactory clientFactory) { return new LoadBalancerFeignClient(new Client.Default(null, null), cachingFactory, clientFactory); } } FeignLoadBalancerAutoConfiguration类： @ConditionalOnClass(Feign.class) @ConditionalOnBean(BlockingLoadBalancerClient.class) @AutoConfigureBefore(FeignAutoConfiguration.class) // 重点1：在FeignRibbonClientAutoConfiguration之后装载配置 @AutoConfigureAfter(FeignRibbonClientAutoConfiguration.class) @EnableConfigurationProperties(FeignHttpClientProperties.class) @Configuration(proxyBeanMethods = false) @Import({ HttpClientFeignLoadBalancerConfiguration.class, OkHttpFeignLoadBalancerConfiguration.class, DefaultFeignLoadBalancerConfiguration.class }) // 重点2： public class FeignLoadBalancerAutoConfiguration { ...非重点... } @Configuration(proxyBeanMethods = false) class DefaultFeignLoadBalancerConfiguration { @Bean @ConditionalOnMissingBean // 如果Client类型的bean已经存在，则不执行 public Client feignClient(BlockingLoadBalancerClient loadBalancerClient) { return new FeignBlockingLoadBalancerClient(new Client.Default(null, null), loadBalancerClient); } } 到这里就非常清晰了： 通过spring.factories注入了两个配置类，并通过 @AutoConfigureBefore 和 @AutoConfigureAfter强制了两个配置类的装载顺序！ 这两个配置类各import 了 另一个配置类：DefaultFeignLoadBalancedConfiguration（注入：LoadBalancerFeignClient） 和 DefaultFeignLoadBalancerConfiguration（注入：FeignBlockingLoadBalancerClient），但是由于@ConditionalOnMissingBean注解，只有一个前面那个会注入成功！ 所以，结论就是：IOC容器中的“Client”类型的 Bean 为 “LoadBalancerFeignClient” 所以，最终的结论就是：Openfeign通过的动态代理，为每个“@FeignClient”注解修饰的接口生成一个类型为 “LoadBalancerFeignClient”的代理类。 三 FeignClient代理类执行时，是如何使用Ribbon的 1 当执行LoadBalancerFeignClient.execute()方法时： // org.springframework.cloud.openfeign.ribbon.LoadBalancerFeignClient#execute public Response execute(Request request, Request.Options options) throws IOException { try { URI asUri = URI.create(request.url()); String clientName = asUri.getHost(); URI uriWithoutHost = cleanUrl(request.url(), clientName); FeignLoadBalancer.RibbonRequest ribbonRequest = new FeignLoadBalancer.RibbonRequest( this.delegate, request, uriWithoutHost); // 可以获取到nacos的一些信息 IClientConfig requestConfig = getClientConfig(options, clientName); // lbClient(clientName)：可以获得具体的代理类，每个@FeignClient修饰的接口代理类时独立的 return lbClient(clientName) .executeWithLoadBalancer(ribbonRequest, requestConfig).toResponse(); } catch (ClientException e) { ...... } } 我们可以很清楚的看到，这里肯定会要用到Ribbon； 2 通过负载均衡器，执行调用，并返回结果 // com.netflix.client.AbstractLoadBalancerAwareClient#executeWithLoadBalancer(...) public T executeWithLoadBalancer(final S request, final IClientConfig requestConfig) throws ClientException { LoadBalancerCommand&lt;T&gt; command = buildLoadBalancerCommand(request, requestConfig); try { return command.submit( // 这步会返回具体的，负载均衡选定好的Server new ServerOperation&lt;T&gt;() { @Override public Observable&lt;T&gt; call(Server server) { // submit选好的Server作为入参 URI finalUri = reconstructURIWithServer(server, request.getUri()); S requestForServer = (S) request.replaceUri(finalUri); try { return Observable.just(AbstractLoadBalancerAwareClient.this.execute(requestForServer, requestConfig)); } catch (Exception e) { return Observable.error(e); } } }) .toBlocking() .single(); } catch (Exception e) { ...... } } // com.netflix.loadbalancer.reactive.LoadBalancerCommand#submit public Observable&lt;T&gt; submit(final ServerOperation&lt;T&gt; operation) { final ExecutionInfoContext context = new ExecutionInfoContext(); ...... final int maxRetrysSame = retryHandler.getMaxRetriesOnSameServer(); final int maxRetrysNext = retryHandler.getMaxRetriesOnNextServer(); // selectServer()方法会通过负载均衡选择一台Server Observable&lt;T&gt; o = (server == null ? selectServer() : Observable.just(server)) .concatMap(new Func1&lt;Server, Observable&lt;T&gt;&gt;() { ...... }); } 3 selectServer()具体方法 // com.netflix.loadbalancer.reactive.LoadBalancerCommand#selectServer private Observable&lt;Server&gt; selectServer() { return Observable.create(new OnSubscribe&lt;Server&gt;() { @Override public void call(Subscriber&lt;? super Server&gt; next) { try { Server server = loadBalancerContext.getServerFromLoadBalancer(loadBalancerURI, loadBalancerKey); next.onNext(server); next.onCompleted(); } catch (Exception e) { next.onError(e); } } }); } 4 这里将使用到非常重要的一个ZoneAwareLoadBalancer // com.netflix.loadbalancer.LoadBalancerContext#getServerFromLoadBalancer public Server getServerFromLoadBalancer(@Nullable URI original, @Nullable Object loadBalancerKey) throws ClientException { String host = null; int port = -1; if (original != null) { host = original.getHost(); } if (original != null) { Pair&lt;String, Integer&gt; schemeAndPort = deriveSchemeAndPortFromPartialUri(original); port = schemeAndPort.second(); } // 关键一步，获得了一个ILoadBalancer ILoadBalancer lb = getLoadBalancer(); if (host == null) { // 重要：这里其实获得的是ZoneAwareLoadBalancer类的Bean if (lb != null){ Server svc = lb.chooseServer(loadBalancerKey); if (svc == null){ throw new ClientException(ClientException.ErrorType.GENERAL, &quot;Load balancer does not have available server for client: &quot; + clientName); } host = svc.getHost(); return svc; } else { ...... } } else { ...... } return new Server(host, port); } // com.netflix.loadbalancer.BaseLoadBalancer#chooseServer public Server chooseServer(Object key) { if (!ENABLED.get() || getLoadBalancerStats().getAvailableZones().size() &lt;= 1) { logger.debug(&quot;Zone aware logic disabled or there is only one zone&quot;); return super.chooseServer(key); } ...其它分支在国内一般不会走，没有zone的概念... } // com.netflix.loadbalancer.BaseLoadBalancer#chooseServer public Server chooseServer(Object key) { if (counter == null) { counter = createCounter(); } counter.increment(); if (rule == null) { return null; } else { try { return rule.choose(key); // PredicateBasedRule } catch (Exception e) { return null; } } } // com.netflix.loadbalancer.PredicateBasedRule#choose public Server choose(Object key) { ILoadBalancer lb = getLoadBalancer(); Optional&lt;Server&gt; server = getPredicate().chooseRoundRobinAfterFiltering(lb.getAllServers(), key); if (server.isPresent()) { return server.get(); } else { return null; } } 5 可以看到 lb.getAllServer()，正是ZoneAwareLoadBalancer.getAllServers()方法 // com.netflix.loadbalancer.BaseLoadBalancer#getAllServers public class BaseLoadBalancer extends AbstractLoadBalancer { protected volatile List&lt;Server&gt; allServerList = Collections.synchronizedList(new ArrayList&lt;Server&gt;()); public List&lt;Server&gt; getAllServers() { return Collections.unmodifiableList(allServerList); } } 所以，我们现在用弄清楚的有两点： 为什么重要的ILoadBalancer的实现就是ZoneAwareLoadBalancer； ZoneAwareLoadBalancer中的allServerList属性是何时被赋值的； 四 ZoneAwareLoadBalancer是何时注入的 1 在spring-cloud-netfliex-ribbon包的spring.factories中有RibbonAutoConfiguration类 我们看到这个类的构造方法，创建了一个SpringClientFactory工厂类： @Configuration @Conditional(RibbonAutoConfiguration.RibbonClassesConditions.class) @RibbonClients @AutoConfigureAfter(name = &quot;org.springframework.cloud.netflix.eureka.EurekaClientAutoConfiguration&quot;) @AutoConfigureBefore({ LoadBalancerAutoConfiguration.class, AsyncLoadBalancerAutoConfiguration.class }) @EnableConfigurationProperties({ RibbonEagerLoadProperties.class, ServerIntrospectorProperties.class }) public class RibbonAutoConfiguration { @Autowired(required = false) private List&lt;RibbonClientSpecification&gt; configurations = new ArrayList&lt;&gt;(); @Autowired private RibbonEagerLoadProperties ribbonEagerLoadProperties; @Bean public HasFeatures ribbonFeature() { return HasFeatures.namedFeature(&quot;Ribbon&quot;, Ribbon.class); } @Bean public SpringClientFactory springClientFactory() { // SpringClientFactory这个Client工厂类很关键 SpringClientFactory factory = new SpringClientFactory(); factory.setConfigurations(this.configurations); return factory; } } 我们再看看这个工厂类的构造方法做了什么事： public class SpringClientFactory extends NamedContextFactory&lt;RibbonClientSpecification&gt; { static final String NAMESPACE = &quot;ribbon&quot;; public SpringClientFactory() { super(RibbonClientConfiguration.class, NAMESPACE, &quot;ribbon.client.name&quot;); } } public abstract class NamedContextFactory&lt;C extends NamedContextFactory.Specification&gt; implements DisposableBean, ApplicationContextAware { private final String propertySourceName; private final String propertyName; private Map&lt;String, AnnotationConfigApplicationContext&gt; contexts = new ConcurrentHashMap(); private Map&lt;String, C&gt; configurations = new ConcurrentHashMap(); private ApplicationContext parent; private Class&lt;?&gt; defaultConfigType; public NamedContextFactory(Class&lt;?&gt; defaultConfigType, String propertySourceName, String propertyName) { this.defaultConfigType = defaultConfigType; this.propertySourceName = propertySourceName; this.propertyName = propertyName; } } 显然，构造了一个上下工厂“NamedContextFactory”类，这个工厂类的默认配置类是“RibbonClientConfiguraion”，这个在之后Feign的调用过程中非常关键； 2 LoadBalancerFeignClient这个代理类的execute()方法中 // org.springframework.cloud.openfeign.ribbon.LoadBalancerFeignClient#execute public Response execute(Request request, Request.Options options) throws IOException { try { URI asUri = URI.create(request.url()); String clientName = asUri.getHost(); URI uriWithoutHost = cleanUrl(request.url(), clientName); FeignLoadBalancer.RibbonRequest ribbonRequest = new FeignLoadBalancer.RibbonRequest( this.delegate, request, uriWithoutHost); // 这个方法不用讲，得到的IclientConfig肯定是RibbonClientConfiguraion IClientConfig requestConfig = getClientConfig(options, clientName); return lbClient(clientName) .executeWithLoadBalancer(ribbonRequest, requestConfig).toResponse(); } catch (ClientException e) { ...... } } // org.springframework.cloud.openfeign.ribbon.LoadBalancerFeignClient#getClientConfig IClientConfig getClientConfig(Request.Options options, String clientName) { ...... requestConfig = this.clientFactory.getClientConfig(clientName); ...... } // org.springframework.cloud.netflix.ribbon.SpringClientFactory#getClientConfig public IClientConfig getClientConfig(String name) { return getInstance(name, IClientConfig.class); } // org.springframework.cloud.netflix.ribbon.SpringClientFactory#getInstance public &lt;C&gt; C getInstance(String name, Class&lt;C&gt; type) { // SpringClientFactory的super父类就是NamedContextFactory C instance = super.getInstance(name, type); if (instance != null) { return instance; } IClientConfig config = getInstance(name, IClientConfig.class); return instantiateWithConfig(getContext(name), type, config); } // org.springframework.cloud.context.named.NamedContextFactory#getInstance public &lt;T&gt; T getInstance(String name, Class&lt;T&gt; type) { AnnotationConfigApplicationContext context = this.getContext(name); return BeanFactoryUtils.beanNamesForTypeIncludingAncestors(context, type).length &gt; 0 ? context.getBean(type) : null; } 找到了**NamedContextFactory ，就和第一点串起来了，它的 默认 配置类正是RibbonClientConfiguration **； 3 RibbonClientConfiguration又是如何注入我们需要的ZoneAwareLoadBalancer的 @Configuration(proxyBeanMethods = false) @EnableConfigurationProperties // Order is important here, last should be the default, first should be optional // see // https://github.com/spring-cloud/spring-cloud-netflix/issues/2086#issuecomment-316281653 @Import({ HttpClientConfiguration.class, OkHttpRibbonConfiguration.class, RestClientRibbonConfiguration.class, HttpClientRibbonConfiguration.class }) public class RibbonClientConfiguration { @Bean @ConditionalOnMissingBean public ILoadBalancer ribbonLoadBalancer(IClientConfig config, ServerList&lt;Server&gt; serverList, ServerListFilter&lt;Server&gt; serverListFilter, IRule rule, IPing ping, ServerListUpdater serverListUpdater) { if (this.propertiesFactory.isSet(ILoadBalancer.class, name)) { return this.propertiesFactory.get(ILoadBalancer.class, config, name); } // 默认注入的类正是ZoneAwareLoadBalancer return new ZoneAwareLoadBalancer&lt;&gt;(config, rule, ping, serverList, serverListFilter, serverListUpdater); } } 所以，这就解释了，为什么在后面的 ILoadBalancer lb = getLoadBalancer(); 获得到的就是ZoneAwareLoadBalancer！ 同时，这个类是在动态代理被执行execute()的时候被调起的，所以Ribbon也是在被使用时才从Nacos获取注册表的，并不是在容器启动时！ 五 Ribbon又是何时从Nacos服务端获取allServerList的 1 我们需要跟踪ZoneAwareLoadBalancer对应的Bean的构造过程 // com.netflix.loadbalancer.ZoneAwareLoadBalancer#ZoneAwareLoadBalancer public ZoneAwareLoadBalancer(IClientConfig clientConfig, IRule rule, IPing ping, ServerList&lt;T&gt; serverList, ServerListFilter&lt;T&gt; filter, ServerListUpdater serverListUpdater) { super(clientConfig, rule, ping, serverList, filter, serverListUpdater); } // com.netflix.loadbalancer.DynamicServerListLoadBalancer#DynamicServerListLoadBalancer public DynamicServerListLoadBalancer(IClientConfig clientConfig, IRule rule, IPing ping, ServerList&lt;T&gt; serverList, ServerListFilter&lt;T&gt; filter, ServerListUpdater serverListUpdater) { super(clientConfig, rule, ping); this.serverListImpl = serverList; this.filter = filter; this.serverListUpdater = serverListUpdater; if (filter instanceof AbstractServerListFilter) { ((AbstractServerListFilter) filter).setLoadBalancerStats(getLoadBalancerStats()); } //执行远程调用，并初始化BaseLoadBalancer中的allServerList restOfInit(clientConfig); } 2 远程调用Nacos并初始化restOfInit()方法的逻辑 void restOfInit(IClientConfig clientConfig) { boolean primeConnection = this.isEnablePrimingConnections(); // turn this off to avoid duplicated asynchronous priming done in BaseLoadBalancer.setServerList() this.setEnablePrimingConnections(false); // 获取服务列表功能 enableAndInitLearnNewServersFeature(); updateListOfServers(); if (primeConnection &amp;&amp; this.getPrimeConnections() != null) { this.getPrimeConnections() .primeConnections(getReachableServers()); } this.setEnablePrimingConnections(primeConnection); } public void enableAndInitLearnNewServersFeature() { serverListUpdater.start(updateAction); } 这个updateAction变量是可执行的： protected final ServerListUpdater.UpdateAction updateAction = new ServerListUpdater.UpdateAction() { @Override public void doUpdate() { updateListOfServers(); // 更新Servers列表 } }; 3、updateListOfServer()方法如何从Nacos服务端获取服务列表： // com.netflix.loadbalancer.DynamicServerListLoadBalancer#updateListOfServers public void updateListOfServers() { List&lt;T&gt; servers = new ArrayList&lt;T&gt;(); if (serverListImpl != null) { // 其中的serverListImpl有三个实现，其中之一就是Nacos servers = serverListImpl.getUpdatedListOfServers(); LOGGER.debug(&quot;List of Servers for {} obtained from Discovery client: {}&quot;, getIdentifier(), servers); if (filter != null) { servers = filter.getFilteredListOfServers(servers); LOGGER.debug(&quot;Filtered List of Servers for {} obtained from Discovery client: {}&quot;, getIdentifier(), servers); } } // 用获得到的服务列表更新BaseLoadBalancer中的allServerList updateAllServerList(servers); } 不用想了，肯定是NacosServerList，这个简单追踪就不赘述了！ NacosServerList.getUpdatedListOfServers()最终会调用nacos客户端的核心类 NacosNamingServer.selectInstances() 方法，该方法只会返回健康实例列表； 4 根据从nacos获得到的Servers，更新本地的allServerList属性 // com.netflix.loadbalancer.DynamicServerListLoadBalancer#updateAllServerList protected void updateAllServerList(List&lt;T&gt; ls) { // other threads might be doing this - in which case, we pass if (serverListUpdateInProgress.compareAndSet(false, true)) { try { for (T s : ls) { s.setAlive(true); // set so that clients can start using these // servers right away instead // of having to wait out the ping cycle. } setServersList(ls); super.forceQuickPing(); } finally { serverListUpdateInProgress.set(false); } } } public void setServersList(List lsrv) { super.setServersList(lsrv); // 核心 List&lt;T&gt; serverList = (List&lt;T&gt;) lsrv; Map&lt;String, List&lt;Server&gt;&gt; serversInZones = new HashMap&lt;String, List&lt;Server&gt;&gt;(); for (Server server : serverList) { // make sure ServerStats is created to avoid creating them on hot // path getLoadBalancerStats().getSingleServerStat(server); String zone = server.getZone(); if (zone != null) { zone = zone.toLowerCase(); List&lt;Server&gt; servers = serversInZones.get(zone); if (servers == null) { servers = new ArrayList&lt;Server&gt;(); serversInZones.put(zone, servers); } servers.add(server); } } setServerListForZones(serversInZones); } // com.netflix.loadbalancer.BaseLoadBalancer#setServersList public void setServersList(List lsrv) { Lock writeLock = allServerLock.writeLock(); logger.debug(&quot;LoadBalancer [{}]: clearing server list (SET op)&quot;, name); ArrayList&lt;Server&gt; newServers = new ArrayList&lt;Server&gt;(); writeLock.lock(); try { ArrayList&lt;Server&gt; allServers = new ArrayList&lt;Server&gt;(); ...对每一个Server进行下包装... allServerList = allServers; //本地的allServerList赋值 if (canSkipPing()) { for (Server s : allServerList) { s.setAlive(true); } upServerList = allServerList; } else if (listChanged) { forceQuickPing(); } } finally { writeLock.unlock(); } } 到这里，总算和 二.5 章节中的allServerList进行呼应了！ 至此，整个从FeignClient注解生成动态代理类LoadBalancerFeignClient； ——&gt; Client执行时会生成ribbon下的ZoneAwareLoadBalancer类，调用nacos服务端获取服务列表； ——&gt; 在通过ZoneAwareLoadBalancer的父类的BaseLoadBalancer.chooseServer()方法根据负载均衡算法挑选一台服务器； ——&gt; 最后交给Feign的的Client通过URL类完成调用。 六 Feign和Ribbon的重试机制 1 首先我们做两个事情 服务提供者feign-provider： @GetMapping(&quot;/get/{id}&quot;) public String getById(@PathVariable(&quot;id&quot;) String id){ System.out.println(&quot;被调用，时间：&quot; + System.currentTimeMillis()); try { TimeUnit.SECONDS.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } return &quot;feign-provider:com.jiguiquan.springcloud.controller.OrderController#getById=&quot; + id; } 服务消费者feign-consumer： @GetMapping(&quot;test&quot;) public String test(){ return orderService.getById(&quot;100&quot;); } 2 不配置任何重试机制，使用默认值 调用接口后，看服务提供者的日志（重试了2次，时间间隔为1秒）： 3 我们为系统注入Feign的默认Retryer重试机制 @Bean public Retryer retryer(){ return new Retryer.Default(); } 再次调用，再看日志（重试了10次，）： 4 Feign的默认重试器的核心代码（默认是不开启的） public static class Default implements Retryer { private final int maxAttempts; private final long period; private final long maxPeriod; int attempt; long sleptForMillis; public Default() { // 默认的重试间隔时间100ms，最大间隔时间为1s，最大重试次数为5次 this(100L, TimeUnit.SECONDS.toMillis(1L), 5); } // 默认下次重试间隔时间 100 * 1.5^(尝试轮次-1) 次方 // 第一次尝试：100 * 1.5(2 -1) = 150ms; 以此类推 long nextMaxInterval() { long interval = (long)((double)this.period * Math.pow(1.5D, (double)(this.attempt - 1))); return interval &gt; this.maxPeriod ? this.maxPeriod : interval; } } 5 Ribbon和Feign的重试器的源码就不过度追溯了，直接上结论 Ribbon的重试机制默认是开启的，重试1次，共调用两次； Feign的Retryer重试器默认是关闭的 NEVER_RETRY; Feign如果想开启默认重试器，直接在Spring容器中注入 Retryer.Default 即可，默认重试5次； 6 修改Ribbon的默认重试机制 # Ribbon 配置 ribbon: # 单节点最大重试次数(不包含默认调用的1次)，达到最大值时，切换到下一个示例 MaxAutoRetries: 0 # 0 相当于关闭ribbon重试 # 更换下一个重试节点的最大次数，可以设置为服务提供者副本数（副本数 = 总机器数 - 1），也是就每个副本都查询一次 MaxAutoRetriesNextServer: 0 # 是否对所有请求进行重试，默认fasle，则只会对GET请求进行重试，建议配置为false，不然添加数据接口，会造成多条重复，也就是幂等性问题。 OkToRetryOnAllOperations: false 7 自定义Feign重试机制（直接给 Retryer.Default 构造方法传参即可） @Bean public Retryer retryer(){ return new Retryer.Default(100, 1000, 3); // 调用3次（包含原本的一次调用） } 当然，如果对 Retryer.Dafault 的默认的方法逻辑不认可，可以直接实现一个自己的CustomRetryer注入到spring容器中即可： @Bean public Retryer customerRetryer(){ return new Retryer() { @Override public void continueOrPropagate(RetryableException e) { } @Override public Retryer clone() { return null; } }; } ","link":"https://tinaxiawuhao.github.io/post/Y45PUcltI/"},{"title":"Nacos核心源码剖析——配置中心","content":"Nacos官方文档：https://nacos.io/zh-cn/docs/quick-start.html 服务端对外暴露的API：https://nacos.io/zh-cn/docs/open-api.html Nacos的Server端其实就是一个Web服务，对外提供了Http服务接口，所有的客户端与服务端的通讯都通过Http调用完成（短链接）。 Nacos注册服务核心类：NacosNamingService Nacos配置中心核心类：NacosConfigService Nacos配置中心的nameSpace/Group和注册中心类似，但是没有集群Cluster的概念！ 配置文件的核心主键是DataId（与注册中心不一样，注册中心为ServiceName） spring: application: name: nacos-config-client cloud: nacos: config: server-addr: 10.206.73.156:8848 namespace: haier-iot group: dev file-extension: yaml # 上面的配置，拼装成的最高优先级配置文件为 haier-iot/dev/ nacos-config-client-dev.yaml Nacos还支持扩展配置：extension-configs，和共享配置：shared-configs，以支持各种复杂的应用场景！ 官方github wiki地址：https://github.com/alibaba/spring-cloud-alibaba/wiki/Nacos-config Client客户端核心知识点： 当需要获取配置时，先尝试从本地配置文件获取，获取不到时，再去远端Server获取；从远端获取成功后，保存到本地配置文件，后面通过ClientWorker中的长轮询完成配置的实时更新! ClientWorker中有两个线程executor和executorService： 单线程executor定时任务：每10ms执行checkConfigInfo()方法，看看本地配置信息是否有变化，以3000个为一组，判断是否要增加新的LongPollingRunnable长轮询任务； 多线程executorService执行LongPollingRunnable长轮询任务核心逻辑：根据本地配置项的dataId，group，Md5值，tenant拼接字符串，调用服务端“监听配置——长轮询接口”看看这批次的配置是否有变化，有变化的话，就遍历调用“获取配置详情接口”获取最新配置值，没变化的不用动，任务最后，再次执行this，循环往复执行此长轮询任务！ 当本地的配置文件发生改变时，会回调注册在这些配置上的监听器的回调方法，从而完成应用程序的配置更新！（refresh(context)后完成Nacos监听器的注册） 服务端核心知识点： 服务端即使配置了mysql，每次请求也不是直接去查询mysql的，而是借助 本地内存缓存中的元数据 + 本地磁盘中的配置文件； Mysql主要用于集群节点启动时的数据加载（全量加载、增量加载） 和 数据变动时的刷新同步； 某节点处理配置发布请求时，不是着急更新自己的状态，而是会先写入mysql数据库，之后通过ConfigDataChangeEvent事件实现异步处理，通知所有节点更新自己的内存缓存和本地磁盘文件；（包括自己） AP集群，存在数据的短时间不一致，但是可以保证最终一致性，对客户端的影响也就是可能配置更新慢那么一点。 一 nacos客户端加载配置的核心逻辑 1 nacos核心配置NacosPropertySourceLocator类的定位： 如果要弄清楚Nacos配置文件加载到Spring容器中的流程，还需要熟悉Springcloud的源码流程，了解Springcloud中的配置，还有重要的Bean是如果装载到Spring容器中的； 这里只能大概聊一下： Springboot项目启动时，在prepareEnvironment()阶段，会通过spring.factories文件中的BootstrapConfiguration类找到NacosConfigBootstrapConfiguration配置类，并完成注入： NacosConfigBootstrapConfiguration配置类，会向Spring容器中注入几个Nacos配置读取重要的类 public class NacosConfigBootstrapConfiguration { public NacosConfigBootstrapConfiguration() { } @Bean @ConditionalOnMissingBean public NacosConfigProperties nacosConfigProperties() { return new NacosConfigProperties(); } @Bean @ConditionalOnMissingBean public NacosConfigManager nacosConfigManager(NacosConfigProperties nacosConfigProperties) { return new NacosConfigManager(nacosConfigProperties); } // NacosPropertySourceLocator能够一步步地把把Nacos的配置文件都找到 @Bean public NacosPropertySourceLocator nacosPropertySourceLocator(NacosConfigManager nacosConfigManager) { return new NacosPropertySourceLocator(nacosConfigManager); } } 然后在prepareContext()上下文的时候，会通过之前从spring.factories中读取到的ApplicationInitializer初始化器，这里遍历执行时，就会执行到springcloud的PropertySpurceBootstrapConfiguration类的initialize()方法： 可以看到PropertySpurceBootstrapConfiguration.initialize()方法中需要用到PropertySourceLocator接口的实现类，而我们配置的Nacos正好为这个接口提供了实现类NacosPropertySourceLocator 2 通过NacosPropertySourceLocator类，理清Nacos各中配置文件的优先级： // com.alibaba.cloud.nacos.client.NacosPropertySourceLocator#locate public PropertySource&lt;?&gt; locate(Environment env) { this.nacosConfigProperties.setEnvironment(env); ConfigService configService = this.nacosConfigManager.getConfigService(); if (null == configService) { log.warn(&quot;no instance of config service found, can't load config from nacos&quot;); return null; } else { long timeout = (long)this.nacosConfigProperties.getTimeout(); this.nacosPropertySourceBuilder = new NacosPropertySourceBuilder(configService, timeout); String name = this.nacosConfigProperties.getName(); String dataIdPrefix = this.nacosConfigProperties.getPrefix(); if (StringUtils.isEmpty(dataIdPrefix)) { dataIdPrefix = name; } if (StringUtils.isEmpty(dataIdPrefix)) { dataIdPrefix = env.getProperty(&quot;spring.application.name&quot;); } CompositePropertySource composite = new CompositePropertySource(&quot;NACOS&quot;); // 1. 先加载共享配置文件 this.loadSharedConfiguration(composite); // 2. 再加载扩展配置文件 this.loadExtConfiguration(composite); // 3. 最后才加载本应用自己的配置文件 this.loadApplicationConfiguration(composite, dataIdPrefix, this.nacosConfigProperties, env); return composite; } } 本应用自己的配置文件也是可以存在多个的，也是有优先级的： // 入参中的dataIdPrefix，理解为就是spring.application.name private void loadApplicationConfiguration(CompositePropertySource compositePropertySource, String dataIdPrefix, NacosConfigProperties properties, Environment environment) { String fileExtension = properties.getFileExtension(); String nacosGroup = properties.getGroup(); // 1. 先尝试加载：纯微服务名称对应的配置文件，如：order-config this.loadNacosDataIfPresent(compositePropertySource, dataIdPrefix, nacosGroup, fileExtension, true); // 2. 再尝试加载：微服务名称 + &quot;.&quot; + 文件扩展名的文件，如：order-config.yaml this.loadNacosDataIfPresent(compositePropertySource, dataIdPrefix + &quot;.&quot; + fileExtension, nacosGroup, fileExtension, true); String[] var7 = environment.getActiveProfiles(); int var8 = var7.length; // 3. 最后再尝试加载：微服务名称 + &quot;-&quot; + profile + &quot;.&quot; + 文件扩展名的文件，如：order-config-dev.yaml for(int var9 = 0; var9 &lt; var8; ++var9) { String profile = var7[var9]; String dataId = dataIdPrefix + &quot;-&quot; + profile + &quot;.&quot; + fileExtension; this.loadNacosDataIfPresent(compositePropertySource, dataId, nacosGroup, fileExtension, true); } } 根据后加载的覆盖先加载的原则，最后我们很容易就可以知道整个服务的配置文件的优先级为： order-config-dev.yaml &gt; order-config.yaml &gt; order-config &gt; extension-configs &gt; shared-configs 二 Nacos配置中心的核心类NacosConfigService的引入 1 客户端是何时从远程配置Nacos服务端拉取配置的 // 还记得刚开始时候，通过spring.factories注入了一个配置类NacosConfigBootstrapConfiguration // 该配置类，会向Spring容器中注入一个Bean：NacosConfigManager @Bean @ConditionalOnMissingBean public NacosConfigManager nacosConfigManager(NacosConfigProperties nacosConfigProperties) { return new NacosConfigManager(nacosConfigProperties); } //而NacosConfigManager的构造方法中，就会“写死”为我们create一个ConfigService，赋值给静态变量service private static ConfigService service = createConfigService(nacosConfigProperties); | public static ConfigService createConfigService(Properties properties) throws NacosException { try { Class&lt;?&gt; driverImplClass = Class.forName(&quot;com.alibaba.nacos.client.config.NacosConfigService&quot;); Constructor constructor = driverImplClass.getConstructor(Properties.class); ConfigService vendorImpl = (ConfigService)constructor.newInstance(properties); return vendorImpl; } catch (Throwable var4) { throw new NacosException(-400, var4); } } 此时，NacosConfigService闪亮登场！ 2 NacosConfigService类的构造方法中会创建重要的两个属性agent和worker： public NacosConfigService(Properties properties) throws NacosException { ValidatorUtils.checkInitParam(properties); String encodeTmp = properties.getProperty(PropertyKeyConst.ENCODE); if (StringUtils.isBlank(encodeTmp)) { this.encode = Constants.ENCODE; } else { this.encode = encodeTmp.trim(); } initNamespace(properties); // agent是一个http代理，如果需要登录验证等操作，ServerHttpAgent构造时会完成验证 this.agent = new MetricsHttpAgent(new ServerHttpAgent(properties)); this.agent.start(); // 客户端的实际工作者ClientWorker，其中的agent也就是上面创建的agent this.worker = new ClientWorker(this.agent, this.configFilterChainManager, properties); } ####3 NacosConfigService中的核心获取配置方法getConfig() ——&gt; getConfigInner()： 客户端需要使用配置文件时，不是直接去调用远端Server获取，而是先尝试从本地Failover文件获取； 如果本地文件不存在，则才会从远端Server获取； 从远端Server成功获取配置后，会向本地文件保存快照，以备后用；（本地文件的更新由后面的长轮询完成） private String getConfigInner(String tenant, String dataId, String group, long timeoutMs) throws NacosException { group = null2defaultGroup(group); ParamUtils.checkKeyParam(dataId, group); ConfigResponse cr = new ConfigResponse(); cr.setDataId(dataId); cr.setTenant(tenant); cr.setGroup(group); // 优先使用本地配置 String content = LocalConfigInfoProcessor.getFailover(agent.getName(), dataId, group, tenant); if (content != null) { cr.setContent(content); configFilterChainManager.doFilter(null, cr); content = cr.getContent(); return content; } try { // 本地没有后，就尝试从远端获取配置 String[] ct = worker.getServerConfig(dataId, group, tenant, timeoutMs); cr.setContent(ct[0]); configFilterChainManager.doFilter(null, cr); content = cr.getContent(); return content; } catch (NacosException ioe) { ...... } ...... return content; } 所以Nacos配置中心与注册中心类似，都是先尝试从本地获取配置，只不过注册中心比配置中心多了一份内存注册表！ 注册中心：本地Failover故障转移文件 ——&gt; 本地内存注册表 ——&gt; 远端请求服务列表 配置中心：本地Failover故障转移文件（也就是本地配置快照文件）——&gt; 远端请求配置文件 worker.getServerConfig()远端配置请求成功后，还会往本地配置文件存一份Snapshot： // worker.getServerConfig() public String[] getServerConfig(String dataId, String group, String tenant, long readTimeout){ // 从远端Server获取配置，这里的agent就是NacosConfigService构造方法中创建的agent代理 result = agent.httpGet(Constants.CONFIG_CONTROLLER_PATH, null, params, agent.getEncode(), readTimeout); switch (result.getCode()) { case HttpURLConnection.HTTP_OK: // 往本地配置快照文件存一份 LocalConfigInfoProcessor.saveSnapshot(agent.getName(), dataId, group, tenant, result.getData()); ct[0] = result.getData(); if (result.getHeader().getValue(CONFIG_TYPE) != null) { ct[1] = result.getHeader().getValue(CONFIG_TYPE); } else { ct[1] = ConfigType.TEXT.getType(); } return ct; case HttpURLConnection.HTTP_NOT_FOUND: LocalConfigInfoProcessor.saveSnapshot(agent.getName(), dataId, group, tenant, null); return ct; case HttpURLConnection.HTTP_CONFLICT: { ... } case HttpURLConnection.HTTP_FORBIDDEN: { ... } default: { ... } } } 4 Client第一次获取到Server端配置后，之后如何进行定时更新？ // ClientWorker构造时，会创建2个线程池 // 1. executor(单线程) ：定时每10毫秒执行checkConfigInfo()方法 // 2. executorService(1~核数/2): 具体的执行长轮询的线程 public ClientWorker(final HttpAgent agent, final ConfigFilterChainManager configFilterChainManager,final Properties properties) { this.agent = agent; this.configFilterChainManager = configFilterChainManager; // Initialize the timeout parameter init(properties); this.executor = Executors.newScheduledThreadPool(1, new ThreadFactory() { @Override public Thread newThread(Runnable r) { Thread t = new Thread(r); t.setName(&quot;com.alibaba.nacos.client.Worker.&quot; + agent.getName()); t.setDaemon(true); return t; } }); this.executorService = Executors .newScheduledThreadPool(Runtime.getRuntime().availableProcessors(), new ThreadFactory() { @Override public Thread newThread(Runnable r) { Thread t = new Thread(r); t.setName(&quot;com.alibaba.nacos.client.Worker.longPolling.&quot; + agent.getName()); t.setDaemon(true); return t; } }); this.executor.scheduleWithFixedDelay(new Runnable() { @Override public void run() { try { checkConfigInfo(); } catch (Throwable e) { LOGGER.error(&quot;[&quot; + agent.getName() + &quot;] [sub-check] rotate check error&quot;, e); } } }, 1L, 10L, TimeUnit.MILLISECONDS); } public void checkConfigInfo() { // 将总的需要监听的配置数，以3000个为一组，创建长轮询LongPollingRunnable任务，监听配置更新！ int listenerSize = cacheMap.get().size(); int longingTaskCount = (int) Math.ceil(listenerSize / ParamUtil.getPerTaskConfigSize()); if (longingTaskCount &gt; currentLongingTaskCount) { for (int i = (int) currentLongingTaskCount; i &lt; longingTaskCount; i++) { // 实际的干活线程，从远端拉取最新的config，与本地的config对比MD5值，看是否发生变化 executorService.execute(new LongPollingRunnable(i)); } currentLongingTaskCount = longingTaskCount; } } 总结：ClientWorker实例化时，会创建两个线程：executor和executorService executor（单线程）：每隔10ms检查本地的配置数是否发生改变，以3000为一批次创建长轮询任务，不足的话，不另外创建长轮询任务； executorService（多线程1~核数/2）:具体执行长轮询LongPolling任务的工作线程； 5 Nacos客户端长轮询LongPolling任务的核心逻辑（md5比对）： 监听配置的长轮询接口API：https://nacos.io/zh-cn/docs/open-api.html // LongPollingRunnable.run() public void run() { List&lt;CacheData&gt; cacheDatas = new ArrayList&lt;CacheData&gt;(); List&lt;String&gt; inInitializingCacheList = new ArrayList&lt;String&gt;(); try { // 检查本地的配置文件 for (CacheData cacheData : cacheMap.get().values()) { if (cacheData.getTaskId() == taskId) { cacheDatas.add(cacheData); try { checkLocalConfig(cacheData); if (cacheData.isUseLocalConfigInfo()) { cacheData.checkListenerMd5(); } } catch (Exception e) { LOGGER.error(&quot;get local config info error&quot;, e); } } } // 会根据上面得到的cacheDatas，组装参数，调用服务端的监听配置的长轮询接口/nacos/v1/cs/configs/listener // 长轮询的返回值是dataId^2group^2tenant^1，空串代表无变化 List&lt;String&gt; changedGroupKeys = checkUpdateDataIds(cacheDatas, inInitializingCacheList); if (!CollectionUtils.isEmpty(changedGroupKeys)) { LOGGER.info(&quot;get changedGroupKeys:&quot; + changedGroupKeys); } for (String groupKey : changedGroupKeys) { String[] key = GroupKey.parseKey(groupKey); String dataId = key[0]; String group = key[1]; String tenant = null; if (key.length == 3) { tenant = key[2]; } try { // 会根据返回值中有变化的dataId配置项，单独去获取最新的配置值回本地 String[] ct = getServerConfig(dataId, group, tenant, 3000L); CacheData cache = cacheMap.get().get(GroupKey.getKeyTenant(dataId, group, tenant)); cache.setContent(ct[0]); if (null != ct[1]) { cache.setType(ct[1]); } LOGGER.info(&quot;[{}] [data-received] dataId={}, group={}, tenant={}, md5={}, content={}, type={}&quot;, agent.getName(), dataId, group, tenant, cache.getMd5(), ContentUtils.truncateContent(ct[0]), ct[1]); } catch (NacosException ioe) { String message = String .format(&quot;[%s] [get-update] get changed config exception. dataId=%s, group=%s, tenant=%s&quot;, agent.getName(), dataId, group, tenant); LOGGER.error(message, ioe); } } for (CacheData cacheData : cacheDatas) { if (!cacheData.isInitializing() || inInitializingCacheList .contains(GroupKey.getKeyTenant(cacheData.dataId, cacheData.group, cacheData.tenant))) { cacheData.checkListenerMd5(); cacheData.setInitializing(false); } } inInitializingCacheList.clear(); // 再次执行该方法，不断循环，不停监听配置文件的更新 executorService.execute(this); } catch (Throwable e) { // If the rotation training task is abnormal, the next execution time of the task will be punished LOGGER.error(&quot;longPolling error : &quot;, e); executorService.schedule(this, taskPenaltyTime, TimeUnit.MILLISECONDS); } } checkUpdateDataIds(cacheDatas, inInitializingCacheList)核心逻辑： // 拼接本地所有的dataId为一个长字符串 List&lt;String&gt; checkUpdateDataIds(List&lt;CacheData&gt; cacheDatas, List&lt;String&gt; inInitializingCacheList) throws Exception { StringBuilder sb = new StringBuilder(); for (CacheData cacheData : cacheDatas) { if (!cacheData.isUseLocalConfigInfo()) { sb.append(cacheData.dataId).append(WORD_SEPARATOR); sb.append(cacheData.group).append(WORD_SEPARATOR); if (StringUtils.isBlank(cacheData.tenant)) { sb.append(cacheData.getMd5()).append(LINE_SEPARATOR); } else { sb.append(cacheData.getMd5()).append(WORD_SEPARATOR); sb.append(cacheData.getTenant()).append(LINE_SEPARATOR); } if (cacheData.isInitializing()) { // It updates when cacheData occours in cacheMap by first time. inInitializingCacheList .add(GroupKey.getKeyTenant(cacheData.dataId, cacheData.group, cacheData.tenant)); } } } boolean isInitializingCacheList = !inInitializingCacheList.isEmpty(); return checkUpdateConfigStr(sb.toString(), isInitializingCacheList); } // 向服务端发起长轮询的查询逻辑，超时为30秒，不要挂起我 List&lt;String&gt; checkUpdateConfigStr(String probeUpdateString, boolean isInitializingCacheList) { Map&lt;String, String&gt; params = new HashMap&lt;String, String&gt;(2); params.put(Constants.PROBE_MODIFY_REQUEST, probeUpdateString); Map&lt;String, String&gt; headers = new HashMap&lt;String, String&gt;(2); headers.put(&quot;Long-Pulling-Timeout&quot;, &quot;&quot; + timeout); if (isInitializingCacheList) { headers.put(&quot;Long-Pulling-Timeout-No-Hangup&quot;, &quot;true&quot;); } if (StringUtils.isBlank(probeUpdateString)) { return Collections.emptyList(); } try { // In order to prevent the server from handling the delay of the client's long task, // increase the client's read timeout to avoid this problem. long readTimeoutMs = timeout + (long) Math.round(timeout &gt;&gt; 1); HttpRestResult&lt;String&gt; result = agent .httpPost(Constants.CONFIG_CONTROLLER_PATH + &quot;/listener&quot;, headers, params, agent.getEncode(), readTimeoutMs); if (result.ok()) { setHealthServer(true); return parseUpdateDataIdResponse(result.getData()); } else { setHealthServer(false); LOGGER.error(&quot;[{}] [check-update] get changed dataId error, code: {}&quot;, agent.getName(), result.getCode()); } } catch (Exception e) { setHealthServer(false); LOGGER.error(&quot;[&quot; + agent.getName() + &quot;] [check-update] get changed dataId exception&quot;, e); throw e; } return Collections.emptyList(); } 总结：长轮询LongPollingRunnable长轮询任务的核心逻辑就是： 先查询本地配置文件Failover文件（本地配置快照Snapshot文件）； 根据本地配置文件，得到所有配置项，拼接请求参数（dataId/group/Md5/tenant），向服务端提供的长轮询接口（监听配置）：POST：/nacos/v1/cs/configs/listener 发起长轮询调用； 如果本批次的配置项有变动，服务端就会返回有变动的配置项字符串数组，如果没有变动，就返回空串； 如果返回不为空，说明有变动，就遍历这些变动项，然后通过具体的获取配置接口：GET：/nacos/v1/cs/configs 获取该配置项的最新配置； 最后，再次执行本次的 LongPollingRunnable 任务，循环往复，完成配置的实时监听更新！ 6 经过长轮询后，如果本地配置文件更新后，该如何通知到我们的应用程序（注册监听器）？ 在Springboot项目完成refresh(context)后，会调用 listeners.running(context) 方法，这个方法会向系统发出ApplicationReadyEvent事件，其它的监听了这个事件的Listener就可以做对应的工作了，而NacosContextRefresher就是其中之一！ public class NacosContextRefresher implements ApplicationListener&lt;ApplicationReadyEvent&gt;, ApplicationContextAware { // 监听到ApplicationReadyEvent事件后，开始注册Nacos的监听器 public void onApplicationEvent(ApplicationReadyEvent event) { if (this.ready.compareAndSet(false, true)) { this.registerNacosListenersForApplications(); } } private void registerNacosListenersForApplications() { if (this.isRefreshEnabled()) { Iterator var1 = NacosPropertySourceRepository.getAll().iterator(); while(var1.hasNext()) { NacosPropertySource propertySource = (NacosPropertySource)var1.next(); if (propertySource.isRefreshable()) { // 默认使开启的 String dataId = propertySource.getDataId(); this.registerNacosListener(propertySource.getGroup(), dataId); } } } } } 有了这一系列的监听器，当客户端知道配置发生改变时，就会回调对应的监听器的回调方法，通知应用程序更新对应的Bean（从IOC容器中删除旧Bean，放入新的Bean）。 三 AP模式nacos集群，服务端如何工作 首先一定要清楚，即使集群情况下，配置了mysql，当客户端查询配置时，也不是直接从mysql获取的，而是从每个节点的本地文件读取； 各节点本地文件：具体的配置； 各节点内存缓存：存储配置的元信息，如Md5值等； Mysql：具体的配置的最新值和历史记录，方便新节点启动时加载，以及节点之间的数据同步； 1 配置文件的Dump加载：DumpService DumpService由两个实现类：EmbeddedDumpService（derby） 和 ExternalDumpService（mysql）； 当新节点启动时，需要从mysql中加载配置数据，如果最后心跳时间&gt;6h，则从mysql加载全量数据；如果最后心跳时间&lt;6h，则从mysql加载增量数据； 全量加载：删除本地的配置文件，全部从mysql加载配置数据；（每次捞取1000条） 增量加载：捞取最近6小时的新增配置，更新本地和内存元数据后，与mysql数据库中的配置进行比对，如果不一致，则再同步一次！ 2 新的配置被发布后，如何在集群间进行同步？ AP集群，每个节点地位平等，发布配置后，根据轮询机制，会由某一台Server节点处理本地请求，该节点首先会将配置写入到Mysql数据库中； 该节点会发布一个ConfigDataChangeEvent事件，该事件会被自己的监听器处理，处理时，会通过HTTP调用集群的所有节点，告知配置发生改变（包括自己，因为上面只是写mysql，自己的本地文件和内存文件也都没有被修改） 所有节点接收到本地配置修改的通知后，会到Mysql中同步最新的配置，刷新内存缓存 和 本地磁盘文件； ","link":"https://tinaxiawuhao.github.io/post/bp8DT_bHg/"},{"title":"Nacos核心源码剖析（CP架构）——注册中心","content":"一 Nacos CP集群架构的基础知识 1 Nacos集群部署后，可以同时支持AP和CP（注意，不是同时支持CAP） AP架构：临时实例 CP架构：持久化实例 在注册服务时，如果我们让我们的节点注册为：持久化实例，即自动会走CP架构！ spring: application: name: nacos-config-client cloud: nacos: discovery: server-addr: 10.206.73.156:8848 namespace: haier-iot group: dev cluster-name: BJ ephemeral: true # 持久化实例走CP架构 2 Nacos CP架构使用的分布式一致性协议？（简化版的Raft） Raft分布式一致性协议 和 Zookeeper使用的ZAB原子广播协议非常相似； 都是一个Leader带领多个Follower，区别在于，Leader选举时的投票机制： ZAB投票时：所有候选节点都会发起投票，然后进行选票PK，决定谁胜出； Raft投票时：会让所有节点随机睡眠，先睡醒的节点发起投票，投自己，并将选票发到其它节点等待结果； 但是，对结果的判断，ZAB 和 Raft 都遵循“半数机制”。 二 CP架构下，持久化实例的注册逻辑 1 注册实例的API接口不变：/nacos/v1/ns/instance 这里当然时选择RaftConsistencyServiceImpl的实现： public void put(String key, Record value) throws NacosException { checkIsStopWork(); try { raftCore.signalPublish(key, value); } catch (Exception e) { ...... } } 2 整个CP架构下的主节点注册逻辑都在signalPublish()方法中 public void signalPublish(String key, Record value) throws Exception { if (stopWork) { throw new IllegalStateException(&quot;old raft protocol already stop work&quot;); } // 判断自己是不是Leader if (!isLeader()) { ObjectNode params = JacksonUtils.createEmptyJsonNode(); params.put(&quot;key&quot;, key); params.replace(&quot;value&quot;, JacksonUtils.transferToJsonNode(value)); Map&lt;String, String&gt; parameters = new HashMap&lt;&gt;(1); parameters.put(&quot;key&quot;, key); final RaftPeer leader = getLeader(); // 如果本节点不是Leader，就把请求转发给Leader节点处理 raftProxy.proxyPostLarge(leader.ip, API_PUB, params.toString(), parameters); return; } OPERATE_LOCK.lock(); try { ...... // 核心方法，写本地数据，写内存缓存，发布事件——更新内存服务注册表 onPublish(datum, peers.local()); final String content = json.toString(); // 通过CountDownLatch实现半数ack的统计，如果获得到半数以上的ack，则Countdownlatch逻辑才可以继续向下走！ final CountDownLatch latch = new CountDownLatch(peers.majorityCount()); for (final String server : peers.allServersIncludeMyself()) { if (isLeader(server)) { // 首先自己的钥匙先用上 latch.countDown(); continue; } final String url = buildUrl(server, API_ON_PUB); // /v1/ns/raft/datum/commit HttpClient.asyncHttpPostLarge(url, Arrays.asList(&quot;key&quot;, key), content, new Callback&lt;String&gt;() { @Override public void onReceive(RestResult&lt;String&gt; result) { latch.countDown(); // Http调用正常后，则当作一次ack } @Override public void onError(Throwable throwable) { } @Override public void onCancel() { } }); } } finally { OPERATE_LOCK.unlock(); } 其实Nacos实现的简单Raft协议，逻辑有点不太严谨，就是：即使本次同步不成功，但是主节点的本地磁盘文件 + 内存文件 已经都修改过了，不像Zookeeper的两阶段提交； 后期Nacos的一致性协议会修改为JRaft，这点肯定会解决！ 3、Leader本节点保存数据的逻辑：onPublish(datum, peers.local()) public void onPublish(Datum datum, RaftPeer source) throws Exception { ...... // 逻辑能走到这，这个if正常都为true if (KeyBuilder.matchPersistentKey(datum.key)) { // 核心，将数据写到本地磁盘 raftStore.write(datum); } // 往内存中保存一些注册表信息 datums.put(datum.key, datum); if (isLeader()) { local.term.addAndGet(PUBLISH_TERM_INCREASE_COUNT); } else { if (local.term.get() + PUBLISH_TERM_INCREASE_COUNT &gt; source.term.get()) { //set leader term: getLeader().term.set(source.term.get()); local.term.set(getLeader().term.get()); } else { local.term.addAndGet(PUBLISH_TERM_INCREASE_COUNT); } } raftStore.updateTerm(local.term.get()); // 发布一个ValueChangeEvent事件，PersistentNotifier.onEvent(ValueChangeEvent)会去处理这个事件 NotifyCenter.publishEvent(ValueChangeEvent.builder().key(datum.key).action(DataOperation.CHANGE).build()); Loggers.RAFT.info(&quot;data added/updated, key={}, term={}&quot;, datum.key, local.term); } Leader保存本节点数据，分为三个过程： 1. 保存数据到磁盘文件 2. 保存部分信息到缓存datums 3. 保存文件到内存中的服务注册表（双层ConcurrentHashMap）—— 但不是同步保存，而是通过事件发布，实现异步保存 需要保存到内存中的服务注册表时，会发布一个ValueChangeEvent事件，该事件会被PersisNotifier.onEvent(ValueChangeEvent)捕捉到，并进行处理： // com.alibaba.nacos.naming.consistency.persistent.PersistentNotifier#onEvent public void onEvent(ValueChangeEvent event) { notify(event.getKey(), event.getAction(), find.apply(event.getKey())); } public &lt;T extends Record&gt; void notify(final String key, final DataOperation action, final T value) { ...... for (RecordListener listener : listenerMap.get(key)) { try { if (action == DataOperation.CHANGE) { listener.onChange(key, value); //执行updateIps更新内存注册表 continue; } if (action == DataOperation.DELETE) { listener.onDelete(key); } } catch (Throwable e) { ...... } } } public void onChange(String key, Instances value) throws Exception { ...... // 更新注册表（这个方法，在AP架构师，重点介绍过） updateIPs(value.getInstanceList(), KeyBuilder.matchEphemeralInstanceListKey(key)); recalculateChecksum(); } 至此，整个Leader节点的持久化节点注册逻辑就完成了！ 同步数据给其它Follower节点，就是HTTP调用“raft/datum/commit”接口，处理逻辑比主节点简单！ 三 Nacos CP集群选举过程 1 集群节点启动时，会执行RaftCore.init()核心方法 这个Init()核心方法，会启动2个核心定时任务（每500ms执行一次）： 选举任务：new MasterElection() 心跳任务：new HeartBeat() @Component public class RaftCore implements Closeable { @PostConstruct public void init() throws Exception { // 启动CP集群节点 raftStore.loadDatums(notifier, datums); //从本地磁盘文件加载数据 // 如果Leader周期不存在，则置为0 setTerm(NumberUtils.toLong(raftStore.loadMeta().getProperty(&quot;term&quot;), 0L)); initialized = true; // 每500ms做一次选举任务 masterTask = GlobalExecutor.registerMasterElection(new MasterElection()); // 每500ms做一次心跳任务 heartbeatTask = GlobalExecutor.registerHeartbeat(new HeartBeat()); versionJudgement.registerObserver(isAllNewVersion -&gt; { stopWork = isAllNewVersion; if (stopWork) { try { shutdown(); raftListener.removeOldRaftMetadata(); } catch (NacosException e) { throw new NacosRuntimeException(NacosException.SERVER_ERROR, e); } } }, 100); // 注册PersistentNotifier监听器，用来监听处理 ValueChangeEvent 事件，保存内存中服务注册表时用的 NotifyCenter.registerSubscriber(notifier); } } 2 选举任务的核心run()方法 public class MasterElection implements Runnable { @Override public void run() { try { RaftPeer local = peers.local(); local.leaderDueMs -= GlobalExecutor.TICK_PERIOD_MS; if (local.leaderDueMs &gt; 0) { return; } // Raft选举前的随机休眠阶段（15s到20s之间的随机值） local.resetLeaderDue(); // 重新心跳时间为5s local.resetHeartbeatDue(); // 率先跳出休眠的节点，发起投票 sendVote(); } catch (Exception e) { } } private void sendVote() { RaftPeer local = peers.get(NetUtils.localServer()); // 重置集群节点投票 peers.reset(); // 选举周期+1 local.term.incrementAndGet(); // 默认投给自己 local.voteFor = local.ip; // 把自己的状态改为 “候选者” local.state = RaftPeer.State.CANDIDATE; Map&lt;String, String&gt; params = new HashMap&lt;&gt;(1); params.put(&quot;vote&quot;, JacksonUtils.toJson(local)); for (final String server : peers.allServersWithoutMySelf()) { // 其它节点的API接口为：/raft/vote final String url = buildUrl(server, API_VOTE); try { // 向其它节点发出选票 HttpClient.asyncHttpPost(url, null, params, new Callback&lt;String&gt;() { @Override //其它节点给本节点的响应 public void onReceive(RestResult&lt;String&gt; result) { if (!result.ok()) { Loggers.RAFT.error(&quot;NACOS-RAFT vote failed: {}, url: {}&quot;, result.getCode(), url); return; } RaftPeer peer = JacksonUtils.toObj(result.getData(), RaftPeer.class); // 判断是否达到半数选票，成为Leader peers.decideLeader(peer); } @Override public void onError(Throwable throwable) { } @Override public void onCancel() { } }); } catch (Exception e) { } } } } 3 其它节点收到投票后的处理逻辑 如果收到的候选节点的term小于自己本地节点的term，则voteFor自己；（我更适合做Leader，这一票我投给自己） 否则，重置自己的election timeout，设置voteFor为收到的候选节点，更新集群周期term为候选节点的term；（我同意收到的节点做Leader） 给Http的调用方返回response； 四 Nacos CP集群的心跳任务 1 心跳任务由Leader节点发出，有2个作用 确定Follower节点在线； 帮助Follower节点判断数据是否一致；（因为服务注册或变更时，Leader节点自己修改了，且收到了“过半”以上节点的ack，但是不排除有些节点没有执行成功，所以通过心跳任务，进行纠错容错） 2 心跳任务的核心run()方法(只有Leader节点才可以向其它节点发送心跳包) public class HeartBeat implements Runnable { @Override public void run() { try { if (stopWork) { return; } if (!peers.isReady()) { return; } RaftPeer local = peers.local(); // 任务每0.5s执行一次，每次减0.5s，总共5s减完后，就可以开始sendBeat() local.heartbeatDueMs -= GlobalExecutor.TICK_PERIOD_MS; if (local.heartbeatDueMs &gt; 0) { return; } // 重置心跳间隔时间为5s local.resetHeartbeatDue(); sendBeat(); } catch (Exception e) { Loggers.RAFT.warn(&quot;[RAFT] error while sending beat {}&quot;, e); } } private void sendBeat() throws IOException, InterruptedException { RaftPeer local = peers.local(); // 如果当前是单机模式，或者本节点不是Leader节点，则无权发送心跳，直接跳过 if (EnvUtil.getStandaloneMode() || local.state != RaftPeer.State.LEADER) { return; } local.resetLeaderDue(); // build data ObjectNode packet = JacksonUtils.createEmptyJsonNode(); packet.replace(&quot;peer&quot;, JacksonUtils.transferToJsonNode(local)); ArrayNode array = JacksonUtils.createEmptyArrayNode(); if (switchDomain.isSendBeatOnly()) { Loggers.RAFT.info(&quot;[SEND-BEAT-ONLY] {}&quot;, switchDomain.isSendBeatOnly()); } // 封装心跳包，从内存中获取Leader节点注册表缓存中抽取数据的key和timestamp值 if (!switchDomain.isSendBeatOnly()) { for (Datum datum : datums.values()) { ObjectNode element = JacksonUtils.createEmptyJsonNode(); if (KeyBuilder.matchServiceMetaKey(datum.key)) { element.put(&quot;key&quot;, KeyBuilder.briefServiceMetaKey(datum.key)); } else if (KeyBuilder.matchInstanceListKey(datum.key)) { element.put(&quot;key&quot;, KeyBuilder.briefInstanceListkey(datum.key)); } element.put(&quot;timestamp&quot;, datum.timestamp.get()); array.add(element); } } packet.replace(&quot;datums&quot;, array); // broadcast Map&lt;String, String&gt; params = new HashMap&lt;String, String&gt;(1); params.put(&quot;beat&quot;, JacksonUtils.toJson(packet)); String content = JacksonUtils.toJson(params); // 对心跳包做gzip压缩 ByteArrayOutputStream out = new ByteArrayOutputStream(); GZIPOutputStream gzip = new GZIPOutputStream(out); gzip.write(content.getBytes(StandardCharsets.UTF_8)); gzip.close(); byte[] compressedBytes = out.toByteArray(); String compressedContent = new String(compressedBytes, StandardCharsets.UTF_8); // 把压缩后的心跳包，发送给除自己外的其他所有节点 for (final String server : peers.allServersWithoutMySelf()) { try { // 心跳包的API为：/raft/beat final String url = buildUrl(server, API_BEAT); HttpClient.asyncHttpPostLarge(url, null, compressedBytes, new Callback&lt;String&gt;() { @Override public void onReceive(RestResult&lt;String&gt; result) { peers.update(JacksonUtils.toObj(result.getData(), RaftPeer.class)); } @Override public void onError(Throwable throwable) { } @Override public void onCancel() { } }); } catch (Exception e) { } } } } 3 Follower节点收到心跳包后的处理逻辑 Leader发出的心跳包中，包含了数据中的所有key和timestamp，Follower节点通过遍历对比，可以排查自己数据是否为最新最全数据； 如果数据不是最新或最全的，则批量从Leader节点获取不一致的数据的最新值；（Leader节点新增或修改的数据） 同时要删除掉自己比Leader多出来的数据；（Leader节点删除掉的数据） public RaftPeer receivedBeat(JsonNode beat) throws Exception { ...从心跳包中解析数据... // 设置Leader为发送心跳包给我的机器，因为只有Leader才可以发送心跳包 peers.makeLeader(remote); if (!switchDomain.isSendBeatOnly()) { // receivedKeysMap 的作用是判断出本节点 比 Leader节点多出来的数据（见方法的最后） Map&lt;String, Integer&gt; receivedKeysMap = new HashMap&lt;&gt;(datums.size()); for (Map.Entry&lt;String, Datum&gt; entry : datums.entrySet()) { // 如果这个Map中的数据为0，则代表是本地自己的数据； // 接收到的主节点数据时，把对应的值改为1； // 那么直到处理最后，这个Map中还有0，说明这条数据在主节点并没有，只有一种可能，这条数据在主节点中被删除掉了！（妙） // 最后，可以把这些数据，在本地清除掉 receivedKeysMap.put(entry.getKey(), 0); } // batch用来收集本节点没有的数据，或者不是最新的数据 List&lt;String&gt; batch = new ArrayList&lt;&gt;(); int processedCount = 0; // 已处理的数据条数 for (Object object : beatDatums) { processedCount = processedCount + 1; ...... receivedKeysMap.put(datumKey, 1); try { // 包含，且我自己缓存中这条key对应的数据的时间戳&gt;=收到的心跳中的数据，则代表这条数据我有，就可以跳出本轮 if (datums.containsKey(datumKey) &amp;&amp; datums.get(datumKey).timestamp.get() &gt;= timestamp &amp;&amp; processedCount &lt; beatDatums.size()) { continue; } // 取反，不满足上面的条件，则说明这条数据我没有，或者不是最新的，则收集到batch中 // 因为节点变化的时候，虽然Leader节点收到了半数以上的ack，但是毕竟还有可能有些节点没有收到，或者处理不成功，所以这里通过心跳包进行数据同步的容错处理 if (!(datums.containsKey(datumKey) &amp;&amp; datums.get(datumKey).timestamp.get() &gt;= timestamp)) { batch.add(datumKey); } // 当batch的数据量&gt;=50或者数据已经全部处理完，则就可以继续下面向Leader节点发起批量请求数据的逻辑； // 反过来，如果batch&lt;50，且数据还没有处理完，那么这里先跳过，不要向Leader节点发起批量获取数据的请求 if (batch.size() &lt; 50 &amp;&amp; processedCount &lt; beatDatums.size()) { continue; } String keys = StringUtils.join(batch, &quot;,&quot;); // 如果batch为空，当然也不用发请求 if (batch.size() &lt;= 0) { continue; } // update datum entry String url = buildUrl(remote.ip, API_GET); Map&lt;String, String&gt; queryParam = new HashMap&lt;&gt;(1); queryParam.put(&quot;keys&quot;, URLEncoder.encode(keys, &quot;UTF-8&quot;)); // 从Leader批量获取本节点缺少的或过时的数据 HttpClient.asyncHttpGet(url, null, queryParam, new Callback&lt;String&gt;() { @Override public void onReceive(RestResult&lt;String&gt; result) { ...获取缺少的或过时的数据成功后... for (JsonNode datumJson : datumList) { Datum newDatum = null; OPERATE_LOCK.lock(); try { ...... // 和上面Leader节点新增数据时候逻辑相同，写内存注册表 raftStore.write(newDatum); datums.put(newDatum.key, newDatum); notifier.notify(newDatum.key, DataOperation.CHANGE, newDatum.value); ...... } catch (Throwable e) { } finally { OPERATE_LOCK.unlock(); } } return; } @Override public void onError(Throwable throwable) { } @Override public void onCancel() { } }); batch.clear(); } catch (Exception e) { } } // 如果最后receivedKeysMap中还有value为0的数据，说明这些数据在主节点已经被删除了，那我们从节点也主动删除一下 List&lt;String&gt; deadKeys = new ArrayList&lt;&gt;(); for (Map.Entry&lt;String, Integer&gt; entry : receivedKeysMap.entrySet()) { if (entry.getValue() == 0) { deadKeys.add(entry.getKey()); } } for (String deadKey : deadKeys) { try { deleteDatum(deadKey); //删除本节点多出来的数据逻辑 } catch (Exception e) { } } } return local; } ","link":"https://tinaxiawuhao.github.io/post/zolXTKRvU/"},{"title":"Nacos核心源码剖析（AP架构）——注册中心","content":"Nacos官方文档：https://nacos.io/zh-cn/docs/quick-start.html 服务端对外暴露的API：https://nacos.io/zh-cn/docs/open-api.html Nacos的Server端其实就是一个Web服务，对外提供了Http服务接口，所有的客户端与服务端的通讯都通过Http调用完成（短链接）。 Nacos注册服务核心类：NacosNamingService Nacos配置中心核心类：NacosConfigService 一 微服务中常用的注册中心对比 Zookeeper（Apache）：典型的CP架构，有Leader节点，在选举Leader的过程中，整个集群对外不可用，为了强一致性，牺牲高可用性！（Client与Server之间为心跳维持的TCP长连接） Eureka（Netflix）：AP架构，为了高可用性，牺牲强一致性；服务提供者新节点注册后，消费者需要一定的时间后才能拿到最新服务列表，最长可达60s； Nacos（阿里）：参考了Zookeeper+Eureka，同时支持AP/CP架构，集群默认为AP架构，也可以通过配置切换为CP架构（Raft）；服务列表变动后，消费者获取最新列表最然会有一点延迟，但是比Eureka好很多，而且还可以通过udp实时通知，虽然UDP可靠性无法保证！（Client与Server之间为短链接Http调用） 二 NACOS的服务架构图 **服务注册+服务心跳：**首先无论是“服务提供者”还是“服务消费者”都会将自己注册到nacos，并维持心跳。（每5秒发送一次心跳包） **服务健康检查：**服务端再启动后，会以Service为单位，开启ClientBeatCheckTask心跳检查任务。（每5秒检查一次，如果某个客户端最后一次心跳超过15秒，标记为不健康，超过30秒踢除） 服务发现：“服务消费者”会根据需要自己的自己所需的目标服务的namespace/group/serviceName/cluster只根据需要查询对应的服务注册表，保存在本地。（定时每10s去服务端更新一次）！ **服务同步：**服务端集群之间会同步服务注册表，用来保证服务信息的一致性！（注意AP架构的集群中，即使配置了mysql，也不是用来存放注册表） 三 Nacos的核心注册表结构（双层ConcurrentHashMap） 1 Nacos和Eureka的注册表底层都是双层ConcurrentHashMap // 本篇只介绍Nacos public class ServiceManager implements RecordListener&lt;Service&gt; { // Nacos服务注册表的实际存储结构（双层Map） Map&lt;String, Map&lt;String, Service&gt;&gt; serviceMap = new ConcurrentHashMap&lt;&gt;() //Map&lt;nameSpaceId, Map&lt;group::serverName, Service&gt;&gt; ————&gt; 通过nameSpaceId + group::serviceName定位到具体的服务(Service) //其中的Service服务实例的结构： public class Service { private Map&lt;String, Cluster&gt; clusterMap = new HashMap&lt;&gt;(); //Map&lt;clusterName, Cluster&gt; ————&gt; 在具体的serviceInstance内通过clusterName定位到具体的Cluster集群 //而Cluster集群，又是这样的结构 public class Cluster { private Set&lt;Instance&gt; ephemeralInstances = new HashSet&lt;&gt;(); //这就是实际可以对外提供的单个服务（serviceInstanceItem） } } } 总结：Nacos底层数据结构，显示一个双层Map， —— 1、服务发现阶段，通过nameSpaceId, group::serviceName找到对应的服务 Service服务 —— 2、在服务Service内通过clusterName定位到具体的集群Cluster —— 3、在Cluster集群里面以HashSet的形式，存放着所有能够提供服务的每个实例Instance（这个Instance中有访问它的详细信息），最后把整个Set列表返回给客户端即可! 2 Nacos这么多层的配置，该如何使用？ 最佳实践一（中小型公司）： // namespace：用来区分不通的项目，如haier-iot / haier-code / cold-chain // group：用来区分不通项目的 prod / test / dev 等环境 // ————spring.application.name———— // cluster：可以以低于来划分集群：BJ / NJ / SH // 示例： spring: application: name: haier-iot-device-manager cloud: nacos: discovery: server-addr: 10.206.73.156:8848 namespace: haier-iot group: dev cluster-name: BJ //可以不区分 config: server-addr: 10.206.73.156:8848 file-extension: yaml namespace: haier-iot group: dev cluster-name: BJ //可以不区分 最佳实践二（大型公司）： //与最佳实践一的区别在于，直接使用nacos项目专用，直接使用 namespace 区分环境 // namespace：直接用来区分 prod / test / dev 等环境 // group：使用 DEFAULT_GROUP，因为微服务有可能太多，管理容易混乱；同时这一层可以做扩展，比如多个小服务可能属于另一个大服务下； // ————spring.application.name———— // cluster：可以以低于来划分集群：BJ / NJ / SH // 示例： spring: application: name: haier-iot-device-manager cloud: nacos: discovery: server-addr: 10.206.73.156:8848 namespace: dev group: DEFAULT_GROUP //默认GROUP可以不指定 cluster-name: BJ //可以不区分 config: server-addr: 10.206.73.156:8848 namespace: dev group: DEFAULT_GROUP //默认GROUP可以不指定 file-extension: yaml 3 为什么Nacos要设计这么复杂的数据结构 因为Nacos是一个开放的产品，为了适应绝大多数使用者的使用场景，所以扩展性一定要好，这么多层的设计，几乎可以满足任意复杂的业务场景！ 三 Nacos的注册表写入性能保证 1 Nacos怎么负责的注册表结构，如何支撑高并发场景？（阻塞队列、异步注册） // 使用内存阻塞队列实现异步注册 —— 当接收到provider的注册时，Nacos服务端会将任务封装成Task public class DistroConsistencyServiceImpl{ @PostConstruct public void init() { GlobalExecutor.submitDistroNotifyTask(notifier); } // 而notifier是一个线程，单线程处理服务注册任务，也避免了“并发覆盖”问题！ public class Notifier implements Runnable { private BlockingQueue&lt;Pair&lt;String, DataOperation&gt;&gt; tasks = new ArrayBlockingQueue&lt;&gt;(1024 * 1024); // run()方法就是在处理放入到tasks队列中的Task任务 @Override public void run() { for (; ; ) { // 死循环，即使出现异常也不会退出 try { Pair&lt;String, DataOperation&gt; pair = tasks.take(); //阻塞队列不消耗CPU handle(pair); } catch (Throwable e) { Loggers.DISTRO.error(&quot;[NACOS-DISTRO] Error while handling notifying task&quot;, e); } } } } // 新的Instance任务被封装成Task任务，放入到Notifier中 public void put(String key, Record value) throws NacosException { onPut(key, value); // 任务被封装成 distroProtocol.sync(new DistroKey(key, KeyBuilder.INSTANCE_LIST_KEY_PREFIX), DataOperation.CHANGE, globalConfig.getTaskDispatchPeriod() / 2); } public void onPut(String key, Record value) { ...边缘逻辑... notifier.addTask(key, DataOperation.CHANGE); } } 2 使用阻塞队列实现异步注册，会不会存在不一致问题，还没注册成功就给客户端返回结果？ 是的，肯定会存在这个问题，但是这是一个取舍，高性能的中间件内部都使用了大量的异步操作； 想一想，我们的应用程序可能依赖很多第三方服务，如果第三方中间件都用同步的方式去执行自己的内部逻辑，那么应用程序的启动将变得非常地缓慢，最后的效果肯定是难以接受的； —— 支持高并发！ —— 要说不及时，之前的Eureka更严重！ 其实正常情况下，几乎不会太过阻塞，因为几乎没有多少公司，是一次性增加n多台服务的，都是慢慢添加的，而且即使个别慢了，也是可以接受的，先用其它服务节点即可，站在服务消费者的角度，也就是provider服务起得有点慢而已。 3 为了解决高并发下的读写冲突问题，Nacos使用了CopyOnWrite方案 // 在Notifier.run()方法中： listener.onChange(datumKey, dataStore.get(datumKey).value); | com.alibaba.nacos.naming.core.Service#onChange(){ updateIPs(value.getInstanceList(), KeyBuilder.matchEphemeralInstanceListKey(key)); } | com.alibaba.nacos.naming.core.Service#updateIPs(){ clusterMap.get(entry.getKey()).updateIps(entryIPs, ephemeral); } | com.alibaba.nacos.naming.core.Cluster#updateIps{ Set&lt;Instance&gt; toUpdateInstances = ephemeral ? ephemeralInstances : persistentInstances; // 将旧的临时实例ephemeralInstances列表，复制转化为一个Map进行更新操作 HashMap&lt;String, Instance&gt; oldIpMap = new HashMap&lt;&gt;(toUpdateInstances.size()); for (Instance ip : toUpdateInstances) { oldIpMap.put(ip.getDatumKey(), ip); } //...对旧Set拷贝后转化为HashMap进行更新操作... toUpdateInstances = new HashSet&lt;&gt;(ips); if (ephemeral) { ephemeralInstances = toUpdateInstances; } else { persistentInstances = toUpdateInstances; } } 4 同时n多个实例注册或更新，都进行CopyOnWrite，岂不是会存在“更新覆盖”？ // 1、首先，根据上面的第1条，Notifier的执行是一个单线程执行任务： /// Notifier所在类DistroConsistencyServiceImpl是一个单例Service，@PostConstruct决定了init方法只会被调用一次： //// 而GlobalExecutor 是一个单线程的线程池，所以处理实例注册的最终线程只会有一个 @PostConstruct public void init() { GlobalExecutor.submitDistroNotifyTask(notifier); } // 2、CopyOnWrite后的集合中的元素不能直接修改，因为集合中的元素是引用！ // —— 当新增时，直接在新集合中新增Instance，然后替换原注册表中的集合即可！ // —— 当删除时，直接将新集合中的对应Instance删除，然后替换原注册表中的集合即可！ // —— 当更新时，新增一个Instance，然后删除原集合中的Instance元素，增加新的Instance元素即可！ // ————原则就是：永远是替换，不直接修改原Instance对象！ 5 随着注册表的不断增大，进行CopyOnWrite时候的成本是不是变得非常大？ // 当然不是每次直接Copy整张注册表，那样开销肯定很大 // 每次Copy的粒度是缩小到Service下对应的Cluster中的Set&lt;Instance&gt;集合，这个粒度是很小的！ public class Cluster extends com.alibaba.nacos.api.naming.pojo.Cluster implements Cloneable { private Set&lt;Instance&gt; persistentInstances = new HashSet&lt;&gt;(); // AP模式实例列表（服务发现得到的列表就是它） private Set&lt;Instance&gt; ephemeralInstances = new HashSet&lt;&gt;(); // CP模式实例列表（服务发现得到的列表就是它） // 更新节点的操作（Cluster级别） public void updateIps(List&lt;Instance&gt; ips, boolean ephemeral) { Set&lt;Instance&gt; toUpdateInstances = ephemeral ? ephemeralInstances : persistentInstances; HashMap&lt;String, Instance&gt; oldIpMap = new HashMap&lt;&gt;(toUpdateInstances.size()); //...对旧Set拷贝后转化为HashMap进行更新操作... toUpdateInstances = new HashSet&lt;&gt;(ips); if (ephemeral) { ephemeralInstances = toUpdateInstances; } else { persistentInstances = toUpdateInstances; } } } // 为了性能，CopyOnWrite的粒度一定要越小越好！ 四 Nacos的心跳机制（定时去调Nacos服务端Http接口） 核心类：NacosNamingService 1 Client在向服务端注册服务的同时，开启定时任务向服务端发送心跳请求 // com.alibaba.nacos.client.naming.NacosNamingService#registerInstance() // 既是注册服务的核心代码，也是发送心跳的核心代码 public void registerInstance(String serviceName, String groupName, Instance instance) throws NacosException { String groupedServiceName = NamingUtils.getGroupedName(serviceName, groupName); if (instance.isEphemeral()) { BeatInfo beatInfo = beatReactor.buildBeatInfo(groupedServiceName, instance); beatReactor.addBeatInfo(groupedServiceName, beatInfo); // 发送心跳 } serverProxy.registerService(groupedServiceName, groupName, instance); // 注册实例 } | public void addBeatInfo(String serviceName, BeatInfo beatInfo) { ... // 第一次调用 = 触发心跳任务 executorService.schedule(new BeatTask(beatInfo), beatInfo.getPeriod(), TimeUnit.MILLISECONDS); ... } | //BeatTask.run()任务核心代码： public void run() { if (!this.beatInfo.isStopped()) { // 计算下一次发送的时间 long nextTime = this.beatInfo.getPeriod(); try { //此处就是去调用“发送心跳API” JsonNode result = BeatReactor.this.serverProxy.sendBeat(this.beatInfo, BeatReactor.this.lightBeatEnabled); ...对心跳发送结果进行处理... } catch (NacosException var11) { ...log... } //第二次发送心跳，循环进行，就形成定时发送心跳的效果 BeatReactor.this.executorService.schedule(BeatReactor.this.new BeatTask(this.beatInfo), nextTime, TimeUnit.MILLISECONDS); } } 我们再看看执行“心跳任务”的线程长啥样： // 定时任务线程 this.executorService = new ScheduledThreadPoolExecutor(threadCount, new ThreadFactory() { @Override public Thread newThread(Runnable r) { Thread thread = new Thread(r); thread.setDaemon(true); // 守护线程（所有用户线程结束后，守护线程会自动结束） thread.setName(&quot;com.alibaba.nacos.naming.beat.sender&quot;); return thread; } }); 所以“心跳包”的核心就是：通过一个定时任务守护线程，定时去调用Nacos服务端的发送心跳包的API接口！ 2 “心跳包”的默认间隔时间是多少？（5-15-30） // 构建心跳包的信息 BeatInfo beatInfo = this.beatReactor.buildBeatInfo(groupedServiceName, instance); beatInfo.setPeriod(instance.getInstanceHeartBeatInterval()); public long getInstanceHeartBeatInterval() { return this.getMetaDataByKeyWithDefault(&quot;preserved.heart.beat.interval&quot;, Constants.DEFAULT_HEART_BEAT_INTERVAL); } //而Constants.DEFAULT_HEART_BEAT_INTERVAL是个常量 static { DEFAULT_HEART_BEAT_TIMEOUT = TimeUnit.SECONDS.toMillis(15L); //15秒 收不到心跳，则会被标记为“不健康” DEFAULT_IP_DELETE_TIMEOUT = TimeUnit.SECONDS.toMillis(30L); //30秒 收不到心跳，则“剔除”该实例IP DEFAULT_HEART_BEAT_INTERVAL = TimeUnit.SECONDS.toMillis(5L); //默认心跳时间 5秒 3 Nacos服务端对心跳包的处理逻辑(服务健康检查)？(定时任务，每5秒健康检查) // 以Service为单位，每个Service在被初始化时，都会创建一个健康检查器HealthCheckReactor public class Service { public void init() { // HealthCheckReactor 健康检查器，也是通过schduled线程池去做检查的 HealthCheckReactor.scheduleCheck(clientBeatCheckTask); for (Map.Entry&lt;String, Cluster&gt; entry : clusterMap.entrySet()) { entry.getValue().setService(this); entry.getValue().init(); } } } | // HealthCheckReactor.scheduleCheck()方法就是开启定时任务线程 |线程池大小为1~核数/2 public static void scheduleCheck(BeatCheckTask task) { futureMap.putIfAbsent(task.taskKey(), GlobalExecutor.scheduleNamingHealth(task, 5000, 5000, TimeUnit.MILLISECONDS)); //延迟5秒后，每5秒执行一次 } 健康检查任务的核心 run() 方法逻辑： public class ClientBeatCheckTask implements BeatCheckTask { @Override public void run() { //拿出Service中所有的实例（后面遍历检查） List&lt;Instance&gt; instances = service.allIPs(true); // 系统当前时间 - 最后一次心跳时间 &gt; 不健康阈值（15秒），标记为不健康 for (Instance instance : instances) { if (System.currentTimeMillis() - instance.getLastBeat() &gt; instance.getInstanceHeartBeatTimeOut()) { if (!instance.isMarked()) { if (instance.isHealthy()) { instance.setHealthy(false); Loggers.EVT_LOG .info(&quot;{POS} {IP-DISABLED} valid: {}:{}@{}@{}, region: {}, msg: client timeout after {}, last beat: {}&quot;, instance.getIp(), instance.getPort(), instance.getClusterName(), service.getName(), UtilsAndCommons.LOCALHOST_SITE, instance.getInstanceHeartBeatTimeOut(), instance.getLastBeat()); getPushService().serviceChanged(service); } } } } if (!getGlobalConfig().isExpireInstance()) { return; } // 系统当前时间 - 最后一次心跳时间 &gt; 可剔除阈值（30秒），直接剔除 for (Instance instance : instances) { if (System.currentTimeMillis() - instance.getLastBeat() &gt; instance.getIpDeleteTimeout()) { // delete instance Loggers.SRV_LOG.info(&quot;[AUTO-DELETE-IP] service: {}, ip: {}&quot;, service.getName(), JacksonUtils.toJson(instance)); deleteIp(instance); } } } } 所以“服务健康检查”的逻辑就是：服务端以Service为单位，使用定时任务线程池，每5秒检查一次Service中所有实例的状态： 最后心跳时间距当前超过15秒，标记为不健康； 最后心跳时间距当前超过30秒，将此实例踢除！ 五 服务发现 当服务消费者需要查询自己需要的服务列表时，会优先从本地缓存注册表获取数据，第一次获取为空时，才会从远程Server端获取； 从远程Server获取服务列表的粒度为Cluster粒度，同时还会将自己的udp端口告诉Server端，便于Server变化时的主动通知； 从远程Server获取列表的同时，还会启动定时任务，每隔10秒从Server端同步一次自己的注册表（只同步自己需要的）； udp通知的可靠性不能保证，但是影响不大，因为有定时任务同步托底！ 1 获取服务实例列表核心方法：NacosNamingService#getAllInstances() public List&lt;Instance&gt; getAllInstances(String serviceName, String groupName, List&lt;String&gt; clusters, boolean subscribe) { ServiceInfo serviceInfo; if (subscribe) { // 默认是开启订阅（udp通知），所以走这一分支 serviceInfo = hostReactor.getServiceInfo(NamingUtils.getGroupedName(serviceName, groupName), StringUtils.join(clusters, &quot;,&quot;)); } else { serviceInfo = hostReactor .getServiceInfoDirectlyFromServer(NamingUtils.getGroupedName(serviceName, groupName), StringUtils.join(clusters, &quot;,&quot;)); } List&lt;Instance&gt; list; if (serviceInfo == null || CollectionUtils.isEmpty(list = serviceInfo.getHosts())) { return new ArrayList&lt;Instance&gt;(); } return list; } | public ServiceInfo getServiceInfo(final String serviceName, final String clusters) { String key = ServiceInfo.getKey(serviceName, clusters); if (failoverReactor.isFailoverSwitch()) { return failoverReactor.getService(key); // 故障转移功能，从故障转移文件获取服务列表 } // 从本地缓存的注册表获取服务列表 ServiceInfo serviceObj = getServiceInfo0(serviceName, clusters); if (null == serviceObj) { // 第一次启动时候，缓存肯定为空，所以会走这一分支 serviceObj = new ServiceInfo(serviceName, clusters); serviceInfoMap.put(serviceObj.getKey(), serviceObj); updatingMap.put(serviceName, new Object()); updateServiceNow(serviceName, clusters); // 核心去远程获取服务列表的方法 updatingMap.remove(serviceName); } else if (updatingMap.containsKey(serviceName)) { if (UPDATE_HOLD_INTERVAL &gt; 0) { // hold a moment waiting for update finish synchronized (serviceObj) { try { serviceObj.wait(UPDATE_HOLD_INTERVAL); } catch (InterruptedException e) { NAMING_LOGGER .error(&quot;[getServiceInfo] serviceName:&quot; + serviceName + &quot;, clusters:&quot; + clusters, e); } } } } scheduleUpdateIfAbsent(serviceName, clusters); // 开启定时任务，定时更新本地注册表 return serviceInfoMap.get(serviceObj.getKey()); } 从远程获取服务列表没啥看的，我们重点看看定时任务更新本地缓存注册表的逻辑： public void scheduleUpdateIfAbsent(String serviceName, String clusters) { synchronized (futureMap) { if (futureMap.get(ServiceInfo.getKey(serviceName, clusters)) != null) { return; } // UpdateTask见名知意 ScheduledFuture&lt;?&gt; future = addTask(new UpdateTask(serviceName, clusters)); futureMap.put(ServiceInfo.getKey(serviceName, clusters), future); } } | // 看看UpdateTask.run()核心方法： public void run() { long delayTime = -1; try { ...一系列逻辑，但是最终都会走finally中的逻辑... delayTime = serviceObj.getCacheMillis(); } catch (Throwable e) { NAMING_LOGGER.warn(&quot;[NA] failed to update serviceName: &quot; + serviceName, e); } finally { if (delayTime &gt; 0) { // delayTime默认为10秒 executor.schedule(this, delayTime, TimeUnit.MILLISECONDS); } } } 所以“服务发现”的逻辑就是：在客户端启动时，会根据需要从Nacos服务端获取自己需要的服务列表（Cluster级别）， 并保存到本地缓存中的注册表中，并开启一个定时任务，每隔10秒去服务端同步一下对应的注册表； 之后每次需要时，都是从本地缓存中的注册表获取服务列表即可！ 2 如何尽可能地保证本地注册表的实时性？(开启订阅，开放udp端口) 从第一条中我们看到一个开启订阅的逻辑，在对应的分支中，从服务端获取服务列表时： updateServiceNow(serviceName, clusters); String result = serverProxy.queryList(serviceName, clusters, pushReceiver.getUdpPort(), false); // pushReceiver.getUdpPort() // 可以知道，从服务端获取服务列表时，顺便把自己的udp端口也传给了服务端 // 那么当服务端发现对应的服务列表有变动时，就可以通过此Udp端口通知到本Client 六 服务同步 ​ Nacos集群即使配置了外部mysql数据库，注册表信息也是存储在每个节点的内存中的，而不是存储在mysql中，而当Client向Nacos服务端注册时，只会选择一个Nacos Server节点注册，那么就必须有一套机制能够实现Nacos集群的各个节点都能同步到数据，Nacos自己实现了一套Distro协议，以实现分布式集群各节点之间的数据最终一致性！ 1 什么时Distro协议？ Distro协议时Nacos社区自研的一套AP分布式协议，为了集群的高可用，牺牲强一致性，只追求最终一致性！ Nacos集群的每个节点时平等的，都可以处理读写请求，同时把数据同步到其他节点； 每个节点只负责部分数据（服务健康检查等），定时发送自己负责的数据的校验值到其他节点，以保证数据的一致性； 每个节点独立处理请求，不需要经过其他节点同意，及时从本地发起对Client端的相应！ 2 Nacos集群中的节点，如何知道其它节点的存在？ 得熟悉Nacos AP集群得部署方式，Nacos集群在部署时，需要在配置 cluster.conf 文件中配置集群得各个节点，这样每台机器就都知道集群中得其它节点得ip:port了; @Component(&quot;serverListManager&quot;) public class ServerListManager extends MemberChangeListener { @PostConstruct public void init() { // 集群节点状态同步任务，它会每2秒调用集群其它节点的状态接口，以判断节点是否还在线！ GlobalExecutor.registerServerStatusReporter(new ServerStatusReporter(), 2000); GlobalExecutor.registerServerInfoUpdater(new ServerInfoUpdater()); } // 集群节点状态同步任务，每2秒执行一次， ServerStatusReporter.run(){ // 很简单，就是调用其它节点的状态接口，告诉其它机器自己还活着（集群中每两台机器直接都会互相调用）； // 如果某个节点在一定时间内，没有收到其它某个节点的状态报告，那就认为这个节点挂了，就会更新自己本地认为的集群存活节点数； // 集群存活节点数会直接影响到“服务健康检查”的目标机器核心变量，从而决定每个Service，将会在哪个Server节点被执行健康检查！ synchronizer.send(server.getAddress(), msg); } } 3 “服务注册”任务由哪个节点负责？如何同步数据到其他节点？ “服务注册”任务，有Client端发起，根据负载均衡算法挑选一台Server机器进行注册； 被挑选到的Server节点，处理自己的注册任务的同时，通过Distro协议，同步到集群中的其它节点； // com.alibaba.nacos.naming.consistency.ephemeral.distro.DistroConsistencyServiceImpl#put public void put(String key, Record value) throws NacosException { // 在本机处理服务注册请求 onPut(key, value); // 同步给其它机器进行注册 distroProtocol.sync(new DistroKey(key, KeyBuilder.INSTANCE_LIST_KEY_PREFIX), DataOperation.CHANGE, globalConfig.getTaskDispatchPeriod() / 2); } 4 如何判断各个Service的健康检查任务，由集群中的哪个节点负责检查？ // 我们找到心跳检查任务的run()方法： ClientBeatCheckTask.run(){ // 判断是否该由本节点负责该Service的心跳检查任务 if (!getDistroMapper().responsible(service.getName())) { return; } ...如果是自己负责该Service的心跳检查，才会继续执行心跳检查任务... } // 判断逻辑 public boolean responsible(String serviceName) { final List&lt;String&gt; servers = healthyList; if (!switchDomain.isDistroEnabled() || EnvUtil.getStandaloneMode()) { return true; } if (CollectionUtils.isEmpty(servers)) { // means distro config is not ready yet return false; } int index = servers.indexOf(EnvUtil.getLocalAddress()); int lastIndex = servers.lastIndexOf(EnvUtil.getLocalAddress()); if (lastIndex &lt; 0 || index &lt; 0) { return true; // 自己不在集群列表中，那可能当前就不是集群部署，所以自己得检查 } // 对serviceName进行hash后，对当前集群节点数量取余，看看是不是自己 // 如果不是自己，不用担心，其它机器在被注册时，也会走到这条逻辑，总有一台机器是负责该Service的“健康检查”的 int target = distroHash(serviceName) % servers.size(); return target &gt;= index &amp;&amp; target &lt;= lastIndex; } 5 集群间两个重要的同步任务 1. ServerListManager下的ServerStatusReporter任务： —— 上面已经讲过，在集群之间通过定时调用状态接口的方式，同步集群各节点的在线状态！ 2. ServiceManager下的ServiceReporter任务： —— 当某个节点执行完健康检查后，如果发现某个Service实例状态改变了，它必须要同步给集群中其它节点，修改各自注册表中的状态（通过调用InstanceController中的API接口实现） 6 如果有新节点加入集群，如果从其它节点同步数据？ // 每个节点启动时，会注入一个DistroProtocol的Bean @Component public class DistroProtocol { // 在DistroProtocol的构造函数中，会启动DistroTask数据同步任务 public DistroProtocol(ServerMemberManager memberManager, DistroComponentHolder distroComponentHolder, DistroTaskEngineHolder distroTaskEngineHolder, DistroConfig distroConfig) { this.memberManager = memberManager; this.distroComponentHolder = distroComponentHolder; this.distroTaskEngineHolder = distroTaskEngineHolder; this.distroConfig = distroConfig; startDistroTask(); } private void startDistroTask() { // 如果时单节点运行，就不用同步啦 if (EnvUtil.getStandaloneMode()) { isInitialized = true; return; } startVerifyTask(); startLoadTask(); // 开启数据加载任务 } private void startLoadTask() { DistroCallback loadCallback = new DistroCallback() { @Override public void onSuccess() { isInitialized = true; } @Override public void onFailed(Throwable throwable) { isInitialized = false; } }; GlobalExecutor.submitLoadDataTask( new DistroLoadDataTask(memberManager, distroComponentHolder, distroConfig, loadCallback)); } } //DistroLoadDataTask任务的核心run()方法： DistroLoadDataTask.run(){ try { load(); // 从其它节点加载数据 if (!checkCompleted()) { // 如果不成功，就开个延时任务，过会儿继续尝试去加载 GlobalExecutor.submitLoadDataTask(this, distroConfig.getLoadDataRetryDelayMillis()); } else { loadCallback.onSuccess(); Loggers.DISTRO.info(&quot;[DISTRO-INIT] load snapshot data success&quot;); } } catch (Exception e) { loadCallback.onFailed(e); Loggers.DISTRO.error(&quot;[DISTRO-INIT] load snapshot data failed. &quot;, e); } } 真正的load()从远程加载逻辑： private void load() throws Exception { while (memberManager.allMembersWithoutSelf().isEmpty()) { Loggers.DISTRO.info(&quot;[DISTRO-INIT] waiting server list init...&quot;); TimeUnit.SECONDS.sleep(1); } while (distroComponentHolder.getDataStorageTypes().isEmpty()) { Loggers.DISTRO.info(&quot;[DISTRO-INIT] waiting distro data storage register...&quot;); TimeUnit.SECONDS.sleep(1); } for (String each : distroComponentHolder.getDataStorageTypes()) { if (!loadCompletedMap.containsKey(each) || !loadCompletedMap.get(each)) { loadCompletedMap.put(each, loadAllDataSnapshotFromRemote(each)); } } } // for循环尝试从所有远程节点获取注册表全量文件，只要有一个成功，则跳出for循环 private boolean loadAllDataSnapshotFromRemote(String resourceType) { DistroTransportAgent transportAgent = distroComponentHolder.findTransportAgent(resourceType); DistroDataProcessor dataProcessor = distroComponentHolder.findDataProcessor(resourceType); if (null == transportAgent || null == dataProcessor) { Loggers.DISTRO.warn(&quot;[DISTRO-INIT] Can't find component for type {}, transportAgent: {}, dataProcessor: {}&quot;, resourceType, transportAgent, dataProcessor); return false; } for (Member each : memberManager.allMembersWithoutSelf()) { try { // 调取远程节点的获取DatumSnapshot快照数据接口 DistroData distroData = transportAgent.getDatumSnapshot(each.getAddress()); // 处理数据，加载到本节点内存的注册表中，完成新节点数据初始化 boolean result = dataProcessor.processSnapshot(distroData); if (result) { return true; // 有一个节点成功，则跳出全部for循环，直接返回成功结果 } } catch (Exception e) { ...... } } return false; } ","link":"https://tinaxiawuhao.github.io/post/zMtbXOeeT/"},{"title":"上传文件到亚马逊云S3对象存储","content":"一 在亚马逊云S3创建一个存储桶，并设置权限 1 创建一个存储桶，并将权限设置为公开，因为我正常时候是用来存放网站图片 2 配置存储卷策略 { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Sid&quot;: &quot;PublicReadGetObject&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: &quot;*&quot;, &quot;Action&quot;: &quot;s3:GetObject&quot;, &quot;Resource&quot;: &quot;arn:aws:s3:::test.jiguiquan.com/*&quot; } ] } 3 配置跨域策略 [ { &quot;AllowedHeaders&quot;: [ &quot;*&quot; ], &quot;AllowedMethods&quot;: [ &quot;PUT&quot;, &quot;POST&quot;, &quot;GET&quot; ], &quot;AllowedOrigins&quot;: [ &quot;*&quot; ], &quot;ExposeHeaders&quot;: [ &quot;x-amz-server-side-encryption&quot;, &quot;x-amz-request-id&quot;, &quot;x-amz-id-2&quot; ], &quot;MaxAgeSeconds&quot;: 3000 } ] 到这里，存储卷的配置就算是OK啦，满足正常网站图片的使用了！ 二 编写Java类，完成文件的上传 1 在项目中引入 aws-sdk 的依赖 &lt;dependency&gt; &lt;groupId&gt;com.amazonaws&lt;/groupId&gt; &lt;artifactId&gt;aws-java-sdk-s3&lt;/artifactId&gt; &lt;version&gt;1.11.347&lt;/version&gt; &lt;/dependency&gt; 2 编写上传文件的接口 @Api(tags = &quot;Auth——第三方服务模块&quot;) @RestController @RequiredArgsConstructor public class ThirdpartyController { @Value(&quot;${s3.accessKeyId}&quot;) private String s3AccessKeyId; @Value(&quot;${s3.accessKeySecret}&quot;) private String s3AccessKeySecret; @Value(&quot;${s3.bucketName}&quot;) private String s3BucketName; @Value(&quot;${s3.region}&quot;) private String s3Region; private static BasicAWSCredentials awsCreds; private static AmazonS3 s3; @PostConstruct private void init(){ awsCreds = new BasicAWSCredentials(s3AccessKeyId, s3AccessKeySecret); s3 = AmazonS3ClientBuilder.standard() .withCredentials(new AWSStaticCredentialsProvider(awsCreds)) //设置服务器所属地区 .withRegion(s3Region) .build(); } @ApiResponses({@ApiResponse(code = 200, message = &quot;文件url&quot;)}) @ApiOperation(&quot;后端直接上传文件到亚马逊云S3&quot;) @PostMapping(&quot;/auth/s3/upload&quot;) public BaseResponse&lt;String&gt; uploadFileToS3(@RequestParam(&quot;file&quot;) MultipartFile file) { if (file.getSize() == 0){ throw ZidanApiException.create(BmoonResponseCode.FILE_SIZE_ZERO); } if (StringUtils.isBlank(file.getOriginalFilename())){ throw ZidanApiException.create(BmoonResponseCode.FILE_NAME_EMPTY); } // String host = &quot;https://s3.&quot; + s3Region + &quot;.amazonaws.com/&quot; + s3BucketName; String host = &quot;https://s3.ap-northeast-2.amazonaws.com/test.jiguiquan.com&quot;; String format = new SimpleDateFormat(&quot;yyyyMMdd&quot;).format(new Date()); String uploadName = format + &quot;/&quot; + UUID.randomUUID().toString() + file.getOriginalFilename(); ObjectMetadata metadata = new ObjectMetadata(); metadata.setContentType(file.getContentType()); metadata.setContentLength(file.getSize()); InputStream inputStream = null; try { inputStream = file.getInputStream(); com.amazonaws.services.s3.model.PutObjectResult result = s3.putObject(new PutObjectRequest(s3BucketName, uploadName, inputStream, metadata)); System.out.println(uploadName + &quot;:文件的Md5为：&quot; + result.getContentMd5()); return BaseResponse.success(host + &quot;/&quot; + uploadName); }catch (Exception e) { e.printStackTrace(); throw ZidanApiException.create(BmoonResponseCode.FILE_UPLOAD_FAILED); } finally { if (inputStream != null){ try { inputStream.close(); } catch (IOException e) { e.printStackTrace(); } } } } } 三 测试文件的上传与访问 浏览器访问： ","link":"https://tinaxiawuhao.github.io/post/PTmSAlH_U/"},{"title":"netty","content":"1. Netty简单介绍 1. 原生NIO存在的问题 为什么有了 NIO ，还会出现 Netty，因为 NIO 有如下问题： NIO 的类库和 API 繁杂，使用麻烦：需要熟练掌握 Selector、ServerSocketChannel、 SocketChannel、ByteBuffer 等。 需要具备其他的额外技能：要熟悉 Java 多线程编程，因为 NIO 编程涉及到 Reactor 模式，你必须对多线程和网络编程非常熟悉，才能编写出高质量的 NIO 程序。 开发工作量和难度都非常大：例如客户端面临断连重连、网络闪断、半包读写、失败缓存、网络拥塞和异常流的处理等等。 JDK NIO 的 Bug：例如臭名昭著的 Epoll Bug，它会导致 Selector 空轮询（死循环），最终导致 CPU 100%。直到 JDK 1.7 版本该问题仍旧存在，没有被根本解决 2. Netty官网说明 官网地址 ：https://netty.io/ Netty是一个异步事件驱动的网络应用程序框架，用于快速开发可维护的高性能的服务器和客户端。 Netty 是由 JBOSS 提供的一个 Java 开源框架。Netty 提供异步的、基于事件驱动的网络应用程序框架，用以快速开发高性能、高可靠性的网络 IO 程序。 Netty 可以帮助你快速、简单的开发出一个网络应用，相当于简化和流程化了 NIO 的 开发过程。 Netty 是目前最流行的 NIO 框架，Netty 在互联网领域、大数据分布式计算领域、游戏行业、通信行业等获得了广泛的应用，知名的 Elasticsearch 、Dubbo 框架内部都采 用了 Netty。 3. Netty的优点 Netty 对 JDK 自带的 NIO 的 API 进行了封装，解决了上述问题。 设计优雅：适用于各种传输类型的统一 API 阻塞和非阻塞 Socket；基于灵活且可扩展的事件模型，可以清晰地分离关注点；高度可定制的线程模型 —— 单线程，一个或多个 线程池. 使用方便：详细记录的 Javadoc，用户指南和示例；没有其他依赖项 (JDK 5 -&gt; Netty 3.x ; 6 -&gt; Netty 4.x 就可以支持了）。 高性能、吞吐量更高：延迟更低；减少资源消耗；最小化不必要的内存复制。 安全：完整的 SSL/TLS 和 StartTLS 支持。 社区活跃、不断更新：社区活跃，版本迭代周期短，发现的 Bug 可以被及时修复， 同时，更多的新功能会被加入 4. Netty版本说明 netty版本分为 netty3.x 和 netty4.x、netty5.x 因为Netty5出现重大bug，已经被官网废弃了，目前推荐使用的是Netty4.x的稳定版本 目前在官网可下载的版本 netty3.x netty4.0.x 和 netty4.1.x 这里使用的是 Netty4.1.x 版本 netty 下载地址： https://bintray.com/netty/downloads/netty/ 2. 各线程模式 1. 传统阻塞 I/O 服务模型 模型特点 采用阻塞IO模式获取输入的数据 每个连接都需要独立的线程完成数据的输入（read），业务处理， 数据返回（send） 问题分析 当并发数很大，就会创建大量的线程，占用很大系统资源 连接创建后，如果当前线程暂时没有数据可读，该线程会阻塞在read 操作，造成线程资源浪费 2. Reactor 模式 针对传统阻塞 I/O 服务模型的 2 个缺点，解决方案： 基于 I/O 复用模型：多个连接共用一个阻塞对象，应用程序只需要在一个阻塞对象等待，无需阻塞等待所有连接。当某个连接有新的数据可以处理时，操作系统通知应用程序，线程从阻塞状态返回，开始进行业务处理 基于线程池复用线程资源：不必再为每个连接创建线程，将连接完成后的业务处理任务分配给线程进行处理，一个线程可以处理多个连接的业务。 Reactor 对应的叫法: 反应器模式 分发者模式(Dispatcher) 通知者模式(notifier) 说明: Reactor 模式，通过一个或多个输入同时传递给服务处理器的模式 (基于事件驱动) 服务器端程序处理传入的多个请求, 并将它们同步分派到相应的处理线程， 因此Reactor（反应器）模式也叫 Dispatcher（分发者） 模式 Reactor 模式使用IO复用监听事件, 收到事件后，分发给某个线程(进程), 这点就是网络服务器高并发处理关键 核心组成： Reactor：Reactor 在一个单独的线程中运行，负责监听和分发事件，分发给适当的处 理程序来对 IO 事件做出反应。 它就像公司的电话接线员，它接听来自客户的电话并 将线路转移到适当的联系人； Handlers：处理程序执行 I/O 事件要完成的实际事件，类似于客户想要与之交谈的公司中的实际官员。Reactor 通过调度适当的处理程序来响应 I/O 事件，处理程序执行非阻塞操作 Reactor 模式分类： 单 Reactor 单线程 单 Reactor 多线程 主从 Reactor 多线程 3. 单Reactor-单线程 说明： Select 是前面 I/O 复用模型介绍的标准网络编程 API，可以实现应用程序通过一个阻塞对象监听多路连接请求 Reactor 对象通过 Select 监控客户端请求事件，收到事件后通过 Dispatch 进行分发 如果是建立连接请求事件，则由 Acceptor 通过 Accept 处理连接请求，然后创建一个 Handler 对象处理连接完成后的后续业务处理 如果不是建立连接事件，则 Reactor 会分发调用连接对应的 Handler 来响应 Handler 会完成 Read→业务处理→Send 的完整业务流程 结合实例：服务器端用一个线程通过多路复用搞定所有的 IO 操作（包括连接，读、写 等），编码简单，清晰明了，但是如果客户端连接数量较多，将无法支撑，前面的 NIO 案例就属于这种模型。 优点： 模型简单，没有多线程、进程通信、竞争的问题，全部都在一个线程中完成 缺点： 性能问题，只有一个线程，无法完全发挥多核 CPU 的性能。Handler 在处理某 个连接上的业务时，整个进程无法处理其他连接事件，很容易导致性能瓶颈 可靠性问题，线程意外终止，或者进入死循环，会导致整个系统通信模块不 可用，不能接收和处理外部消息，造成节点故障 使用场景：客户端的数量有限，业务处理非常快速，比如 Redis在业务处理的时间复 杂度 O(1) 的情况 4. 单 Reactor 多线程 说明： Reactor 对象通过 select 监控客户端请求事件, 收到事件后，通过 dispatch 进行分发 如果建立连接请求, 则由 Acceptor 通过 accept 处理连接请求, 然后创建一个Handler对象处理完成连接后的各种事件 如果不是连接请求，则由 Reactor 分发调用连接对 应的Handler 来处理 Handler 只负责响应事件，不做具体的业务处理, 通过 read 读取数据后，会分发给后面的 Worker 线程池的某个线程处理业务 Worker 线程池会分配独立线程完成真正的业务， 并将结果返回给 Handler Handler 收到响应后，通过 send 将结果返回给 Client 优点：可以充分的利用多核cpu 的处理能力 缺点：多线程数据共享和访问比较复杂,单 Reactor 处理所有的事件的监听和响应，在单线程运行， 在高并发场景容易出现性能瓶颈 5. 主从 Reactor 多线程 说明 1: Reactor 主线程 MainReactor 对象通过 select 监听连接事件, 收到事件后，通过 Acceptor 处理连接事件(主 Reactor 只处理连接事件) 2：当 Acceptor 处理连接事件后，MainReactor 将连接分配给 SubReactor 3：SubReactor 将连接加入到连接队列进行监听,并创建 Handler 进行各种事件处理 4：当有新事件发生时， SubReactor 就会调用对应的 Handler处理，Handler 通过 read 读取数据，分发给后面的 （Worker 线程池）处理 5：（Worker 线程池）分配独立的 （Worker 线程）进行业务处理，并返 回结果 6：Handler 收到响应的结果后，再通过 send 将结果返回给 Client ps：一个 MainReactor 可以关联多个 SubReactor 优点： 父线程与子线程的数据交互简单职责明确，父线程只需要接收新连接，子线程完成后续的业务处理。 父线程与子线程的数据交互简单，Reactor 主线程只需要把新连接传给子线程，子线程无需返回数据。 缺点：编程复杂度较高 结合实例：这种模型在许多项目中广泛使用，包括 Nginx 主从 Reactor 多进程模型， Memcached 主从多线程，Netty 主从多线程模型的支持 6. Reactor 模式小结 3 种模式用生活案例来理解 单 Reactor 单线程，前台接待员和服务员是同一个人，全程为顾客服 单 Reactor 多线程，1 个前台接待员，多个服务员，接待员只负责接待 主从 Reactor 多线程，多个前台接待员，多个服务生 Reactor 模式具有如下的优点： 响应快，不必为单个同步时间所阻塞，虽然 Reactor 本身依然是同步的 可以最大程度的避免复杂的多线程及同步问题，并且避免了多线程/进程 的切换开销 扩展性好，可以方便的通过增加 Reactor 实例个数来充分利用 CPU 资源 复用性好，Reactor 模型本身与具体事件处理逻辑无关，具有很高的复用性 3. Netty模型 1. 简单版 说明 ： BossGroup 线程维护Selector , 只关注Accecpt 当接收到Accept事件，获取到对应的 SocketChannel, 封装成 NIOScoketChannel并注册到 Worker 线程(事件循环), 并进行维护 当Worker线程监听到 Selector 中通道发生自己感 兴趣的事件后，就进行处理(就由 Handler 处理)， 注意 Handler 已经加入到通道 2. 进阶版 3. 完整版 非常重要 说明 ： 1：Netty 抽象出两组线程池 BossGroup 专门负责接收客户端的连接, WorkerGroup 专门负责网络的读写 2：BossGroup 和 WorkerGroup 类型都是 NioEventLoopGroup 3：NioEventLoopGroup 相当于一个事件循环组, 这个组中含有多个事件循环 ，每一个事件循环是 NioEventLoop 4：NioEventLoop 表示一个不断循环的执行处理任务的线程， 每个 NioEventLoop 都有一个 Selector , 用于监听绑定在其上的 Socket 的网络通讯 5:NioEventLoopGroup(BossGroup、WorkerGroup) 可以有多个线程, 即可以含有多个 NioEventLoop 6:每个Boss 的 NioEventLoop 循环执行的步骤有3步 (1):轮询accept 事件 (2):处理accept 事件 , 与client建立连接 , 生成NioScocketChannel , 并将其注册到 Worker 的 (3):NIOEventLoop 上的 Selector 处理任务队列的任务 ， 即 runAllTasks 7：每个 Worker 的 NIOEventLoop 循环执行的步骤 (1):轮询read, write 事件 (2):处理i/o事件， 即read , write 事件，在对应NioScocketChannel 处理 (3):处理任务队列的任务 ， 即 runAllTasks 每个Worker NIOEventLoop 处理业务时，会使用 Pipeline(管道), Pipeline 中包含了 Channel , 即通过 Pipeline 可以获取到对应通道, 管道中维护了很多的处理器。管道可以使用 Netty 提供的，也可以自定义 4. 理解重制版 服务器端首先创建一个ServerSocketChannel，bossGroup只处理客户端连接请求,workGroup处理读写事件，此二者为线程组，其中每一个NIOEventLoop都是线程组其中一个线程 客户端发送连接请求通过ServerSocketChannel被NIOLoopEventGroup线程组中的一个线程NIOLoopEvent的selector选择器监听，并将事件放入taskqueue队列进行轮询，一旦有accept事件就会封装NIOSocketChannel对象，并通过其去注册到workGroup的线程的selector中让其监听 一旦workGroup的线程的taskqueue轮询到读写事件，在对应的NIOSocketChannel进行处理 4. Netty实例分析 1. BossGroup 和 WorkGroup 怎么确定自己有多少个 NIOEventLoop BossGroup 和 WorkerGroup 含有的子线程数（NioEventLoop）默认为 CPU 核数*2 由源码中的构造方法可知 —— 想要设置线程数只要在参数中输入即可 2. WorkerGroup 是如何分配这些进程的 设置 BossGroup 进程数为 1 ； WorkerGroup 进程数为 4 ； Client 数位 8 在默认情况下，WorkerGroup 分配的逻辑就是按顺序循环分配的 3. BossGroup 和 WorkerGroup 中的 Selector 和 TaskQueue 打断点进行 Debug 每个子线程都具有自己的 Selector、TaskQueue…… 4. CTX 上下文、Channel、Pipeline 之间关系 修改 NettyServerHandler ，并添加端点 先看 CTX 上下文中的信息 Pipeline Channel CTX 上下文、Channel、Pipeline 三者关系示意图 5. 设置通道参数 childOption() 方法 给每条child channel 连接设置一些TCP底层相关的属性，比如上面，我们设置了两种TCP属性，其中 ChannelOption.SO_KEEPALIVE表示是否开启TCP底层心跳机制，true为开 option() 方法 对于server bootstrap而言，这个方法，是给parent channel 连接设置一些TCP底层相关的属性。 TCP连接的参数详细介绍如下。SO_RCVBUF ，SO_SNDBUF 这两个选项就是来设置TCP连接的两个buffer尺寸的。 每个TCP socket在内核中都有一个发送缓冲区和一个接收缓冲区，TCP的全双工的工作模式以及TCP的滑动窗口便是依赖于这两个独立的buffer以及此buffer的填充状态。 SO_SNDBUF Socket参数，TCP数据发送缓冲区大小。该缓冲区即TCP发送滑动窗口，linux操作系统可使用命令：cat /proc/sys/net/ipv4/tcp_smem 查询其大小。 TCP_NODELAY TCP参数，立即发送数据，默认值为Ture（Netty默认为True而操作系统默认为False）。该值设置Nagle算法的启用，改算法将小的碎片数据连接成更大的报文来最小化所发送的报文的数量，如果需要发送一些较小的报文，则需要禁用该算法。Netty默认禁用该算法，从而最小化报文传输延时。 这个参数，与是否开启Nagle算法是反着来的，true表示关闭，false表示开启。通俗地说，如果要求高实时性，有数据发送时就马上发送，就关闭，如果需要减少发送次数减少网络交互，就开启。 SO_KEEPALIVE 底层TCP协议的心跳机制。Socket参数，连接保活，默认值为False。启用该功能时，TCP会主动探测空闲连接的有效性。可以将此功能视为TCP的心跳机制，需要注意的是：默认的心跳间隔是7200s即2小时。Netty默认关闭该功能。 SO_REUSEADDR Socket参数，地址复用，默认值False。有四种情况可以使用： (1).当有一个有相同本地地址和端口的socket1处于TIME_WAIT状态时，而你希望启动的程序的socket2要占用该地址和端口，比如重启服务且保持先前端口。 (2).有多块网卡或用IP Alias技术的机器在同一端口启动多个进程，但每个进程绑定的本地IP地址不能相同。 (3).单个进程绑定相同的端口到多个socket上，但每个socket绑定的ip地址不同。(4).完全相同的地址和端口的重复绑定。但这只用于UDP的多播，不用于TCP。 SO_LINGER Socket参数，关闭Socket的延迟时间，默认值为-1，表示禁用该功能。-1表示socket.close()方法立即返回，但OS底层会将发送缓冲区全部发送到对端。0表示socket.close()方法立即返回，OS放弃发送缓冲区的数据直接向对端发送RST包，对端收到复位错误。非0整数值表示调用socket.close()方法的线程被阻塞直到延迟时间到或发送缓冲区中的数据发送完毕，若超时，则对端会收到复位错误。 SO_BACKLOG Socket参数，服务端接受连接的队列长度，如果队列已满，客户端连接将被拒绝。默认值，Windows为200，其他为128。 b.option(ChannelOption.SO_BACKLOG, 1024) 表示系统用于临时存放已完成三次握手的请求的队列的最大长度，如果连接建立频繁，服务器处理创建新连接较慢，可以适当调大这个参数. SO_BROADCAST Socket参数，设置广播模式。 5. TaskQueue 任务队列 任务队列中的 Task 有 3 种典型使用场景 用户程序自定义的普通任务 用户自定义定时任务 非当前 Reactor 线程调用 Channel 的各种方法 6. 异步模型 1. 工作示意图 说明 1:在使用 Netty 进行编程时，拦截操作和转换出入站数据只需要您提供 callback 或利用 future 即可。这使得链式操作简单、高效, 并有利于编写可重用的、通用的代码。 2:Netty 框架的目标就是让你的业务逻辑从网络基础应用编码中分离出来 2. Future-Listener 机制 当 Future 对象刚刚创建时，处于非完成状态，调用者可以通过返回的 ChannelFuture 来获取操作执行的状态，注册监听函数来执行完成后的操作。 常见有如下操作 • 通过 isDone 方法来判断当前操作是否完成； • 通过 isSuccess 方法来判断已完成的当前操作是否成功； • 通过 getCause 方法来获取已完成的当前操作失败的原因； • 通过 isCancelled 方法来判断已完成的当前操作是否被取消； • 通过 addListener 方法来注册监听器，当操作已完成(isDone 方法返回完成)，将会通知 指定的监听器；如果 Future 对象已完成，则通知指定的监听器 代码示例 给一个 ChannelFuture 注册监听器，来监控我们关系的事件 channelFuture.addListener(new ChannelFutureListener() { @Override public void operationComplete(ChannelFuture channelFuture) throws Exception { if (channelFuture.isSuccess()){ System.out.println(&quot;监听端口 6668 成功&quot;); }else { System.out.println(&quot;监听端口 6668 失败&quot;); } } }); 3. 快速入门实例-HTTP服务 Netty 可以做Http服务开发，并且理解Handler实例 和客户端及其请求的关系 编写代码 —— 服务端代码 # 编写服务端： HttpServer public static void main(String[] args) throws Exception { //创建BossGroup ,workGroup EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workGroup = new NioEventLoopGroup(); try { ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.group(bossGroup,workGroup) .channel(NioServerSocketChannel.class) .childHandler(new TestServerInitializer()); ChannelFuture channelFuture = serverBootstrap.bind(8080).sync(); channelFuture.channel().closeFuture().sync(); } finally { bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); } } 编写 服务初始化器 ：HttpServerInitialize public class TestServerInitializer extends ChannelInitializer&lt;SocketChannel&gt; { @Override protected void initChannel(SocketChannel socketChannel) throws Exception { //向管道加入处理器 ChannelPipeline pipeline = socketChannel.pipeline(); //加入netty提供的httpServerCodec,netty提供的处理Http的编-解码器 pipeline.addLast(&quot;MyHttpServerCodec&quot;,new HttpServerCodec()); pipeline.addLast(&quot;MyTestHttpServerHandler&quot;,new TestHttpServerHandler()); } } 编写 服务处理器 ：HttpServerHandler /** * * 1. SimpleChannelInboundHandler 是之前使用的 ChannelInboundHandlerAdapter 的子类 * 2. HttpObject 这个类型表示， 客户端、服务端 相互通信的数据需要被封装成什么类型 * */ public class TestHttpServerHandler extends SimpleChannelInboundHandler&lt;HttpObject&gt; { /** * 读取客户端数据 * @param ctx * @param msg * @throws Exception */ @Override protected void channelRead0(ChannelHandlerContext ctx, HttpObject msg) throws Exception { //判断msg 是不是HttpRequest 请求 if(msg instanceof HttpRequest){ System.out.println(&quot;msg 类型 = &quot;+msg.getClass()); System.out.println(&quot;客户端地址&quot;+ctx.channel().remoteAddress()); // 获取请求的 URI HttpRequest httpRequest = (HttpRequest) msg; URI uri = new URI(httpRequest.uri()); // 判断请求路径为 /favicon.ico，就不做处理 if (&quot;/favicon.ico&quot;.equals(uri.getPath())){ System.out.println(&quot;请求了 图标 资源，不做响应&quot;); return; } //回复信息给浏览器【HTTP协议】 ByteBuf content = Unpooled.copiedBuffer(&quot;Hello I am server&quot;, CharsetUtil.UTF_8); //构造一个http回应，即httpResponse,HttpResponseStatus:状态码 FullHttpResponse response = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.OK,content); response.headers().set(HttpHeaderNames.CONTENT_TYPE,&quot;text/plain&quot;); response.headers().set(HttpHeaderNames.CONTENT_LENGTH,content.readableBytes()); //将构建好的response返回 ctx.writeAndFlush(response); } } 编写代码 —— 对特定资源的过滤 上面的服务端启动后，在页面上不止接收到了文本，还接收到了一个网页的图标 现在把它过滤掉 // 修改 HttpServerHandler // 获取请求的 URI HttpRequest httpRequest = (HttpRequest) msg; URI uri = new URI(httpRequest.uri()); // 判断请求路径为 /favicon.ico，就不做处理 if (&quot;/favicon.ico&quot;.equals(uri.getPath())){ System.out.println(&quot;请求了 图标 资源，不做响应&quot;); return; } 7. netty核心组件 1. Bootstrap 和 ServerBootstrap Bootstrap 意思是引导，一个 Netty 应用通常由一个 Bootstrap 开始，主要作用是配置整个 Netty 程序，串联各个组件，Netty 中 Bootstrap类是客户端程序的启动引导类， ServerBootstrap是服务端启动引导类 常见的方法有 • public ServerBootstrap group(EventLoopGroup parentGroup, EventLoopGroup childGroup)，该方法用于服务器端，用来设置两个 EventLoop • public B channel(Class&lt;? extends C&gt; channelClass)，该方法用来设置一个服务器端的通道实现 • public ChannelFuture bind(int inetPort) ，该方法用于服务器端，用来设置占用的端口号 • public B option(ChannelOption option, T value)，用来给 ServerChannel 添加配置 • public B group(EventLoopGroup group) ，该方法用于客户端，用来设置一个 EventLoop • public ChannelFuture connect(String inetHost, int inetPort) ，该方法用于客户端，用来连接服务器 端 • public ServerBootstrap childOption(ChannelOption childOption, T value)，用来给接收到的通道添加配置 • public ServerBootstrap childHandler(ChannelHandler childHandler)，该方法用来设置业务处理类 （自定义的 handler） .childHandler(new TestServerInitializer())//对应workGroup .handler(null)//对应bossGroup 2. Future 和 ChannelFuture Netty 中所有的 IO 操作都是异步的，不能立刻得知消息是否被正确处理。但是可以过一会等它执行完成或者直接注册一个监听，具体的实现就是通过 Future 和 ChannelFutures，他们可以注册一个监听，当操作执行成功或失败时监听会自动触发注册的监听事件 常见的方法有 • Channel channel()，返回当前正在进行 IO 操作的通道 • ChannelFuture sync()，等待异步操作执行完毕 3. Channel Netty 网络通信的组件，能够用于执行网络 I/O 操作。 通过Channel 可获得当前网络连接的通道的状态 通过Channel 可获得网络连接的配置参数 （例如接收缓冲区大小） Channel 提供异步的网络 I/O 操作(如建立连接，读写，绑定端口)，异步调用意味着任何 I/O 调用都将立即返回，并且不保证在调用结束时所请求的 I/O 操作已完成 调用立即返回一个 ChannelFuture 实例，通过注册监听器到 ChannelFuture 上，可以 I/O 操作成功、失败或取消时回调通知调用方 支持关联 I/O 操作与对应的处理程序 不同协议、不同的阻塞类型的连接都有不同的 Channel 类型与之对应 常用的 Channel 类型 • NioSocketChannel，异步的客户端 TCP Socket 连接。 • NioServerSocketChannel，异步的服务器端 TCP Socket 连接。 • NioDatagramChannel，异步的 UDP 连接。 • NioSctpChannel，异步的客户端 Sctp 连接。 • NioSctpServerChannel，异步的 Sctp 服务器端连接，这些通道涵盖了 UDP 和 TCP 网络 IO 以及文件 IO。 4. Selector Netty 基于 Selector 对象实现 I/O 多路复用，通过 Selector 一个线程可以监听多个连接的 Channel 事件。 当向一个 Selector 中注册 Channel 后，Selector 内部的机制就可以自动不断地查询 (Select) 这些注册的 Channel 是否有已就绪的 I/O 事件（例如可读，可写，网络连接 完成等），这样程序就可以很简单地使用一个线程高效地管理多个 Channel 5. ChannelHandler 我们经常需要自定义一 个 Handler 类去继承 ChannelInboundHandlerA dapter，然后通过重写相应方法实现业务逻辑 常用的方法 public class ChannelInboundHandlerAdapter extends ChannelHandlerAdapter implements ChannelInboundHandler { // 通道注册事件 public void channelRegistered(ChannelHandlerContext ctx) throws Exception { ctx.fireChannelRegistered(); } // 通道注销事件 public void channelUnregistered(ChannelHandlerContext ctx) throws Exception { ctx.fireChannelUnregistered(); } // 通道就绪事件 public void channelActive(ChannelHandlerContext ctx) throws Exception { ctx.fireChannelActive(); } // 通道读取数据事件 public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { ctx.fireChannelRead(msg); } // 通道读取数据完毕事件 public void channelReadComplete(ChannelHandlerContext ctx) throws Exception { ctx.fireChannelReadComplete(); } // 通道发生异常事件 public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { ctx.fireExceptionCaught(cause); } } 6. Pipeline 和 ChannelPipeline ChannelPipeline 是一个 Handler 的集合，它负责处理和拦截 inbound（入栈） 或者 outbound（出栈） 的事件和操作，相当于一个贯穿 Netty 的链。(也可以这样理解： ChannelPipeline 是 保存 ChannelHandler 的 List，用于处理或拦截 Channel 的入站 和出站 事件 / 操作) ChannelPipeline 实现了一种高级形式的拦截过滤器模式，使用户可以完全控制事件的处理方式，以及 Channel 中各个的 ChannelHandler 如何相互交互 在 Netty 中每个 Channel 都有且仅有一个 ChannelPipeline 与之对应，它们的组成关系如下 说明 ： • 一个 Channel 包含了一个 ChannelPipeline，而 ChannelPipeline 中又维护了一个由 ChannelHandlerContext 组成的双向链表，并且每个 ChannelHandlerContext 中又关联着一个 ChannelHandler • 入站事件和出站事件在一个双向链表中，入站事件会从链表 head 往后传递到最后一个入站的 handler， 出站事件会从链表 tail 往前传递到最前一个出站的 handler，两种类型的 handler 互不干扰 常用方法 //把一个业务处理类（handler） 添加到链中的第一个位置 ChannelPipeline addFirst(ChannelHandler… handlers) //把一个业务处理类（handler） 添加到链中的最后一个位置 ChannelPipeline addLast(ChannelHandler… handlers) 8. Netty 群聊 要求： 编写一个 Netty 群聊系统，实现服务器端和客户端之间的数据简单通讯（非阻塞） 实现多人群聊 服务器端：可以监测用户上线，离线，并实现消息转发功能 客户端：通过channel 可以无阻塞发送消息给其它所有用户，同时可以接受其它用 户发送的消息(有服务器转发得到) 1. server端 public class ChatServer { private int port; public ChatServer(int port) { this.port = port; } public void run() throws Exception{ //创建bossGroup,workGroup EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workGroup = new NioEventLoopGroup(); try { //创建辅助工具 ServerBootstrap serverBootstrap = new ServerBootstrap(); //循环事件组 serverBootstrap.group(bossGroup,workGroup)//线程组 .channel(NioServerSocketChannel.class)//通道类型 .option(ChannelOption.SO_BACKLOG,128) .childOption(ChannelOption.SO_KEEPALIVE,true) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override protected void initChannel(SocketChannel socketChannel) throws Exception { ChannelPipeline pipeline = socketChannel.pipeline(); //加入解码器 pipeline.addLast(&quot;decoder&quot;,new StringDecoder()); //加入编码器 pipeline.addLast(&quot;encoder&quot;,new StringEncoder()); pipeline.addLast(new ChatServerHandler()); } }); System.out.println(&quot;server is ok&quot;); ChannelFuture channelFuture = serverBootstrap.bind(port).sync(); channelFuture.channel().closeFuture().sync(); } finally { bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); } } public static void main(String[] args) throws Exception { new ChatServer(8080).run(); } } 2. serverHandler public class ChatServerHandler extends SimpleChannelInboundHandler&lt;String&gt; { /** * 定义一个 Channel 线程组，管理所有的 Channel, 参数 执行器 * GlobalEventExecutor =&gt; 全局事件执行器 * INSTANCE =&gt; 表示是单例的 */ private static ChannelGroup channelGroup = new DefaultChannelGroup(GlobalEventExecutor.INSTANCE); SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); /** * 当连接建立之后，第一个被执行 * 一连接成功，就把当前的 Channel 加入到 ChannelGroup，并将上线消息推送给其他客户 */ @Override public void handlerAdded(ChannelHandlerContext ctx) throws Exception { Channel channel = ctx.channel(); // 将该客户上线的信息，推送给其他在线的 客户端 // 该方法，会将 ChannelGroup 中所有的 Channel 遍历，并发送消息 Date date = new Date(System.currentTimeMillis()); channelGroup.writeAndFlush(&quot;[client]&quot; +channel.remoteAddress()+&quot;[&quot;+simpleDateFormat.format(date)+&quot;]&quot;+&quot;加入聊天&quot;); channelGroup.add(channel); } /** * 当断开连接激活，将 XXX 退出群聊消息推送给当前在线的客户 * 当某个 Channel 执行到这个方法，会自动从 ChannelGroup 中移除 */ @Override public void handlerRemoved(ChannelHandlerContext ctx) throws Exception { Channel channel = ctx.channel(); Date date = new Date(System.currentTimeMillis()); channelGroup.writeAndFlush(&quot;[client]&quot;+channel.remoteAddress()+&quot;[&quot;+simpleDateFormat.format(date)+&quot;]&quot;+&quot;离开聊天&quot;); //channelGroup.remove(channel); 不需要，handlerRemoved（）直接删除了channel } /** * 提示客户端离线状态 * @param ctx * @throws Exception */ @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception { Date date = new Date(System.currentTimeMillis()); System.out.println(ctx.channel().remoteAddress()+&quot;[&quot;+simpleDateFormat.format(date)+&quot;]&quot;+&quot;下线了&quot;); } /** * 提示客户端上线状态 * @param ctx * @throws Exception */ @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { Date date = new Date(System.currentTimeMillis()); System.out.println(ctx.channel().remoteAddress()+&quot;[&quot;+simpleDateFormat.format(date)+&quot;]&quot;+&quot;上线了&quot;);; } /** * 读取消息，转发数据 * @param ctx * @param msg * @throws Exception */ @Override protected void channelRead0(ChannelHandlerContext ctx, String msg) throws Exception { Channel channel = ctx.channel(); Date date = new Date(System.currentTimeMillis()); //遍历，根据不同对象发送不同数据 channelGroup.forEach(ch -&gt;{ if(channel != ch){//不是自己的channel ch.writeAndFlush(&quot;[客户]&quot;+channel.remoteAddress()+&quot;[&quot;+simpleDateFormat.format(date)+&quot;]&quot;+&quot;发送&quot;+msg+&quot;/n&quot;); } else{ ch.writeAndFlush(&quot;[自己]发送了消息&quot;+&quot;[&quot;+simpleDateFormat.format(date)+&quot;]&quot;+msg+&quot;/n&quot;); } } ); } /** * 异常处理 * @param ctx * @param cause * @throws Exception */ @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { ctx.close(); } } 3. client端 public class ChatClient { private final String HOST; private final int PORT; public ChatClient(String host, int port) { HOST = host; PORT = port; } public void run() throws InterruptedException { EventLoopGroup eventLoopGroup = new NioEventLoopGroup(); try { Bootstrap bootstrap = new Bootstrap(); bootstrap.group(eventLoopGroup) .channel(NioSocketChannel.class) .handler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override protected void initChannel(SocketChannel socketChannel) throws Exception { ChannelPipeline pipeline = socketChannel.pipeline(); //加入解码器 pipeline.addLast(&quot;decoder&quot;,new StringDecoder()); //加入编码器 pipeline.addLast(&quot;encoder&quot;,new StringEncoder()); pipeline.addLast(new ChatClientHandler()); } }); ChannelFuture channelFuture = bootstrap.connect(HOST, PORT).sync(); System.out.println(&quot;client prepare is ok&quot;); Channel channel = channelFuture.channel(); //客户端需要输入信息，定义扫描器 Scanner scanner = new Scanner(System.in); while(scanner.hasNextLine()){ String s = scanner.nextLine(); channel.writeAndFlush(s); } channel.closeFuture().sync(); } finally { eventLoopGroup.shutdownGracefully(); } } public static void main(String[] args) throws InterruptedException { new ChatClient(&quot;localhost&quot;,8080).run(); } } 4. clientHandler public class ChatClientHandler extends SimpleChannelInboundHandler&lt;String&gt; { @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, String s) throws Exception { // 直接输出从服务端获得的信息 System.out.println(s.trim()); } } 9. 心跳机制 1. server端： public class MyServer { public static void main(String[] args) throws Exception{ //创建bossGroup,workGroup EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workGroup = new NioEventLoopGroup(); try { //创建辅助工具 ServerBootstrap serverBootstrap = new ServerBootstrap(); //循环事件组 serverBootstrap.group(bossGroup,workGroup)//线程组 .channel(NioServerSocketChannel.class)//通道类型 .handler(new LoggingHandler(LogLevel.INFO)) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override protected void initChannel(SocketChannel socketChannel) throws Exception { ChannelPipeline pipeline = socketChannel.pipeline(); /* 说明： 1. IdleStateHandler 是 Netty 提供的 空闲状态处理器 2. 四个参数： readerIdleTime : 表示多久没有 读 事件后，就会发送一个心跳检测包，检测是否还是连接状态 writerIdleTime : 表示多久没有 写 事件后，就会发送一个心跳检测包，检测是否还是连接状态 allIdleTime : 表示多久时间既没读也没写 后，就会发送一个心跳检测包，检测是否还是连接状态 TimeUnit : 时间单位 1. 当 Channel 一段时间内没有执行 读 / 写 / 读写 事件后，就会触发一个 IdleStateEvent 空闲状态事件 2. 当 IdleStateEvent 触发后，就会传递给 Pipeline 中的下一个 Handler 去处理，通过回调下一个 Handler 的 userEventTriggered 方法，在该方法中处理 IdleStateEvent */ pipeline.addLast(new IdleStateHandler(3,5,7, TimeUnit.SECONDS)); pipeline.addLast(new MyServerHandler()); } }); System.out.println(&quot;server is ok&quot;); ChannelFuture channelFuture = serverBootstrap.bind(8080).sync(); channelFuture.channel().closeFuture().sync(); } finally { bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); } } } 2. handler public class MyServerHandler extends ChannelInboundHandlerAdapter { /** * @param ctx 上下文 @param evt 事件 * @throws Exception */ @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception { if(evt instanceof IdleStateEvent){ //将 evt 向下转型 IdleStateEvent IdleStateEvent event = (IdleStateEvent)evt; String eventType = null; //IdleStateEvent 枚举 switch (event.state()){ case READER_IDLE: eventType = &quot;读空闲&quot;; break; case WRITER_IDLE: eventType = &quot;写空闲&quot;; break; case ALL_IDLE: eventType = &quot;读写空闲&quot;; } System.out.println(ctx.channel().remoteAddress()+&quot;---超时事件---&quot;+eventType); } } } 说明： 1: IdleStateHandler 是 Netty 提供的 空闲状态处理器 2: 四个参数： readerIdleTime : 表示多久没有 读 事件后，就会发送一个心跳检测包，检测是否还是连接状态 writerIdleTime : 表示多久没有 写 事件后，就会发送一个心跳检测包，检测是否还是连接状态 allIdleTime : 表示多久时间既没读也没写 后，就会发送一个心跳检测包，检测是否还是连接状态 TimeUnit : 时间单位 3: 当 Channel 一段时间内没有执行 读 / 写 / 读写 事件后，就会触发一个 IdleStateEvent 空闲状态事件 4: 当 IdleStateEvent 触发后，就会传递给 Pipeline 中的下一个 Handler 去处理，通过回调下一个 Handler 的 userEventTriggered 方法，在该方法中处理 IdleStateEvent 10. 长连接 要求： 实现基于webSocket的长连接 的全双工的交互 改变Http协议多次请求的约束，实 现长连接了， 服务器可以发送消息 给浏览器 客户端浏览器和服务器端会相互感 知，比如服务器关闭了，浏览器会 感知，同样浏览器关闭了，服务器 会感知 代码实现 服务端 ：WebServer public class MyServer { public static void main(String[] args) throws InterruptedException { //创建bossGroup,workGroup EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workGroup = new NioEventLoopGroup(); try { //创建辅助工具 ServerBootstrap serverBootstrap = new ServerBootstrap(); //循环事件组 serverBootstrap.group(bossGroup,workGroup)//线程组 .channel(NioServerSocketChannel.class)//通道类型 .handler(new LoggingHandler(LogLevel.INFO)) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override protected void initChannel(SocketChannel socketChannel) throws Exception { ChannelPipeline pipeline = socketChannel.pipeline(); //基于http协议使用http的编码和解码器 pipeline.addLast(new HttpServerCodec()); // 添加块处理器 pipeline.addLast(new ChunkedWriteHandler()); /* 说明： 1. 因为 HTTP 数据传输时是分段的，HttpObjectAggregator 可以将多个端聚合 2. 这就是为什么浏览器发送大量数据时，就会发出多次 HTTP 请求 */ pipeline.addLast(new HttpObjectAggregator(8192)); /* 说明： 1. 对于 WebSocket 是以 帧 的形式传递的 2. 后面的参数表示 ：请求的 URL 3. WebSocketServerProtocolHandler 将 HTTP 协议升级为 WebSocket 协议，即保持长连接 4. 切换协议通过一个状态码101 */ pipeline.addLast(new WebSocketServerProtocolHandler(&quot;/hello&quot;)); // 自定义的 Handler pipeline.addLast(new MyTextWebSocketFrameHandler()); } }); System.out.println(&quot;server is ok&quot;); ChannelFuture channelFuture = serverBootstrap.bind(8080).sync(); channelFuture.channel().closeFuture().sync(); } finally { bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); } } } 服务端的处理器 ：MyTextWebSocketFrameHandler /** * TextWebSocketFrame 类型，表示一个文本帧（flame） */ public class MyTextWebSocketFrameHandler extends SimpleChannelInboundHandler&lt;TextWebSocketFrame&gt; { @Override protected void channelRead0(ChannelHandlerContext ctx, TextWebSocketFrame msg) throws Exception { System.out.println(&quot;服务器收到消息&quot;+msg.text()); //回复消息 ctx.channel().writeAndFlush(new TextWebSocketFrame(&quot;服务器时间&quot;+ LocalDateTime.now()+msg.text())); } /** * 客户端连接后，触发方法 * @param ctx * @throws Exception */ @Override public void handlerAdded(ChannelHandlerContext ctx) throws Exception { //id表示唯一的值，Longtext是唯一的shortText 不是唯一 System.out.println(&quot;handlerAdded 被调用&quot;+ctx.channel().id().asLongText()); System.out.println(&quot;handlerAdded 被调用&quot;+ctx.channel().id().asShortText()); } @Override public void handlerRemoved(ChannelHandlerContext ctx) throws Exception { System.out.println(&quot;handlerRemove被调用&quot;+ctx.channel().id().asLongText()); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { System.out.println(&quot;异常发生&quot;+cause.getMessage()); //关闭连接 ctx.close(); } } hello.html(浏览器) &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form onsubmit=&quot;return false&quot;&gt; &lt;textarea id=&quot;message&quot; name=&quot;message&quot; style=&quot;height: 300px; width: 300px&quot;&gt;&lt;/textarea&gt; &lt;input type=&quot;button&quot; value=&quot;发送消息&quot; onclick=&quot;send(this.form.message.value)&quot;&gt; &lt;textarea id=&quot;responseText&quot; style=&quot;height: 300px; width: 300px&quot;&gt;&lt;/textarea&gt; &lt;input type=&quot;button&quot; value=&quot;清空内容&quot; onclick=&quot;document.getElementById('responseText').value=''&quot;&gt; &lt;/form&gt; &lt;script&gt; var socket; // 判断当前浏览器是否支持 WebSocket if (window.WebSocket){ socket = new WebSocket(&quot;ws://localhost:8080/hello&quot;); // 相当于 channelRead0 方法，ev 收到服务器端回送的消息 socket.onmessage = function (ev){ var rt = document.getElementById(&quot;responseText&quot;); rt.value = rt.value + &quot;\\n&quot; + ev.data; } // 相当于连接开启，感知到连接开启 socket.onopen = function (){ var rt = document.getElementById(&quot;responseText&quot;); rt.value = rt.value + &quot;\\n&quot; + &quot;连接开启……&quot;; } // 感知连接关闭 socket.onclose = function (){ var rt = document.getElementById(&quot;responseText&quot;); rt.value = rt.value + &quot;\\n&quot; + &quot;连接关闭……&quot;; } }else { alert(&quot;不支持 WebSocket&quot;); } // 发送消息到服务器 function send(message){ // 判断 WebSocket 是否创建好了 if (!window.socket){ return ; } // 判断 WebSocket 是否开启 if (socket.readyState == WebSocket.OPEN){ // 通过 Socket 发送消息 socket.send(message); }else { alert(&quot;连接未开启&quot;); } } &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; ","link":"https://tinaxiawuhao.github.io/post/Hbra5M-xo/"},{"title":"NIO","content":"1. 简介 Netty 是由 JBOSS 提供的一个 Java 开源框架，现为 Github上的独立项目。 Netty 是一个异步的、基于事件驱动（客户端的行为、读写事件）的网络应用框架，用以快速开发高性能、高可靠性的网络 IO 程序。 Netty主要针对在TCP协议下，面向Clients端的高并发应用，或者Peer-to-Peer场景下的大量数据持续传输的应用。 Netty本质是一个NIO框架，适用于服务器通讯相关的多种应用场景 要透彻理解Netty ， 需要先学习 NIO ， 这样我们才能阅读 Netty 的源码。 2. I/O 模型基本说明 I/O 模型简单的理解：就是用什么样的通道进行数据的发送和接收，很大程度上决定了程 序通信的性能 2.1. Java共支持3种网络编程模型/IO模式： BIO、NIO、AIO Java BIO ： 同步并阻塞(传统阻塞型)，服务器实现模式为一个连接一个线程，即客户端 有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成 不必要的线程开销 Java NIO ： 同步非阻塞，服务器实现模式为一个线程处理多个请求(连接)，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求就进行处理 Java AIO(NIO.2) ： 异步非阻塞，AIO 引入异步通道的概念，采用了 Proactor 模式，简化了程序编写，有效的请求才启动线程，它的特点是先由操作系统完成后才通知服务端程 2.2. BIO、NIO、AIO适用场景分析 BIO方式 适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高， 并发局限于应用中，JDK1.4以前的唯一选择，但程序简单易理解。 NIO方式 适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，弹幕 系统，服务器间通讯等。编程比较复杂，JDK1.4开始支持。 AIO方式 使用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分 调用OS参与并发操作，编程比较复杂，JDK7开始支持。 3. NIO具体介绍 Java NIO 全称 java non-blocking IO，是指 JDK 提供的新 API。从 JDK1.4 开始，Java 提供了一系列改进的输入/输出 的新特性，被统称为 NIO(即 New IO)，是同步非阻塞的 NIO 相关类都被放在 java.nio 包及子包下，并且对原 java.io 包中的很多类进行改写。 NIO 有三大核心部分：Channel(通道)，Buffer(缓冲区), Selector(选择器) NIO是 面向缓冲区 ，或者面向块编程的。数据读取到一个 它稍后处理的缓冲区，需要时可在缓冲区中前后移动，这就 增加了处理过程中的灵活性，使用它可以提供非阻塞式的高 伸缩性网络 Java NIO的非阻塞模式，使一个线程从某通道发送请求或者读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取，而不是保持线程阻塞，所以直至数据变的可以读取之前，该线程可以继续做其他的事情。 非阻塞写也是如此，一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这 个线程同时可以去做别的事情。 通俗理解：NIO是可以做到用一个线程来处理多个操作的。假设有10000个请求过来, 根据实际情况，可以分配50或者100个线程来处理。不像之前的阻塞IO那样，非得分配10000个。 HTTP2.0使用了多路复用的技术，做到同一个连接并发处理多个请求，而且并发请求 的数量比HTTP1.1大了好几个数量级 3.1. NIO 三大核心 Selector 、 Channel 和 Buffer 的简单关系图 关系图的说明: 线程是非阻塞，buffer起很大的作用 每个 Channel 都会对应一个 Buffer Selector 对应一个线程， 一个 Selector 对应多个 Channel(连接) 该图反应了有三个 Channel 注册到该 selector 程序切换到哪个 Channel 是由事件决定的, Event 就是一个重要的概念 Selector 会根据不同的事件，在各个 Channel（通道）上切换 Buffer 就是一个内存块 ， 底层是有一个数组 数据的读取写入是通过 Buffer, 这个和BIO , BIO 中要么是输入流，或者是 输出流, 不能双向，但是NIO的 Buffer 是可以读也可以写, 需要 flip 方法切换 3.2. 常用Buffer子类一览 ByteBuffer，存储字节数据到缓冲区 =最常用= ShortBuffer，存储字符串数据到缓冲区 CharBuffer，存储字符数据到缓冲区 IntBuffer，存储整数数据到缓冲区 LongBuffer，存储长整型数据到缓冲区 DoubleBuffer，存储小数到缓冲区 FloatBuffer，存储小数到缓冲区 3.3. buffer常用方法 public abstract class Buffer { //JDK1.4时，引入的api public final int capacity( )// ★ 返回此缓冲区的容量 public final int position( )// ★ 返回此缓冲区的位置 public final Buffer position (int newPositio)// ★ 设置此缓冲区的位置 public final int limit( )// ★ 返回此缓冲区的限制 public final Buffer limit (int newLimit)// ★ 设置此缓冲区的限制 public final Buffer mark( )//在此缓冲区的位置设置标记 public final Buffer reset( )//将此缓冲区的位置重置为以前标记的位置 public final Buffer clear( )// ★ 清除此缓冲区, 即将各个标记恢复到初始状态，但是数据并没有真正擦除, 后面操作会覆盖 public final Buffer flip( )// ★ 反转此缓冲区 public final Buffer rewind( )//重绕此缓冲区 public final int remaining( )//返回当前位置与限制之间的元素数 public final boolean hasRemaining( )// ★ 告知在当前位置和限制之间是否有元素 public abstract boolean isReadOnly( );// ★ 告知此缓冲区是否为只读缓冲区 //JDK1.6时引入的api public abstract boolean hasArray();// ★ 告知此缓冲区是否具有可访问的底层实现数组 public abstract Object array();// ★ 返回此缓冲区的底层实现数组 public abstract int arrayOffset();//返回此缓冲区的底层实现数组中第一个缓冲区元素的偏移量 public abstract boolean isDirect();//告知此缓冲区是否为直接缓冲区 } 3.4. 通道(Channel) 1：基本介绍 NIO的通道类似于流，但有些区别如下： • 通道可以同时进行读写，而流只能读或者只能写 • 通道可以实现异步读写数据 • 通道可以从缓冲读数据，也可以写数据到缓冲: FileChannel主要用来对本地文件进行 IO 操作，常见的方法有 public int read(ByteBuffer dst) ，从通道读取数据并放到缓冲区中 public int write(ByteBuffer src) ，把缓冲区的数据写到通道中 public long transferFrom(ReadableByteChannel src, long position, long count)，从目标通道 中复制数据到当前通道 public long transferTo(long position, long count, WritableByteChannel target)，把数据从当 前通道复制给目标通道 首先channel与文件联立，判断是输入输出流看channel与buffer之间的关系 3.5. 关于Buffer 和 Channel的注意事项和细节 1：ByteBuffer 支持类型化的 put 和 get, put 放入的是什么数据类型，get 就应该使用相应的数据类型来取出（取出的顺序也要和存入的顺序一致），否则可能有 BufferUnderflowException 异常。 2： 可以将一个普通Buffer 转成只读Buffer，如果对一个只读类型的 Buffer 进行写操作会报错 ReadOnlyBufferException ByteBuffer buffer = ByteBuffer.allocate(3); ByteBuffer byteBuffer = buffer.asReadOnlyBuffer(); System.out.println(buffer); System.out.println(byteBuffer); 3：NIO 还提供了 MappedByteBuffer， 可以让文件直接在内存（堆外的内存）中进行修改， 而如何同步到文件由NIO 来完成. /* 说明 1. MappedByteBuffer 可以让文件直接在内存中修改，这样操作系统并不需要拷贝一次 2. MappedByteBuffer 实际类型是 DirectByteBuffer */ public static void main(String[] args) throws Exception { RandomAccessFile randomAccessFile = new RandomAccessFile(&quot;D:\\\\file01.txt&quot;, &quot;rw&quot;); // 获取对应的文件通道 FileChannel channel = randomAccessFile.getChannel(); // 参数 ：使用 只读/只写/读写 模式 ； 可以修改的起始位置 ； 映射到内存的大小，即可以将文件的多少个字节映射到内存 // 这里就表示，可以对 file01.txt 文件中 [0,5) 的字节进行 读写操作 MappedByteBuffer map = channel.map(FileChannel.MapMode.READ_WRITE, 0, 5); // 进行修改操作 map.put(0, (byte) 'A'); map.put(3, (byte) '3'); // 关闭通道 channel.close(); } 4：前面我们讲的读写操作，都是通过一个Buffer 完成的，NIO 还支持 通过多个 Buffer (即 Buffer 数组) 完成读写操作，即 Scattering 和 Gathering ，遵循 依次写入，依次读取。 public static void main(String[] args) throws Exception { // 使用 ServerSocketChannel 和 InetSocketAddress 网络 ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); InetSocketAddress inetSocketAddress = new InetSocketAddress(7000); //绑定端口到socket，启动 serverSocketChannel.socket().bind(inetSocketAddress); //创建buffer的2个数组 ByteBuffer[] byteBuffers = new ByteBuffer[2]; byteBuffers[0] = ByteBuffer.allocate(5); byteBuffers[1] = ByteBuffer.allocate(3); //等待客户端连接 SocketChannel socketChannel = serverSocketChannel.accept(); int messageLength = 8; //循环的读取 while(true){ // 表示累计读取的字节数 int byteRead = 0; // 假设从客户端最多接收 8 个字节 while (byteRead &lt; messageLength){ // 自动把数据分配到 byteBuffers-0、byteBuffers-1 long read = socketChannel.read(byteBuffers); byteRead += read; // 使用流打印，查看当前 Buffer 的 Position 和 Limit Arrays.asList(byteBuffers).stream(). map(byteBuffer -&gt; &quot;{position: &quot;+byteBuffer.position()+&quot;, limit: &quot;+byteBuffer.limit()+&quot;}&quot;) .forEach(System.out::println); } // 将所有的 Buffer 进行反转，为后面的其他操作做准备 Arrays.asList(byteBuffers).forEach(Buffer -&gt;Buffer.flip()); // 将数据读出，显示到客户端 int byteWrite = 0; while (byteWrite &lt; messageLength){ long write = socketChannel.write(byteBuffers); byteWrite += write; } // 将所有的 Buffer 进行清空，为后面的其他操作做准备 Arrays.asList(byteBuffers).forEach(Buffer-&gt;Buffer.clear()); // 打印处理的字节数 System.out.println(&quot;{byteRead: &quot;+byteRead+&quot;, byteWrite: &quot;+byteWrite+&quot;}&quot;); } } 3.6. Selector选择器 1. 基本介绍 1: Java 的 NIO，用非阻塞的 IO 方式。可以用一个线程，处理多个的客户端连接，就会使用到Selector(选择器) 2: Selector 能够检测多个注册的通道上是否有事件发生(注意:多个Channel以事件的方式可以注册到同一个Selector)，如果有事件发生，便获取事件然 后针对每个事件进行相应的处理。这样就可以只用一个单线程去管理多个通道，也就是管理多个连接和请求。 3: 只有在 连接/通道 真正有读写事件发生时，才会进行读写，就大大地减少 了系统开销，并且不必为每个连接都创建一个线程，不用去维护多个线程 4: 避免了多线程之间的上下文切换导致的开销 2. Selector(选择器) 示意图 说明: Netty 的 IO 线程 NioEventLoop 聚合了 Selector(选择器， 也叫多路复用器)，可以同时并发处理成百上千个客 户端连接。 当线程从某客户端 Socket 通道进行读写数据时，若没 有数据可用时，该线程可以进行其他任务。 线程通常将非阻塞 IO 的空闲时间用于在其他通道上 执行 IO 操作，所以单独的线程可以管理多个输入和 输出通道。 由于读写操作都是非阻塞的，这就可以充分提升 IO 线程的运行效率，避免由于频繁 I/O 阻塞导致的线程 挂起。 一个 I/O 线程可以并发处理 N 个客户端连接和读写操作，这从根本上解决了传统同步阻塞 I/O 一连接一线 程模型，架构的性能、弹性伸缩能力和可靠性都得到 了极大的提升。 3. Selector类相关方法 Selector 类是一个抽象类, 常用方法和说明如下 public abstract class Selector implements Closeable { public static Selector open();//得到一个选择器对象 public int select(long timeout);//监控所有注册的通道，当其 中有 IO 操作可以进行时，将 对应的 SelectionKey 加入到内部集合中并返回，参数用来 设置超时时间 public Set&lt;SelectionKey&gt; selectedKeys();//从内部集合中得 到所有的 SelectionKey } 4. 注意事项 NIO中的 ServerSocketChannel功能类似ServerSocket，SocketChannel功能类 似Socket selector 相关方法说明 selector.select()//阻塞 selector.select(1000);//阻塞1000毫秒，在1000毫秒后返回 selector.wakeup();//唤醒 selector selector.selectNow();//不阻塞，立马返还 3.7. NIO 非阻塞 网络编程原理分析 NIO 非阻塞 网络编程相关的(Selector、SelectionKey、 ServerScoketChannel和SocketChannel) 关系梳理图 说明： ServerSocketChannel 需要在selector注册，一旦有register事件（客户端一旦连接）就会建立SocketChannel 客户端连接时需要会通过ServerSocketChannel 在selector注册，一旦有读写事件，反向获取通道channel，把channel数据读出到buffer 事件发生通过SelectorKey来判断 代码演示：server端 public static void main(String[] args) throws Exception { //服务器端创建ServerSocketChannel -&gt;ServerSocketChannel ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); //得到一个Selector对象 Selector selector = Selector.open(); //绑定一个端口6666，在服务器端监听 serverSocketChannel.socket().bind(new InetSocketAddress(6666)); //设置为非阻塞 serverSocketChannel.configureBlocking(false); //把serverSocketChannel 注册到 selector 关心的事件：op_ACCEPT serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); //循环等待客户端连接 while(true){ if(selector.select(1000) == 0){//没有事件发生 System.out.println(&quot;服务器等待1s,无连接&quot;); continue; } //返回大于0，获取相关的selectionKey集合(获取到关注的事件) //selector.selectedKeys() 返回关注的事件集合 Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); //遍历,使用迭代器 Iterator&lt;SelectionKey&gt; keyIterator = selectionKeys.iterator(); while(keyIterator.hasNext()){ //获取到SelectionKey SelectionKey key = keyIterator.next(); //根据key 对应的通道发生的事件做相应的处理 if(key.isAcceptable()){//如果是OP_ACCEPT,有新的客户端连接 //该客户端生成一个SocketChannel SocketChannel socketChannel = serverSocketChannel.accept(); System.out.println(&quot;客户端连接成功，生成一个socketChannel&quot;); //将socketChannel设置为非阻塞，这时线程可以做其他事 socketChannel.configureBlocking(false); //将socketChannel 注册到Selector，关注OP_READ，同时给socketChannel关联一个buffer socketChannel.register(selector,SelectionKey.OP_READ, ByteBuffer.allocate(1024)); } if(key.isReadable()){//发生OP_READ //通过key 反向获取对应的channel SocketChannel channel = (SocketChannel) key.channel(); //获取改channel关联的buffer ByteBuffer buffer = (ByteBuffer) key.attachment(); channel.read(buffer); System.out.println(&quot;form client &quot;+ new String(buffer.array())); } //手动从集合中移动当前的selectionKey,防止重复操作 keyIterator.remove(); } } } client端： public static void main(String[] args) throws Exception { //得到一个网络通道 SocketChannel socketChannel = SocketChannel.open(); //设置非阻塞模式 socketChannel.configureBlocking(false); //提供服务器端的ip和端口 InetSocketAddress inetSocketAddress = new InetSocketAddress(&quot;127.0.0.1&quot;,6666); //连接服务器 if(!socketChannel.connect(inetSocketAddress)){ while(!socketChannel.finishConnect()){ System.out.println(&quot;连接需要时间，客户端不会阻塞，可做其他工作&quot;); } } //连接成功，发送数据 String str = &quot;hello,world&quot;; //获得字节数组到buffer中,且buffer大小与字节长度一致 ByteBuffer buffer = ByteBuffer.wrap(str.getBytes()); //发送数据，将buffer 数据写入channel socketChannel.write(buffer); System.in.read(); } 3.8. SelectionKey SelectionKey，表示 Selector 和网络通道的注册关系（OPS）, 共四种： int OP_ACCEPT：有新的网络连接可以 accept，值为 16 int OP_CONNECT：代表连接已经建立，值为 8 int OP_READ：代表读操作，值为 1 int OP_WRITE：代表写操作，值为 4 源码中： public static final int OP_READ = 1 &lt;&lt; 0; public static final int OP_WRITE = 1 &lt;&lt; 2; public static final int OP_CONNECT = 1 &lt;&lt; 3; public static final int OP_ACCEPT = 1 &lt;&lt; 4; SelectionKey相关方法 public abstract class SelectionKey { public abstract Selector selector();//得到与之关联的 Selector 对象 public abstract SelectableChannel channel();//得到与之关 联的通道 public final Object attachment();//得到与之关联的共享数 据 public abstract SelectionKey interestOps(int ops);//设置或改 变监听事件 public final boolean isAcceptable();//是否可以 accept public final boolean isReadable();//是否可以读 public final boolean isWritable();//是否可以写 } 3.9. ServerSocketChannel ServerSocketChannel 在服务器端监听新的客户端 Socket 连接 相关方法如下: public abstract class ServerSocketChannel extends AbstractSelectableChannel implements NetworkChannel{ public static ServerSocketChannel open()//得到一个 ServerSocketChannel 通道 public final ServerSocketChannel bind(SocketAddress local)//设置服务器端端口 号 public final SelectableChannel configureBlocking(boolean block)//设置阻塞或非 阻塞模式，取值 false 表示采用非阻塞模式 public SocketChannel accept()//接受一个连接，返回代表这个连接的通道对象 public final SelectionKey register(Selector sel, int ops)//注册一个选择器并设置 监听事件 } 3.10. SocketChannel SocketChannel，网络 IO 通道，具体负责进行读写操作。NIO 把缓冲区的数据写入通 道，或者把通道里的数据读到缓冲区。 相关方法如下 public abstract class SocketChannel extends AbstractSelectableChannel implements ByteChannel, ScatteringByteChannel, GatheringByteChannel, NetworkChannel{ public static SocketChannel open();//得到一个 SocketChannel 通道 public final SelectableChannel configureBlocking(boolean block);//设置阻塞或非阻塞 模式，取值 false 表示采用非阻塞模式 public boolean connect(SocketAddress remote);//连接服务器 public boolean finishConnect();//如果上面的方法连接失败，接下来就要通过该方法 完成连接操作 public int write(ByteBuffer src);//往通道里写数据 public int read(ByteBuffer dst);//从通道里读数据 public final SelectionKey register(Selector sel, int ops, Object att);//注册一个选择器并 设置监听事件，最后一个参数可以设置共享数据 public final void close();//关闭通道 } 3.11. NIO 网络编程应用实例-群聊系统 要求: 编写一个 NIO 群聊系统，实现服务器端和客户端之间的数据简单通讯（非阻塞） 实现多人群聊 服务器端：可以监测用户上线，离线， 并实现消息转发功能 客户端：通过channel 可以无阻塞发送 消息给其它所有用户，同时可以接受 其它用户发送的消息(有服务器转发得到) 目的：进一步理解NIO非阻塞网络编程 机制 代码实现：服务端 package com.atguigu.groupchat; import java.io.IOException; import java.net.InetSocketAddress; import java.nio.ByteBuffer; import java.nio.channels.*; import java.util.Iterator; public class GroupChatServer { //定义属性 private Selector selector; private ServerSocketChannel listenChannel; private static final int PORT = 6667; //构造器 //初始化工作 public GroupChatServer(){ try { //得到选择器 selector = Selector.open(); //得到ServerSocketChannel listenChannel = ServerSocketChannel.open(); //绑定端口 listenChannel.socket().bind(new InetSocketAddress(PORT)); //设置非阻塞 listenChannel.configureBlocking(false); //listenChannel注册到Selector listenChannel.register(selector, SelectionKey.OP_ACCEPT); } catch (IOException e) { e.printStackTrace(); } } //监听 public void listen(){ try { //循环处理监听 while(true){ int count = selector.select(); if(count&gt;0){//有事件 //遍历得到selectionKey 集合 Iterator&lt;SelectionKey&gt; iterator = selector.selectedKeys().iterator(); while(iterator.hasNext()){ //取得selectionKey SelectionKey key = iterator.next(); if(key.isAcceptable()){//连接事件 SocketChannel socketChannel = listenChannel.accept(); socketChannel.configureBlocking(false); socketChannel.register(selector,SelectionKey.OP_READ); //提示客户上线 System.out.println(socketChannel.getRemoteAddress()+&quot;上线了&quot;); } else if(key.isReadable()){//read事件 readDate(key); } //手动从集合中移动当前的selectionKey,防止重复操作 iterator.remove(); } } else{ System.out.println(&quot;waiting event&quot;); } } } catch (Exception e) { e.printStackTrace(); }finally { } } //读取client数据 public void readDate(SelectionKey key){ //定义一个SocketChannel SocketChannel socketChannel = null; try { //取到关联的channel socketChannel = (SocketChannel)key.channel(); //创建缓存 ByteBuffer byteBuffer = ByteBuffer.allocate(1024); int count = socketChannel.read(byteBuffer); if(count&gt;0){ //buffer的数据转成字符串 String msg = new String(byteBuffer.array()); //输出msg System.out.println(&quot;form client&quot;+msg); //向其他客户端转发消息 sendInfoToOtherClient(msg,socketChannel); } } catch (IOException e) { try { System.out.println(socketChannel.getRemoteAddress()+&quot;离线了&quot;); //取消注册 key.cancel(); //关闭通道 socketChannel.close(); } catch (IOException ioException) { ioException.printStackTrace(); } } } //转发消息给其他消息 private void sendInfoToOtherClient(String msg,SocketChannel self) throws IOException{ //遍历所有注册到selector的SocketChannel for(SelectionKey key: selector.keys()){ Channel targetChannel = key.channel(); //排除自己 if(targetChannel != self &amp;&amp; targetChannel instanceof SocketChannel){ //转发 SocketChannel dest = (SocketChannel)targetChannel; //写入buffer ByteBuffer buffer = ByteBuffer.wrap(msg.getBytes()); //写入通道 dest.write(buffer); } } } public static void main(String[] args) { // 创建一个服务器对象 GroupChatServer server = new GroupChatServer(); server.listen(); } } 代码实现：客户端 package com.atguigu.groupchat; import java.io.IOException; import java.net.InetSocketAddress; import java.nio.ByteBuffer; import java.nio.channels.SelectionKey; import java.nio.channels.Selector; import java.nio.channels.SocketChannel; import java.util.Iterator; import java.util.Scanner; public class GroupChatClient { // 定义相关属性 // 服务器的IP private final String HOST = &quot;127.0.0.1&quot;; // 服务器的端口 private final int PORT = 6667; private Selector selector; private SocketChannel socketChannel; private String username; // 构造器 public GroupChatClient() throws IOException { // 完成初始化 selector = Selector.open(); // 连接服务器 socketChannel = SocketChannel.open(new InetSocketAddress(HOST, PORT)); // 设置 非阻塞 socketChannel.configureBlocking(false); // 将 socketChannel 注册到 Selector socketChannel.register(selector, SelectionKey.OP_READ); // 得到 username username = socketChannel.getLocalAddress().toString().substring(1); System.out.println(username + &quot;is OK!&quot;); } // 向服务器发送消息 public void sendMessage(String message){ message = username + &quot;说：&quot;+ message; try { // 把 message 写入 buffer socketChannel.write(ByteBuffer.wrap(message.getBytes())); // 读取从服务器端回复的消息 }catch (Exception e){ e.printStackTrace(); }finally { } } public void readmessage(){ try { int select = selector.select(); if (select &gt; 0){ // 有事件发生的通道 Iterator&lt;SelectionKey&gt; iterator = selector.selectedKeys().iterator(); while (iterator.hasNext()){ SelectionKey key = iterator.next(); if (key.isReadable()){ // 得到相关的通道 SocketChannel channel = (SocketChannel) key.channel(); ByteBuffer buffer = ByteBuffer.allocate(1024); channel.read(buffer); String msg = new String(buffer.array()); System.out.println(msg.trim()); } } iterator.remove();//删除当前的selectionKey }else { // System.out.println(&quot;没有可用的通道&quot;); } }catch (Exception e){ e.printStackTrace(); }finally { } } public static void main(String[] args) throws IOException { // 启动客户端 GroupChatClient client = new GroupChatClient(); // 启动一个线程,每个三秒读取从服务器端读取数据 new Thread(){ public void run(){ while (true){ client.readmessage(); try { Thread.sleep(3000); }catch (Exception e){ e.printStackTrace(); } } } }.start(); // 发送数据给服务端 Scanner scanner = new Scanner(System.in); while (scanner.hasNextLine()){ String line = scanner.nextLine(); client.sendMessage(line); } } } ","link":"https://tinaxiawuhao.github.io/post/X2beO2j1l/"},{"title":"前端简单部署","content":"部署前端项目，将打包后的文件dist放入nginx，基于nginx的基础镜像创建前端项目的docker镜像即可 一、前端项目的文件准备 1、文件准备： 1,dockerfile(docker部署镜像打包文件) 2,dist(待部署前端文件夹) 3,default.conf 2、Dockerfile文件： FROM nginx RUN rm /etc/nginx/conf.d/default.conf ADD default.conf /etc/nginx/conf.d/ COPY dist/ /usr/share/nginx/html/ EXPOSE 80 3、default.conf： server { listen 80; server_name localhost; # 修改为docker服务宿主机的ip location / { root /usr/share/nginx/html; index index.html index.htm; try_files $uri $uri/ /index.html =404; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } location /api/ { proxy_pass http://后端服务名:IP端口; } } 二、vue项目的自动化部署 1、安装Jenkins Jenkins的安装：https://tinaxiawuhao.github.io/post/MN_sOyfo3/ 2、安装node/npm/yarn 安装node与node/npm/yarn：https://tinaxiawuhao.github.io/post/XMNtsw8ig/ 3、Jenkins中配置harbor仓库的密钥 我们需要通过jenkinsfile文件推送docker镜像到指定的harbor仓库，需要在jenkins中配置仓库登录凭据 4、编写Jenkinsfile pipeline{ agent any environment { WS = &quot;${WORKSPACE}&quot; HARBOR_URL=&quot;harbor的url&quot; HARBOR_ID=&quot;harbor密钥&quot; } stages { stage(&quot;环境检查&quot;){ steps { sh 'docker version' sh 'npm -v' sh 'yarn -v' } } stage('yarn安装与编译') { steps { sh &quot;echo ${WS} &amp;&amp; ls -alh&quot; sh &quot;cd ${WS} &amp;&amp; yarn install&quot; sh &quot;cd ${WS} &amp;&amp; yarn build:hd&quot; } } stage('docker镜像与推送') { steps { sh &quot;cd ${WS} &amp;&amp; docker build -f Dockerfile -t ${HARBOR_URL}/test/web:latest .&quot; withCredentials([usernamePassword(credentialsId: &quot;${HARBOR_ID}&quot;, passwordVariable: 'password', usernameVariable: 'username')]) { sh &quot;docker login -u ${username} -p ${password} ${HARBOR_URL}&quot; sh &quot;docker push ${HARBOR_URL}/test/web:latest&quot; } } } } post { success { echo 'success!' } failure { echo 'failed...' } } } harbor密钥获取 点进入刚才添加的凭证获取凭证 最后创建一个pipeline项目（可选择Pipeline或者多分支流水线），pipeline脚本定义选择Pipeline script from SCM，选择git项目 修改关注分支 三、模仿后端实现环境变量注入 借助Nginx1.19以后docker镜像的template新功能，可以实现配置文件环境变量的动态注入 1、准备 default.conf.template： server { listen 80; listen [::]:80; server_name localhost; # 修改为docker服务宿主机的ip location / { root /usr/share/nginx/html; index index.html index.htm; ssi on; try_files $uri $uri/ /index.html; if ($request_filename ~* ^.*?\\.(eot)|(ttf)|(woff)|(svg)|(otf)$) { add_header Access-Control-Allow-Origin *; } } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } location /api/ { ssi on; # 借助ssi proxy_pass ${BACKEND_SERVICE_URL}; } } 2、编写Dockerfile文件： FROM nginx:1.21 COPY dist /usr/share/nginx/html # 注意和上面文件对比位置的改变 COPY default.conf.template /etc/nginx/templates/ EXPOSE 80 3、设置环境变量 部署docker时候，设置环境变量-e BACKEND_SERVICE_URL= http://ip:port 后，实际的nginx配置就变为了： $BACKEND_SERVICE_URL = http://ip:port #后端服务的地址； server { listen 80; listen [::]:80; server_name localhost; # 修改为docker服务宿主机的ip location / { root /usr/share/nginx/html; index index.html index.htm; ssi on; try_files $uri $uri/ /index.html; if ($request_filename ~* ^.*?\\.(eot)|(ttf)|(woff)|(svg)|(otf)$) { add_header Access-Control-Allow-Origin *; } } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } location /api/ { ssi on; # 借助ssi proxy_pass http://ip:port; } } 切换服务就可以修改环境变量，切换不同的服务后端 ","link":"https://tinaxiawuhao.github.io/post/KEyilqyTn/"},{"title":"gitlab提交代码自动触发Jenkins构建操作","content":"Jenkins配置 插件安装 系统管理=====》插件管理=====》可选插件=====》搜索要按照的插件(gitlab hook-plugin和gitlab-plugin) 如果找不到上面两个插件，安装gitlab和gitlab hook即可 创建测试项目 gitlab配置 打开要关联Jenkins项目的设置选项找到webhooks选项，把Jenkins中的项目触发器url以及Secret token配置到gitlab的webhooks选项中 URL Secret token 验证效果 我们测试下点击刚刚关联的构建动作，Jenkins会不会自动构建 当出现请求状态码200的时候证明我们的关联动作已经执行 下面我们去到Jenkins中看下是否有构建历史 再试下手动push代码到gitlab会不会触发Jenkins的构建任务 手动去到服务器上向远程gitlab分支推送文件 ","link":"https://tinaxiawuhao.github.io/post/ymh_BwIfn/"},{"title":"linux安装node","content":"以当前node版本16.13.0为例，新建演示目录/usr/local/node 1、下载 wget https://nodejs.org/dist/v16.13.0/node-v16.13.0-linux-x64.tar.xz 2、移动到指定目录 mv node-v16.13.0-linux-x64.tar.xz /usr/local/node 3、解压 xz -d node-v16.13.0-linux-x64.tar.xz tar -xvf node-v16.13.0-linux-x64.tar 测试下： /root/sdemo/node-v16.13.0-linux-x64/bin/node -v v16.13.0 /root/sdemo/node-v16.13.0-linux-x64/bin/npm -v 8.1.0 4、设置软连接，相当于windows设置环境变量 ln -s /usr/local/node/node-v16.13.0-linux-x64/bin/node /usr/bin/node ln -s /usr/local/node/node-v16.13.0-linux-x64/bin/npm /usr/bin/npm 5、安装yarn,并设置yarn的软连 npm install yarn -g ln -s /usr/local/node/node-v16.13.0-linux-x64/bin/yarn /usr/bin/yarn 测试下： yarn -v 1.22.17 6、安装cnpm npm install -g cnpm --registry=https://registry.npm.taobao.org ln -s /usr/local/node/lib/node-v16.13.0-linux-x64/bin/cnpm /usr/bin/cnpm ","link":"https://tinaxiawuhao.github.io/post/XMNtsw8ig/"},{"title":"linux安装jenkins","content":"1、安装JDK yum install -y java 2、安装jenkins 添加Jenkins库到yum库，Jenkins将从这里下载安装。 wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat/jenkins.repo rpm --import https://jenkins-ci.org/redhat/jenkins-ci.org.key yum install -y jenkins 或 yum install jenkins -y --nogpgcheck 3、修改jenkins端口 [root@tools ~]# rpm -qa | grep jenkins jenkins-2.340-1.1.noarch [root@tools ~]# rpm -ql jenkins-2.340-1.1.noarch /etc/init.d/jenkins /etc/logrotate.d/jenkins /etc/sysconfig/jenkins /usr/bin/jenkins /usr/lib/systemd/system/jenkins.service /usr/sbin/rcjenkins /usr/share/java/jenkins.war /usr/share/jenkins /usr/share/jenkins/migrate /var/cache/jenkins /var/lib/jenkins /var/log/jenkins [root@tools ~]# vim /usr/lib/systemd/system/jenkins.service vi /etc/sysconfig/jenkins 找到修改端口号： JENKINS_PORT=“9000” #此端口不冲突可以不修改 4、启动jenkins systemctl start jenkins.service [root@tools ~]# netstat -nultp 安装成功后Jenkins将作为一个守护进程随系统启动 系统会创建一个“jenkins”用户来允许这个服务，如果改变服务所有者，同时需要修改/var/log/jenkins, /var/lib/jenkins, 和/var/cache/jenkins的所有者 启动的时候将从/etc/sysconfig/jenkins获取配置参数 默认情况下，Jenkins运行在8080端口，在浏览器中直接访问该端进行服务配置 Jenkins的RPM仓库配置被加到/etc/yum.repos.d/jenkins.repo 5、打开jenkins 在浏览器中访问 http://192.168.3.200:9000/ 首次进入会要求输入初始密码 初始密码在：/var/lib/jenkins/secrets/initialAdminPassword 重置密码：admin/admin ","link":"https://tinaxiawuhao.github.io/post/MN_sOyfo3/"},{"title":"k8s高可用集群搭建","content":"整体环境 3台master节点，3台node节点。采用了Centos 7，有网络，互相可以ping通。 准备工作查看单集群部署 1.安装keepalived 和 haproxy 1.1安装keepalived和 haproxy 1yum install keepalived haproxy -y 1.2 配置 keepalived cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id k8s } vrrp_script check_haproxy { script &quot;/bin/bash -c 'if [[ $(netstat -nlp | grep 16443) ]]; then exit 0; else exit 1; fi'&quot; interval 3 weight -2 fall 10 rise 2 } vrrp_instance VI_1 { state MASTER interface ens33 virtual_router_id 51 priority 250 advert_int 1 authentication { auth_type PASS auth_pass ceb1b3ec013d66163d6ab } virtual_ipaddress { 192.168.40.19 } track_script { check_haproxy } } EOF cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id k8s } vrrp_script check_haproxy { script &quot;/bin/bash -c 'if [[ $(netstat -nlp | grep 16443) ]]; then exit 0; else exit 1; fi'&quot; interval 3 weight -2 fall 10 rise 2 } vrrp_instance VI_1 { state BACKUP interface ens33 virtual_router_id 51 priority 150 advert_int 1 authentication { auth_type PASS auth_pass ceb1b3ec013d66163d6ab } virtual_ipaddress { 192.168.40.19 } track_script { check_haproxy } } EOF cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id k8s } vrrp_script check_haproxy { script &quot;/bin/bash -c 'if [[ $(netstat -nlp | grep 16443) ]]; then exit 0; else exit 1; fi'&quot; interval 3 weight -2 fall 10 rise 2 } vrrp_instance VI_1 { state BACKUP interface ens33 virtual_router_id 51 priority 200 advert_int 1 authentication { auth_type PASS auth_pass ceb1b3ec013d66163d6ab } virtual_ipaddress { 192.168.40.19 } track_script { check_haproxy } } EOF vrrp_script 用于检测 haproxy 是否正常。如果本机的 haproxy 挂掉，即使 keepalived 劫持vip，也无法将流量负载到 apiserver。 我所查阅的网络教程全部为检测进程, 类似 killall -0 haproxy。这种方式用在主机部署上可以，但容器部署时，在 keepalived 容器中无法知道另一个容器 haproxy 的活跃情况，因此我在此处通过检测端口号来判断 haproxy 的健康状况。 weight 可正可负。为正时检测成功 +weight，相当与节点检测失败时本身 priority 不变，但其他检测成功节点 priority 增加。为负时检测失败本身 priority 减少。 另外很多文章中没有强调 nopreempt 参数，意为不可抢占，此时 master 节点失败后，backup 节点也不能接管 vip，因此我将此配置删去 1.3 配置haproxy cat &lt;&lt;EOF &gt; /etc/haproxy/haproxy.cfg #--------------------------------------------------------------------- # Global settings #--------------------------------------------------------------------- global # to have these messages end up in /var/log/haproxy.log you will # need to: # 1) configure syslog to accept network log events. This is done # by adding the '-r' option to the SYSLOGD_OPTIONS in # /etc/sysconfig/syslog # 2) configure local2 events to go to the /var/log/haproxy.log # file. A line like the following can be added to # /etc/sysconfig/syslog # # local2.* /var/log/haproxy.log # log 127.0.0.1 local0 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon # turn on stats unix socket stats socket /var/lib/haproxy/stats #--------------------------------------------------------------------- # common defaults that all the 'listen' and 'backend' sections will # use if not designated in their block #--------------------------------------------------------------------- defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000 #--------------------------------------------------------------------- # kubernetes apiserver frontend which proxys to the backends #--------------------------------------------------------------------- frontend kubernetes-apiserver mode tcp bind *:16443 option tcplog default_backend kubernetes-apiserver #--------------------------------------------------------------------- # round robin balancing between the various backends #--------------------------------------------------------------------- backend kubernetes-apiserver mode tcp balance roundrobin server gk8s-master 192.168.40.20:6443 check server gk8s-master2 192.168.40.21:6443 check server gk8s-master3 192.168.40.22:6443 check #--------------------------------------------------------------------- # collection haproxy statistics message #--------------------------------------------------------------------- listen stats bind *:1080 stats auth admin:awesomePassword stats refresh 5s stats realm HAProxy\\ Statistics stats uri /admin?stats EOF 设置开机启动 systemctl enable haproxy &amp;&amp; systemctl start haproxy systemctl enable keepalived &amp;&amp; systemctl start keepalived 2.安装kubeadm、kubelet、kubectl 1.配置文件修改 cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 2.安装启用 sudo yum install -y kubelet-1.24.1 kubeadm-1.24.1 kubectl-1.24.1 --disableexcludes=kubernetes sudo systemctl enable kubelet &amp;&amp; systemctl start kubelet 3.修改kubelet的配置文件 先查看配置文件位置 systemctl status kubelet vi /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf 并添加以下内容(使用和docker相同的cgroup-driver)。 Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --cgroup-driver=systemd&quot; 4.重启kubelet systemctl daemon-reload &amp;&amp; systemctl restart kubelet 3.获取K8S镜像（可忽略） 1.获取镜像列表 使用阿里云镜像仓库下载（国内环境该命令可不执行，下步骤kubeadm init --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers已经默认为国内环境） 由于官方镜像地址被墙，所以我们需要首先获取所需镜像以及它们的版本。然后从国内镜像站获取。 kubeadm config images list 获取镜像列表后可以通过下面的脚本从阿里云获取： vi /usr/local/k8s/k8s-images.sh 下面的镜像应该去除&quot;k8s.gcr.io/&quot;的前缀，版本换成上面获取到的版本 images=( kube-apiserver:v1.24.1 kube-controller-manager:v1.24.1 kube-scheduler:v1.24.1 kube-proxy:v1.24.1 pause:3.7 etcd:3.5.3-0 coredns:v1.8.6 ) for imageName in ${images[@]} ; do docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName done 2.赋权执行 chmod +x k8s-images.sh &amp;&amp; ./k8s-images.sh 以上操作在所有机器执行 4.初始化环境（master操作） 1.安装镜像 采用模板配置文件加载 kubeadm config print init-defaults &gt; kubeadm-config.yaml # 模板随版本更新 [root@master1 ~]# cat kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta2 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 192.168.40.131 # 本机IP bindPort: 6443 nodeRegistration: criSocket: unix:///var/run/cri-docker.sock # 此处千万不要忘记修改，如果不修改等于没有替换。(此处已经更改完了) #criSocket: unix:///run/containerd/containerd.sock # 此处千万不要忘记修改，如果不修改等于没有替换。(此处已经更改完了) name: master1 # 本主机名 taints: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiServer: certSANs: - k8s-master-01 - k8s-master-02 - k8s-master-03 - master.k8s.io - 192.168.40.19 - 192.168.40.20 - 192.168.40.21 - 192.168.40.22 - 127.0.0.1 extraArgs: authorization-mode: Node,RBAC timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta2 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controlPlaneEndpoint: &quot;192.168.40.19:16443&quot; # 虚拟IP和haproxy端口 controllerManager: {} dns: type: CoreDNS etcd: local: dataDir: /var/lib/etcd imageRepository: registry.aliyuncs.com/google_containers # 镜像仓库源要根据自己实际情况修改 kind: ClusterConfiguration kubernetesVersion: v1.24.1 # k8s版本 networking: dnsDomain: cluster.local podSubnet: &quot;10.244.0.0/16&quot; #设置网段，和下面网络插件对应 serviceSubnet: 10.96.0.0/12 scheduler: {} --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration featureGates: SupportIPVSProxyMode: true mode: ipvs 2.查看kubeadm版本，修改命令参数 kubeadm version 这个就很简单了，只需要简单的一个命令： #直接使用已经下载好的镜像 kubeadm init --kubernetes-version=v1.24.1 --control-plane-endpoint &quot;192.168.40.19:16443&quot; --apiserver-advertise-address=192.168.40.20 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap --cri-socket unix:///var/run/cri-docker.sock | tee kubeadm-init.log #或者采用aliyuncs镜像下载 kubeadm init --kubernetes-version=v1.24.1 --control-plane-endpoint &quot;192.168.40.19:16443&quot; --apiserver-advertise-address=192.168.40.20 --image-repository registry.aliyuncs.com/google_containers --service-cidr=10.1.0.0/16 --pod-network-cidr=10.244.0.0/16 --cri-socket unix:///var/run/cri-docker.sock| tee kubeadm-init.log #使用上面系统生成配置文件加载 kubeadm init --config kubeadm-config.yaml 3.初始化命令说明： 虚拟ip节点端口号 --control-plane-endpoint 指明用 Master 的哪个 interface 与 Cluster 的其他节点通信。如果 Master 有多个 interface，建议明确指定，如果不指定，kubeadm 会自动选择有默认网关的 interface。 --apiserver-advertise-address 指定 Pod 网络的范围。Kubernetes 支持多种网络方案，而且不同网络方案对 --pod-network-cidr 有自己的要求，这里设置为 10.244.0.0/16 是因为我们将使用 flannel 网络方案，必须设置成这个 CIDR。 --pod-network-cidr Kubenetes默认Registries地址是 k8s.gcr.io，在国内并不能访问 gcr.io，在1.19.3版本中我们可以增加–image-repository参数，默认值是 k8s.gcr.io，将其指定为阿里云镜像地址：registry.aliyuncs.com/google_containers。 --image-repository 关闭版本探测，因为它的默认值是stable-1，会导致从https://dl.k8s.io/release/stable-1.txt下载最新的版本号，我们可以将其指定为固定版本（最新版：v1.24.1）来跳过网络请求。 --kubernetes-version=v1.24.1 指定启动时使用cri-docker调用docker --cri-socket unix:///var/run/cri-docker.sock 4.错误启动重置 # 重置 如果有需要 kubeadm reset --cri-socket unix:///var/run/cri-docker.sock 5.初始化成功后，为顺利使用kubectl，执行以下命令： mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 6.添加其他主节点 1.复制密钥及相关文件 ssh root@192.168.40.21 mkdir -p /etc/kubernetes/pki/etcd scp /etc/kubernetes/admin.conf root@192.168.40.21:/etc/kubernetes scp /etc/kubernetes/pki/{ca.*,sa.*,front-proxy-ca.*}root@192.168.40.21:/etc/kubernetes/pki scp /etc/kubernetes/pki/etcd/ca.* root@192.168.40.21:/etc/kubernetes/pki/etcd ssh root@192.168.40.22 mkdir -p /etc/kubernetes/pki/etcd scp /etc/kubernetes/admin.conf root@192.168.40.22:/etc/kubernetes scp /etc/kubernetes/pki/{ca.*,sa.*,front-proxy-ca.*} root@192.168.40.22:/etc/kubernetes/pki scp /etc/kubernetes/pki/etcd/ca.* root@192.168.40.22:/etc/kubernetes/pki/etcd 2.添加节点 kubeadm join 192.168.40.19:16443 --token pqir66.66fy6pexw3kprt2b --discovery-token-ca-cert-hash sha256:cd4c42e956fdc7e0ad48c990484c22cfd43da63cb3f3887bedc481e7f33a0be1 --control-plane --cri-socket unix:///var/run/cri-docker.sock 7.执行kubectl get nodes，查看master节点状态： kubectl get node 8.通过如下命令查看kubelet状态： journalctl -xef -u kubelet -n 20 提示未安装cni 网络插件。 5.1安装flannel网络插件(CNI) master执行以下命令安装flannel即可： kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml kube-flannel.yaml文件中的net-conf.json-&gt;Network地址默认为命令中–pod-network-cidr=值相同 输入命令kubectl get pods -n kube-system,等待所有插件为running状态。 待所有pod status为Running的时候，再次执行kubectl get nodes： [root@k8s-master ~]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master Ready master 16m v1.24.1 如上所示，master状态变为，表明Master节点部署成功！ 5.2安装calico网络(功能更完善) 1.在master上下载配置calico网络的yaml。 kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml 2.提前下载所需要的镜像。 # 查看此文件用哪些镜像： [root@k8s-master ~]# grep image calico.yaml image: docker.io/calico/cni:v3.23.1 image: docker.io/calico/node:v3.23.1 image: docker.io/calico/kube-controllers:v3.23.1 3.安装calico网络。 在master上执行如下命令： kubectl apply -f calico.yaml 5.验证结果。 再次在master上运行命令 kubectl get nodes查看运行结果： [root@k8s-master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION master01 Ready control-plane,master 21h v1.23.4 worker01 Ready control-plane,master 16h v1.23.4 worker02 Ready control-plane,master 16h v1.23.4 6.部署k8s-node1、k8s-node2、k8s-node3集群 1、在k8s-node1、k8s-node2、k8s-node3等三台虚拟机中重复执行上面的步骤，安装好docker、kubelet、kubectl、kubeadm。 1.node节点加入集群 在上面第初始化master节点成功后，输出了下面的kubeadm join命令： kubeadm join 192.168.40.131:6443 --token zj0u08.ge77y7uv76flqgdk --discovery-token-ca-cert-hash sha256:7cd23cec6afb192b2d34c5c719b378082a6315a9d91a22d91b83066c870d4db5 --cri-socket unix:///var/run/cri-docker.sock 该命令就是node加入集群的命令，分别在k8s-node1、k8s-node2上执行该命令加入集群。 如果忘记该命令，可以通过以下命令重新生成： kubeadm token create --print-join-command 2.在master节点执行下面命令查看集群状态： kubectl get nodes [root@k8s-master ~]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master Ready master 24m v1.24.1 k8s-master2 Ready master 24m v1.24.1 k8s-master3 Ready master 24m v1.24.1 k8s-node1 Ready &lt;none&gt; 5m50s v1.24.1 k8s-node2 Ready &lt;none&gt; 5m21s v1.24.1 k8s-node3 Ready &lt;none&gt; 5m21s v1.24.1 如上所示，所有节点都为ready，集群搭建成功。 卸载集群命令 #建议所有服务器都执行 #!/bin/bash kubeadm reset -f modprobe -r ipip lsmod rm -rf ~/.kube/ rm -rf /etc/kubernetes/ rm -rf /etc/systemd/system/kubelet.service.d rm -rf /etc/systemd/system/kubelet.service rm -rf /usr/bin/kube* rm -rf /etc/cni rm -rf /opt/cni rm -rf /var/lib/etcd rm -rf /var/etcd yum -y remove kubeadm* kubectl* kubelet* docker* reboot ","link":"https://tinaxiawuhao.github.io/post/McgbXRFnx/"},{"title":"Helm简介","content":"一、简介 Helm是Kubernetes的包管理器。包管理器类似于Ubuntu中使用的apt、Python中的pip一样，能快速查找、下载和安装软件包。 Helm解决的痛点 在Kubernetes中部署一个可以使用的应用，需要涉及到很多的Kubernetes资源的共同协作。比如安装一个WordPress，用到了一些Kubernetes的一些资源对象，包括Deployment用于部署应用、Service提供服务发现、Secret配置 WordPress的用户名和密码，可能还需要pv和pvc来提供持久化服务。并且WordPress数据是存储在mariadb里面的，所以需要mariadb启动就绪后才能启动 WordPress。这些k8s资源过于分散，不方便进行管理。 Helm把Kubernetes资源(比如deployments、services或ingress等) 打包到一个chart中，而chart被保存到chart仓库。通过chart仓库可用来存储和分享chart。Helm使发布可配置，支持发布应用配置的版本管理，简化了Kubernetes部署应用的版本控制、打包、发布、删除、更新等操作。 Helm相关组件及概念 helm是一个命令行工具，主要用于Kubernetes应用程序Chart的创建、打包、发布以及创建和管理本地和远程的Chart仓库。chart Helm的软件包，采用TAR格式。类似于APT的DEB包或者YUM的RPM包，其包含了一组定义Kubernetes资源相关的 YAML文件。 Repoistory Helm的软件仓库，Repository本质上是一个Web服务器，该服务器保存了一系列的Chart软件包以供用户下载，并且提供了一个该Repository的Chart包的清单文件以供查询。Helm可以同时管理多个不同的Repository。 release使用helm install命令在Kubernetes集群中部署的Chart称为Release。可以理解为Helm使用Chart包部署的一个应用实例。 创建release helm 客户端从指定的目录或本地tar文件或远程repo仓库解析出chart的结构信息 helm 客户端根据 chart 和 values 生成一个 release helm 将install release请求直接传递给 kube-apiserver 删除release helm 客户端从指定的目录或本地tar文件或远程repo仓库解析出chart的结构信息 helm 客户端根据 chart 和 values 生成一个 release helm 将delete release请求直接传递给 kube-apiserver 更新release helm 客户端从指定的目录或本地tar文件或远程repo仓库解析出chart的结构信息 helm 将收到的信息生成新的 release，并同时更新这个 release 的 history helm 将新的 release 传递给 kube-apiserver 进行更新 chart的基本结构 ## Helm的打包格式叫做chart，所谓chart就是一系列文件, 它描述了一组相关的 k8s 集群资源。 ## Chart中的文件安装特定的目录结构组织, 最简单的chart 目录如下所示： ./ ├── charts # 目录存放依赖的chart ├── Chart.yaml # 包含Chart的基本信息，包括chart版本，名称等 ├── templates # 目录下存放应用一系列k8s资源的yaml模板 │ ├── deployment.yaml │ ├── _helpers.tpl # 此文件中定义一些可重用的模板片断，此文件中的定义在任何资源定义模板中可用 │ ├── ingress.yaml │ ├── NOTES.txt # 介绍chart部署后的帮助信息，如何使用chart等 │ ├── serviceaccount.yaml │ ├── service.yaml │ └── tests │ └── test-connection.yaml └── values.yaml # 包含了必要的值定义（默认值）, 用于存储templates目录中模板文件中用到变量的值 二、安装Helm 1 安装Helm [root@Ansible01 ~]# wget https://get.helm.sh/helm-v3.8.2-linux-amd64.tar.gz [root@Ansible01 ~]# tar -zxvf helm-v3.0.0-linux-amd64.tar.gz [root@Ansible01 ~]# mv linux-amd64/helm /usr/local/bin/helm [root@Ansible01 ~]# helm version version.BuildInfo{Version:&quot;v3.8.2&quot;, GitCommit:&quot;6e3701edea09e5d55a8ca2aae03a68917630e91b&quot;, GitTreeState:&quot;clean&quot;, GoVersion:&quot;go1.17.5&quot;} 2 添加常用repo 添加存储库 [root@Ansible01 ~]# helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts &quot;aliyun&quot; has been added to your repositories 更新存储库 [root@Ansible01 ~]# helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the &quot;aliyun&quot; chart repository ...Successfully got an update from the &quot;stable&quot; chart repository ...Successfully got an update from the &quot;prometheus-community&quot; chart repository ...Successfully got an update from the &quot;bitnami&quot; chart repository Update Complete. ⎈Happy Helming!⎈ 查看存储库 [root@Ansible01 ~]# helm repo list NAME URL bitnami https://charts.bitnami.com/bitnami prometheus-community https://prometheus-community.github.io/helm-charts stable http://mirror.azure.cn/kubernetes/charts aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts 删除存储库 [root@Ansible01 ~]# helm repo remove aliyun &quot;aliyun&quot; has been removed from your repositories 三、使用Helm 1 使用chart部署一个mysql # 查找所有repo下的所有chart helm search repo # 查找stable这个repo下的chart mysql helm search repo stable/mysql # 查看chart信息： helm show chart stable/mysql # 安装包： helm install db stable/mysql** # 查看发布状态： helm status db [root@Ansible01 ~]# kubectl get pod -n default NAME READY STATUS RESTARTS AGE db-mysql-599d764c8c-knfqc 0/1 Pending 0 2m10s [root@Ansible01 ~]# kubectl describe pod db-mysql-599d764c8c-knfqc -n default ...... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 17s (x3 over 2m33s) default-scheduler 0/4 nodes are available: 4 pod has unbound immediate PersistentVolumeClaims. 2 安装前自定义chart配置选项 上面部署的mysql并没有成功，这是因为并不是所有的chart都能按照默认配置运行成功，可能会需要一些环境依赖，例如PV。 所以需要自定义chart配置选项，安装过程中有两种方法可以传递配置数据：–values（或-f）：指定带有覆盖的YAML文件。这可以多次指定，最右边的文件优先。 –set：在命令行上指定替代。如果两者都用，–set优先级高 创建满足的PV [root@Ansible01 mysql]# cat pv.yaml apiVersion: v1 kind: PersistentVolume metadata: name: mysql-pv-volume labels: type: local spec: storageClassName: &quot;managed-nfs-storage&quot; capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: &quot;/mnt/data&quot; [root@Ansible01 hello-world]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE mysql-pv-volume 10Gi RWO Retain Available managed-nfs-storage 4s [root@Ansible01 hello-world]# helm uninstall db release &quot;db&quot; uninstalled [root@Ansible01 hello-world]# cat config.yaml persistence: enabled: true storageClass: &quot;managed-nfs-storage&quot; accessMode: ReadWriteOnce size: 8Gi mysqlUser: &quot;k8s&quot; mysqlPassword: &quot;123456&quot; mysqlDatabase: &quot;k8s&quot; [root@Ansible01 hello-world]# helm install db -f config.yaml stable/mysql [root@Ansible01 hello-world]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE mysql-pv-volume 10Gi RWO Retain Bound default/db-mysql managed-nfs-storage 17s [root@Ansible01 hello-world]# kubectl get pod NAME READY STATUS RESTARTS AGE db-mysql-f7fbfdd68-tc8cm 1/1 Running 0 11s 3 构建一个Helm Chart 命令来创建一个名为mychart 的helm chart [root@Ansible01 2022-05-21]# helm create mychart Creating mychart [root@Ansible01 2022-05-21]# ls mychart 创建后会在目录创建一个mychart目录 [root@Ansible01 2022-05-21]# tree mychart/ mychart/ ├── charts ├── Chart.yaml ├── templates │ ├── deployment.yaml │ ├── _helpers.tpl │ ├── hpa.yaml │ ├── ingress.yaml │ ├── NOTES.txt │ ├── serviceaccount.yaml │ ├── service.yaml │ └── tests │ └── test-connection.yaml └── values.yaml 3 directories, 10 files 其中mychart目录下的templates目录中保存有部署的模板文件，values.yaml中定义了部署的变量，Chart.yaml文件包含有version（chart版本）和appVersion（包含应用的版本）。 [root@Ansible01 2022-05-21]# cat mychart/Chart.yaml |grep -v &quot;^#&quot; |grep -v ^$ apiVersion: v2 name: mychart description: A Helm chart for Kubernetes type: application version: 0.1.0 appVersion: &quot;1.16.0&quot; 选择镜像及标签和副本数（这里设置1个） [root@Ansible01 2022-05-21]# vi mychart/values.yaml replicaCount: 1 image: repository: myapp pullPolicy: IfNotPresent # Overrides the image tag whose default is the chart appVersion. tag: &quot;v1&quot; 编辑完成后检查依赖及模板配置是否正确 [root@Ansible01 2022-05-21]# cd mychart/ [root@Ansible01 mychart]# helm lint . ==&gt; Linting . [INFO] Chart.yaml: icon is recommended 1 chart(s) linted, 0 chart(s) failed 打包应用，其中0.1.0为在Chart.yaml文件中定义的version（chart版本）信息 [root@Ansible01 2022-05-21]# helm package mychart/ Successfully packaged chart and saved it to: /root/2022-05-21/mychart-0.1.0.tgz [root@Ansible01 2022-05-21]# ls mychart mychart-0.1.0.tgz 升级、回滚和删除 # 发布新版本的chart时，或者当您要更改发布的配置时，可以使用该helm upgrade 命令。 helm upgrade --set imageTag=1.17 web mychart helm upgrade -f values.yaml web mychart # 如果在发布后没有达到预期的效果，则可以使用helm rollback回滚到之前的版本。 例如将应用回滚到第一个版本： helm rollback web 2 # 卸载发行版，请使用以下helm uninstall命令： helm uninstall web # 查看历史版本配置信息 helm get --revision 1 web 下载、上传tar包 [root@Ansible01 2022-05-21]# helm pull stable/traefik [root@Ansible01 2022-05-21]# ls traefik-1.87.7.tgz # 一种方式是直接上传charts文件夹，seldon-mab是charts目录，harbor-test-helm是harbor charts repo名称。 helm push seldon-mab harbor-test-helm # 另一种是将charts package文件包push helm push seldon-core-operator-1.5.1.tgz harbor-test-helm 安装到k8s的其它空间：--namespace=monitoring helm install --name prometheus-operator --set rbacEnable=true --namespace=monitoring stable/prometheus-operator 四、Helm使用minio搭建私有仓库 minio介绍 我们一般是从本地的目录结构中的chart去进行部署，如果要集中管理chart,就需要涉及到repository的问题，因为helmrepository都是指到外面的地址，接下来我们可以通过minio建立一个企业私有的存放仓库。 Minio提供对象存储服务。它的应用场景被设定在了非结构化的数据的存储之上了。众所周知，非结构化对象诸如图像/音频/视频/log文件/系统备份/镜像文件…等等保存起来管理总是不那么方便，size变化很大，类型很多，再有云端的结合会使得情况更加复杂，minio就是解决此种场景的一个解决方案。Minio号称其能很好的适应非结构化的数据，支持AWS的S3，非结构化的文件从数KB到5TB都能很好的支持。 Minio的使用比较简单，只有两个文件，服务端minio,客户访问端mc,比较简单。 在项目中，我们可以直接找一台虚拟机作为Minio Server,提供服务，当然minio也支持作为Pod部署。 1 helm3存储库更改 在Helm 2中，默认情况下包括稳定的图表存储库。在Helm 3中，默认情况下不包含任何存储库。因此需要做的第一件事就是添加一个存储库。官方图表存储库将在有限的时间内继续接收补丁，但是将不再作为默认存储库包含在Helm客户端中。 2 minio介绍 MinIO 是一个基于Apache License v2.0开源协议的对象存储服务。它兼容亚马逊S3云存储服务接口，非常适合于存储大容量非结构化的数据，例如图片、视频、日志文件、备份数据和容器/虚拟机镜像等，而一个对象文件可以是任意大小，从几kb到最大5T不等。 MinIO是一个非常轻量的服务,可以很简单的和其他应用的结合，类似 NodeJS, Redis 或者 MySQL。 3 安装minio服务端 1使用容器安装服务端 docker pull minio/minio docker run -p 9000:9000 minio/minio server /data 2使用二进制安装服务端 wget https://dl.min.io/server/minio/release/linux-amd64/minio chmod +x minio mkdir -p /chart ./minio server /chart 访问Browser Access地址： 在启动日志中获取access key和secret key 看到这个页面则表示登陆成功 至此服务端部署完成。 4 安装minio客户端 1.使用容器安装客户端 docker pull minio/mc docker run minio/mc ls play 2.使用二进制安装客户端 wget https://dl.min.io/client/mc/release/linux-amd64/mc chmod +x mc ./mc 5 连接至服务端 ./mc config host add myminio http://172.17.0.1:9000 XH2LCA4AJIP52RDB4P5M CDDCuoS2FNsdW8S0bodkcs2729N+TH5lFov+rrT3 服务端启动时候的access key和secret key 6 mc的shell使用别名 ls=mc ls cp=mc cp cat=mc cat mkdir=mc mb pipe=mc pipe find=mc find 7 创建bucket ./mc mb myminio/minio-helm-repo 8 设置bucket和objects匿名访问 ./mc policy set download myminio/minio-helm-repo 9 helm创建与仓库连接的index.yaml文件 mkdir /root/helm/repo helm repo index helm/repo/ 10 helm与minio仓库进行连接 1.将index.yaml文件推送到backet中去 ./mc cp helm/repo/index.yaml myminio/minio-helm-repo 2.helm连接私仓 helm repo add fengnan http://192.168.0.119:9000/minio-helm-repo 3.更新repo仓库 helm repo update 4.查看repo helm repo list 5.查看repo中的文件 ./mc ls myminio/minio-helm-repo 6.登录服务端web界面查看 ","link":"https://tinaxiawuhao.github.io/post/zIWVYloAQ/"},{"title":"k8s资源操作","content":"1.资源类型 资源分类 类型 具体资源 名称空间级别 工作负载型资源 Pod(pod:k8s 系统中可以创建和管理的最小单元) 名称空间级别 工作负载型资源 ReplicaSet(rs:用来确保容器应用的副本数始终保持在用户定义的副本数) 名称空间级别 工作负载型资源 Deployment(deployment:为 Pod 和 ReplicaSet 提供了一个 声明式定义 (declarative) 方法) 名称空间级别 工作负载型资源 StatefulSet(sts:为了解决有状态服务的问题) 名称空间级别 工作负载型资源 DaemonSet(ds:) 名称空间级别 工作负载型资源 Job(jobs:负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod) 名称空间级别 工作负载型资源 CronJob(jobs:管理基于时间的 Job) 名称空间级别 服务发现及负载均衡型资源 Service(svc:为一组功能相同的Pod提供一个供外界访问的地址) 名称空间级别 服务发现及负载均衡型资源 Ingress(ing:官方只能实现四层代理，INGRESS 可以实现七层代理) 名称空间级别 配置与存储型资源 Volume(存储卷) 名称空间级别 配置与存储型资源 CSI(容器存储接口-- 第三方存储卷) 名称空间级别 特殊类型的存储卷 ConfigMap(当配置中心来使用的资源类型) 名称空间级别 特殊类型的存储卷 Secret(保存敏感数据) 名称空间级别 特殊类型的存储卷 DownwardApi(把外部环境中的信息输出给容器) 集群级资源 集群级资源 Namespace(命名空间) 集群级资源 集群级资源 Node(集群节点) 集群级资源 集群级资源 Role(角色) 集群级资源 集群级资源 ClusterRole(集群角色) 集群级资源 集群级资源 RoleBinding(角色绑定) 集群级资源 集群级资源 ClusterRoleBinging(集群角色绑定) 元数据型资源 元数据型资源 HPA(根据 Pod的 CPU 利用率扩所容) 元数据型资源 元数据型资源 PodTemplate(pod资源模板) 元数据型资源 元数据型资源 LimitRange(资源限制) 2.操作指令 命令名 类型 作用 get 查 列出某个类型的下属资源 describe 查 查看某个资源的详细信息 logs 查 查看某个 pod 的日志 create 增 新建资源 explain 查 查看某个资源的配置项 delete 删 删除某个资源 edit 改 修改某个资源的配置项 apply 改 应用某个资源的配置项 3.查看和进入空间 命令名 作用 kubectl get pod -n名称空间 查看对应名称空间内的pod kubectl exec -it pod名字 -n名称空间 bash 进入对应名称空间的pod内 kubectl get nodes -o wide 获取节点和服务版本信息，并查看附加信息 kubectl get pod 获取pod信息，默认是default名称空间 kubectl get pod -o wide 获取pod信息，默认是default名称空间，并查看附加信息 kubectl get pod -n kube-system 获取指定名称空间的pod kubectl get pod -n kube-system podName 获取指定名称空间中的指定pod kubectl get pod -A 获取所有名称空间的pod kubectl get pods -o yamlkubectl get pods -o json 查看pod的详细信息，以yaml格式或json格式显示 kubectl get pod -A --show-labels 查看pod的标签信息 kubectl get pod -A --selector=“k8s-app=kube-dns” 根据Selector（label query）来查询pod kubectl exec podName env 查看运行pod的环境变量 kubectl logs -f --tail 500 -n kube-system kube-apiserver-k8s-master 查看指定pod的日志 kubectl get svc -A 查看所有名称空间的service信息 kubectl get svc -n kube-system 查看指定名称空间的service信息 kubectl get cs 查看componentstatuses信息 kubectl get cm -A 查看所有configmaps信息 kubectl get sa -A 查看所有serviceaccounts信息 kubectl get ds -A 查看所有daemonsets信息 kubectl get deploy -A 查看所有deployments信息 kubectl get rs -A 查看所有replicasets信息 kubectl get sts -A 查看所有statefulsets信息 kubectl get jobs -A 查看所有jobs信息 kubectl get ing -A 查看所有ingresses信息 kubectl get ns 查看有哪些名称空间 kubectl describe pod podNamekubectl describe pod -n kube-system kube-apiserver-k8s-master 查看pod的描述信息 kubectl describe deploy -n kube-system coredns 查看指定名称空间中指定deploy的描述信息 kubectl top nodekubectl top pod 查看node或pod的资源使用情况（需要heapster 或metrics-server支持） kubectl cluster-info kubectl cluster-info dump 查看集群信息 kubectl -s https://172.16.1.110:6443 get componentstatuses 查看各组件信息【172.16.1.110为master机器】 4.进入pod启动的容器 命令名 作用 kubectl exec -it podName -n nsName /bin/sh 进入容器 kubectl exec -it podName -n nsName /bin/bash 进入容器 5.添加label值 命令名 作用 kubectl label nodes k8s-node01 zone=north 为指定节点添加标签 kubectl label nodes k8s-node01 zone 为指定节点删除标签 kubectl label pod podName -n nsName role-name=test 为指定pod添加标签 kubectl label pod podName -n nsName role-name=dev --overwrite 修改lable标签值 kubectl label pod podName -n nsName role-name 删除lable标签 6.滚动升级 命令名 作用 kubectl apply -f myapp-deployment-v2.yaml 通过配置文件滚动升级 kubectl set image deploy/myapp-deployment myapp=“registry.cn-beijing.aliyuncs.com/google_registry/myapp:v3” 通过命令滚动升级 kubectl rollout undo deploy/myapp-deployment kubectl rollout undo deploy myapp-deployment pod回滚到前一个版本 kubectl rollout undo deploy/myapp-deployment --to-revision=2 回滚到指定历史版本 7.动态伸缩 命令名 作用 kubectl scale deploy myapp-deployment --replicas=5 动态伸缩 kubectl scale --replicas=8 -f myapp-deployment-v2.yaml 动态伸缩（根据yaml文件） 8.操作类命令 命令名 作用 kubectl create -f xxx.yaml 创建资源 kubectl apply -f xxx.yaml 应用资源 kubectl apply -f 应用资源，该目录下的所有 .yaml, .yml, 或 .json 文件都会被使用 kubectl create namespace test 创建test名称空间 kubectl delete -f xxx.yamlkubectl delete -f 删除资源 kubectl delete pod podName 删除指定的pod kubectl delete pod -n test podName 删除指定名称空间的指定pod kubectl delete svc svcNamekubectl delete deploy deployNamekubectl delete ns nsName 删除其他资源 kubectl delete pod podName -n nsName --grace-period=0 --forcekubectl delete pod podName -n nsName --grace-period=1kubectl delete pod podName -n nsName --now 强制删除 kubectl edit pod podName 编辑资源 9.状态 状态名 含义 Running 运行中 Error 异常，无法提供服务 Pending 准备中，暂时无法提供服务 Terminaling 结束中，即将被移除 Unknown 未知状态，多发生于节点宕机 PullImageBackOff 镜像拉取失败 必须存在的属性 参数名 字段类型 说明 version String K8S API版本, 可以使用kubectl api-version命令查询 kind String 指的是yaml文件定义的资源类型和角色, 比如Pod metadata Object 元数据对象, 固定值就写metadata metadata.name String 元数据对象的名字, 比如命名Pod的名字 metadata.namespace String 元数据对象的命名空间 Spec Object 详细定义对象, 固定值就写Spec spec.containers[] list Spec对象的容器列表定义, 是个列表 spec.containers[].name String 容器的名字 spec.containers[].image String 使用到的镜像名称 主要对象 apiVersion: v1 #必选，版本号，例如v1，可以用 kubectl api-versions 查询到 kind: Pod #必选，指yaml文件定义的k8s 资源类型或角色，比如：Pod metadata: #必选，元数据对象 name: string #必选，元数据对象的名字，自己定义，比如命名Pod的名字 namespace: string #必选，元数据对象的名称空间，默认为&quot;default&quot; labels: #自定义标签 key1: value1 #自定义标签键值对1 key2: value2 #自定义标签键值对2 annotations: #自定义注解 key1: value1 #自定义注解键值对1 key2: value2 #自定义注解键值对2 spec: #必选，对象【如pod】的详细定义 containers: #必选，spec对象的容器信息 - name: string #必选，容器名称 image: string #必选，要用到的镜像名称 imagePullPolicy: [Always|Never|IfNotPresent] #获取镜像的策略；(1)Always：意思是每次都尝试重新拉取镜像；(2)Never：表示仅使用本地镜像，即使本地没有镜像也不拉取；(3) IfNotPresent：如果本地有镜像就使用本地镜像，没有就拉取远程镜像。默认：Always command: [string] #指定容器启动命令，由于是数组因此可以指定多个。不指定则使用镜像打包时指定的启动命令。 args: [string] #指定容器启动命令参数，由于是数组因此可以指定多个 workingDir: string #指定容器的工作目录 volumeMounts: #指定容器内部的存储卷配置 - name: string #指定可以被容器挂载的存储卷的名称。跟下面volume字段的name值相同表示使用这个存储卷 mountPath: string #指定可以被容器挂载的存储卷的路径，应少于512字符 readOnly: boolean #设置存储卷路径的读写模式，true或者false，默认为读写模式false ports: #需要暴露的端口号列表 - name: string #端口的名称 containerPort: int #容器监听的端口号 #除非绝对必要，否则不要为 Pod 指定 hostPort。将 Pod 绑定到hostPort时，它会限制 Pod 可以调度的位置数 #DaemonSet 中的 Pod 可以使用 hostPort，从而可以通过节点 IP 访问到 Pod；因为DaemonSet模式下Pod不会被调度到其他节点。 #一般情况下 containerPort与hostPort值相同 hostPort: int #可以通过宿主机+hostPort的方式访问该Pod。例如：pod在/调度到了k8s-node02【172.16.1.112】，hostPort为8090，那么该Pod可以通过172.16.1.112:8090方式进行访问。 protocol: string #端口协议，支持TCP和UDP，默认TCP env: #容器运行前需设置的环境变量列表 - name: string #环境变量名称 value: string #环境变量的值 resources: #资源限制和资源请求的设置（设置容器的资源上线） limits: #容器运行时资源使用的上线 cpu: string #CPU限制，单位为core数，允许浮点数，如0.1等价于100m，0.5等价于500m；因此如果小于1那么优先选择如100m的形式，精度为1m。这个数字用作 docker run 命令中的 --cpu-quota 参数。 memory: string #内存限制，单位：E,P,T,G,M,K；或者Ei,Pi,Ti,Gi,Mi,Ki；或者字节数。将用于docker run --memory参数 requests: #容器启动和调度时的限制设定 cpu: string #CPU请求，容器启动时初始化可用数量，单位为core数，允许浮点数，如0.1等价于100m，0.5等价于500m；因此如果小于1那么优先选择如100m的形式，精度为1m。这个数字用作 docker run 命令中的 --cpu-shares 参数。 memory: string #内存请求,容器启动的初始化可用数量。单位：E,P,T,G,M,K；或者Ei,Pi,Ti,Gi,Mi,Ki；或者字节数 # 参见官网地址：https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/ livenessProbe: #对Pod内各容器健康检查的设置，当探测无响应几次后将自动重启该容器，检查方法有exec、httpGet和tcpSocket，对一个容器【只需设置其中一种方法即可】 exec: #对Pod内容器健康检查方式设置为exec方式 command: [string] #exec方式需要制定的命令或脚本 httpGet: #对Pod内容器健康检查方法设置为HttpGet，需要制定Path、port path: string #访问 HTTP 服务的路径 port: number #访问容器的端口号或者端口名。如果数字必须在 1 ~ 65535 之间。 host: string #当没有定义 &quot;host&quot; 时，使用 &quot;PodIP&quot; scheme: string #当没有定义 &quot;scheme&quot; 时，使用 &quot;HTTP&quot;，scheme 只允许 &quot;HTTP&quot; 和 &quot;HTTPS&quot; HttpHeaders: #请求中自定义的 HTTP 头。HTTP 头字段允许重复。 - name: string value: string tcpSocket: #对Pod内容器健康检查方式设置为tcpSocket方式 port: number initialDelaySeconds: 5 #容器启动完成后，kubelet在执行第一次探测前应该等待 5 秒。默认是 0 秒，最小值是 0。 periodSeconds: 60 #指定 kubelet 每隔 60 秒执行一次存活探测。默认是 10 秒。最小值是 1 timeoutSeconds: 3 #对容器健康检查探测等待响应的超时时间为 3 秒，默认1秒 successThreshold: 1 #检测到有1次成功则认为服务是`就绪` failureThreshold: 5 #检测到有5次失败则认为服务是`未就绪`。默认值是 3，最小值是 1。 restartPolicy: [Always|Never|OnFailure] #Pod的重启策略，默认Always。Always表示一旦不管以何种方式终止运行，kubelet都将重启；OnFailure表示只有Pod以非0退出码退出才重启；Nerver表示不再重启该Pod nodeSelector: #定义Node的label过滤标签，以key：value的格式指定。节点选择，先给主机打标签kubectl label nodes kube-node01 key1=value1 key1: value1 imagePullSecrets: #Pull镜像时使用的secret名称，以name：secretKeyName格式指定 - name: string hostNetwork: false #是否使用主机网络模式，默认为false。如果设置为true，表示使用宿主机网络，不使用docker网桥 # volumes 和 containers 是同层级 ****************************** # 参见官网地址：https://kubernetes.io/zh/docs/concepts/storage/volumes/ volumes: #定义了paues容器关联的宿主机或分布式文件系统存储卷列表 （volumes类型有很多种，选其中一种即可） - name: string #共享存储卷名称。 emptyDir: {} #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。当Pod因为某些原因被从节点上删除时，emptyDir卷中的数据也会永久删除。 hostPath: string #类型为hostPath的存储卷，表示挂载Pod所在宿主机的文件或目录 path: string #在宿主机上文件或目录的路径 type: [|DirectoryOrCreate|Directory|FileOrCreate|File] #空字符串（默认）用于向后兼容，这意味着在安装 hostPath 卷之前不会执行任何检查。DirectoryOrCreate：如果给定目录不存在则创建，权限设置为 0755，具有与 Kubelet 相同的组和所有权。Directory：给定目录必须存在。FileOrCreate：如果给定文件不存在，则创建空文件，权限设置为 0644，具有与 Kubelet 相同的组和所有权。File：给定文件必须存在。 secret: #类型为secret的存储卷，挂载集群预定义的secre对象到容器内部。Secret 是一种包含少量敏感信息例如密码、token 或 key 的对象。放在一个 secret 对象中可以更好地控制它的用途，并降低意外暴露的风险。 secretName: string #secret 对象的名字 items: #可选，修改key 的目标路径 - key: username #username secret存储在/etc/foo/my-group/my-username 文件中而不是 /etc/foo/username 中。【此时存在spec.containers[].volumeMounts[].mountPath为/etc/foo】 path: my-group/my-username configMap: #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部。ConfigMap 允许您将配置文件与镜像文件分离，以使容器化的应用程序具有可移植性。 name: string #提供你想要挂载的 ConfigMap 的名字 kubectl explain pod命令可以查看资源的模板 kubectl explain pod.apiVersion 查看详细字段 ","link":"https://tinaxiawuhao.github.io/post/HhauOMn26/"},{"title":"k8s-ingress安装使用","content":"1.Nginx ingress安装 首先需要安装Nginx Ingress Controller控制器，控制器安装方式包含两种：DaemonSets和Deployments。 DaemonSets通过hostPort的方式暴露80和443端口，可通过Node的调度由专门的节点实现部署 Deployments则通过NodePort的方式实现控制器端口的暴露，借助外部负载均衡实现高可用负载均衡 除此之外，还需要部署Namespace，ServiceAccount，RBAC，Secrets，Custom Resource Definitions等资源，如下开始部署。 1.1 基础依赖环境准备 1、github中下载源码包,安装部署文件在kubernetes-ingress/deployments/目录下 git clone https://github.com/nginxinc/kubernetes-ingress.git kubernetes-ingress/deployments/ ├── common │ ├── custom-resource-definitions.yaml 自定义资源 │ ├── default-server-secret.yaml Secrets │ ├── nginx-config.yaml │ └── ns-and-sa.yaml Namspace+ServiceAccount ├── daemon-set │ ├── nginx-ingress.yaml DaemonSets控制器 │ └── nginx-plus-ingress.yaml ├── deployment │ ├── nginx-ingress.yaml Deployments控制器 │ └── nginx-plus-ingress.yaml ├── helm-chart Helm安装包 │ ├── chart-icon.png │ ├── Chart.yaml │ ├── README.md │ ├── templates │ │ ├── controller-configmap.yaml │ │ ├── controller-custom-resources.yaml │ │ ├── controller-daemonset.yaml │ │ ├── controller-deployment.yaml │ │ ├── controller-leader-election-configmap.yaml │ │ ├── controller-secret.yaml │ │ ├── controller-serviceaccount.yaml │ │ ├── controller-service.yaml │ │ ├── controller-wildcard-secret.yaml │ │ ├── _helpers.tpl │ │ ├── NOTES.txt │ │ └── rbac.yaml │ ├── values-icp.yaml │ ├── values-plus.yaml │ └── values.yaml ├── rbac RBAC认证授权 │ └── rbac.yaml ├── README.md └── service Service定义 ├── loadbalancer-aws-elb.yaml ├── loadbalancer.yaml DaemonSets暴露服务方式 └── nodeport.yaml Deployments暴露服务方式 **2、创建Namespace和ServiceAccount ** cat common/ns-and-sa.yaml apiVersion: v1 kind: Namespace metadata: name: nginx-ingress --- apiVersion: v1 kind: ServiceAccount metadata: name: nginx-ingress namespace: nginx-ingress kubectl apply -f common/ns-and-sa.yaml 3、创建Secrets自签名证书 kubectl apply -f common/default-server-secret.yaml apiVersion: v1 kind: Secret metadata: name: default-server-secret namespace: nginx-ingress type: Opaque data: tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN2akNDQWFZQ0NRREFPRjl0THNhWFhEQU5CZ2txaGtpRzl3MEJBUXNGQURBaE1SOHdIUVlEVlFRRERCWk8KUjBsT1dFbHVaM0psYzNORGIyNTBjbTlzYkdWeU1CNFhEVEU0TURreE1qRTRNRE16TlZvWERUSXpNRGt4TVRFNApNRE16TlZvd0lURWZNQjBHQTFVRUF3d1dUa2RKVGxoSmJtZHlaWE56UTI5dWRISnZiR3hsY2pDQ0FTSXdEUVlKCktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQUwvN2hIUEtFWGRMdjNyaUM3QlBrMTNpWkt5eTlyQ08KR2xZUXYyK2EzUDF0azIrS3YwVGF5aGRCbDRrcnNUcTZzZm8vWUk1Y2Vhbkw4WGM3U1pyQkVRYm9EN2REbWs1Qgo4eDZLS2xHWU5IWlg0Rm5UZ0VPaStlM2ptTFFxRlBSY1kzVnNPazFFeUZBL0JnWlJVbkNHZUtGeERSN0tQdGhyCmtqSXVuektURXUyaDU4Tlp0S21ScUJHdDEwcTNRYzhZT3ExM2FnbmovUWRjc0ZYYTJnMjB1K1lYZDdoZ3krZksKWk4vVUkxQUQ0YzZyM1lma1ZWUmVHd1lxQVp1WXN2V0RKbW1GNWRwdEMzN011cDBPRUxVTExSakZJOTZXNXIwSAo1TmdPc25NWFJNV1hYVlpiNWRxT3R0SmRtS3FhZ25TZ1JQQVpQN2MwQjFQU2FqYzZjNGZRVXpNQ0F3RUFBVEFOCkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQWpLb2tRdGRPcEsrTzhibWVPc3lySmdJSXJycVFVY2ZOUitjb0hZVUoKdGhrYnhITFMzR3VBTWI5dm15VExPY2xxeC9aYzJPblEwMEJCLzlTb0swcitFZ1U2UlVrRWtWcitTTFA3NTdUWgozZWI4dmdPdEduMS9ienM3bzNBaS9kclkrcUI5Q2k1S3lPc3FHTG1US2xFaUtOYkcyR1ZyTWxjS0ZYQU80YTY3Cklnc1hzYktNbTQwV1U3cG9mcGltU1ZmaXFSdkV5YmN3N0NYODF6cFErUyt1eHRYK2VBZ3V0NHh3VlI5d2IyVXYKelhuZk9HbWhWNThDd1dIQnNKa0kxNXhaa2VUWXdSN0diaEFMSkZUUkk3dkhvQXprTWIzbjAxQjQyWjNrN3RXNQpJUDFmTlpIOFUvOWxiUHNoT21FRFZkdjF5ZytVRVJxbStGSis2R0oxeFJGcGZnPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= tls.key: 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 4、创建ConfigMap自定义配置文 kubectl apply -f common/nginx-config.yaml kind: ConfigMap apiVersion: v1 metadata: name: nginx-config namespace: nginx-ingress data: 5、为主机和主机路由定义自定义资源，支持自定义主机和路由 kubectl apply -f common/custom-resource-definitions.yaml apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: virtualservers.k8s.nginx.org spec: group: k8s.nginx.org versions: - name: v1 served: true storage: true scope: Namespaced names: plural: virtualservers singular: virtualserver kind: VirtualServer shortNames: - vs --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: virtualserverroutes.k8s.nginx.org spec: group: k8s.nginx.org versions: - name: v1 served: true storage: true scope: Namespaced names: plural: virtualserverroutes singular: virtualserverroute kind: VirtualServerRoute shortNames: - vsr 6、配置RBAC认证授权，实现ingress控制器访问集群中的其他资源 kubectl apply -f rbac/rbac.yaml kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: nginx-ingress rules: - apiGroups: - &quot;&quot; resources: - services - endpoints verbs: - get - list - watch - apiGroups: - &quot;&quot; resources: - secrets verbs: - get - list - watch - apiGroups: - &quot;&quot; resources: - configmaps verbs: - get - list - watch - update - create - apiGroups: - &quot;&quot; resources: - pods verbs: - list - watch - apiGroups: - &quot;&quot; resources: - events verbs: - create - patch - apiGroups: - extensions resources: - ingresses verbs: - list - watch - get - apiGroups: - &quot;extensions&quot; resources: - ingresses/status verbs: - update - apiGroups: - k8s.nginx.org resources: - virtualservers - virtualserverroutes verbs: - list - watch - get --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: nginx-ingress subjects: - kind: ServiceAccount name: nginx-ingress namespace: nginx-ingress roleRef: kind: ClusterRole name: nginx-ingress apiGroup: rbac.authorization.k8s.io 1.2 部署Ingress控制器 1、 部署控制器，控制器可以DaemonSets和Deployment的形式部署，如下是DaemonSets的配置文件 apiVersion: apps/v1 kind: DaemonSet metadata: name: nginx-ingress namespace: nginx-ingress spec: selector: matchLabels: app: nginx-ingress template: metadata: labels: app: nginx-ingress #annotations: #prometheus.io/scrape: &quot;true&quot; #prometheus.io/port: &quot;9113&quot; spec: serviceAccountName: nginx-ingress containers: - image: nginx/nginx-ingress:edge imagePullPolicy: Always name: nginx-ingress ports: - name: http containerPort: 80 hostPort: 80 #通过hostPort的方式暴露端口 - name: https containerPort: 443 hostPort: 443 #- name: prometheus #containerPort: 9113 securityContext: allowPrivilegeEscalation: true runAsUser: 101 #nginx capabilities: drop: - ALL add: - NET_BIND_SERVICE env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name args: - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret #- -v=3 # Enables extensive logging. Useful for troubleshooting. #- -report-ingress-status #- -external-service=nginx-ingress #- -enable-leader-election #- -enable-prometheus-metrics Deployments的配置文件 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-ingress namespace: nginx-ingress spec: replicas: 1 #副本的个数 selector: matchLabels: app: nginx-ingress template: metadata: labels: app: nginx-ingress #annotations: #prometheus.io/scrape: &quot;true&quot; #prometheus.io/port: &quot;9113&quot; spec: serviceAccountName: nginx-ingress containers: - image: nginx/nginx-ingress:edge imagePullPolicy: Always name: nginx-ingress ports: #内部暴露的服务端口，需要通过NodePort的方式暴露给外部 - name: http containerPort: 80 - name: https containerPort: 443 #- name: prometheus #containerPort: 9113 securityContext: allowPrivilegeEscalation: true runAsUser: 101 #nginx capabilities: drop: - ALL add: - NET_BIND_SERVICE env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name args: - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret #- -v=3 # Enables extensive logging. Useful for troubleshooting. #- -report-ingress-status #- -external-service=nginx-ingress #- -enable-leader-election #- -enable-prometheus-metrics 2、我们以DaemonSets的方式部署，DaemonSet部署集群中各个节点都是对等，如果有外部LoadBalancer则通过外部负载均衡路由至Ingress中 [root@node-1 deployments]# kubectl apply -f daemon-set/nginx-ingress.yaml daemonset.apps/nginx-ingress created [root@node-1 deployments]# kubectl get daemonsets -n nginx-ingress NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE nginx-ingress 3 3 3 3 3 &lt;none&gt; 15s [root@node-1 ~]# kubectl get pods -n nginx-ingress -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-ingress-7mpfc 1/1 Running 0 2m44s 10.244.0.50 node-1 &lt;none&gt; &lt;none&gt; nginx-ingress-l2rtj 1/1 Running 0 2m44s 10.244.1.144 node-2 &lt;none&gt; &lt;none&gt; nginx-ingress-tgf6r 1/1 Running 0 2m44s 10.244.2.160 node-3 &lt;none&gt; &lt;none&gt; 3、校验Nginx Ingress安装情况 此时三个节点均是对等，即访问任意一个节点均能实现相同的效果，统一入口则通过外部负载均衡，如果在云环境下执行kubectl apply -f service/loadbalancer.yaml创建外部负载均衡实现入口调度，自建的可以通过lvs或nginx等负载均衡实现接入。 备注说明：如果以Deployments的方式部署，则需要执行service/nodeport.yaml创建NodePort类型的Service，实现的效果和DaemonSets类似。 2. Ingress资源定义 上面的已安装了一个Nginx Ingress Controller控制器，有了Ingress控制器后，我们就可以定义Ingress资源来实现七层负载转发了，大体上Ingress支持三种使用方式：1. 基于虚拟主机转发，2. 基于虚拟机主机URI转发，3. 支持TLS加密转发。 2.1 Ingress定义 1、环境准备，先创建一个nginx的Deployment应用，包含2个副本 [root@node-1 ~]# kubectl run ingress-demo --image=nginx:1.7.9 --port=80 --replicas=2 [root@node-1 ~]# kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE ingress-demo 2/2 2 2 116s 2、以service方式暴露服务端口 [root@node-1 ~]# kubectl expose deployment ingress-demo --port=80 --protocol=TCP --target-port=80 service/ingress-demo exposed [root@node-1 ~]# kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-demo ClusterIP 10.109.33.91 &lt;none&gt; 80/TCP 2m15s 3、上述两个步骤已创建了一个service，如下我们定义一个ingress对象将流量转发至ingress-demo这个service，通过ingress.class指定控制器的类型为nginx [root@node-1 nginx-ingress]# cat nginx-ingress-demo.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress-demo labels: ingres-controller: nginx annotations: kubernets.io/ingress.class: nginx spec: rules: - host: www.test.cn http: paths: - path: / backend: serviceName: ingress-demo servicePort: 80 4、创建ingress对象 [root@node-1 nginx-ingress]# kubectl apply -f nginx-ingress-demo.yaml ingress.extensions/nginx-ingress-demo created 查看ingress资源列表 [root@node-1 nginx-ingress]# kubectl get ingresses NAME HOSTS ADDRESS PORTS AGE nginx-ingress-demo www.test.cn 80 4m4s 5、查看ingress详情，可以在Rules规则中看到后端Pod的列表，自动发现和关联相关Pod [root@node-1 ~]# kubectl describe ingresses nginx-ingress-demo Name: nginx-ingress-demo Namespace: default Address: Default backend: default-http-backend:80 (&lt;none&gt;) Rules: Host Path Backends ---- ---- -------- www.test.cn / ingress-demo:80 (10.244.1.146:80,10.244.2.162:80) Annotations: kubectl.kubernetes.io/last-applied-configuration: {&quot;apiVersion&quot;:&quot;extensions/v1beta1&quot;,&quot;kind&quot;:&quot;Ingress&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubernets.io/ingress.class&quot;:&quot;nginx&quot;},&quot;labels&quot;:{&quot;ingres-controller&quot;:&quot;nginx&quot;},&quot;name&quot;:&quot;nginx-ingress-demo&quot;,&quot;namespace&quot;:&quot;default&quot;},&quot;spec&quot;:{&quot;rules&quot;:[{&quot;host&quot;:&quot;www.happylaulab.cn&quot;,&quot;http&quot;:{&quot;paths&quot;:[{&quot;backend&quot;:{&quot;serviceName&quot;:&quot;ingress-demo&quot;,&quot;servicePort&quot;:80},&quot;path&quot;:&quot;/&quot;}]}}]}} kubernets.io/ingress.class: nginx Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal AddedOrUpdated 9m7s nginx-ingress-controller Configuration for default/nginx-ingress-demo was added or updated Normal AddedOrUpdated 9m7s nginx-ingress-controller Configuration for default/nginx-ingress-demo was added or updated Normal AddedOrUpdated 9m7s nginx-ingress-controller Configuration for default/nginx-ingress-demo was added or updated 6、测试验证 ingress规则的配置信息已注入到Ingress Controller中，环境中Ingress Controller是以DaemonSets的方式部署在集群中，如果有外部的负载均衡，则将www.test.cn域名的地址解析为负载均衡VIP。由于测试环境没有搭建负载均衡，将hosts解析执行node-1，node-2或者node-3任意一个IP都能实现相同的功能。 上述测试解析正常，当然也可以解析为node-1和node-2的IP，如下： [root@node-1 ~]# curl -I http://www.happylau.cn --resolve www.happylau.cn:80:10.254.100.101 HTTP/1.1 200 OK Server: nginx/1.17.6 Date: Tue, 24 Dec 2019 10:32:22 GMT Content-Type: text/html Content-Length: 612 Connection: keep-alive Last-Modified: Tue, 23 Dec 2014 16:25:09 GMT ETag: &quot;54999765-264&quot; Accept-Ranges: bytes [root@node-1 ~]# curl -I http://www.happylau.cn --resolve www.happylau.cn:80:10.254.100.102 HTTP/1.1 200 OK Server: nginx/1.17.6 Date: Tue, 24 Dec 2019 10:32:24 GMT Content-Type: text/html Content-Length: 612 Connection: keep-alive Last-Modified: Tue, 23 Dec 2014 16:25:09 GMT ETag: &quot;54999765-264&quot; Accept-Ranges: bytes 3 Ingress动态配置 上面介绍了ingress资源对象的申明配置，这里我们探究一下Nginx Ingress Controller的实现机制和动态配置更新机制，以方便了解Ingress控制器的工作机制。 1、 查看Nginx Controller控制器的配置文件，在nginx-ingress pod中存储着ingress的配置文件 [root@node-1 ~]# kubectl get pods -n nginx-ingress NAME READY STATUS RESTARTS AGE nginx-ingress-7mpfc 1/1 Running 0 6h15m nginx-ingress-l2rtj 1/1 Running 0 6h15m nginx-ingress-tgf6r 1/1 Running 0 6h15m #查看配置文件，每个ingress生成一个配置文件，文件名为：命名空间-ingres名称.conf [root@node-1 ~]# kubectl exec -it nginx-ingress-7mpfc -n nginx-ingress -- ls -l /etc/nginx/conf.d total 4 -rw-r--r-- 1 nginx nginx 1005 Dec 24 10:06 default-nginx-ingress-demo.conf #查看配置文件 [root@node-1 ~]# kubectl exec -it nginx-ingress-7mpfc -n nginx-ingress -- cat /etc/nginx/conf.d/default-nginx-ingress-demo.conf # configuration for default/nginx-ingress-demo #upstream的配置，会用least_conn算法，通过service服务发现机制动态识别到后端的Pod upstream default-nginx-ingress-demo-www.test.cn-ingress-demo-80 { zone default-nginx-ingress-demo-www.test.cn-ingress-demo-80 256k; random two least_conn; server 10.244.1.146:80 max_fails=1 fail_timeout=10s max_conns=0; server 10.244.2.162:80 max_fails=1 fail_timeout=10s max_conns=0; } server { listen 80; server_tokens on; server_name www.test.cn; location / { proxy_http_version 1.1; proxy_connect_timeout 60s; proxy_read_timeout 60s; proxy_send_timeout 60s; client_max_body_size 1m; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_buffering on; proxy_pass http://default-nginx-ingress-demo-www.test.cn-ingress-demo-80; #调用upstream实现代理 } } 通过上述查看配置文件可得知，Nginx Ingress Controller实际是根据ingress规则生成对应的nginx配置文件，以实现代理转发的功能，加入Deployments的副本数变更后nginx的配置文件会发生什么改变呢？ 2、更新控制器的副本数，由2个Pod副本扩容至3个 [root@node-1 ~]# kubectl scale --replicas=3 deployment ingress-demo deployment.extensions/ingress-demo scaled [root@node-1 ~]# kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE ingress-demo 3/3 3 3 123m 3、再次查看nginx的配置文件，ingress借助于service的服务发现机制，将加入的Pod自动加入到nginx upstream中 4、查看nginx pod的日志（kubectl logs nginx-ingress-7mpfc -n nginx-ingress），有reload优雅重启的记录，即通过更新配置文件+reload实现配置动态更新。 通过上述的配置可知，ingress调用kubernetes api去感知kubernetes集群中的变化情况，Pod的增加或减少这些变化，然后动态更新nginx ingress controller的配置文件，并重新载入配置。当集群规模越大时，会频繁涉及到配置文件的变动和重载，因此nginx这方面会存在先天的劣势，专门为微服务负载均衡应运而生，如Traefik，Envoy，Istio，这些负载均衡工具能够提供大规模，频繁动态更新的场景，但性能相比Nginx，HAproxy还存在一定的劣势。 4 Ingress路径转发 Ingress支持URI格式的转发方式，同时支持URL重写，如下以两个service为例演示，service-1安装nginx，service-2安装httpd，分别用http://demo.happylau.cn/news和http://demo.happylau.cn/sports转发到两个不同的service 1、环境准备，创建两个应用并实现service暴露，创建deployments时指定--explose创建service [root@node-1 ~]# kubectl run service-1 --image=nginx:1.7.9 --port=80 --replicas=1 --expose=true service/service-1 created deployment.apps/service-1 created [root@node-1 ~]# kubectl run service-2 --image=httpd --port=80 --replicas=1 --expose=true service/service-2 created deployment.apps/service-2 created 查看deployment状态 [root@node-1 ~]# kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE ingress-demo 4/4 4 4 4h36m service-1 1/1 1 1 65s service-2 1/1 1 1 52s 查看service状态，服务已经正常 [root@node-1 ~]# kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-demo ClusterIP 10.109.33.91 &lt;none&gt; 80/TCP 4h36m kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 101d service-1 ClusterIP 10.106.245.71 &lt;none&gt; 80/TCP 68s service-2 ClusterIP 10.104.204.158 &lt;none&gt; 80/TCP 55s 2、创建ingress对象，通过一个域名将请求转发至后端两个service [root@node-1 nginx-ingress]# cat nginx-ingress-uri-demo.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress-uri-demo labels: ingres-controller: nginx annotations: kubernets.io/ingress.class: nginx nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: http: paths: - path: /news backend: serviceName: service-1 servicePort: 80 - path: /sports backend: serviceName: service-2 servicePort: 80 3、创建ingress规则，查看详情 [root@node-1 nginx-ingress]# kubectl apply -f nginx-ingress-uri-demo.yaml ingress.extensions/nginx-ingress-uri-demo created #查看详情 [root@node-1 nginx-ingress]# kubectl get ingresses. NAME HOSTS ADDRESS PORTS AGE nginx-ingress-demo www.happylau.cn 80 4h35m nginx-ingress-uri-demo demo.happylau.cn 80 4s [root@node-1 nginx-ingress]# kubectl describe ingresses nginx-ingress-uri-demo Name: nginx-ingress-uri-demo Namespace: default Address: Default backend: default-http-backend:80 (&lt;none&gt;) Rules: #对应的转发url规则 Host Path Backends ---- ---- -------- demo.happylau.cn /news service-1:80 (10.244.2.163:80) /sports service-2:80 (10.244.1.148:80) Annotations: kubectl.kubernetes.io/last-applied-configuration: {&quot;apiVersion&quot;:&quot;extensions/v1beta1&quot;,&quot;kind&quot;:&quot;Ingress&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubernets.io/ingress.class&quot;:&quot;nginx&quot;,&quot;nginx.ingress.kubernetes.io/rewrite-target&quot;:&quot;/&quot;},&quot;labels&quot;:{&quot;ingres-controller&quot;:&quot;nginx&quot;},&quot;name&quot;:&quot;nginx-ingress-uri-demo&quot;,&quot;namespace&quot;:&quot;default&quot;},&quot;spec&quot;:{&quot;rules&quot;:[{&quot;host&quot;:&quot;demo.happylau.cn&quot;,&quot;http&quot;:{&quot;paths&quot;:[{&quot;backend&quot;:{&quot;serviceName&quot;:&quot;service-1&quot;,&quot;servicePort&quot;:80},&quot;path&quot;:&quot;/news&quot;},{&quot;backend&quot;:{&quot;serviceName&quot;:&quot;service-2&quot;,&quot;servicePort&quot;:80},&quot;path&quot;:&quot;/sports&quot;}]}}]}} kubernets.io/ingress.class: nginx nginx.ingress.kubernetes.io/rewrite-target: / Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal AddedOrUpdated 11s nginx-ingress-controller Configuration for default/nginx-ingress-uri-demo was added or updated Normal AddedOrUpdated 11s nginx-ingress-controller Configuration for default/nginx-ingress-uri-demo was added or updated Normal AddedOrUpdated 11s nginx-ingress-controller Configuration for default/nginx-ingress-uri-demo was added or updated 4、准备测试，站点中创建对应的路径 [root@node-1 ~]# kubectl exec -it service-1-7b66bf758f-xj9jh /bin/bash root@service-1-7b66bf758f-xj9jh:/# echo &quot;service-1 website page&quot; &gt;/usr/share/nginx/html/news [root@node-1 ~]# kubectl exec -it service-2-7c7444684d-w9cv9 /bin/bash root@service-2-7c7444684d-w9cv9:/usr/local/apache2# echo &quot;service-2 website page&quot; &gt;/usr/local/apache2/htdocs/sports 5、测试验证 [root@node-1 ~]# curl http://demo.happylau.cn/news --resolve demo.happylau.cn:80:10.254.100.101 service-1 website page [root@node-1 ~]# curl http://demo.happylau.cn/sports --resolve demo.happylau.cn:80:10.254.100.101 service-2 website page 6、总结 通过上述的验证测试可以得知，ingress支持URI的路由方式转发，其对应在ingress中的配置文件内容是怎样的呢，我们看下ingress controller生成对应的nginx配置文件内容，实际是通过ingress的location来实现，将不同的localtion转发至不同的upstream以实现service的关联，配置文件如下： [root@node-1 ~]# kubectl exec -it nginx-ingress-7mpfc -n nginx-ingress /bin/bash nginx@nginx-ingress-7mpfc:/$ cat /etc/nginx/conf.d/default-nginx-ingress-uri-demo.conf |grep -v &quot;^$&quot; # configuration for default/nginx-ingress-uri-demo #定义两个upstream和后端的service关联 upstream default-nginx-ingress-uri-demo-demo.happylau.cn-service-1-80 { zone default-nginx-ingress-uri-demo-demo.happylau.cn-service-1-80 256k; random two least_conn; server 10.244.2.163:80 max_fails=1 fail_timeout=10s max_conns=0; } upstream default-nginx-ingress-uri-demo-demo.happylau.cn-service-2-80 { zone default-nginx-ingress-uri-demo-demo.happylau.cn-service-2-80 256k; random two least_conn; server 10.244.1.148:80 max_fails=1 fail_timeout=10s max_conns=0; } server { listen 80; server_tokens on; server_name demo.happylau.cn; #定义location实现代理，通过proxy_pass和后端的service关联 location /news { proxy_http_version 1.1; proxy_connect_timeout 60s; proxy_read_timeout 60s; proxy_send_timeout 60s; client_max_body_size 1m; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_buffering on; proxy_pass http://default-nginx-ingress-uri-demo-demo.happylau.cn-service-1-80; } location /sports { proxy_http_version 1.1; proxy_connect_timeout 60s; proxy_read_timeout 60s; proxy_send_timeout 60s; client_max_body_size 1m; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_buffering on; proxy_pass http://default-nginx-ingress-uri-demo-demo.happylau.cn-service-2-80; } } 5 Ingress虚拟主机 ingress支持基于名称的虚拟主机，实现单个IP多个域名转发的需求，通过请求头部携带主机名方式区分开，将上个章节的ingress删除，使用service-1和service-2两个service来做演示。 1、创建ingress规则，通过主机名实现转发规则 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress-virtualname-demo labels: ingres-controller: nginx annotations: kubernets.io/ingress.class: nginx spec: rules: - host: news.happylau.cn http: paths: - path: / backend: serviceName: service-1 servicePort: 80 - host: sports.happylau.cn http: paths: - path: / backend: serviceName: service-2 servicePort: 80 2、生成ingress规则并查看详情，一个ingress对应两个HOSTS [root@node-1 nginx-ingress]# kubectl apply -f nginx-ingress-virtualname.yaml ingress.extensions/nginx-ingress-virtualname-demo created #查看列表 [root@node-1 nginx-ingress]# kubectl get ingresses nginx-ingress-virtualname-demo NAME HOSTS ADDRESS PORTS AGE nginx-ingress-virtualname-demo news.happylau.cn,sports.happylau.cn 80 12s #查看详情 [root@node-1 nginx-ingress]# kubectl describe ingresses nginx-ingress-virtualname-demo Name: nginx-ingress-virtualname-demo Namespace: default Address: Default backend: default-http-backend:80 (&lt;none&gt;) Rules: Host Path Backends ---- ---- -------- news.happylau.cn / service-1:80 (10.244.2.163:80) sports.happylau.cn / service-2:80 (10.244.1.148:80) Annotations: kubectl.kubernetes.io/last-applied-configuration: {&quot;apiVersion&quot;:&quot;extensions/v1beta1&quot;,&quot;kind&quot;:&quot;Ingress&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubernets.io/ingress.class&quot;:&quot;nginx&quot;},&quot;labels&quot;:{&quot;ingres-controller&quot;:&quot;nginx&quot;},&quot;name&quot;:&quot;nginx-ingress-virtualname-demo&quot;,&quot;namespace&quot;:&quot;default&quot;},&quot;spec&quot;:{&quot;rules&quot;:[{&quot;host&quot;:&quot;news.happylau.cn&quot;,&quot;http&quot;:{&quot;paths&quot;:[{&quot;backend&quot;:{&quot;serviceName&quot;:&quot;service-1&quot;,&quot;servicePort&quot;:80},&quot;path&quot;:&quot;/&quot;}]}},{&quot;host&quot;:&quot;sports.happylau.cn&quot;,&quot;http&quot;:{&quot;paths&quot;:[{&quot;backend&quot;:{&quot;serviceName&quot;:&quot;service-2&quot;,&quot;servicePort&quot;:80},&quot;path&quot;:&quot;/&quot;}]}}]}} kubernets.io/ingress.class: nginx Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal AddedOrUpdated 28s nginx-ingress-controller Configuration for default/nginx-ingress-virtualname-demo was added or updated Normal AddedOrUpdated 28s nginx-ingress-controller Configuration for default/nginx-ingress-virtualname-demo was added or updated Normal AddedOrUpdated 28s nginx-ingress-controller Configuration for default/nginx-ingress-virtualname-demo was added or updated 3、准备测试数据并测试 [root@node-1 ~]# kubectl exec -it service-1-7b66bf758f-xj9jh /bin/bash root@service-1-7b66bf758f-xj9jh:/# echo &quot;news demo&quot; &gt;/usr/share/nginx/html/index.html [root@node-1 ~]# kubectl exec -it service-2-7c7444684d-w9cv9 /bin/bash root@service-2-7c7444684d-w9cv9:/usr/local/apache2# echo &quot;sports demo&quot; &gt;/usr/local/apache2/htdocs/index.html 测试： [root@node-1 ~]# curl http://news.happylau.cn --resolve news.happylau.cn:80:10.254.100.102 news demo [root@node-1 ~]# curl http://sports.happylau.cn --resolve sports.happylau.cn:80:10.254.100.102 sports demo 4、查看nginx的配置文件内容，通过在server中定义不同的server_name以区分，代理到不同的upstream以实现service的代理。 # configuration for default/nginx-ingress-virtualname-demo upstream default-nginx-ingress-virtualname-demo-news.happylau.cn-service-1-80 { zone default-nginx-ingress-virtualname-demo-news.happylau.cn-service-1-80 256k; random two least_conn; server 10.244.2.163:80 max_fails=1 fail_timeout=10s max_conns=0; } upstream default-nginx-ingress-virtualname-demo-sports.happylau.cn-service-2-80 { zone default-nginx-ingress-virtualname-demo-sports.happylau.cn-service-2-80 256k; random two least_conn; server 10.244.1.148:80 max_fails=1 fail_timeout=10s max_conns=0; } server { listen 80; server_tokens on; server_name news.happylau.cn; location / { proxy_http_version 1.1; proxy_connect_timeout 60s; proxy_read_timeout 60s; proxy_send_timeout 60s; client_max_body_size 1m; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_buffering on; proxy_pass http://default-nginx-ingress-virtualname-demo-news.happylau.cn-service-1-80; } } server { listen 80; server_tokens on; server_name sports.happylau.cn; location / { proxy_http_version 1.1; proxy_connect_timeout 60s; proxy_read_timeout 60s; proxy_send_timeout 60s; client_max_body_size 1m; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_buffering on; proxy_pass http://default-nginx-ingress-virtualname-demo-sports.happylau.cn-service-2-80; } } 6 Ingress TLS加密 四层的负载均衡无法支持https请求，当前大部分业务都要求以https方式接入，Ingress能支持https的方式接入，通过Secrets存储证书+私钥，实现https接入，同时还能支持http跳转功能。对于用户的请求流量来说，客户端到ingress controller是https流量，ingress controller到后端service则是http，提高用户访问性能，如下介绍ingress TLS功能实现步骤。 1、生成自签名证书和私钥 [root@node-1 ~]# openssl req -x509 -newkey rsa:2048 -nodes -days 365 -keyout tls.key -out tls.crt Generating a 2048 bit RSA private key ....................................................+++ ........................................+++ writing new private key to 'tls.key' ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [XX]:CN #国家 State or Province Name (full name) []:GD #省份 Locality Name (eg, city) [Default City]:ShenZhen #城市 Organization Name (eg, company) [Default Company Ltd]:Tencent #公司 Organizational Unit Name (eg, section) []:HappyLau #组织 Common Name (eg, your name or your server's hostname) []:www.happylau.cn #域名 Email Address []:573302346@qq.com #邮箱地址 #tls.crt为证书，tls.key为私钥 [root@node-1 ~]# ls tls.* -l -rw-r--r-- 1 root root 1428 12月 26 13:21 tls.crt -rw-r--r-- 1 root root 1708 12月 26 13:21 tls.key 2、配置Secrets，将证书和私钥配置到Secrets中 [root@node-1 ~]# kubectl create secret tls happylau-sslkey --cert=tls.crt --key=tls.key secret/happylau-sslkey created 查看Secrets详情,证书和私要包含在data中，文件名为两个不同的key：tls.crt和tls.key [root@node-1 ~]# kubectl describe secrets happylau-sslkey Name: happylau-sslkey Namespace: default Labels: &lt;none&gt; Annotations: &lt;none&gt; Type: kubernetes.io/tls Data ==== tls.crt: 1428 bytes tls.key: 1708 bytes 3、配置ingress调用Secrets实现SSL证书加密 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress-tls-demo labels: ingres-controller: nginx annotations: kubernets.io/ingress.class: nginx spec: tls: - hosts: - news.happylau.cn - sports.happylau.cn secretName: happylau-sslkey rules: - host: news.happylau.cn http: paths: - path: / backend: serviceName: service-1 servicePort: 80 - host: sports.happylau.cn http: paths: - path: / backend: serviceName: service-2 servicePort: 80 4、创建ingress并查看ingress详情 [root@node-1 nginx-ingress]# kubectl describe ingresses nginx-ingress-tls-demo Name: nginx-ingress-tls-demo Namespace: default Address: Default backend: default-http-backend:80 (&lt;none&gt;) TLS: happylau-sslkey terminates news.happylau.cn,sports.happylau.cn Rules: Host Path Backends ---- ---- -------- news.happylau.cn / service-1:80 (10.244.2.163:80) sports.happylau.cn / service-2:80 (10.244.1.148:80) Annotations: kubectl.kubernetes.io/last-applied-configuration: {&quot;apiVersion&quot;:&quot;extensions/v1beta1&quot;,&quot;kind&quot;:&quot;Ingress&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubernets.io/ingress.class&quot;:&quot;nginx&quot;},&quot;labels&quot;:{&quot;ingres-controller&quot;:&quot;nginx&quot;},&quot;name&quot;:&quot;nginx-ingress-tls-demo&quot;,&quot;namespace&quot;:&quot;default&quot;},&quot;spec&quot;:{&quot;rules&quot;:[{&quot;host&quot;:&quot;news.happylau.cn&quot;,&quot;http&quot;:{&quot;paths&quot;:[{&quot;backend&quot;:{&quot;serviceName&quot;:&quot;service-1&quot;,&quot;servicePort&quot;:80},&quot;path&quot;:&quot;/&quot;}]}},{&quot;host&quot;:&quot;sports.happylau.cn&quot;,&quot;http&quot;:{&quot;paths&quot;:[{&quot;backend&quot;:{&quot;serviceName&quot;:&quot;service-2&quot;,&quot;servicePort&quot;:80},&quot;path&quot;:&quot;/&quot;}]}}],&quot;tls&quot;:[{&quot;hosts&quot;:[&quot;news.happylau.cn&quot;,&quot;sports.happylau.cn&quot;],&quot;secretName&quot;:&quot;happylau-sslkey&quot;}]}} kubernets.io/ingress.class: nginx Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal AddedOrUpdated 22s nginx-ingress-controller Configuration for default/nginx-ingress-tls-demo was added or updated Normal AddedOrUpdated 22s nginx-ingress-controller Configuration for default/nginx-ingress-tls-demo was added or updated Normal AddedOrUpdated 22s nginx-ingress-controller Configuration for default/nginx-ingress-tls-demo was added or updated 5、访问 将news.happylau.cn和sports.happylau.cn写入到hosts文件中，并通过https://news.happylau.cn 的方式访问，浏览器访问内容提示证书如下，信任证书即可访问到站点内容。 查看证书详情，正是我们制作的自签名证书，生产实际使用时，推荐使用CA机构颁发签名证书。 6、接下来查看一下tls配置https的nginx配置文件内容，可以看到在server块启用了https并配置证书，同时配置了http跳转，因此直接访问http也能够实现自动跳转到https功能。 # configuration for default/nginx-ingress-tls-demo upstream default-nginx-ingress-tls-demo-news.happylau.cn-service-1-80 { zone default-nginx-ingress-tls-demo-news.happylau.cn-service-1-80 256k; random two least_conn; server 10.244.2.163:80 max_fails=1 fail_timeout=10s max_conns=0; } upstream default-nginx-ingress-tls-demo-sports.happylau.cn-service-2-80 { zone default-nginx-ingress-tls-demo-sports.happylau.cn-service-2-80 256k; random two least_conn; server 10.244.1.148:80 max_fails=1 fail_timeout=10s max_conns=0; } server { listen 80; listen 443 ssl; #https监听端口，证书和key，实现和Secrets关联 ssl_certificate /etc/nginx/secrets/default-happylau-sslkey; ssl_certificate_key /etc/nginx/secrets/default-happylau-sslkey; server_tokens on; server_name news.happylau.cn; #http跳转功能，即访问http会自动跳转至https if ($scheme = http) { return 301 https://$host:443$request_uri; } location / { proxy_http_version 1.1; proxy_connect_timeout 60s; proxy_read_timeout 60s; proxy_send_timeout 60s; client_max_body_size 1m; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_buffering on; proxy_pass http://default-nginx-ingress-tls-demo-news.happylau.cn-service-1-80; } } server { listen 80; listen 443 ssl; ssl_certificate /etc/nginx/secrets/default-happylau-sslkey; ssl_certificate_key /etc/nginx/secrets/default-happylau-sslkey; server_tokens on; server_name sports.happylau.cn; if ($scheme = http) { return 301 https://$host:443$request_uri; } location / { proxy_http_version 1.1; proxy_connect_timeout 60s; proxy_read_timeout 60s; proxy_send_timeout 60s; client_max_body_size 1m; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_buffering on; proxy_pass http://default-nginx-ingress-tls-demo-sports.happylau.cn-service-2-80; } } 7. Nginx Ingress高级功能 7.1 定制化参数 ingress controller提供了基础反向代理的功能，如果需要定制化nginx的特性或参数，需要通过ConfigMap和Annotations来实现，两者实现的方式有所不同，ConfigMap用于指定整个ingress集群资源的基本参数，修改后会被所有的ingress对象所继承；Annotations则被某个具体的ingress对象所使用，修改只会影响某个具体的ingress资源，冲突时其优先级高于ConfigMap。 7.1.1 ConfigMap自定义参数 安装nginx ingress controller时默认会包含一个空的ConfigMap，可以通过ConfigMap来自定义nginx controller的默认参数，如下以修改一些参数为例： 1、 定义ConfigMap参数 kind: ConfigMap apiVersion: v1 metadata: name: nginx-config namespace: nginx-ingress data: proxy-connect-timeout: &quot;10s&quot; proxy-read-timeout: &quot;10s&quot; proxy-send-timeout: &quot;10&quot; client-max-body-size: &quot;3m&quot; 2、 应用配置并查看ConfigMap配置 [root@node-1 ~]# kubectl get configmaps -n nginx-ingress nginx-config -o yaml apiVersion: v1 data: client-max-body-size: 3m proxy-connect-timeout: 10s proxy-read-timeout: 10s proxy-send-timeout: 10s kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:{&quot;client-max-body-size&quot;:&quot;3m&quot;,&quot;proxy-connect-timeout&quot;:&quot;10s&quot;,&quot;proxy-read-timeout&quot;:&quot;10s&quot;,&quot;proxy-send-timeout&quot;:&quot;10&quot;},&quot;kind&quot;:&quot;ConfigMap&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;nginx-config&quot;,&quot;namespace&quot;:&quot;nginx-ingress&quot;}} creationTimestamp: &quot;2019-12-24T04:39:23Z&quot; name: nginx-config namespace: nginx-ingress resourceVersion: &quot;13845543&quot; selfLink: /api/v1/namespaces/nginx-ingress/configmaps/nginx-config uid: 9313ae47-a0f0-463e-a25a-1658f1ca0d57 3 、此时，ConfigMap定义的配置参数会被集群中所有的Ingress资源继承（除了annotations定义之外） 有很多参数可以定义，详情配置可参考方文档说明：https://github.com/nginxinc/kubernetes-ingress/blob/master/docs/configmap-and-annotations.md#Summary-of-ConfigMap-and-Annotations 7.1.2 Annotations自定义参数 ConfigMap定义的是全局的配置参数，修改后所有的配置都会受影响，如果想针对某个具体的ingress资源自定义参数，则可以通过Annotations来实现，下面开始以实际的例子演示Annotations的使用。 1、修改ingress资源，添加annotations的定义,通过nginx.org组修改了一些参数，如proxy-connect-timeout，调度算法为round_robin（默认为least _conn） apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress-demo labels: ingres-controller: nginx annotations: kubernets.io/ingress.class: nginx nginx.org/proxy-connect-timeout: &quot;30s&quot; nginx.org/proxy-send-timeout: &quot;20s&quot; nginx.org/proxy-read-timeout: &quot;20s&quot; nginx.org/client-max-body-size: &quot;2m&quot; nginx.org/fail-timeout: &quot;5s&quot; nginx.org/lb-method: &quot;round_robin&quot; spec: rules: - host: www.happylau.cn http: paths: - path: / backend: serviceName: ingress-demo servicePort: 80 2、 重新应用ingress对象并查看参数配置情况 由上面的演示可得知，Annotations的优先级高于ConfigMapMap，Annotations修改参数只会影响到某一个具体的ingress资源，其定义的方法和ConfigMap相相近似，但又有差别，部分ConfigMap的参数Annotations无法支持，反过来Annotations定义的参数ConfigMap也不一定支持，下图列举一下常规支持参数情况： 通用参数 日志支持 请求头部 认证和安全 upstream支持 7.2 虚拟主机和路由 安装nginx ingress时我们安装了一个customresourcedefinitions自定义资源，其能够提供除了默认ingress功能之外的一些高级特性如 虚拟主机VirtualServer 虚拟路由VirtualServerRoute 健康检查Healthcheck 流量切割Split 会话保持SessionCookie 重定向Redirect 这些功能大部分依赖于Nginx Plus高级版本的支持，社区版本仅支持部分，对于企业级开发而言，丰富更多的功能可以购买企业级Nginx Plus版本。如下以通过VirtualServer和VirtualServerRoute定义upstream配置为例演示功能使用。 1、定义VirtualServer资源,其配置和ingress资源对象类似，能支持的功能会更丰富一点 apiVersion: k8s.nginx.org/v1 kind: VirtualServer metadata: name: cafe spec: host: cafe.example.com tls: secret: cafe-secret upstreams: - name: tea service: tea-svc port: 80 name: tea service: ingress-demo subselector: version: canary lb-method: round_robin fail-timeout: 10s max-fails: 1 max-conns: 32 keepalive: 32 connect-timeout: 30s read-timeout: 30s send-timeout: 30s next-upstream: &quot;error timeout non_idempotent&quot; next-upstream-timeout: 5s next-upstream-tries: 10 client-max-body-size: 2m tls: enable: true routes: - path: /tea action: pass: tea 2、 应用资源并查看VirtualServer资源列表 [root@node-1 ~]# kubectl apply -f vs.yaml virtualserver.k8s.nginx.org/cafe unchanged [root@node-1 ~]# kubectl get virtualserver NAME AGE cafe 2m52s 3、检查ingress控制器的配置文件情况,生成的配置和upstream定义一致 nginx@nginx-ingress-7mpfc:/etc/nginx/conf.d$ cat vs_default_cafe.conf upstream vs_default_cafe_tea { zone vs_default_cafe_tea 256k; server 10.244.0.51:80 max_fails=1 fail_timeout=10s max_conns=32; server 10.244.1.146:80 max_fails=1 fail_timeout=10s max_conns=32; server 10.244.1.147:80 max_fails=1 fail_timeout=10s max_conns=32; server 10.244.2.162:80 max_fails=1 fail_timeout=10s max_conns=32; keepalive 32; } server { listen 80; server_name cafe.example.com; listen 443 ssl; ssl_certificate /etc/nginx/secrets/default; ssl_certificate_key /etc/nginx/secrets/default; ssl_ciphers NULL; server_tokens &quot;on&quot;; location /tea { proxy_connect_timeout 30s; proxy_read_timeout 30s; proxy_send_timeout 30s; client_max_body_size 2m; proxy_max_temp_file_size 1024m; proxy_buffering on; proxy_http_version 1.1; set $default_connection_header &quot;&quot;; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $vs_connection_header; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_pass https://vs_default_cafe_tea; proxy_next_upstream error timeout non_idempotent; proxy_next_upstream_timeout 5s; proxy_next_upstream_tries 10; } } ","link":"https://tinaxiawuhao.github.io/post/TX9ZIPxbq/"},{"title":"k8s界面kubesphere安装","content":"在 Kubernetes 上安装 KubeSphere 3.2.1，您的 Kubernetes 版本必须为：1.19.x、1.20.x、1.21.x 或 1.22.x（实验性支持） 1 搭建NFS作为默认sc（（主节点作为服务器，主节点操作） 1.1 配置NFS服务器 yum install -y nfs-utils rpcbind &amp;&amp; echo &quot;/nfs *(insecure,rw,sync,no_root_squash)&quot; &gt; /etc/exports 1.2 创建nfs服务器目录 mkdir -p /nfs 1.3 启动rpcbind,nfs服务命令 systemctl restart rpcbind &amp;&amp; systemctl enable rpcbind systemctl restart nfs-server &amp;&amp; systemctl enable nfs-server 1.4 检查配置是否生效 exportfs -r exportfs 1.5 测试Pod直接挂载NFS了（主节点操作） 1.5.1 在opt目录下创建一个nginx.yaml的文件 vi nginx.yaml 1.5.2 写入以下的命令 apiVersion: v1 kind: Pod metadata: name: vol-nfs namespace: default spec: volumes: - name: html nfs: path: /nfs server: 192.168.40.131 #自己的nfs服务器地址 containers: - name: myapp image: nginx volumeMounts: - name: html mountPath: /usr/share/nginx/html/ 1.5.3 应用该yaml的pod服务 kubectl apply -f nginx.yaml 1.5.4 检查该pod是否允许状态 kubectl get pod kubectl get pods -A 这里需要注意的是，必须等所有的状态为Runing才能进行下一步操作。 2 搭建NFS-Client（node节点操作） 服务器端防火墙开放111、662、875、892、2049的 tcp / udp 允许，否则远端客户无法连接。 2.1 安装客户端工具 yum install -y nfs-utils rpcbind showmount -e 192.168.40.131 该IP地址是master的IP地址 2.2 创建同步文件夹 mkdir /nfs/data/ 2.3 将客户端的/nfs/data和/nfs/做同步（node节点操作） mount -t nfs 192.168.40.131:/nfs/ /nfs/data/ 192.168.40.131：是nfs的服务器的地址，这里是master的IP地址。 3 设置动态供应 3.1 创建provisioner（NFS环境前面已经搭好） 字段名称 填入内容 备注 名称 nfs-storage 自定义存储类名称 NFS Server 192.168.40.131 NFS服务的IP地址 NFS Path /nfs NFS服务所共享的路径 3.1.1 先创建授权（master节点操作） vim nfs-rbac.yaml #在opt目录下 新建内容如下： --- apiVersion: v1 kind: ServiceAccount metadata: name: nfs-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-provisioner-runner rules: - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumes&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumeclaims&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;] - apiGroups: [&quot;storage.k8s.io&quot;] resources: [&quot;storageclasses&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;events&quot;] verbs: [&quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;services&quot;, &quot;endpoints&quot;] verbs: [&quot;get&quot;,&quot;create&quot;,&quot;list&quot;, &quot;watch&quot;,&quot;update&quot;] - apiGroups: [&quot;extensions&quot;] resources: [&quot;podsecuritypolicies&quot;] resourceNames: [&quot;nfs-provisioner&quot;] verbs: [&quot;use&quot;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-provisioner subjects: - kind: ServiceAccount name: nfs-provisioner namespace: default roleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Deployment apiVersion: apps/v1 metadata: name: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-client-provisioner image: lizhenliang/nfs-client-provisioner volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: storage.pri/nfs - name: NFS_SERVER value: 192.168.40.131 - name: NFS_PATH value: /nfs volumes: - name: nfs-client-root nfs: server: 192.168.40.131 path: /nfs 这个镜像中volume的mountPath默认为/persistentvolumes，不能修改，否则运行时会报错。ip的必须是自己的master的IP地址。 3.1.2 执行创建nfs的yaml文件信息 kubectl apply -f nfs-rbac.yaml 3.1.3 如果发现pod有问题，想删除pod进行重新kubectl apply-f nfs-rbac.yaml的话，可以参照这个博客文档： https://blog.csdn.net/qq_43542988/article/details/101277263?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param 3.1.4 查看pod的状态信息 kubectl get pods -A # 如果报错：查看报错信息，这个命令： kubectl describe pod xxx -n kube-system 3.1.5 创建storageclass（master节点操作） vim storageclass-nfs.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: storage-nfs provisioner: storage.pri/nfs reclaimPolicy: Delete 3.1.6 应用storageclass-nfs.yaml文件 kubectl apply -f storageclass-nfs.yaml 3.1.7 修改默认的驱动 kubectl patch storageclass storage-nfs -p '{&quot;metadata&quot;: {&quot;annotations&quot;:{&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;}}}' kubectl get sc 4 安装metrics-server 4.1 准备metrics-server.yaml文件（主节点操作） vim metrics-server.yaml 4.2 编写以下的内容 --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:aggregated-metrics-reader labels: rbac.authorization.k8s.io/aggregate-to-view: &quot;true&quot; rbac.authorization.k8s.io/aggregate-to-edit: &quot;true&quot; rbac.authorization.k8s.io/aggregate-to-admin: &quot;true&quot; rules: - apiGroups: [&quot;metrics.k8s.io&quot;] resources: [&quot;pods&quot;, &quot;nodes&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: metrics-server:system:auth-delegator roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegator subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: metrics-server-auth-reader namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: extension-apiserver-authentication-reader subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: apiregistration.k8s.io/v1 kind: APIService metadata: name: v1beta1.metrics.k8s.io spec: service: name: metrics-server namespace: kube-system group: metrics.k8s.io version: v1beta1 insecureSkipTLSVerify: true groupPriorityMinimum: 100 versionPriority: 100 --- apiVersion: v1 kind: ServiceAccount metadata: name: metrics-server namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: metrics-server namespace: kube-system labels: k8s-app: metrics-server spec: selector: matchLabels: k8s-app: metrics-server template: metadata: name: metrics-server labels: k8s-app: metrics-server spec: serviceAccountName: metrics-server volumes: # mount in tmp so we can safely use from-scratch images and/or read-only containers - name: tmp-dir emptyDir: {} containers: - name: metrics-server image: mirrorgooglecontainers/metrics-server-amd64:v0.3.6 imagePullPolicy: IfNotPresent args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-insecure-tls - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname ports: - name: main-port containerPort: 4443 protocol: TCP securityContext: readOnlyRootFilesystem: true runAsNonRoot: true runAsUser: 1000 volumeMounts: - name: tmp-dir mountPath: /tmp nodeSelector: kubernetes.io/os: linux kubernetes.io/arch: &quot;amd64&quot; --- apiVersion: v1 kind: Service metadata: name: metrics-server namespace: kube-system labels: kubernetes.io/name: &quot;Metrics-server&quot; kubernetes.io/cluster-service: &quot;true&quot; spec: selector: k8s-app: metrics-server ports: - port: 443 protocol: TCP targetPort: main-port --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:metrics-server rules: - apiGroups: - &quot;&quot; resources: - pods - nodes - nodes/stats - namespaces - configmaps verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:metrics-server roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:metrics-server subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system 4.3 应用该文件pod kubectl apply -f metrics-server.yaml 4.4 查看部署的应用信息状态 kubectl get pod -A 4.5 查看系统的监控状态 kubectl top nodes 如果运行kubectl top nodes这个命令，爆metrics not available yet 这个命令还没有用，那就稍等一会，就能用了 这里，kubesphere3.0的前置环境全部结束。 5 安装kubesphere v3.0.0 5.1 文档地址 https://kubesphere.com.cn/ 1.5.2 部署文档地址 https://kubesphere.com.cn/docs/quick-start/minimal-kubesphere-on-k8s/ 1.5.3 安装步骤说明（master节点） 1.5.3.1 安装集群配置文件 1.5.3.1.1 准备配置文件cluster-configuration.yaml vim cluster-configuration.yaml 1.5.3.1.2 编写以下的内容配置 --- apiVersion: installer.kubesphere.io/v1alpha1 kind: ClusterConfiguration metadata: name: ks-installer namespace: kubesphere-system labels: version: v3.2.1 spec: persistence: storageClass: &quot;&quot; # If there is no default StorageClass in your cluster, you need to specify an existing StorageClass here. authentication: jwtSecret: &quot;&quot; # Keep the jwtSecret consistent with the Host Cluster. Retrieve the jwtSecret by executing &quot;kubectl -n kubesphere-system get cm kubesphere-config -o yaml | grep -v &quot;apiVersion&quot; | grep jwtSecret&quot; on the Host Cluster. local_registry: &quot;&quot; # Add your private registry address if it is needed. # dev_tag: &quot;&quot; # Add your kubesphere image tag you want to install, by default it's same as ks-install release version. etcd: monitoring: false # Enable or disable etcd monitoring dashboard installation. You have to create a Secret for etcd before you enable it. endpointIps: localhost # etcd cluster EndpointIps. It can be a bunch of IPs here. port: 2379 # etcd port. tlsEnable: true common: core: console: enableMultiLogin: true # Enable or disable simultaneous logins. It allows different users to log in with the same account at the same time. port: 30880 type: NodePort # apiserver: # Enlarge the apiserver and controller manager's resource requests and limits for the large cluster # resources: {} # controllerManager: # resources: {} redis: enabled: false volumeSize: 2Gi # Redis PVC size. openldap: enabled: false volumeSize: 2Gi # openldap PVC size. minio: volumeSize: 20Gi # Minio PVC size. monitoring: # type: external # Whether to specify the external prometheus stack, and need to modify the endpoint at the next line. endpoint: http://prometheus-operated.kubesphere-monitoring-system.svc:9090 # Prometheus endpoint to get metrics data. GPUMonitoring: # Enable or disable the GPU-related metrics. If you enable this switch but have no GPU resources, Kubesphere will set it to zero. enabled: false gpu: # Install GPUKinds. The default GPU kind is nvidia.com/gpu. Other GPU kinds can be added here according to your needs. kinds: - resourceName: &quot;nvidia.com/gpu&quot; resourceType: &quot;GPU&quot; default: true es: # Storage backend for logging, events and auditing. # master: # volumeSize: 4Gi # The volume size of Elasticsearch master nodes. # replicas: 1 # The total number of master nodes. Even numbers are not allowed. # resources: {} # data: # volumeSize: 20Gi # The volume size of Elasticsearch data nodes. # replicas: 1 # The total number of data nodes. # resources: {} logMaxAge: 7 # Log retention time in built-in Elasticsearch. It is 7 days by default. elkPrefix: logstash # The string making up index names. The index name will be formatted as ks-&lt;elk_prefix&gt;-log. basicAuth: enabled: false username: &quot;&quot; password: &quot;&quot; externalElasticsearchUrl: &quot;&quot; externalElasticsearchPort: &quot;&quot; alerting: # (CPU: 0.1 Core, Memory: 100 MiB) It enables users to customize alerting policies to send messages to receivers in time with different time intervals and alerting levels to choose from. enabled: false # Enable or disable the KubeSphere Alerting System. # thanosruler: # replicas: 1 # resources: {} auditing: # Provide a security-relevant chronological set of records，recording the sequence of activities happening on the platform, initiated by different tenants. enabled: false # Enable or disable the KubeSphere Auditing Log System. # operator: # resources: {} # webhook: # resources: {} devops: # (CPU: 0.47 Core, Memory: 8.6 G) Provide an out-of-the-box CI/CD system based on Jenkins, and automated workflow tools including Source-to-Image &amp; Binary-to-Image. enabled: false # Enable or disable the KubeSphere DevOps System. # resources: {} jenkinsMemoryLim: 2Gi # Jenkins memory limit. jenkinsMemoryReq: 1500Mi # Jenkins memory request. jenkinsVolumeSize: 8Gi # Jenkins volume size. jenkinsJavaOpts_Xms: 512m # The following three fields are JVM parameters. jenkinsJavaOpts_Xmx: 512m jenkinsJavaOpts_MaxRAM: 2g events: # Provide a graphical web console for Kubernetes Events exporting, filtering and alerting in multi-tenant Kubernetes clusters. enabled: false # Enable or disable the KubeSphere Events System. # operator: # resources: {} # exporter: # resources: {} # ruler: # enabled: true # replicas: 2 # resources: {} logging: # (CPU: 57 m, Memory: 2.76 G) Flexible logging functions are provided for log query, collection and management in a unified console. Additional log collectors can be added, such as Elasticsearch, Kafka and Fluentd. enabled: false # Enable or disable the KubeSphere Logging System. containerruntime: docker logsidecar: enabled: true replicas: 2 # resources: {} metrics_server: # (CPU: 56 m, Memory: 44.35 MiB) It enables HPA (Horizontal Pod Autoscaler). enabled: false # Enable or disable metrics-server. monitoring: storageClass: &quot;&quot; # If there is an independent StorageClass you need for Prometheus, you can specify it here. The default StorageClass is used by default. # kube_rbac_proxy: # resources: {} # kube_state_metrics: # resources: {} # prometheus: # replicas: 1 # Prometheus replicas are responsible for monitoring different segments of data source and providing high availability. # volumeSize: 20Gi # Prometheus PVC size. # resources: {} # operator: # resources: {} # adapter: # resources: {} # node_exporter: # resources: {} # alertmanager: # replicas: 1 # AlertManager Replicas. # resources: {} # notification_manager: # resources: {} # operator: # resources: {} # proxy: # resources: {} gpu: # GPU monitoring-related plug-in installation. nvidia_dcgm_exporter: # Ensure that gpu resources on your hosts can be used normally, otherwise this plug-in will not work properly. enabled: false # Check whether the labels on the GPU hosts contain &quot;nvidia.com/gpu.present=true&quot; to ensure that the DCGM pod is scheduled to these nodes. # resources: {} multicluster: clusterRole: none # host | member | none # You can install a solo cluster, or specify it as the Host or Member Cluster. network: networkpolicy: # Network policies allow network isolation within the same cluster, which means firewalls can be set up between certain instances (Pods). # Make sure that the CNI network plugin used by the cluster supports NetworkPolicy. There are a number of CNI network plugins that support NetworkPolicy, including Calico, Cilium, Kube-router, Romana and Weave Net. enabled: false # Enable or disable network policies. ippool: # Use Pod IP Pools to manage the Pod network address space. Pods to be created can be assigned IP addresses from a Pod IP Pool. type: none # Specify &quot;calico&quot; for this field if Calico is used as your CNI plugin. &quot;none&quot; means that Pod IP Pools are disabled. topology: # Use Service Topology to view Service-to-Service communication based on Weave Scope. type: none # Specify &quot;weave-scope&quot; for this field to enable Service Topology. &quot;none&quot; means that Service Topology is disabled. openpitrix: # An App Store that is accessible to all platform tenants. You can use it to manage apps across their entire lifecycle. store: enabled: false # Enable or disable the KubeSphere App Store. servicemesh: # (0.3 Core, 300 MiB) Provide fine-grained traffic management, observability and tracing, and visualized traffic topology. enabled: false # Base component (pilot). Enable or disable KubeSphere Service Mesh (Istio-based). kubeedge: # Add edge nodes to your cluster and deploy workloads on edge nodes. enabled: false # Enable or disable KubeEdge. cloudCore: nodeSelector: {&quot;node-role.kubernetes.io/worker&quot;: &quot;&quot;} tolerations: [] cloudhubPort: &quot;10000&quot; cloudhubQuicPort: &quot;10001&quot; cloudhubHttpsPort: &quot;10002&quot; cloudstreamPort: &quot;10003&quot; tunnelPort: &quot;10004&quot; cloudHub: advertiseAddress: # At least a public IP address or an IP address which can be accessed by edge nodes must be provided. - &quot;&quot; # Note that once KubeEdge is enabled, CloudCore will malfunction if the address is not provided. nodeLimit: &quot;100&quot; service: cloudhubNodePort: &quot;30000&quot; cloudhubQuicNodePort: &quot;30001&quot; cloudhubHttpsNodePort: &quot;30002&quot; cloudstreamNodePort: &quot;30003&quot; tunnelNodePort: &quot;30004&quot; edgeWatcher: nodeSelector: {&quot;node-role.kubernetes.io/worker&quot;: &quot;&quot;} tolerations: [] edgeWatcherAgent: nodeSelector: {&quot;node-role.kubernetes.io/worker&quot;: &quot;&quot;} tolerations: [] endpointIps: 192.168.40.131：master节点的地址。 1.5.3.1.3 准备配置文件kubesphere-installer.yaml vim kubesphere-installer.yaml 1.5.3.1.4 编写以下的内容配置 --- apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: clusterconfigurations.installer.kubesphere.io spec: group: installer.kubesphere.io versions: - name: v1alpha1 served: true storage: true schema: openAPIV3Schema: type: object properties: spec: type: object x-kubernetes-preserve-unknown-fields: true status: type: object x-kubernetes-preserve-unknown-fields: true scope: Namespaced names: plural: clusterconfigurations singular: clusterconfiguration kind: ClusterConfiguration shortNames: - cc --- apiVersion: v1 kind: Namespace metadata: name: kubesphere-system --- apiVersion: v1 kind: ServiceAccount metadata: name: ks-installer namespace: kubesphere-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: ks-installer rules: - apiGroups: - &quot;&quot; resources: - '*' verbs: - '*' - apiGroups: - apps resources: - '*' verbs: - '*' - apiGroups: - extensions resources: - '*' verbs: - '*' - apiGroups: - batch resources: - '*' verbs: - '*' - apiGroups: - rbac.authorization.k8s.io resources: - '*' verbs: - '*' - apiGroups: - apiregistration.k8s.io resources: - '*' verbs: - '*' - apiGroups: - apiextensions.k8s.io resources: - '*' verbs: - '*' - apiGroups: - tenant.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - certificates.k8s.io resources: - '*' verbs: - '*' - apiGroups: - devops.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - monitoring.coreos.com resources: - '*' verbs: - '*' - apiGroups: - logging.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - jaegertracing.io resources: - '*' verbs: - '*' - apiGroups: - storage.k8s.io resources: - '*' verbs: - '*' - apiGroups: - admissionregistration.k8s.io resources: - '*' verbs: - '*' - apiGroups: - policy resources: - '*' verbs: - '*' - apiGroups: - autoscaling resources: - '*' verbs: - '*' - apiGroups: - networking.istio.io resources: - '*' verbs: - '*' - apiGroups: - config.istio.io resources: - '*' verbs: - '*' - apiGroups: - iam.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - notification.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - auditing.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - events.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - core.kubefed.io resources: - '*' verbs: - '*' - apiGroups: - installer.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - storage.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - security.istio.io resources: - '*' verbs: - '*' - apiGroups: - monitoring.kiali.io resources: - '*' verbs: - '*' - apiGroups: - kiali.io resources: - '*' verbs: - '*' - apiGroups: - networking.k8s.io resources: - '*' verbs: - '*' - apiGroups: - kubeedge.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - types.kubefed.io resources: - '*' verbs: - '*' - apiGroups: - monitoring.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - application.kubesphere.io resources: - '*' verbs: - '*' --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: ks-installer subjects: - kind: ServiceAccount name: ks-installer namespace: kubesphere-system roleRef: kind: ClusterRole name: ks-installer apiGroup: rbac.authorization.k8s.io --- apiVersion: apps/v1 kind: Deployment metadata: name: ks-installer namespace: kubesphere-system labels: app: ks-install spec: replicas: 1 selector: matchLabels: app: ks-install template: metadata: labels: app: ks-install spec: serviceAccountName: ks-installer containers: - name: installer image: kubesphere/ks-installer:v3.2.1 imagePullPolicy: &quot;Always&quot; resources: limits: cpu: &quot;1&quot; memory: 1Gi requests: cpu: 20m memory: 100Mi volumeMounts: - mountPath: /etc/localtime name: host-time readOnly: true volumes: - hostPath: path: /etc/localtime type: &quot;&quot; name: host-time 1.5.3.1.5 分别执行两个文件 kubectl apply -f kubesphere-installer.yaml kubectl apply -f cluster-configuration.yaml #或者直接用网络文件安装 kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.2.1/kubesphere-installer.yaml kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.2.1/cluster-configuration.yaml 1.5.3.1.6 监控安装的日志信息 kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath='{.items[0].metadata.name}') -f 1.5.3.1.7 查看pod启动状态信息 kubectl get pods -A 需要等待漫长的时间。 1.5.4 访问验证是否安装成功 访问地址： http://192.168.40.131:30880/login 帐号：admin 密码：P@88w0rd 1.5.5 harbor目录下重新启动harbor docker-compose stop docker-compose up -d ","link":"https://tinaxiawuhao.github.io/post/ULtZ5rQXA/"},{"title":"harbor自建镜像仓库","content":"centos网络配置 1.设置主机名 [root@localhost ~]# hostnamectl set-hostname harbor 2.添加 Host 解析 [root@harbor ~]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.40.131 k8s-master 192.168.40.132 k8s-node1 192.168.40.133 k8s-node2 192.168.40.150 hub.test.com k8s 集群每个节点添加解析（注意：K8s 每个节点，不是 Harbor） [root@k8s-master ~]# echo &quot;192.168.40.150 hub.test.com&quot; &gt;&gt; /etc/hosts [root@k8s-node1 ~]# echo &quot;192.168.40.150 hub.test.com&quot; &gt;&gt; /etc/hosts [root@k8s-node2 ~]# echo &quot;192.168.40.150 hub.test.com&quot; &gt;&gt; /etc/hosts 3.网络环境设置 vi /etc/sysconfig/network-scripts/ifcfg-ens33 内容替换如下： BOOTPROTO=static #静态连接 ONBOOT=yes #网络设备开机启动 IPADDR=192.168.40.150 NETMASK=255.255.255.0 #子网掩码 GATEWAY=192.168.40.2 #网关 DNS1=114.114.114.114 #DNS解析 网络服务重启 service network restart 安装环境 1.安装Docker-CE # 卸载旧版本 yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine # 安装所需的软件包 yum install -y yum-utils device-mapper-persistent-data lvm2 # 添加docker存储库 yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo #安装最新版的docker-ce yum install -y docker-ce docker-ce-cli containerd.io #启动docker并设置为开机自启动 systemctl enable --now docker 2.配置Docker镜像加速器 tee /etc/docker/daemon.json &lt;&lt;-'EOF' { &quot;registry-mirrors&quot;: [&quot;https://hub-mirror.c.163.com/&quot;], &quot;insecure-registries&quot;: [&quot;https://hub.test.com&quot;] } EOF # 重启 systemctl daemon-reload &amp;&amp; systemctl restart docker 3.安装docker-compose #1、安装pip yum -y install epel-release yum install python3-pip pip3 install --upgrade pip #2、安装docker-compose pip3 install docker-compose #3、查看版本 docker-compose version 4.创建 https 证书 安装 openssl [root@harbor]# yum install openssl -y 创建证书目录，并赋予权限 [root@harbor ~]# mkdir -p /cert/harbor [root@harbor ~]# chmod -R 777 /cert/harbor [root@harbor ~]# cd /cert/harbor 创建服务器证书密钥文件 harbor.key [root@harbor harbor]# openssl genrsa -des3 -out harbor.key 2048 输入密码，确认密码，自己随便定义，但是要记住，后面会用到。 创建服务器证书的申请文件 harbor.csr [root@harbor harbor]# openssl req -new -key harbor.key -out harbor.csr 输入密钥文件的密码, 然后一路回车。 备份一份服务器密钥文件 [root@harbor harbor]# cp harbor.key harbor.key.org 去除文件口令 [root@harbor harbor]# openssl rsa -in harbor.key.org -out harbor.key 输入密钥文件的密码 创建一个自当前日期起为期十年的证书 harbor.crt [root@harbor harbor]# openssl x509 -req -days 3650 -in harbor.csr -signkey harbor.key -out harbor.crt 5.下载Harbor安装包并解压 链接：https://pan.baidu.com/s/1AUEw0Qw3w9lZP-rnAYaWjA 提取码：qutg wget https://github.com/goharbor/harbor/releases/download/v2.5.0/harbor-offline-installer-v2.5.0.tgz tar zxvf harbor-offline-installer-v2.5.0.tgz 6.配置harbor.cfg和安装Harbor vi harbor.yml 将http端口改成10086，因为默认用的80端口已经被占用，http可以指定任意端口； 接下来运行install.sh安装和启动harbor ./prepare ./install.sh 7.测试 Harbor [root@k8s-master01 ~]# docker login https://hub.test.com Username: admin Password: Harbor12345 # 默认密码，可通过 harbor.yml 配置文件修改 登录时报：Error response from daemon: Get https://hub.test.com/v2/: x509: certificate is not valid for any names, but wanted to match hub.test.com 解决：修改客户端（即需要登陆harbor的机器）的docker.service 文件 vi /lib/systemd/system/docker.service 添加 --insecure-registry hub.test.com 重新加载服务配置文件，并且重启docker服务 systemctl daemon-reload &amp;&amp; systemctl restart docker 下载镜像推送到 Harbor [root@k8s-node01 ~]# docker pull nginx [root@k8s-node01 ~]# docker tag nginx:latest hub.test.com/library/test:v1 [root@k8s-node01 ~]# docker push hub.test.com/library/test:v1 8.Windows 访问 Harbor Web界面 Windows 添加 hosts 解析路径 C:\\Windows\\System32\\drivers\\etc\\hosts 添加信息 192.168.40.150 hub.test.com 浏览器访问测试 https://hub.test.com 用户密码：admin / Harbor12345 ","link":"https://tinaxiawuhao.github.io/post/WOO39eqh2/"},{"title":"k8s1.24.1安装（基于Centos 7）","content":"注意：除了Master节点初始化及Node节点添加分别在Master节点和Node节点执行外，其余所有命令均在所有节点执行 整体环境 一台master节点，2台node节点。采用了Centos 7，有网络，互相可以ping通。 1.内核升级（可忽略） 为避免出现不可预知的问题，提升centos 7内核到最新版本 联网升级内核 1. 查看内核版本 uname -r 2. 导入ELRepo软件仓库的公共秘钥 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org 3. 安装ELRepo软件仓库的yum源 rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm 4. 启用 elrepo 软件源并下载安装最新稳定版内核 yum --enablerepo=elrepo-kernel install kernel-ml -y 5. 查看系统可用内核，并设置内核启动顺序 sudo awk -F\\' '$1==&quot;menuentry &quot; {print i++ &quot; : &quot; $2}' /etc/grub2.cfg 6. 生成 grub 配置文件 机器上存在多个内核，我们要使用最新版本，可以通过 grub2-set-default 0 命令生成 grub 配置文件 grub2-set-default 0 #初始化页面的第一个内核将作为默认内核 grub2-mkconfig -o /boot/grub2/grub.cfg #重新创建内核配置 7. 重启系统并验证 yum update reboot uname -r 8. 删除旧内核 yum -y remove kernel kernel-tools 2.centos网络配置文件 网络配置文件名可能会有不同，在输入到ifcfg时，可以连续按两下tab键，获取提示，比如我的机器 为 ifcfg-ens33 vi /etc/sysconfig/network-scripts/ifcfg-ens33 1.内容替换如下： BOOTPROTO=static #静态连接 ONBOOT=yes #网络设备开机启动 IPADDR=192.168.40.131 #192.168.40.132,192.168.40.133. NETMASK=255.255.255.0 #子网掩码 GATEWAY=192.168.40.2 #网关 DNS1=114.114.114.114 #DNS解析 2.网络服务重启 service network restart 3.查看IP地址 3.安装依赖包 yum install -y wget 4.修改yum源（视网络情况操作） 1.备份 cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup 2.下载 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 或者使用清华大学站 sudo sed -e 's|^mirrorlist=|#mirrorlist=|g' -e 's|^#baseurl=http://mirror.centos.org/$contentdir|baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos|g' -i.bak /etc/yum.repos.d/CentOS-Base.repo 3.清除生成缓存 yum clean all # 清除系统所有的yum缓存 yum makecache # 生成yum缓存 5.修改主机名 hostnamectl set-hostname k8s-master hostnamectl set-hostname k8s-node1 hostnamectl set-hostname k8s-node2 查看主机名称 6.添加hosts解析 echo -e &quot;192.168.40.131 k8s-master\\n192.168.40.132 k8s-node1\\n192.168.40.133 k8s-node2&quot; &gt;&gt; /etc/hosts 7.关闭防火墙firewalld systemctl stop firewalld &amp;&amp; systemctl disable firewalld 8.关闭selinux setenforce 0 &amp;&amp; sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config 9.关闭swap分区交换 swapoff -a &amp;&amp; sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab 10.配置内核参数 将桥接的IPv4流量传递倒iptables的链 1.设置内核参数 cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt;EOF net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 net.ipv4.ip_forward=1 net.ipv4.tcp_tw_recycle=0 vm.swappiness=0 # 禁止使用swap空间，只有当系统00M时才允许使用它 vm.overcommit_memory=1 # 不检查物理内存是否够用 vm.panic_on_oom=0 # 开启oom fs.inotify.max_user_instances=8192 fs.inotify.max_user_watches=1048576 fs.file-max=52786963 fs.nr_open=52706963 net.ipv6.conf.all.disable_ipv6=1 net.netfilter.nf_conntrack_max=2310720 EOF 2.加载内核模块 modprobe br_netfilter &amp;&amp; echo &quot;modprobe br_netfilter&quot; &gt;&gt; /etc/rc.local 3.使内核参数生效 sysctl -p /etc/sysctl.d/k8s.conf 11.时间同步 timedatectl set-timezone Asia/Shanghai &amp;&amp; timedatectl set-local-rtc 0 #重启依赖于系统时间的服务 systemctl restart rsyslog &amp;&amp; systemctl restart crond 12.安装iptables，设置空表 yum -y install iptables-services &amp;&amp; systemctl start iptables &amp;&amp; systemctl enable iptables &amp;&amp; iptables -F &amp;&amp; service iptables save 检查服务的的规则：iptables -L -n 13.开启IPVS 由于ipvs已经加入到了内核的主干，所以为kube-proxy开启ipvs的前提需要加载以下的内核模块 cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF #！/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOF 授权启动 chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod 接下来还需要确保各个节点上已经安装了ipset软件包。 为了便于查看ipvs的代理规则，最好安装一下管理工具ipvsadm。 yum install ipset ipvsadm -y service底层实现主要由两个网络模式组成：iptables与IPVS。他们都是有kube-proxy维护 Iptables VS IPVS Iptables： • 灵活，功能强大 • 规则遍历匹配和更新，呈线性时延 IPVS： • 工作在内核态，有更好的性能 • 调度算法丰富：rr，wrr，lc，wlc，ip hash 等集群部署成功，mode由空值修改成ipvs模式 kubectl edit configmap kube-proxy -n kube-system configmap/kube-proxy edited 删除所有kube-proxy的pod,等待重启 kubectl delete pod/kube-proxy-84p9n -n kube-system # 查看ipvs相关规则 ipvsadm 14.安装docker软件 自1.20版本被弃用之后，dockershim组件终于在1.24的kubelet中被删除。从1.24开始，大家需要使用其他受到支持的运行时选项（例如containerd或CRI-O）；如果您选择Docker Engine作为运行时，则需要使用cri-dockerd。 1.删除自带的docker yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine 2.安装依赖包 yum install -y yum-utils device-mapper-persistent-data lvm2 3.安装yum源 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 4.安装docker-ce yum -y install docker-ce 5.设置docker cat &gt; /etc/docker/daemon.conf &lt;&lt;EOF { &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;,&quot;https://heusyzko.mirror.aliyuncs.com&quot;], &quot;insecure-registries&quot;: [&quot;https://hub.test.com&quot;], &quot;exec-opts&quot;:[&quot;native.cgroupdriver=systemd&quot;], &quot;log-driver&quot;:&quot;json-file&quot;, &quot;log-opts&quot;:{ &quot;max-size&quot;: &quot;100m&quot; }, &quot;storage-driver&quot;: &quot;overlay2&quot; } EOF mkdir -p /etc/systemd/system/docker.service.d 6.重启docker服务 systemctl daemon-reload &amp;&amp; systemctl start docker &amp;&amp; systemctl enable docker systemctl daemon-reload &amp;&amp; systemctl restart docker 以下的NO_PROXY表示对这些网段的服务器不使用代理，如果不需要用到代理服务器，以下的配置可以不写，注意，以下的代理是不通的。不建议使用代理，因为国内有资源可以访问到gcr.io需要的镜像，下文会介绍 #放在Type=notify下面 vi /usr/lib/systemd/system/docker.service Environment=&quot;HTTPS_PROXY=http://www.ik8s.io:10080&quot; Environment=&quot;HTTP_PROXY=http://www.ik8s.io:10080&quot; Environment=&quot;NO_PROXY=127.0.0.0/8,172.20.0.0/16&quot; #保存退出后，执行 systemctl daemon-reload #确保如下两个参数值为1，默认为1。 cat /proc/sys/net/bridge/bridge-nf-call-ip6tables cat /proc/sys/net/bridge/bridge-nf-call-iptables #启动docker-ce systemctl restart docker #设置开机启动 systemctl enable docker.service #想要删除容器，则要先停止所有容器（当然，也可以加-f强制删除，但是不推荐）： docker stop $(docker ps -a -q) #删除所有容器 docker rm $(docker ps -a -q) #.删除所有镜像（慎重） docker rmi $(docker images -q) 14.2.cri-dockerd安装 CRI-Dockerd 其实就是从被移除的 Docker Shim 中，独立出来的一个项目，用于解决历史遗留的节点升级 Kubernetes 的问题。 kubelet并没有直接和dockerd交互，而是通过了一个dockershim的组件间接操作dockerd。dockershim提供了一个标准的接口，让kubelet能够专注于容器调度逻辑本身，而不用去适配dockerd的接口变动。而其他实现了相同标准接口的容器技术也可以被kubelet集成使用，这个接口称作CRI。dockershim和CRI的出现也是容器生态系统演化的历史产物。在k8s最早期的版本中是不存在dockershim的，kubelet直接和dockerd交互。但为了支持更多不同的容器技术（避免完全被docker控制容器技术市场），kubelet在之后的版本开始支持另一种容器技术rkt。这给kubelet的维护工作造成了巨大的挑战，因为两种容器技术没有统一的接口和使用逻辑，kubelet同时支持两种技术的使用还要保证一致的容器功能表现，对代码逻辑和功能可靠性都有很大的影响。为了解决这个问题，k8s提出了一个统一接口CRI，kubelet统一通过这个接口来调用容器功能。但是dockerd并不支持CRI，k8s就自己实现了配套的dockershim将CRI接口调用转换成dockerd接口调用来支持CRI。因此，dockershim并不是docker技术的一部分，而是k8s系统的一部分 使用 CRI-Dockerd 项目 项目地址：https://github.com/Mirantis/cri-dockerd 1.下载cri-dockerd二进制包或者源码自己编译 # 下载文件 wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.2.0/cri-dockerd-v0.2.0-linux-amd64.tar.gz # 解压文件 tar -xvf cri-dockerd-v0.2.0-linux-amd64.tar.gz # 复制二进制文件到指定目录 cp cri-dockerd /usr/bin/ 2.配置启动文件 # 配置启动文件 cat &lt;&lt;&quot;EOF&quot; &gt; /usr/lib/systemd/system/cri-docker.service [Unit] Description=CRI Interface for Docker Application Container Engine Documentation=https://docs.mirantis.com After=network-online.target firewalld.service docker.service Wants=network-online.target Requires=cri-docker.socket [Service] Type=notify ExecStart=/usr/bin/cri-dockerd --container-runtime-endpoint=unix:///var/run/cri-docker.sock --network-plugin=cni --cni-bin-dir=/opt/cni/bin \\ --cni-conf-dir=/etc/cni/net.d --image-pull-progress-deadline=30s --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.7 \\ --docker-endpoint=unix:///var/run/docker.sock --cri-dockerd-root-directory=/var/lib/docker ExecReload=/bin/kill -s HUP $MAINPID TimeoutSec=0 RestartSec=2 Restart=always # Note that StartLimit* options were moved from &quot;Service&quot; to &quot;Unit&quot; in systemd 229. # Both the old, and new location are accepted by systemd 229 and up, so using the old location # to make them work for either version of systemd. StartLimitBurst=3 # Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230. # Both the old, and new name are accepted by systemd 230 and up, so using the old name to make # this option work for either version of systemd. StartLimitInterval=60s # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity # Comment TasksMax if your systemd version does not support it. # Only systemd 226 and above support this option. TasksMax=infinity Delegate=yes KillMode=process [Install] WantedBy=multi-user.target EOF # 生成socket 文件 cat &lt;&lt;&quot;EOF&quot; &gt; /usr/lib/systemd/system/cri-docker.socket [Unit] Description=CRI Docker Socket for the API PartOf=cri-docker.service [Socket] ListenStream=/var/run/cri-dockerd.sock SocketMode=0660 SocketUser=root SocketGroup=docker [Install] WantedBy=sockets.target EOF # 启动 cri-dockerd systemctl daemon-reload systemctl start cri-docker #设置开机启动 systemctl enable cri-docker # 查看启动状态 systemctl status cri-docker 3.下载cri-tools验证cri-docker 是否正常 # 下载二进制文件 wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.24.0/crictl-v1.24.0-linux-amd64.tar.gz # 解压 tar -xvf crictl-v1.24.0-linux-amd64.tar.gz # 复制二进制文件到指定目录 cp crictl /usr/bin/ # 创建配置文件 vim /etc/crictl.yaml runtime-endpoint: &quot;unix:///var/run/cri-docker.sock&quot; image-endpoint: &quot;unix:///var/run/cri-docker.sock&quot; timeout: 10 debug: false pull-image-on-create: true disable-pull-on-run: false # 测试能否访问docker # 查看运行的容器 crictl ps # 查看拉取的镜像 crictl images # 拉取镜像 crictl pull busybox [root@k8s-node-4 ~]# crictl pull busybox Image is up to date for busybox@sha256:5ecba83a746c7608ed511dc1533b87c737a0b0fb730301639a0179f9344b13448 返回一切正常cri-dockerd接入docker完整 15.部署 containerd(k8s-1.24版本以上) 服务版本 服务名称 版本号 内核 5.14.3-1.el7.elrepo.x86_64 containerd v1.6.4（加入） ctr v1.6.4 k8s 1.24 1.安装containerd 创建配置文件 mkdir /etc/modules-load.d/containerd.conf 创建完配置文件执行以下命令 modprobe overlay &amp;&amp; modprobe br_netfilter 立即生效 sysctl --system 下载 docker-ce 源 wget http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 或者 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 安装 containerd 服务并加入开机启动 yum install -y containerd.io systemctl enable containerd &amp;&amp; systemctl start containerd 2.配置 containerd 创建路径 mkdir -p /etc/containerd 获取默认配置文件 containerd config default | sudo tee /etc/containerd/config.toml 修改配置文件，新增 &quot;SystemdCgroup = true&quot;，使用 systemd 作为 cgroup 驱动程序 [root@master1 ~]# vi /etc/containerd/config.toml #修改以下内容 [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options] SystemdCgroup = true ## 修改为true 替换默认pause镜像地址 默认情况下k8s.gcr.io无法访问，所以使用阿里云镜像仓库地址即可 # 所有节点更换默认镜像地址 sed -i 's/k8s.gcr.io/registry.cn-beijing.aliyuncs.com\\/abcdocker/' /etc/containerd/config.toml 重启 containerd systemctl restart containerd 查看 containerd 运行状态(以下状态视为正常) [root@master1 ~]# systemctl status containerd ● containerd.service - containerd container runtime Loaded: loaded (/usr/lib/systemd/system/containerd.service; enabled; vendor preset: disabled) Active: active (running) since Sun 2022-03-06 08:09:00 CST; 1h 43min ago Docs: https://containerd.io Process: 931 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS) Main PID: 941 (containerd) Tasks: 11 Memory: 61.4M CGroup: /system.slice/containerd.service └─941 /usr/bin/containerd 16.安装kubeadm、kubelet、kubectl 1.配置文件修改 cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 2.安装启用 sudo yum install -y kubelet-1.24.1 kubeadm-1.24.1 kubectl-1.24.1 --disableexcludes=kubernetes sudo systemctl enable kubelet &amp;&amp; systemctl start kubelet 3.修改kubelet的配置文件 先查看配置文件位置 systemctl status kubelet vi /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf 并添加以下内容(使用和docker相同的cgroup-driver)。 Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --cgroup-driver=systemd&quot; 4.重启kubelet systemctl daemon-reload &amp;&amp; systemctl restart kubelet 17.获取K8S镜像（可忽略） 1.获取镜像列表 使用阿里云镜像仓库下载（国内环境该命令可不执行，下步骤kubeadm init --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers已经默认为国内环境） 由于官方镜像地址被墙，所以我们需要首先获取所需镜像以及它们的版本。然后从国内镜像站获取。 kubeadm config images list 获取镜像列表后可以通过下面的脚本从阿里云获取： vi /usr/local/k8s/k8s-images.sh 下面的镜像应该去除&quot;k8s.gcr.io/&quot;的前缀，版本换成上面获取到的版本 images=( kube-apiserver:v1.24.1 kube-controller-manager:v1.24.1 kube-scheduler:v1.24.1 kube-proxy:v1.24.1 pause:3.7 etcd:3.5.3-0 coredns:v1.8.6 ) for imageName in ${images[@]} ; do docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName done 2.赋权执行 chmod +x k8s-images.sh &amp;&amp; ./k8s-images.sh 以上操作在所有机器执行 18.初始化环境（master操作） 1.安装镜像 采用模板配置文件加载 kubeadm config print init-defaults &gt; kubeadm-config.yaml [root@master1 ~]# cat kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta2 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 192.168.40.131 # 本机IP bindPort: 6443 nodeRegistration: criSocket: unix:///var/run/cri-docker.sock # 此处千万不要忘记修改，如果不修改等于没有替换。(此处已经更改完了) #criSocket: unix:///run/containerd/containerd.sock # 此处千万不要忘记修改，如果不修改等于没有替换。(此处已经更改完了) name: master1 # 本主机名 taints: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta2 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controlPlaneEndpoint: &quot;192.168.40.151:16443&quot; # 虚拟IP和haproxy端口 controllerManager: {} dns: type: CoreDNS etcd: local: dataDir: /var/lib/etcd imageRepository: registry.aliyuncs.com/google_containers # 镜像仓库源要根据自己实际情况修改 kind: ClusterConfiguration kubernetesVersion: v1.24.1 # k8s版本 networking: dnsDomain: cluster.local podSubnet: &quot;10.244.0.0/16&quot; #设置网段，和下面网络插件对应 serviceSubnet: 10.96.0.0/12 scheduler: {} --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration featureGates: SupportIPVSProxyMode: true mode: ipvs 2.查看kubeadm版本，修改命令参数 kubeadm version 这个就很简单了，只需要简单的一个命令： #直接使用已经下载好的镜像 kubeadm init --kubernetes-version=v1.24.1 --apiserver-advertise-address=192.168.40.131 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap --cri-socket unix:///var/run/cri-docker.sock | tee kubeadm-init.log #或者采用aliyuncs镜像下载 kubeadm init --kubernetes-version=v1.24.1 --apiserver-advertise-address=192.168.40.131 --image-repository registry.aliyuncs.com/google_containers --service-cidr=10.1.0.0/16 --pod-network-cidr=10.244.0.0/16 --cri-socket unix:///var/run/cri-docker.sock| tee kubeadm-init.log #使用上面系统生成配置文件加载 kubeadm init --config kubeadm-config.yaml 3.初始化命令说明： 指明用 Master 的哪个 interface 与 Cluster 的其他节点通信。如果 Master 有多个 interface，建议明确指定，如果不指定，kubeadm 会自动选择有默认网关的 interface。 --apiserver-advertise-address 指定 Pod 网络的范围。Kubernetes 支持多种网络方案，而且不同网络方案对 --pod-network-cidr 有自己的要求，这里设置为 10.244.0.0/16 是因为我们将使用 flannel 网络方案，必须设置成这个 CIDR。 --pod-network-cidr Kubenetes默认Registries地址是 k8s.gcr.io，在国内并不能访问 gcr.io，在1.19.3版本中我们可以增加–image-repository参数，默认值是 k8s.gcr.io，将其指定为阿里云镜像地址：registry.aliyuncs.com/google_containers。 --image-repository 关闭版本探测，因为它的默认值是stable-1，会导致从https://dl.k8s.io/release/stable-1.txt下载最新的版本号，我们可以将其指定为固定版本（最新版：v1.24.1）来跳过网络请求。 --kubernetes-version=v1.24.1 指定启动时使用cri-docker调用docker --cri-socket unix:///var/run/cri-docker.sock 4.错误启动重置 # 重置 如果有需要 kubeadm reset --cri-socket unix:///var/run/cri-docker.sock 5.初始化成功后，为顺利使用kubectl，执行以下命令： mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 6.添加节点 kubeadm join 192.168.40.131:6443 --token eyr8v6.j84sxak8aptse8j9 --discovery-token-ca-cert-hash sha256:c082f3c546bdbac02d0d0a3b696de4004b0d449e37838fa38d4752b39682676b --cri-socket unix:///var/run/cri-docker.sock 7.执行kubectl get nodes，查看master节点状态： kubectl get node 8.通过如下命令查看kubelet状态： journalctl -xef -u kubelet -n 20 提示未安装cni 网络插件。 19.1安装flannel网络插件(CNI) master执行以下命令安装flannel即可： kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml kube-flannel.yaml文件中的net-conf.json-&gt;Network地址默认为命令中–pod-network-cidr=值相同 输入命令kubectl get pods -n kube-system,等待所有插件为running状态。 待所有pod status为Running的时候，再次执行kubectl get nodes： [root@k8s-master ~]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master Ready master 16m v1.19.3 如上所示，master状态变为，表明Master节点部署成功！ 19.2安装calico网络(功能更完善) 1.在master上下载配置calico网络的yaml。 kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml 2.提前下载所需要的镜像。 # 查看此文件用哪些镜像： [root@k8s-master ~]# grep image calico.yaml image: docker.io/calico/cni:v3.23.1 image: docker.io/calico/node:v3.23.1 image: docker.io/calico/kube-controllers:v3.23.1 3.安装calico网络。 在master上执行如下命令： kubectl apply -f calico.yaml 5.验证结果。 再次在master上运行命令 kubectl get nodes查看运行结果： [root@k8s-master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION master01 Ready control-plane,master 21h v1.23.4 worker01 Ready &lt;none&gt; 16h v1.23.4 worker02 Ready &lt;none&gt; 16h v1.23.4 20.部署k8s-node1、k8s-node2集群 1、在k8s-node1、k8s-node2等两台虚拟机中重复执行上面的步骤，安装好docker、kubelet、kubectl、kubeadm。 1.node节点加入集群 在上面第初始化master节点成功后，输出了下面的kubeadm join命令： kubeadm join 192.168.40.131:6443 --token zj0u08.ge77y7uv76flqgdk --discovery-token-ca-cert-hash sha256:7cd23cec6afb192b2d34c5c719b378082a6315a9d91a22d91b83066c870d4db5 --cri-socket unix:///var/run/cri-docker.sock 该命令就是node加入集群的命令，分别在k8s-node1、k8s-node2上执行该命令加入集群。 如果忘记该命令，可以通过以下命令重新生成： kubeadm token create --print-join-command 2.在master节点执行下面命令查看集群状态： kubectl get nodes [root@k8s-master ~]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master Ready master 24m v1.19.3 k8s-node1 Ready &lt;none&gt; 5m50s v1.19.3 k8s-node2 Ready &lt;none&gt; 5m21s v1.19.3 如上所示，所有节点都为ready，集群搭建成功。 21.安装ingress-nginx vi ingress-nginx-deploy.yaml apiVersion: v1 kind: Namespace metadata: labels: app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx name: ingress-nginx --- apiVersion: v1 automountServiceAccountToken: true kind: ServiceAccount metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx namespace: ingress-nginx --- apiVersion: v1 kind: ServiceAccount metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-admission namespace: ingress-nginx --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx namespace: ingress-nginx rules: - apiGroups: - &quot;&quot; resources: - namespaces verbs: - get - apiGroups: - &quot;&quot; resources: - configmaps - pods - secrets - endpoints verbs: - get - list - watch - apiGroups: - &quot;&quot; resources: - services verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses/status verbs: - update - apiGroups: - networking.k8s.io resources: - ingressclasses verbs: - get - list - watch - apiGroups: - &quot;&quot; resourceNames: - ingress-controller-leader resources: - configmaps verbs: - get - update - apiGroups: - &quot;&quot; resources: - configmaps verbs: - create - apiGroups: - &quot;&quot; resources: - events verbs: - create - patch --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-admission namespace: ingress-nginx rules: - apiGroups: - &quot;&quot; resources: - secrets verbs: - get - create --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx rules: - apiGroups: - &quot;&quot; resources: - configmaps - endpoints - nodes - pods - secrets - namespaces verbs: - list - watch - apiGroups: - &quot;&quot; resources: - nodes verbs: - get - apiGroups: - &quot;&quot; resources: - services verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses verbs: - get - list - watch - apiGroups: - &quot;&quot; resources: - events verbs: - create - patch - apiGroups: - networking.k8s.io resources: - ingresses/status verbs: - update - apiGroups: - networking.k8s.io resources: - ingressclasses verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-admission rules: - apiGroups: - admissionregistration.k8s.io resources: - validatingwebhookconfigurations verbs: - get - update --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx namespace: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ingress-nginx subjects: - kind: ServiceAccount name: ingress-nginx namespace: ingress-nginx --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-admission namespace: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ingress-nginx-admission subjects: - kind: ServiceAccount name: ingress-nginx-admission namespace: ingress-nginx --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ingress-nginx subjects: - kind: ServiceAccount name: ingress-nginx namespace: ingress-nginx --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-admission roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ingress-nginx-admission subjects: - kind: ServiceAccount name: ingress-nginx-admission namespace: ingress-nginx --- apiVersion: v1 data: allow-snippet-annotations: &quot;true&quot; kind: ConfigMap metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-controller namespace: ingress-nginx --- apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-controller namespace: ingress-nginx spec: ports: - appProtocol: http name: http port: 80 protocol: TCP targetPort: http - appProtocol: https name: https port: 443 protocol: TCP targetPort: https selector: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx type: NodePort --- apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-controller-admission namespace: ingress-nginx spec: ports: - appProtocol: https name: https-webhook port: 443 targetPort: webhook selector: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx type: ClusterIP --- apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-controller namespace: ingress-nginx spec: minReadySeconds: 0 revisionHistoryLimit: 10 selector: matchLabels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx template: metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx spec: containers: - args: - /nginx-ingress-controller - --election-id=ingress-controller-leader - --controller-class=k8s.io/ingress-nginx - --ingress-class=nginx - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller - --validating-webhook=:8443 - --validating-webhook-certificate=/usr/local/certificates/cert - --validating-webhook-key=/usr/local/certificates/key env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: LD_PRELOAD value: /usr/local/lib/libmimalloc.so image: registry.aliyuncs.com/google_containers/nginx-ingress-controller:v1.2.0 imagePullPolicy: IfNotPresent lifecycle: preStop: exec: command: - /wait-shutdown livenessProbe: failureThreshold: 5 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 name: controller ports: - containerPort: 80 name: http protocol: TCP - containerPort: 443 name: https protocol: TCP - containerPort: 8443 name: webhook protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 resources: requests: cpu: 100m memory: 90Mi securityContext: allowPrivilegeEscalation: true capabilities: add: - NET_BIND_SERVICE drop: - ALL runAsUser: 101 volumeMounts: - mountPath: /usr/local/certificates/ name: webhook-cert readOnly: true dnsPolicy: ClusterFirst nodeSelector: kubernetes.io/os: linux serviceAccountName: ingress-nginx terminationGracePeriodSeconds: 300 volumes: - name: webhook-cert secret: secretName: ingress-nginx-admission --- apiVersion: batch/v1 kind: Job metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-admission-create namespace: ingress-nginx spec: template: metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-admission-create spec: containers: - args: - create - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc - --namespace=$(POD_NAMESPACE) - --secret-name=ingress-nginx-admission env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace image: registry.aliyuncs.com/google_containers/kube-webhook-certgen:v1.1.1 imagePullPolicy: IfNotPresent name: create securityContext: allowPrivilegeEscalation: false nodeSelector: kubernetes.io/os: linux restartPolicy: OnFailure securityContext: fsGroup: 2000 runAsNonRoot: true runAsUser: 2000 serviceAccountName: ingress-nginx-admission --- apiVersion: batch/v1 kind: Job metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-admission-patch namespace: ingress-nginx spec: template: metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-admission-patch spec: containers: - args: - patch - --webhook-name=ingress-nginx-admission - --namespace=$(POD_NAMESPACE) - --patch-mutating=false - --secret-name=ingress-nginx-admission - --patch-failure-policy=Fail env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace image: registry.aliyuncs.com/google_containers/kube-webhook-certgen:v1.1.1 imagePullPolicy: IfNotPresent name: patch securityContext: allowPrivilegeEscalation: false nodeSelector: kubernetes.io/os: linux restartPolicy: OnFailure securityContext: fsGroup: 2000 runAsNonRoot: true runAsUser: 2000 serviceAccountName: ingress-nginx-admission --- apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: nginx spec: controller: k8s.io/ingress-nginx --- apiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-admission webhooks: - admissionReviewVersions: - v1 clientConfig: service: name: ingress-nginx-controller-admission namespace: ingress-nginx path: /networking/v1/ingresses failurePolicy: Fail matchPolicy: Equivalent name: validate.nginx.ingress.kubernetes.io rules: - apiGroups: - networking.k8s.io apiVersions: - v1 operations: - CREATE - UPDATE resources: - ingresses sideEffects: None kubectl create -f ingress-nginx-deploy.yaml 卸载集群命令 #建议所有服务器都执行 #!/bin/bash kubeadm reset -f modprobe -r ipip lsmod rm -rf ~/.kube/ rm -rf /etc/kubernetes/ rm -rf /etc/systemd/system/kubelet.service.d rm -rf /etc/systemd/system/kubelet.service rm -rf /usr/bin/kube* rm -rf /etc/cni rm -rf /opt/cni rm -rf /var/lib/etcd rm -rf /var/etcd yum -y remove kubeadm* kubectl* kubelet* docker* reboot ","link":"https://tinaxiawuhao.github.io/post/oqAZb-4wU/"},{"title":"centos7安装Docker详细步骤","content":"一、安装前必读 在安装 Docker 之前，先说一下配置，我这里是Centos7 Linux 内核：官方建议 3.10 以上，3.8以上貌似也可。 注意：本文的命令使用的是 root 用户登录执行，不是 root 的话所有命令前面要加 sudo 1.查看当前的内核版本 uname -r 2.使用 root 权限更新 yum 包（生产环境中此步操作需慎重，看自己情况，学习的话随便搞） yum -y update 这个命令不是必须执行的，看个人情况，后面出现不兼容的情况的话就必须update了 注意 yum -y update：升级所有包同时也升级软件和系统内核； yum -y upgrade：只升级所有包，不升级软件和系统内核 3.卸载旧版本（如果之前安装过的话） yum remove docker docker-common docker-selinux docker-engine 二、安装Docker的详细步骤 1.安装需要的软件包， yum-util 提供yum-config-manager功能，另两个是devicemapper驱动依赖 yum install -y yum-utils device-mapper-persistent-data lvm2 2.设置 yum 源 设置一个yum源，下面两个都可用 yum-config-manager --add-repo http://download.docker.com/linux/centos/docker-ce.repo（中央仓库） yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo（阿里仓库） 3.选择docker版本并安装 （1）查看可用版本有哪些 yum list docker-ce --showduplicates | sort -r （2）选择一个版本并安装：yum install docker-ce-版本号 yum -y install docker-ce-18.03.1.ce 4.启动 Docker 并设置开机自启 systemctl start docker systemctl enable docker ","link":"https://tinaxiawuhao.github.io/post/4kG3rvwit/"},{"title":"HTTP3","content":"HTTP3协议发布 自2017年起HTTP3协议已发布了29个Draft，推出在即，Chrome、Nginx等软件都在跟进实现最新的草案。本文将介绍HTTP3协议规范、应用场景及实现原理。 2015年HTTP2协议正式推出后，已经有接近一半的互联网站点在使用它： HTTP2协议虽然大幅提升了HTTP/1.1的性能，然而，基于TCP实现的HTTP2遗留下3个问题： 有序字节流引出的队头阻塞（Head-of-line blocking），使得HTTP2的多路复用能力大打折扣； TCP与TLS叠加了握手时延，建链时长还有1倍的下降空间； 基于TCP四元组确定一个连接，这种诞生于有线网络的设计，并不适合移动状态下的无线网络，这意味着IP地址的频繁变动会导致TCP连接、TLS会话反复握手，成本高昂。 HTTP3协议解决了这些问题： HTTP3基于UDP协议重新定义了连接，在QUIC层实现了无序、并发字节流的传输，解决了队头阻塞问题（包括基于QPACK解决了动态表的队头阻塞）； HTTP3重新定义了TLS协议加密QUIC头部的方式，既提高了网络攻击成本，又降低了建立连接的速度（仅需1个RTT就可以同时完成建链与密钥协商）； HTTP3 将Packet、QUIC Frame、HTTP3 Frame分离，实现了连接迁移功能，降低了5G环境下高速移动设备的连接维护成本。 本文将会从HTTP3协议的概念讲起，从连接迁移的实现上学习HTTP3的报文格式，再围绕着队头阻塞问题来分析多路复用与QPACK动态表的实现。虽然正式的RFC规范还未推出，但最近的草案Change只有微小的变化，所以现在学习HTTP3正当其时，这将是下一代互联网最重要的基础设施。 HTTP3协议到底是什么？ 就像HTTP2协议一样，HTTP3并没有改变HTTP1的语义。那什么是HTTP语义呢？在我看来，它包括以下3个点： 请求只能由客户端发起，而服务器针对每个请求返回一个响应； 请求与响应都由Header、Body（可选）组成，其中请求必须含有URL和方法，而响应必须含有响应码； Header中各Name对应的含义保持不变。 HTTP3在保持HTTP1语义不变的情况下，更改了编码格式，这由2个原因所致：首先，是为了减少编码长度。下图中HTTP1协议的编码使用了ASCII码，用空格、冒号以及\\r\\n作为分隔符，编码效率很低： HTTP2与HTTP3采用二进制、静态表、动态表与Huffman算法对HTTP Header编码，不只提供了高压缩率，还加快了发送端编码、接收端解码的速度。 其次，由于HTTP1协议不支持多路复用，这样高并发只能通过多开一些TCP连接实现。然而，通过TCP实现高并发有3个弊端： 实现成本高。TCP是由操作系统内核实现的，如果通过多线程实现并发，并发线程数不能太多，否则线程间切换成本会以指数级上升；如果通过异步、非阻塞socket实现并发，开发效率又太低； 每个TCP连接与TLS会话都叠加了2-3个RTT的建链成本； TCP连接有一个防止出现拥塞的慢启动流程，它会对每个TCP连接都产生减速效果。 因此，HTTP2与HTTP3都在应用层实现了多路复用功能： HTTP2协议基于TCP有序字节流实现，因此应用层的多路复用并不能做到无序地并发，在丢包场景下会出现队头阻塞问题。如下面的动态图片所示，服务器返回的绿色响应由5个TCP报文组成，而黄色响应由4个TCP报文组成，当第2个黄色报文丢失后，即使客户端接收到完整的5个绿色报文，但TCP层不会允许应用进程的read函数读取到最后5个报文，并发成了一纸空谈： 当网络繁忙时，丢包概率会很高，多路复用受到了很大限制。因此，HTTP3采用UDP作为传输层协议，重新实现了无序连接，并在此基础上通过有序的QUIC Stream提供了多路复用，如下图所示： 最早这一实验性协议由Google推出，并命名为gQUIC，因此，IETF草案中仍然保留了QUIC概念，用来描述HTTP3协议的传输层和表示层。HTTP3协议规范由以下5个部分组成： QUIC层由https://tools.ietf.org/html/draft-ietf-quic-transport-29描述，它定义了连接、报文的可靠传输、有序字节流的实现； TLS协议会将QUIC层的部分报文头部暴露在明文中，方便代理服务器进行路由。https://tools.ietf.org/html/draft-ietf-quic-tls-29规范定义了QUIC与TLS的结合方式； 丢包检测、RTO重传定时器预估等功能由https://tools.ietf.org/html/draft-ietf-quic-recovery-29定义，目前拥塞控制使用了类似TCP New RENO的算法，未来有可能更换为基于带宽检测的算法（例如BBR）； 基于以上3个规范，https://tools.ietf.org/html/draft-ietf-quic-http-29定义了HTTP语义的实现，包括服务器推送、请求响应的传输等； 在HTTP2中，由HPACK规范定义HTTP头部的压缩算法。由于HPACK动态表的更新具有时序性，无法满足HTTP3的要求。在HTTP3中，QPACK定义HTTP头部的编码：https://tools.ietf.org/html/draft-ietf-quic-qpack-16。注意，以上规范的最新草案都到了29，而QPACK相对简单，它目前更新到16。 自1991年诞生的HTTP/0.9协议已不再使用，但1996推出的HTTP/1.0、1999年推出的HTTP/1.1、2015年推出的HTTP2协议仍然共存于互联网中（HTTP/1.0在企业内网中还在广为使用，例如Nginx与上游的默认协议还是1.0版本），即将面世的HTTP3协议的加入，将会进一步增加协议适配的复杂度。接下来，我们将深入HTTP3协议的细节。 连接迁移功能是怎样实现的？ 对于当下的HTTP1和HTTP2协议，传输请求前需要先完成耗时1个RTT的TCP三次握手、耗时1个RTT的TLS握手（TLS1.3），由于它们分属内核实现的传输层、openssl库实现的表示层，所以难以合并在一起，如下图所示： 在IoT时代，移动设备接入的网络会频繁变动，从而导致设备IP地址改变。对于通过四元组（源IP、源端口、目的IP、目的端口）定位连接的TCP协议来说，这意味着连接需要断开重连，所以上述2个RTT的建链时延、TCP慢启动都需要重新来过。而HTTP3的QUIC层实现了连接迁移功能，允许移动设备更换IP地址后，只要仍保有上下文信息（比如连接ID、TLS密钥等），就可以复用原连接。 在UDP报文头部与HTTP消息之间，共有3层头部，定义连接且实现了Connection Migration主要是在Packet Header中完成的，如下图所示： 这3层Header实现的功能各不相同： Packet Header实现了可靠的连接。当UDP报文丢失后，通过Packet Header中的Packet Number实现报文重传。连接也是通过其中的Connection ID字段定义的； QUIC Frame Header在无序的Packet报文中，基于QUIC Stream概念实现了有序的字节流，这允许HTTP消息可以像在TCP连接上一样传输； HTTP3 Frame Header定义了HTTP Header、Body的格式，以及服务器推送、QPACK编解码流等功能。 为了进一步提升网络传输效率，Packet Header又可以细分为两种： Long Packet Header用于首次建立连接； Short Packet Header用于日常传输数据。 其中，Long Packet Header的格式如下图所示： 建立连接时，连接是由服务器通过Source Connection ID字段分配的，这样，后续传输时，双方只需要固定住Destination Connection ID，就可以在客户端IP地址、端口变化后，绕过UDP四元组（与TCP四元组相同），实现连接迁移功能。下图是Short Packet Header头部的格式，这里就不再需要传输Source Connection ID字段了： 上图中的Packet Number是每个报文独一无二的序号，基于它可以实现丢失报文的精准重发。如果你通过抓包观察Packet Header，会发现Packet Number被TLS层加密保护了，这是为了防范各类网络攻击的一种设计。下图给出了Packet Header中被加密保护的字段： 其中，显示为E（Encrypt）的字段表示被TLS加密过。当然，Packet Header只是描述了最基本的连接信息，其上的Stream层、HTTP消息也是被加密保护的： 现在我们已经对HTTP3协议的格式有了基本的了解，接下来我们通过队头阻塞问题，看看Packet之上的QUIC Frame、HTTP3 Frame帧格式。 Stream多路复用时的队头阻塞是怎样解决的？ 其实，解决队头阻塞的方案，就是允许微观上有序发出的Packet报文，在接收端无序到达后也可以应用于并发请求中。比如上文的动态图中，如果丢失的黄色报文对其后发出的绿色报文不造成影响，队头阻塞问题自然就得到了解决： 在Packet Header之上的QUIC Frame Header，定义了有序字节流Stream，而且Stream之间可以实现真正的并发。HTTP3的Stream，借鉴了HTTP2中的部分概念，所以在讨论QUIC Frame Header格式之前，我们先来看看HTTP2中的Stream长成什么样子： 每个Stream就像HTTP1中的TCP连接，它保证了承载的HEADERS frame（存放HTTP Header）、DATA frame（存放HTTP Body）是有序到达的，多个Stream之间可以并行传输。在HTTP3中，上图中的HTTP2 frame会被拆解为两层，我们先来看底层的QUIC Frame。 一个Packet报文中可以存放多个QUIC Frame，当然所有Frame的长度之和不能大于PMTUD（Path Maximum Transmission Unit Discovery，这是大于1200字节的值），你可以把它与IP路由中的MTU概念对照理解： 每一个Frame都有明确的类型： 前4个字节的Frame Type字段描述的类型不同，接下来的编码也不相同，下表是各类Frame的16进制Type值： 在上表中，我们只要分析0x08-0x0f这8种STREAM类型的Frame，就能弄明白Stream流的实现原理，自然也就清楚队头阻塞是怎样解决的了。Stream Frame用于传递HTTP消息，它的格式如下所示： 可见，Stream Frame头部的3个字段，完成了多路复用、有序字节流以及报文段层面的二进制分隔功能，包括： Stream ID标识了一个有序字节流。当HTTP Body非常大，需要跨越多个Packet时，只要在每个Stream Frame中含有同样的Stream ID，就可以传输任意长度的消息。多个并发传输的HTTP消息，通过不同的Stream ID加以区别； 消息序列化后的“有序”特性，是通过Offset字段完成的，它类似于TCP协议中的Sequence序号，用于实现Stream内多个Frame间的累计确认功能； Length指明了Frame数据的长度。 你可能会奇怪，为什么会有8种Stream Frame呢？这是因为0x08-0x0f 这8种类型其实是由3个二进制位组成，它们实现了以下3 标志位的组合： 第1位表示是否含有Offset，当它为0时，表示这是Stream中的起始Frame，这也是上图中Offset是可选字段的原因； 第2位表示是否含有Length字段； 第3位Fin，表示这是Stream中最后1个Frame，与HTTP2协议Frame帧中的FIN标志位相同。 Stream数据中并不会直接存放HTTP消息，因为HTTP3还需要实现服务器推送、权重优先级设定、流量控制等功能，所以Stream Data中首先存放了HTTP3 Frame： 其中，Length指明了HTTP消息的长度，而Type字段（请注意，低2位有特殊用途，在QPACK章节中会详细介绍）包含了以下类型： 0x00：DATA帧，用于传输HTTP Body包体； 0x01：HEADERS帧，通过QPACK 编码，传输HTTP Header头部； 0x03：CANCEL_PUSH控制帧，用于取消1次服务器推送消息，通常客户端在收到PUSH_PROMISE帧后，通过它告知服务器不需要这次推送； 0x04：SETTINGS控制帧，设置各类通讯参数； 0x05：PUSH_PROMISE帧，用于服务器推送HTTP Body前，先将HTTP Header头部发给客户端，流程与HTTP2相似； 0x07：GOAWAY控制帧，用于关闭连接（注意，不是关闭Stream）； 0x0d：MAX_PUSH_ID，客户端用来限制服务器推送消息数量的控制帧。 总结一下，QUIC Stream Frame定义了有序字节流，且多个Stream间的传输没有时序性要求，这样，HTTP消息基于QUIC Stream就实现了真正的多路复用，队头阻塞问题自然就被解决掉了。 QPACK编码是如何解决队头阻塞问题的？ 最后，我们再看下HTTP Header头部的编码方式，它需要面对另一种队头阻塞问题。 与HTTP2中的HPACK编码方式相似，HTTP3中的QPACK也采用了静态表、动态表及Huffman编码： 先来看静态表的变化。在上图中，GET方法映射为数字2，这是通过客户端、服务器协议实现层的硬编码完成的。在HTTP2中，共有61个静态表项： 而在QPACK中，则上升为98个静态表项，比如Nginx上的ngx_htt_v3_static_table数组所示： 你也可以从这里找到完整的HTTP3静态表。对于Huffman以及整数的编码，QPACK与HPACK并无多大不同，但动态表编解码方式差距很大。 所谓动态表，就是将未包含在静态表中的Header项，在其首次出现时加入动态表，这样后续传输时仅用1个数字表示，大大提升了编码效率。因此，动态表是天然具备时序性的，如果首次出现的请求出现了丢包，后续请求解码HPACK头部时，一定会被阻塞！ QPACK是如何解决队头阻塞问题的呢？事实上，QPACK将动态表的编码、解码独立在单向Stream中传输，仅当单向Stream中的动态表编码成功后，接收端才能解码双向Stream上HTTP消息里的动态表索引。 我们又引入了单向Stream和双向Stream概念，不要头疼，它其实很简单。单向指只有一端可以发送消息，双向则指两端都可以发送消息。还记得上一小节的QUIC Stream Frame头部吗？其中的Stream ID别有玄机，除了标识Stream外，它的低2位还可以表达以下组合： 因此，当Stream ID是0、4、8、12时，这就是客户端发起的双向Stream（HTTP3不支持服务器发起双向Stream），它用于传输HTTP请求与响应。单向Stream有很多用途，所以它在数据前又多出一个Stream Type字段： Stream Type有以下取值： 0x00：控制Stream，传递各类Stream控制消息； 0x01：服务器推送消息； 0x02：用于编码QPACK动态表，比如面对不属于静态表的HTTP请求头部，客户端可以通过这个Stream发送动态表编码； 0x03：用于通知编码端QPACK动态表的更新结果。 由于HTTP3的STREAM之间是乱序传输的，因此，若先发送的编码Stream后到达，双向Stream中的QPACK头部就无法解码，此时传输HTTP消息的双向Stream就会进入Block阻塞状态（两端可以通过控制帧定义阻塞Stream的处理方式）。 小结 最后对本文内容做个小结。 基于四元组定义连接并不适用于下一代IoT网络，HTTP3创造出Connection ID概念实现了连接迁移，通过融合传输层、表示层，既缩短了握手时长，也加密了传输层中的绝大部分字段，提升了网络安全性。 HTTP3在Packet层保障了连接的可靠性，在QUIC Frame层实现了有序字节流，在HTTP3 Frame层实现了HTTP语义，这彻底解开了队头阻塞问题，真正实现了应用层的多路复用。 QPACK使用独立的单向Stream分别传输动态表编码、解码信息，这样乱序、并发传输HTTP消息的Stream既不会出现队头阻塞，也能基于时序性大幅压缩HTTP Header的体积。 ","link":"https://tinaxiawuhao.github.io/post/Um_5N4eGG/"},{"title":"binlog、redolog、undolog","content":"日志是 mysql 数据库的重要组成部分，记录着数据库运行期间各种状态信息。mysql日志主要包括错误日志、查询日志、慢查询日志、事务日志、二进制日志几大类。 作为开发，我们重点需要关注的是二进制日志( binlog )和事务日志(包括redo log 和 undo log )，本文接下来会详细介绍这三种日志。 binlog binlog 用于记录数据库执行的写入性操作(不包括查询)信息，以二进制的形式保存在磁盘中。binlog 是 mysql的逻辑日志，并且由 Server 层进行记录，使用任何存储引擎的 mysql 数据库都会记录 binlog 日志。 逻辑日志：可以简单理解为记录的就是sql语句 。 物理日志：mysql 数据最终是保存在数据页中的，物理日志记录的就是数据页变更 。 binlog 是通过追加的方式进行写入的，可以通过max_binlog_size 参数设置每个 binlog文件的大小，当文件大小达到给定值之后，会生成新的文件来保存日志。 binlog使用场景 在实际应用中， binlog 的主要使用场景有两个，分别是 主从复制 和 数据恢复 。 主从复制 ：在 Master 端开启 binlog ，然后将 binlog发送到各个 Slave 端， Slave 端重放 binlog 从而达到主从数据一致。 数据恢复 ：通过使用 mysqlbinlog 工具来恢复数据。 binlog刷盘时机 对于 InnoDB 存储引擎而言，只有在事务提交时才会记录binlog ，此时记录还在内存中，那么 binlog是什么时候刷到磁盘中的呢？ mysql 通过 sync_binlog 参数控制 binlog 的刷盘时机，取值范围是 0-N： 0：不去强制要求，由系统自行判断何时写入磁盘； 1：每次 commit 的时候都要将 binlog 写入磁盘； N：每N个事务，才会将 binlog 写入磁盘。 从上面可以看出， sync_binlog 最安全的是设置是 1 ，这也是MySQL 5.7.7之后版本的默认值。但是设置一个大一些的值可以提升数据库性能，因此实际情况下也可以将值适当调大，牺牲一定的一致性来获取更好的性能。 binlog日志格式 binlog 日志有三种格式，分别为 STATMENT 、 ROW 和 MIXED。 在 MySQL 5.7.7 之前，默认的格式是 STATEMENT ， MySQL 5.7.7 之后，默认值是 ROW。日志格式通过 binlog-format 指定。 STATMENT：基于SQL 语句的复制( statement-based replication, SBR )，每一条会修改数据的sql语句会记录到binlog 中 。 优点：不需要记录每一行的变化，减少了 binlog 日志量，节约了 IO , 从而提高了性能； 缺点：在某些情况下会导致主从数据不一致，比如执行sysdate() 、 slepp() 等 。 ROW：基于行的复制(row-based replication, RBR )，不记录每条sql语句的上下文信息，仅需记录哪条数据被修改了 。 优点：不会出现某些特定情况下的存储过程、或function、或trigger的调用和触发无法被正确复制的问题 ； 缺点：会产生大量的日志，尤其是alter table 的时候会让日志暴涨 MIXED：基于STATMENT 和 ROW 两种模式的混合复制(mixed-based replication, MBR )，一般的复制使用STATEMENT 模式保存 binlog ，对于 STATEMENT 模式无法复制的操作使用 ROW 模式保存 binlog redo log 我们都知道，事务的四大特性里面有一个是 持久性 ，具体来说就是只要事务提交成功，那么对数据库做的修改就被永久保存下来了，不可能因为任何原因再回到原来的状态 。 那么 mysql是如何保证一致性的呢？ 最简单的做法是在每次事务提交的时候，将该事务涉及修改的数据页全部刷新到磁盘中。但是这么做会有严重的性能问题，主要体现在两个方面： 因为 Innodb 是以 页 为单位进行磁盘交互的，而一个事务很可能只修改一个数据页里面的几个字节，这个时候将完整的数据页刷到磁盘的话，太浪费资源了！ 一个事务可能涉及修改多个数据页，并且这些数据页在物理上并不连续，使用随机IO写入性能太差！ 因此 mysql 设计了 redo log ， 具体来说就是只记录事务对数据页做了哪些修改，这样就能完美地解决性能问题了(相对而言文件更小并且是顺序IO)。 redo log基本概念 redo log 包括两部分：一个是内存中的日志缓冲( redo log buffer )，另一个是磁盘上的日志文件( redo logfile)。 mysql 每执行一条 DML 语句，先将记录写入 redo log buffer，后续某个时间点再一次性将多个操作记录写到 redo log file。这种 先写日志，再写磁盘 的技术就是 MySQL里经常说到的 WAL(Write-Ahead Logging) 技术。 在计算机操作系统中，用户空间( user space )下的缓冲区数据一般情况下是无法直接写入磁盘的，中间必须经过操作系统内核空间( kernel space )缓冲区( OS Buffer )。 因此， redo log buffer 写入 redo logfile 实际上是先写入 OS Buffer ，然后再通过系统调用 fsync() 将其刷到 redo log file 中，过程如下： mysql 支持三种将 redo log buffer 写入 redo log file 的时机，可以通过 innodb_flush_log_at_trx_commit 参数配置，各参数值含义如下： redo log记录形式 前面说过， redo log 实际上记录数据页的变更，而这种变更记录是没必要全部保存，因此 redo log实现上采用了大小固定，循环写入的方式，当写到结尾时，会回到开头循环写日志。如下图： 同时我们很容易得知， 在innodb中，既有redo log 需要刷盘，还有 数据页 也需要刷盘， redo log存在的意义主要就是降低对 数据页 刷盘的要求 。 在上图中， write pos 表示 redo log 当前记录的 LSN (逻辑序列号)位置， check point 表示 数据页更改记录 刷盘后对应 redo log 所处的 LSN(逻辑序列号)位置。 write pos 到 check point 之间的部分是 redo log 空着的部分，用于记录新的记录；check point 到 write pos 之间是 redo log 待落盘的数据页更改记录。当 write pos追上check point 时，会先推动 check point 向前移动，空出位置再记录新的日志。 启动 innodb 的时候，不管上次是正常关闭还是异常关闭，总是会进行恢复操作。因为 redo log记录的是数据页的物理变化，因此恢复的时候速度比逻辑日志(如 binlog )要快很多。 重启innodb 时，首先会检查磁盘中数据页的 LSN ，如果数据页的LSN 小于日志中的 LSN ，则会从 checkpoint 开始恢复。 还有一种情况，在宕机前正处于checkpoint 的刷盘过程，且数据页的刷盘进度超过了日志页的刷盘进度，此时会出现数据页中记录的 LSN 大于日志中的 LSN，这时超出日志进度的部分将不会重做，因为这本身就表示已经做过的事情，无需再重做。 redo log与binlog区别 由 binlog 和 redo log 的区别可知：binlog 日志只用于归档，只依靠 binlog 是没有 crash-safe 能力的。 但只有 redo log 也不行，因为 redo log 是 InnoDB特有的，且日志上的记录落盘后会被覆盖掉。因此需要 binlog和 redo log二者同时记录，才能保证当数据库发生宕机重启时，数据不会丢失。 undo log 数据库事务四大特性中有一个是 原子性 ，具体来说就是 原子性是指对数据库的一系列操作，要么全部成功，要么全部失败，不可能出现部分成功的情况。 实际上， 原子性 底层就是通过 undo log 实现的。undo log主要记录了数据的逻辑变化，比如一条 INSERT 语句，对应一条DELETE 的 undo log ，对于每个 UPDATE 语句，对应一条相反的 UPDATE 的 undo log ，这样在发生错误时，就能回滚到事务之前的数据状态。 同时， undo log 也是 MVCC(多版本并发控制)实现的关键。 ","link":"https://tinaxiawuhao.github.io/post/HHdQUUQvV/"},{"title":"RocketMQ","content":"可用性评估 系统可用性(Availability)是信息工业界用来衡量一个信息系统提供持续服务的能力，它表示的是在给定时间区间内系统或者系统某一能力在特定环境中能够正常工作的概率。 简单地说， 可用性是平均故障间隔时间(MTBF)除以平均故障间隔时间(MTBF)和平均故障修复时间(MTTR)之和所得的结果， 即： 通常业界习惯用N个9来表征系统可用性，表示系统可以正常使用时间与总时间(1年)之比，比如： 99.9%代表3个9的可用性，意味着全年不可用时间在8.76小时以内，表示该系统在连续运行1年时间里最多可能的业务中断时间是8.76小时； 99.99%代表4个9的可用性，意味着全年不可用时间在52.6分钟以内,表示该系统在连续运行1年时间里最多可能的业务中断时间是52.6分钟； 99.999%代表5个9的可用性，意味着全年不可用时间必须保证在5.26分钟以内，缺少故障自动恢复机制的系统将很难达到5个9的高可用性。 那么X个9里的X只代表数字35，为什么没有12，也没有大于6的呢？ 我们接着往下计算： 1个9：(1-90%)*365=36.5天 *2个9：(1-99%)*365=3.65天 6个9：(1-99.9999%)*365*24*60*60=31秒 可以看到1个9和、2个9分别表示一年时间内业务可能中断的时间是36.5天、3.65天，这种级别的可靠性或许还不配使用“可靠性”这个词； 而6个9则表示一年内业务中断时间最多是31秒，那么这个级别的可靠性并非实现不了，而是要做到从“5个9” 到“6个9”的可靠性提升的话，后者需要付出比前者几倍的成本。 RocketMQ架构设计 在介绍RocketMQ高可用之前，首先了解一下RocketMQ架构设计 技术架构 部署架构 技术架构 RocketMQ架构上主要分为四部分，如图所示: Producer：消息发布的角色，支持分布式集群方式部署。Producer通过MQ的负载均衡模块选择相应的Broker集群队列进行消息投递，投递的过程支持快速失败并且低延迟。 Consumer：消息消费的角色，支持分布式集群方式部署。支持以push推，pull拉两种模式对消息进行消费。同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制，可以满足大多数用户的需求。 NameServer：NameServer是一个非常简单的Topic路由注册中心，其角色类似Dubbo中的zookeeper，支持Broker的动态注册与发现。主要包括两个功能：Broker管理，NameServer接受Broker集群的注册信息并且保存下来作为路由信息的基本数据。然后提供心跳检测机制，检查Broker是否还存活；路由信息管理，每个NameServer将保存关于Broker集群的整个路由信息和用于客户端查询的队列信息。然后Producer和Conumser通过NameServer就可以知道整个Broker集群的路由信息，从而进行消息的投递和消费。NameServer通常也是集群的方式部署，各实例间相互不进行信息通讯。Broker是向每一台NameServer注册自己的路由信息，所以每一个NameServer实例上面都保存一份完整的路由信息。当某个NameServer因某种原因下线了，Broker仍然可以向其它NameServer同步其路由信息，Producer,Consumer仍然可以动态感知Broker的路由的信息。 BrokerServer：Broker主要负责消息的存储、投递和查询以及服务高可用保证，为了实现这些功能，Broker包含了以下几个重要子模块。 Remoting Module：整个Broker的实体，负责处理来自clients端的请求。 Client Manager：负责管理客户端(Producer/Consumer)和维护Consumer的Topic订阅信息 Store Service：提供方便简单的API接口处理消息存储到物理硬盘和查询功能。 HA Service：高可用服务，提供Master Broker 和 Slave Broker之间的数据同步功能。 Index Service：根据特定的Message key对投递到Broker的消息进行索引服务，以提供消息的快速查询。 部署架构 RocketMQ的Broker有三种集群部署方式： 1.单台Master部署； 2.多台Master部署； 3.多Master多Slave部署； 基础的rocket高可用，主要采用第3种部署方式 下图是第3种部署方式的简单图： 第3种部署方式网络部署特点 NameServer是一个几乎无状态节点，可集群部署，节点之间无任何信息同步。 Broker部署相对复杂，Broker分为Master与Slave，一个Master可以对应多个Slave，但是一个Slave只能对应一个Master，Master与Slave 的对应关系通过指定相同的BrokerName，不同的BrokerId 来定义，BrokerId为0表示Master，非0表示Slave。Master也可以部署多个。每个Broker与NameServer集群中的所有节点建立长连接，定时注册Topic信息到所有NameServer。 注意：当前RocketMQ版本在部署架构上支持一Master多Slave，但只有BrokerId=1的从服务器才会参与消息的读负载。 Producer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer获取Topic路由信息，并向提供Topic 服务的Master建立长连接，且定时向Master发送心跳。Producer完全无状态，可集群部署。 Consumer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer获取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳。Consumer既可以从Master订阅消息，也可以从Slave订阅消息，消费者在向Master拉取消息时，Master服务器会根据拉取偏移量与最大偏移量的距离（判断是否读老消息，产生读I/O），以及从服务器是否可读等因素建议下一次是从Master还是Slave拉取。 结合部署架构图，描述集群工作流程： 启动NameServer，NameServer起来后监听端口，等待Broker、Producer、Consumer连上来，相当于一个路由控制中心。 Broker启动，跟所有的NameServer保持长连接，定时发送心跳包。心跳包中包含当前Broker信息(IP+端口等)以及存储所有Topic信息。注册成功后，NameServer集群中就有Topic跟Broker的映射关系。 收发消息前，先创建Topic，创建Topic时需要指定该Topic要存储在哪些Broker上，也可以在发送消息时自动创建Topic。 Producer发送消息，启动时先跟NameServer集群中的其中一台建立长连接，并从NameServer中获取当前发送的Topic存在哪些Broker上，轮询从队列列表中选择一个队列，然后与队列所在的Broker建立长连接从而向Broker发消息。 Consumer跟Producer类似，跟其中一台NameServer建立长连接，获取当前订阅Topic存在哪些Broker上，然后直接跟Broker建立连接通道，开始消费消息。 汇总：RocketMQ 集群部署模式 前面介绍到，RocketMQ的Broker有三种集群部署方式： 1.单台Master部署； 2.多台Master部署； 3.多Master多Slave部署； 第三种模式，根据Master和Slave之节的数据同步方式可以分为： 多 master 多 slave 异步复制模式 多 master 多 slave 同步复制模式 同步方式：同步复制和异步复制（指的一组 master 和 slave 之间数据的同步） 所以，总体来说，RocketMQ 集群部署模式为四种： 1.单 master 模式 也就是只有一个 master 节点，如果master节点挂掉了，会导致整个服务不可用，线上不宜使用，适合个人学习使用。 2.多 master 模式 多个 master 节点组成集群，单个 master 节点宕机或者重启对应用没有影响。 优点：所有模式中性能最高 缺点：单个 master 节点宕机期间，未被消费的消息在节点恢复之前不可用，消息的实时性就受到影响。 注意：使用同步刷盘可以保证消息不丢失，同时 Topic 相对应的 queue 应该分布在集群中各个 master 节点，而不是只在某各 master 节点上，否则，该节点宕机会对订阅该 topic 的应用造成影响。 3.多 master 多 slave 异步复制模式 在多 master 模式的基础上，每个 master 节点都有至少一个对应的 slave。 master 节点可读可写，但是 slave 只能读不能写，类似于 mysql 的主备模式。 优点： 在 master 宕机时，消费者可以从 slave 读取消息，消息的实时性不会受影响，性能几乎和多 master 一样。 缺点：使用异步复制的同步方式有可能会有消息丢失的问题。 4.多 master 多 slave 同步双写模式 同多 master 多 slave 异步复制模式类似，区别在于 master 和 slave 之间的数据同步方式。 优点：同步双写的同步模式能保证数据不丢失。 缺点：发送单个消息 RT 会略长，性能相比异步复制低10%左右。 刷盘策略：同步刷盘和异步刷盘（指的是节点自身数据是同步还是异步存储） 注意：要保证数据可靠，需采用同步刷盘和同步双写的方式，但性能会较其他方式低。 RocketMQ与ZooKeeper的爱恨纠葛 说到高性能消息中间件，第一个想到的肯定是LinkedIn开源的Kafka，虽然最初Kafka是为日志传输而生，但也非常适合互联网公司消息服务的应用场景，他们不要求数据实时的强一致性（事务），更多是希望达到数据的最终一致性。 RocketMQ是MetaQ的3.0版本，而MetaQ最初的设计又参考了Kafka。最初的MetaQ 1.x版本由阿里的原作者庄晓丹开发，后面的MetaQ 2.x版本才进行了开源。 MetaQ 1.x和MetaQ 2.x是依赖ZooKeeper的，但RocketMQ（即MetaQ 3.x）却去掉了ZooKeeper依赖，转而采用自己的NameServer。 ZooKeeper是著名的分布式协作框架，提供了Master选举、分布式锁、数据的发布和订阅等诸多功能。为什么RocketMQ没有选择ZooKeeper，而是自己开发了NameServer，我们来具体看看NameServer在RocketMQ集群中的作用就明了了。 RocketMQ的Broker有三种集群部署方式 RocketMQ的Broker有三种集群部署方式： 1.单台Master部署； 2.多台Master部署； 3.多Master多Slave部署； 采用第3种部署方式时，Master和Slave可以采用同步复制和异步复制两种方式。 下图是第3种部署方式的简单图： 当采用多Master方式时，Master与Master之间是不需要知道彼此的，这样的设计直接降低了Broker实现的复杂性。 你可以试想，如果Master与Master之间需要知道彼此的存在，这会需要在Master之中维护一个网络的Master列表，而且必然设计到Master发现和活跃Master数量变更等诸多状态更新问题，所以最简单也最可靠的做法就是Master只做好自己的事情（比如和Slave进行数据同步）即可。 这样，在分布式环境中，某台Master宕机或上线，不会对其他Master造成任何影响。 那么怎么才能知道网络中有多少台Master和Slave呢？ 你会很自然想到用ZooKeeper，每个活跃的Master或Slave都去约定的ZooKeeper节点下注册一个状态节点，但RocketMQ没有使用ZooKeeper，所以这件事就交给了NameServer来做了（看上图）。 NameServer的功能 功能一：NameServer用来保存活跃的broker列表，包括Master和Slave。 功能二：NameServer用来保存所有topic和该topic所有队列的列表。 功能三：NameServer用来保存所有broker的Filter列表。 功能四：NameServer可以理解承担了注册中心的职能 NameServer注册中心职能 NameServer是一个非常简单的路由注册中心，其角色类似Dubbo中的zookeeper，支持Broker的动态注册与发现。 Broker管理，NameServer接受Broker集群的注册信息并且保存下来作为路由信息的基本数据。然后提供心跳检测机制，检查Broker是否还存活； 路由信息管理，每个NameServer将保存关于Broker集群的整个路由信息和用于客户端查询的队列信息。然后Producer和Conumser通过NameServer就可以知道整个Broker集群的路由信息，从而进行消息的投递和消费 整个Rocketmq集群的工作原理如下图所示： 可以看到，Broker集群、Producer集群、Consumer集群都需要与NameServer集群进行通信： Broker集群: Broker用于接收生产者发送消息，或者消费者消费消息的请求。一个Broker集群由多组Master/Slave组成，Master可写可读，Slave只可以读，Master将写入的数据同步给Slave。 每个Broker节点，在启动时，都会遍历NameServer列表，与每个NameServer建立长连接，注册自己的信息，之后定时上报。 Producer集群: 消息的生产者，通过NameServer集群获得Topic的路由信息，包括Topic下面有哪些Queue，这些Queue分布在哪些Broker上等。Producer只会将消息发送到Master节点上，因此只需要与Master节点建立连接。 Consumer集群: 消息的消费者，通过NameServer集群获得Topic的路由信息，连接到对应的Broker上消费消息。注意，由于Master和Slave都可以读取消息，因此Consumer会与Master和Slave都建立连接。 总之： Name Server 是专为 RocketMQ 设计的轻量级注册中心，具有简单、可集群横吐扩展、无状态，节点之间互不通信等特点。 RocketMQ为什么不使用ZooKeeper 来看看RocketMQ为什么不使用ZooKeeper？ ZooKeeper可以提供Master选举功能。比如Kafka用来给每个分区选一个broker作为leader。 但对于RocketMQ来说，topic的数据在每个Master上是对等的，没有哪个Master上有topic上的全部数据，所以这里选举leader没有意义； RockeqMQ集群中，需要有构件来处理一些通用数据，比如broker列表，broker刷新时间。 虽然ZooKeeper也能存放数据，并有一致性保证。但处理数据之间的一些逻辑关系却比较麻烦，而且数据的逻辑解析操作得交给ZooKeeper客户端来做，如果有多种角色的客户端存在，自己解析多级数据确实是个麻烦事情； 既然RocketMQ集群中没有用到ZooKeeper的一些重量级的功能，只是使用ZooKeeper的数据一致性和发布订阅的话，与其依赖重量级的ZooKeeper，还不如写个轻量级的NameServer，NameServer也可以集群部署，NameServer与NameServer之间无任何信息同步，不需要保障数据一致性， 比zk简单太多。 NameServer特性 NameServer通常也是集群的方式部署，各实例间相互不进行信息通讯。Broker是向每一台NameServer注册自己的路由信息，所以每一个NameServer实例上面都保存一份完整的路由信息。当某个NameServer因某种原因下线了，Broker仍然可以向其它NameServer同步其路由信息，Producer，Consumer仍然可以动态感知Broker的路由的信息。 NameServer实例时间互不通信，这本身也是其设计亮点之一，即允许不同NameServer之间数据不同步(像Zookeeper那样保证各节点数据强一致性会带来额外的性能消耗) RocketMQ 的消息类型 RocketMQ 支持普通消息，顺序消息、事务消息，等等多种消息类型： 普通消息：没有特殊功能的消息。 分区顺序消息：以分区纬度保持顺序进行消费的消息。 全局顺序消息：全局顺序消息可以看作是只分一个区，始终在同一个分区上进行消费。 定时/延时消息：消息可以延迟一段特定时间进行消费。 事务消息：二阶段事务消息，先进行prepare投递消息，此时不能进行消息消费，当二阶段发出commit或者rollback的时候才会进行消息的消费或者回滚。 虽然配置种类比较繁多，但是使用的还是普通消息和分区顺序消息。 本文的主要介绍高可用，主要介绍普通消息，其他消息的高可用策略，也是类似侧。 RocketMQ高可用 NameServer 高可用 由于 NameServer 节点是无状态的，且各个节点直接的数据是一致的，故存在多个 NameServer 节点的情况下，部分 NameServer 不可用也可以保证 MQ 服务正常运行 BrokerServer 高可用 RocketMQ是通过 Master 和 Slave 的配合达到 BrokerServer 模块的高可用性的 一个 Master 可以配置多个 Slave，同时也支持配置多个 Master-Slave 组。 当其中一个 Master 出现问题时： 由于Slave只负责读，当 Master 不可用，它对应的 Slave 仍能保证消息被正常消费 由于配置多组 Master-Slave 组，其他的 Master-Slave 组也会保证消息的正常发送和消费 老版本的RocketMQ不支持把Slave自动转成Master，如果机器资源不足， 需要把Slave转成Master，则要手动停止Slave角色的Broker，更改配置文 件，用新的配置文件启动Broker。 新版本的RocketMQ，支持Slave自动转成Master。 consumer高可用 Consumer 的高可用是依赖于 Master-Slave 配置的，由于 Master 能够支持读写消息，Slave 支持读消息，当 Master 不可用或繁忙时， Consumer 会被自动切换到从 Slave 读取(自动切换，无需配置)。 故当 Master 的机器故障后，消息仍可从 Slave 中被消费 producer高可用 在创建Topic的时候，把Topic的多个Message Queue创建在多个Broker组上（相同Broker名称，不同 brokerId的机器组成一个Broker组）. 这样当一个Broker组的Master不可用后，其他组的Master仍然可用，Producer仍然可以发送消息。 实现分布式集群多副本的三种方式 M/S模式 即Master/Slaver模式。 该模式在过去使用的最多，RocketMq之前也是使用这样的主从模式来实现的。 主从模式分为同步模式和异步模式，区别是在同步模式下只有主从复制完毕才会返回给客户端；而在异步模式中，主从的复制是异步的，不用等待即可返回。 同步模式 同步模式特点 异步模式 异步模式特点 基于zookeeper服务 和M/S模式相比zookeeper模式是自动选举的主节点，新版本rocketMq暂时不支持zookeeper。 基于raft 相比zookeeper，raft自身就可以实现选举，raft通过投票的方式实现自身选举leader。去除额外依赖。目前RocketMq 4.5.0已经支持 可用性与可靠性 可用性 由于消息分布在各个broker上，一旦某个broker宕机，则该broker上的消息读写都会受到影响。所以rocketmq提供了master/slave的结构，salve定时从master同步数据，如果master宕机，则slave提供消费服务，但是不能写入消息，此过程对应用透明，由rocketmq内部解决。 这里有两个关键点： 一旦某个broker master宕机，生产者和消费者多久才能发现？受限于rocketmq的网络连接机制，默认情况下，最多需要30秒，但这个时间可由应用设定参数来缩短时间。这个时间段内，发往该broker的消息都是失败的，而且该broker的消息无法消费，因为此时消费者不知道该broker已经挂掉。 消费者得到master宕机通知后，转向slave消费，但是slave不能保证master的消息100%都同步过来了，因此会有少量的消息丢失。但是消息最终不会丢的，一旦master恢复，未同步过去的消息会被消费掉。 可靠性 所有发往broker的消息，有同步刷盘和异步刷盘机制，总的来说，可靠性非常高 同步刷盘时，消息写入物理文件才会返回成功，因此非常可靠 异步刷盘时，只有机器宕机，才会产生消息丢失，broker挂掉可能会发生，但是机器宕机崩溃是很少发生的，除非突然断电 Broker消息的零丢失方案 同步刷盘、异步刷盘 RocketMQ的消息是存储到磁盘上的，这样既能保证断电后恢复，又可以让存储的消息量超出内存的限制。RocketMQ为了提高性能，会尽可能地保证磁盘的顺序写。 消息在通过Producer写入RocketMQ的时候，有两种写磁盘方式： 异步刷盘方式： 在返回写成功状态时，消息可能只是被写入了内存的PAGECACHE，写操作的返回快，吞吐量大；当内存里的消息量积累到一定程度时，统一触发写磁盘操作，快速写入 优点：性能高 缺点：Master宕机，磁盘损坏的情况下，会丢失少量的消息, 导致MQ的消息状态和生产者/消费者的消息状态不一致 同步刷盘方式： 在返回应用写成功状态前，消息已经被写入磁盘。 具体流程是，消息写入内存的PAGECACHE后，立刻通知刷盘线程刷盘，然后等待刷盘完成，刷盘线程执行完成后唤醒等待的线程，给应用返回消息写成功的状态。 优点：可以保持MQ的消息状态和生产者/消费者的消息状态一致 缺点：性能比异步的低 同步刷盘还是异步刷盘，是通过Broker配置文件里的flushDiskType参数设置的，这个参数被设置成SYNC_FLUSH, ASYNC_FLUSH中的一个。 同步复制、异步复制 如果一个broker组有Master和Slave，消息需要从Master复制到Slave上，有同步和异步两种复制方式。 同步复制方式： 等Master和Slave均写成功后才反馈给客户端写成功状态 优点：如果Master出故障，Slave上有全部的备份数据，容易恢复，消费者仍可以从Slave消费, 消息不丢失 缺点：增大数据写入延迟，降低系统吞吐量，性能比异步复制模式略低，大约低10%左右，发送单个Master的响应时间会略高 异步复制方式： 只要Master写成功即可反馈给客户端写成功状态 优点：系统拥有较低的延迟和较高的吞吐量. Master宕机之后，消费者仍可以从Slave消费，此过程对应用透明，不需要人工干预，性能同多个Master模式几乎一样 缺点：如果Master出了故障，有些数据因为没有被写入Slave，而丢失少量消息。 若一个 Broker 组有一个 Master 和 Slave，消息需要从 Master 复制到 Slave 上，有同步复制和异步复制两种方式 同步复制 异步复制 概念 即等 Master 和 Slave 均写成功后才反馈给客户端写成功状态 只要 Master 写成功，就反馈客户端写成功状态 可靠性 可靠性高，若 Master 出现故障，Slave 上有全部的备份数据，容易恢复 若 Master 出现故障，可能存在一些数据还没来得及写入 Slave，可能会丢失 效率 由于是同步复制，会增加数据写入延迟，降低系统吞吐量 由于只要写入 Master 即可，故数据写入延迟较低，吞吐量较高 同步复制和异步复制是通过Broker配置文件里的brokerRole参数进行设置的，这个参数可以被设置成ASYNC_MASTER、SYNC_MASTER、SLAVE三个值中的一个。 三个值的说明： sync_master是同步方式，Master角色Broker中的消息要立刻同步过去。 async_master是异步方式，Master角色Broker中的消息通过异步处理的方式同步到Slave角色的机器上。 SLAVE 表明当前是从节点，无需配置 brokerRole 消息零丢失方案 消息零丢失是一把双刃剑，要想用好，还是要视具体的业务场景，在性能和消息零丢失上做平衡。 实际应用中的推荐把Master和Slave设置成ASYNC_FLUSH的异步刷盘方式，主从之间配置成SYNC_MASTER的同步复制方式，这样即使有一台机器出故障，仍然可以保证数据不丢。 刷盘方式 Master和Slave都设置成ASYNC_FLUSH的异步刷盘 复制方式 Master配置成SYNC_MASTER 同步复制 异步刷盘能够避免频繁触发磁盘写操作，除非服务器宕机，否则不会造成消息丢失。 主从同步复制能够保证消息不丢失，即使 Master 节点异常，也能保证 Slave 节点存储所有消息并被正常消费掉。 producer高可用 producer具备发送到全部master的能力，如果有多个master，消息会发送到所有的master 另外，在topic的不同的queue之间，producer还具备负载均衡能力。 在实例发送消息时，默认会轮询所有订阅了改 Topic 的 broker 节点上的 message queue，让消息平均落在不同的 queue 上，而由于这些 queue 散落在不同的 broker 节点中，即使某个 broker 节点异常，其他存在订阅了这个 Topic 的 message queue 的 broker 依然能消费消息 消息者业务代码出现异常怎么办？ 再来看一下消费者的代码中监听器的部分，它说如果消息处理成功，那么就返回消息状态为 CONSUME_SUCCESS，也有可能发放优惠券、积分等操作出现了异常，比如说数据库挂掉了。这个时候应该怎么处理呢？ consumer.registerMessageListener(new MessageListenerConcurrently() { @Override public ConsumeConcurrentlyStatus consumeMessage(List &lt;MessageExt&gt; list, ConsumeConcurrentlyContext consumeConcurrentlyContext) { // 对消息的处理，比如发放优惠券、积分等 return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; } }); 我们可以把代码改一改，捕获异常之后返回消息的状态为 RECONSUME_LATER 表示稍后重试。 // 这次回调接口，接收消息 consumer.registerMessageListener(new MessageListenerConcurrently() { @Override public ConsumeConcurrentlyStatus consumeMessage(List &lt;MessageExt&gt; list, ConsumeConcurrentlyContext consumeConcurrentlyContext) { try { // 对消息的处理，比如发放优惠券、积分等 return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; } catch (Exception e) { // 万一发生数据库宕机等异常，返回稍后重试消息的状态 return ConsumeConcurrentlyStatus.RECONSUME_LATER; } } }); 这个时候，消息会进入到 RocketMQ 的重试队列中。 重试队列 比如说消费者所属的消息组名称为AAAConsumerGroup 其重试队列名称就叫做**%RETRY%AAAConsumerGroup** 重试队列中的消息过一段时间会再次发送给消费者，如果还是无法正常执行会再次进入重试队列 默认重试16次，还是无法执行，消息就会从重试队列进入到死信队列 死信队列 重试队列中的消息重试16次任然无法执行，将会进入到死信队列 死信队列的名字是 %DLQ%AAAConsumerGroup 死信队列中的消息可以后台开一个线程，订阅**%DLQ%AAAConsumerGroup**，并不停重试 Customer 负载均衡 集群模式 在集群消费模式下，存在多个消费者同时消费消息，同一条消息只会被某一个消费者获取。即消息只需要被投递到订阅了这个 Topic 的消费者Group下的一个实例中即可。 消费者采用主动拉去的方式拉去并消费，在拉取的时候需要明确指定拉取那一条消息队列中的消息。 每当有实例变更，都会触发一次所有消费者实例的负载均衡，这是会按照queue的数量和实例的数量平均分配 queue 给每个消费者实例。 注意： 1）在集群模式下，一个 queue 只允许分配给一个消费者实例，这是由于若多个实例同时消费一个 queue 的小，由于拉取操作是由 consumer 主动发生的，可能导致同一个消息在不同的 consumer 实例中被消费。故算法保证了一个 queue 只会被一个 consumer 实例消费，但一个 consumer 实例能够消费多个 queue 2）控制 consumer 数量，应小于 queue 数量。这是由于一个 queue 只允许分配给一个 consumer 实例，若 consumer 实例数量多于 queue，则多出的 consumer 实例无法分配到 queue消费，会浪费系统资源 广播模式 广播模式其实不是负载均衡，由于每个消费者都能够拿到所有消息，故不能达到负载均衡的要求 消费者的消息重试 顺序消息重试 对于顺序消息，为了保证消息消费的顺序性，当consumer消费失败后，消息队列会自动不断进行消息重试(每次间隔时间为1s)， 这时会导致consumer消费被阻塞的情况，故必须保证应用能够及时监控并处理消费失败的情况，避免阻塞现象的发生 无序消息重试 概述 无序消息即普通、定时、延时、事务消息，当consumer消费消息失败时，可以通过设置返回状态实现消息重试 注意：无序消息的重试只针对集群消费方式（非广播方式）生效 广播方式不提供失败重试特性，即消费失败后，失败的消息不再重试，而是继续消费新消息 重试次数 消息队列 RocketMQ 默认允许每条消息最多重试 16 次，每次重试的间隔时间如下： 第几次重试 与上次重试的间隔时间 第几次重试 与上次重试的间隔时间 1 10 秒 9 7 分钟 2 30 秒 10 8 分钟 3 1 分钟 11 9 分钟 4 2 分钟 12 10 分钟 5 3 分钟 13 20 分钟 6 4 分钟 14 30 分钟 7 5 分钟 15 1 小时 8 6 分钟 16 2 小时 如果消息重试 16 次后仍然失败，消息将不再投递。 如果严格按照上述重试时间间隔计算，某条消息在一直消费失败的前提下，将会在接下来的 4 小时 46 分钟之内进行 16 次重试，超过这个时间范围消息将不再重试投递。 注意： 一条消息无论重试多少次，这些重试消息的 Message ID 不会改变。 消息重试相关的处理方式 消费失败后，需要重试的处理方式 集群消费方式（非广播方式）下，消息消费失败后期望消息重试，需要在消息监听器接口的实现中明确进行配置（三种方式任选一种）： 方式 1：返回 Action.ReconsumeLater（推荐） 方式 2：返回 Null 方式 3：抛出异常 示例代码 public class MessageListenerImpl implements MessageListener { @Override public Action consume(Message message, ConsumeContext context) { //消息处理逻辑抛出异常，消息将重试 doConsumeMessage(message); //方式 1：返回 Action.ReconsumeLater，消息将重试 return Action.ReconsumeLater; //方式 2：返回 null，消息将重试 return null; //方式 3：直接抛出异常，消息将重试 throw new RuntimeException(&quot;Consumer Message exception&quot;); } } 集群消费方式下，消息消费失败后期望消息重试，需要在消息监听器接口的实现中明确进行配置 消费失败后，无需重试的处理方式 集群消费方式下，消息失败后期望消息不重试，需要捕获消费逻辑中可能抛出的异常，最终返回 Action.CommitMessage，此后这条消息将不会再重试。 public class MessageListenerImpl implements MessageListener { @Override public Action consume(Message message, ConsumeContext context) { try { doConsumeMessage(message); } catch (Throwable e) { //捕获消费逻辑中的所有异常，并返回 Action.CommitMessage; return Action.CommitMessage; } //消息处理正常，直接返回 Action.CommitMessage; return Action.CommitMessage; } } 3）自定义消息最大重试次数 消息队列 RocketMQ 允许 Consumer 启动的时候设置最大重试次数，重试时间间隔将按照如下策略： 最大重试次数小于等于 16 次，则重试时间间隔同上表描述。 最大重试次数大于 16 次，超过 16 次的重试时间间隔均为每次 2 小时。 设置方式： consumer.setMaxReconsumeTimes(20); 或者： Properties properties = new Properties(); //配置对应 Group ID 的最大消息重试次数为 20 次，最大重试次数为字符串类型 properties.put(PropertyKeyConst.MaxReconsumeTimes,&quot;20&quot;); Consumer consumer =ONSFactory.createConsumer(properties); 注意： 消息最大重试次数设置，对相同 Group ID 下的所有 Consumer 实例有效。 如果只对相同 Group ID 下两个 Consumer 实例中的其中一个设置了 MaxReconsumeTimes，那么该配置对两个 Consumer 实例均生效。 配置采用覆盖的方式生效，即最后启动的 Consumer 实例会覆盖之前的启动实例的配置 获取消息重试次数 消费者收到消息后，可以获取到消息的重试次数 设置方式： public class MessageListenerImpl implements MessageListener { @Override public Action consume(Message message, ConsumeContext context) { //获取消息的重试次数 System.out.println(message.getReconsumeTimes()); return Action.CommitMessage; } } 死信队列 死信队列概念 在正常情况下无法被消费(超过最大重试次数)的消息称为死信消息(Dead-Letter Message)，存储死信消息的特殊队列就称为死信队列(Dead-Letter Queue) 当一条消息初次消费失败，消息队列 RocketMQ 会自动进行消息重试； 达到最大重试次数后，若消费依然失败，则表明消费者在正常情况下无法正确地消费该消息，此时，消息队列 RocketMQ 不会立刻将消息丢弃，而是将其发送到该消费者对应的死信队列中。 代码正常执行返回消息状态为CONSUME_SUCCESS，执行异常返回RECONSUME_LATER 状态为RECONSUME_LATER的消息会进入到重试队列，重试队列的名称为 %RETRY% + ConsumerGroupName； 重试16次消息任然没有处理成功，消息就会进入到死信队列%DLQ% + ConsumerGroupName; 死信特性 死信消息有以下特点： 不会再被消费者正常消费 有效期与正常消息相同，均为 3 天，3 天后会被自动删除。故死信消息应在产生的 3 天内及时处理 死信队列有以下特点： 一个死信队列对应一个消费者组，而不是对应单个消费者实例 一个死信队列包含了对应的 Group ID 所产生的所有死信消息，不论该消息属于哪个 Topic 若一个 Group ID 没有产生过死信消息，则 RocketMQ 不会为其创建相应的死信队列 查看死信信息和重发 在控制台查看死信队列的主题信息 重发消息 消息幂等性 消费幂等 消费幂等即无论消费者消费多少次，其结果都是一样的。 RocketMQ 是通过业务上的唯一 Key 来对消息做幂等处理 消费幂等的必要性 在网络环境中，由于网络不稳定等因素，消息队列的消息有可能出现重复，大概有以下几种： 发送时消息重复 当一条消息已被成功发送到服务端并完成持久化，此时出现了网络闪断或者客户端宕机，导致服务端对客户端应答失败。 如果此时生产者意识到消息发送失败并尝试再次发送消息，消费者后续会收到两条内容相同并且 Message ID 也相同的消息。 投递时消息重复 消息消费的场景下，消息已投递到消费者并完成业务处理，当客户端给服务端反馈应答的时候网络闪断。 为了保证消息至少被消费一次，消息队列 RocketMQ 的服务端将在网络恢复后再次尝试投递之前已被处理过的消息，消费者后续会收到两条内容相同并且 Message ID 也相同的消息。 负载均衡时消息重复（包括但不限于网络抖动、Broker 重启以及订阅方应用重启） 当消息队列 RocketMQ 的 Broker 或客户端重启、扩容或缩容时，会触发 Rebalance，此时消费者可能会收到重复消息。 结合三种情况，可以发现消息重发的最后结果都是，消费者接收到了重复消息，那么，我们只需要在消费者端统一进行幂等处理就能够实现消息幂等。 处理方式 消费端实现消息幂等性 RocketMQ 只能够保证消息丢失，但不能保证消息不重复投递，且由于高可用和高性能的考虑，应该在消费端实现消息幂等性。 那么 RocketMQ 是怎样解决消息重复的问题呢？还是“恰好”不解决。 造成消息重复的根本原因是：网络不可达。只要通过网络交换数据，就无法避免这个问题。所以解决这个问题的办法就是绕过这个问题。那么问题就变成了：如果消费端收到两条一样的消息，应该怎样处理？ 消费端处理消息的业务逻辑保持幂等性 保证每条消息都有唯一编号且保证消息处理成功与去重表的日志同时出现 第1条很好理解，只要保持幂等性，不管来多少条重复消息，最后处理的结果都一样。 第2条原理就是利用一张日志表来记录已经处理成功的消息的ID，如果新到的消息ID已经在日志表中，那么就不再处理这条消息。 第1条解决方案，很明显应该在消费端实现，不属于消息系统要实现的功能。第2条可以消息系统实现，也可以业务端实现。正常情况下出现重复消息的概率其实很小，如果由消息系统来实现的话，肯定会对消息系统的吞吐量和高可用有影响，所以最好还是由业务端自己处理消息重复的问题，这也是 RocketMQ 不解决消息重复的问题的原因。 RocketMQ 不保证消息不重复，如果你的业务需要保证严格的不重复消息，需要你自己在业务端去重。 在消费端通过业务逻辑实现幂等性操作，最常用的方式就是唯一ID的形式，若已经消费过的消息就不进行处理。例如在秒杀系统中使用订单ID作为关键ID，分布式系统中常用雪花算法生成ID。 注：如果需要彻底了解雪花算法，以及里边的位运算逻辑，请参见尼恩的秒杀视频。 在发送消息时，可以对 Message 设置标识唯一标识： Message message = new Message(); # 设置唯一标识，标识由雪花算法生成message.setKey(idWorker.nextId()); 订阅方收到消息时，可以获取到这个 Key consumer.registerMessageListener(new MessageListenerConcurrently() { @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) { System.out.printf(&quot;%s Receive New Messages: %s %n&quot;, Thread.currentThread().getName(), msgs); for (MessageExt ext : msgs) { System.out.println(ext.getKeys()); } return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; } }); 一、顺序消息 顺序消息（FIFO 消息）是消息队列 RocketMQ 提供的一种严格按照顺序来发布和消费的消息。顺序发布和顺序消费是指对于指定的一个 Topic，生 产者按照一定的先后顺序发布消息；消费者按照既定的先后顺序订阅消息，即先发布的消息一定会先被客户端接收到。 顺序消息分为全局顺序消息和分区顺序消息。 1.1、全局顺序消息 RocketMQ 在默认情况下不保证顺序，要保证全局顺序，需要把 Topic 的读写队列数设置为 1，然后生产者和消费者的并发设置也是 1。所以这样的话 高并发，高吞吐量的功能完全用不上。 1.1.1、适用场景 适用于性能要求不高，所有的消息严格按照 FIFO 原则来发布和消费的场景。 1.1.2、示例 要确保全局顺序消息，需要先把 Topic 的读写队列数设置为 1，然后生产者和消费者的并发设置也是 1。 mqadmin update Topic -t AllOrder -c DefaultCluster -r 1 -w 1 -n 127.0.0.1:9876 在证券处理中，以人民币兑换美元为 Topic，在价格相同的情况下，先出价者优先处理，则可以按照 FIFO 的方式发布和消费全局顺序消息。 1.2、部分顺序消息 对于指定的一个 Topic，所有消息根据 Sharding Key 进行区块分区。同一个分区内的消息按照严格的 FIFO 顺序进行发布和消费。Sharding Key 是顺 序消息中用来区分不同分区的关键字段，和普通消息的 Key 是完全不同的概念。 二、延时消息 2.1、概念介绍 延时消息：Producer 将消息发送到消息队列 RocketMQ 服务端，但并不期望这条消息立马投递，而是延迟一定时间后才投递到 Consumer 进行消费， 该消息即延时消息。 2.2、适用场景 消息生产和消费有时间窗口要求：比如在电商交易中超时未支付关闭订单的场景，在订单创建时会发送一条延时消息。这条消息将会在 30 分钟以 后投递给消费者，消费者收到此消息后需要判断对应的订单是否已完成支付。 如支付未完成，则关闭订单。如已完成支付则忽略。 2.3、使用方式 Apache RocketMQ 目前只支持固定精度的定时消息，因为如果要支持任意的时间精度，在 Broker 层面，必须要做消息排序，如果再涉及到持久化， 那么消息排序要不可避免的产生巨大性能开销。（阿里云 RocketMQ 提供了任意时刻的定时消息功能，Apache 的 RocketMQ 并没有,阿里并没有开源） 发送延时消息时需要设定一个延时时间长度，消息将从当前发送时间点开始延迟固定时间之后才开始投递。 延迟消息是根据延迟队列的 level 来的，延迟队列默认是 **msg.setDelayTimeLevel(5)**代表延迟一分钟 &quot;1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h&quot; 是这 18 个等级（秒（s）、分（m）、小时（h）），level 为 1，表示延迟 1 秒后消费，level 为 5 表示延迟 1 分钟后消费，level 为 18 表示延迟 2 个 小时消费。生产消息跟普通的生产消息类似，只需要在消息上设置延迟队列的 level 即可。消费消息跟普通的消费消息一致。 三、死信队列 3.1、概念介绍 死信队列用于处理无法被正常消费的消息。当一条消息初次消费失败，消息队列 MQ 会自动进行消息重试；达到最大重试次数后，若消费依然失败， 则表明Consumer 在正常情况下无法正确地消费该消息。此时，消息队列MQ不会立刻将消息丢弃，而是将这条消息发送到该 Consumer 对应的特殊队列中。 消息队列 MQ 将这种正常情况下无法被消费的消息称为死信消息（Dead-Letter Message），将存储死信消息的特殊队列称为死信队列 （Dead-Letter Queue)。 3.2适用场景 3.2.1、死信消息的特性 不会再被消费者正常消费。 有效期与正常消息相同，均为 3 天，3 天后会被自动删除。因此，请在死信消息产生后的 3 天内及时处理。 3.2.2、死信队列的特性 一个死信队列对应一个 Group ID， 而不是对应单个消费者实例。 如果一个 Group ID 未产生死信消息，消息队列 MQ 不会为其创建相应的死信队列。 一个死信队列包含了对应 Group ID 产生的所有死信消息，不论该消息属于哪个 Topic。 消息队列 MQ 控制台提供对死信消息的查询的功能。 一般控制台直接查看死信消息会报错。 进入RocketMQ中服务器对应的 RocketMQ 中的/bin 目录，执行以下脚本 sh mqadmin updateTopic -b 192.168.0.128:10911 -n 192.168.0.128:9876 -t %DLQ%group1 -p 6 四、消费幂等 为了防止消息重复消费导致业务处理异常，消息队列 MQ 的消费者在接收到消息后，有必要根据业务上的唯一 Key 对消息做幂等处理。本文介绍消息幂 等的概念、适用场景以及处理方法。 4.1、什么是消息幂等 当出现消费者对某条消息重复消费的情况时，重复消费的结果与消费一次的结果是相同的，并且多次消费并未对业务系统产生任何负面影响，那么 这整个过程就实现可消息幂等。 例如，在支付场景下，消费者消费扣款消息，对一笔订单执行扣款操作，扣款金额为 100 元。如果因网络不稳定等原因导致扣款消息重复投递，消 费者重复消费了该扣款消息，但最终的业务结果是只扣款一次，扣费 100 元，且用户的扣款记录中对应的订单只有一条扣款流水，不会多次扣除费用。 那么这次扣款操作是符合要求的，整个消费过程实现了消费幂等。 4.2、需要处理的场景 在互联网应用中，尤其在网络不稳定的情况下，消息队列 MQ 的消息有可能会出现重复。如果消息重复会影响您的业务处理，请对消息做幂等处理。 消息重复的场景如下： **1. 发送时消息重复 ** 当一条消息已被成功发送到服务端并完成持久化，此时出现了网络闪断或者客户端宕机，导致服务端对客户端应答失败。 如果此时生产者意识到消 息发送失败并尝试再次发送消息，消费者后续会收到两条内容相同并且 Message ID 也相同的消息。 **2. 投递时消息重复 ** 消息消费的场景下，消息已投递到消费者并完成业务处理，当客户端给服务端反馈应答的时候网络闪断。为了保证消息至少被消费一次，消息队列 MQ 的服务端将在网络恢复后再次尝试投递之前已被处理过的消息，消费者后续会收到两条内容相同并且 Message ID 也相同的消息。 3. 负载均衡时消息重复（包括但不限于网络抖动、Broker 重启以及消费者应用重启） 当消息队列 MQ 的 Broker 或客户端重启、扩容或缩容时，会触发 Rebalance，此时消费者可能会收到重复消息。 4.3、处理方法 因为 Message ID 有可能出现冲突（重复）的情况，所以真正安全的幂等处理，不建议以 Message ID 作为处理依据。最好的方式是以业务唯一标识 作为幂等处理的关键依据，而业务的唯一标识可以通过消息 Key 设置。 以支付场景为例，可以将消息的 Key 设置为订单号，作为幂等处理的依据。具体代码示例如下： Message message = new Message(); message.setKey(&quot;ORDERID_100&quot;); SendResult sendResult = producer.send(message); 消费者收到消息时可以根据消息的 Key，即订单号来实现消息幂等： consumer.subscribe(&quot;ons_test&quot;, &quot;*&quot;, new MessageListener() { public Action consume(Message message, ConsumeContext context) { String key = message.getKey() // 根据业务唯一标识的 Key 做幂等处理 } }); ","link":"https://tinaxiawuhao.github.io/post/BvbuvYGnx/"},{"title":"CountDownLatch,CyclicBarrier,Semaphore的用法和区别","content":"CountDownLatch CountDownLatch（也叫闭锁）是一个同步协助类，允许一个或多个线程等待，直到其他线程完成操作集。 CountDownLatch 使用给定的计数值（count）初始化。await 方法会阻塞直到当前的计数值（count）由于 countDown 方法的调用达到 0，count 为 0 之后所有等待的线程都会被释放，并且随后对await方法的调用都会立即返回。 构造方法 //参数count为计数值 public CountDownLatch(int count) {}; 常用方法 // 调用 await() 方法的线程会被挂起，它会等待直到 count 值为 0 才继续执行 public void await() throws InterruptedException {}; // 和 await() 类似，若等待 timeout 时长后，count 值还是没有变为 0，不再等待，继续执行 public boolean await(long timeout, TimeUnit unit) throws InterruptedException {}; // 会将 count 减 1，直至为 0 public void countDown() {}; 使用案例 首先是创建实例 CountDownLatch countDown = new CountDownLatch(2)； 需要同步的线程执行完之后，计数 -1， countDown.countDown()； 需要等待其他线程执行完毕之后，再运行的线程，调用 countDown.await()实现阻塞同步。 应用场景 CountDownLatch 一般用作多线程倒计时计数器，强制它们等待其他一组（CountDownLatch的初始化决定）任务执行完成。 CountDownLatch的两种使用场景： 让多个线程等待，模拟并发。 让单个线程等待，多个线程（任务）完成后，进行汇总合并。 场景 1：模拟并发 import java.util.concurrent.CountDownLatch; /** * 让多个线程等待：模拟并发，让并发线程一起执行 */ public class CountDownLatchTest { public static void main(String[] args) throws InterruptedException { CountDownLatch countDownLatch = new CountDownLatch(1); for (int i = 0; i &lt; 5; i++) { new Thread(() -&gt; { try { // 等待 countDownLatch.await(); String parter = &quot;【&quot; + Thread.currentThread().getName() + &quot;】&quot;; System.out.println(parter + &quot;开始执行……&quot;); } catch (InterruptedException e) { e.printStackTrace(); } }).start(); } Thread.sleep(2000); countDownLatch.countDown(); } } 场景 2：多个线程完成后，进行汇总合并 很多时候，我们的并发任务，存在前后依赖关系；比如数据详情页需要同时调用多个接口获取数据，并发请求获取到数据后、需要进行结果合并；或者多个数据操作完成后，需要数据 check；这其实都是：在多个线程(任务)完成后，进行汇总合并的场景。 import java.util.Map; import java.util.concurrent.ConcurrentHashMap; import java.util.concurrent.CountDownLatch; /** * 让单个线程等待：多个线程(任务)完成后，进行汇总合并 */ public class CountDownLatchTest3 { //用于聚合所有的统计指标 private static Map map = new ConcurrentHashMap(); //创建计数器，这里需要统计4个指标 private static CountDownLatch countDownLatch = new CountDownLatch(4); public static void main(String[] args) throws Exception { //记录开始时间 long startTime = System.currentTimeMillis(); Thread countUserThread = new Thread(() -&gt; { try { System.out.println(&quot;正在统计新增用户数量&quot;); Thread.sleep(3000);//任务执行需要3秒 map.put(&quot;userNumber&quot;, 100);//保存结果值 System.out.println(&quot;统计新增用户数量完毕&quot;); countDownLatch.countDown();//标记已经完成一个任务 } catch (InterruptedException e) { e.printStackTrace(); } }); Thread countOrderThread = new Thread(() -&gt; { try { System.out.println(&quot;正在统计订单数量&quot;); Thread.sleep(3000);//任务执行需要3秒 map.put(&quot;countOrder&quot;, 20);//保存结果值 System.out.println(&quot;统计订单数量完毕&quot;); countDownLatch.countDown();//标记已经完成一个任务 } catch (InterruptedException e) { e.printStackTrace(); } }); Thread countGoodsThread = new Thread(() -&gt; { try { System.out.println(&quot;正在商品销量&quot;); Thread.sleep(3000);//任务执行需要3秒 map.put(&quot;countGoods&quot;, 300);//保存结果值 System.out.println(&quot;统计商品销量完毕&quot;); countDownLatch.countDown();//标记已经完成一个任务 } catch (InterruptedException e) { e.printStackTrace(); } }); Thread countmoneyThread = new Thread(() -&gt; { try { System.out.println(&quot;正在总销售额&quot;); Thread.sleep(3000);//任务执行需要3秒 map.put(&quot;countMoney&quot;, 40000);//保存结果值 System.out.println(&quot;统计销售额完毕&quot;); countDownLatch.countDown();//标记已经完成一个任务 } catch (InterruptedException e) { e.printStackTrace(); } }); //启动子线程执行任务 countUserThread.start(); countGoodsThread.start(); countOrderThread.start(); countmoneyThread.start(); try { //主线程等待所有统计指标执行完毕 countDownLatch.await(); long endTime = System.currentTimeMillis();//记录结束时间 System.out.println(&quot;------统计指标全部完成--------&quot;); System.out.println(&quot;统计结果为：&quot; + map); System.out.println(&quot;任务总执行时间为&quot; + (endTime - startTime) + &quot;ms&quot;); } catch (InterruptedException e) { e.printStackTrace(); } } } CylicBarrier 从字面上的意思可以知道，这个类的中文意思是“循环栅栏”。大概的意思就是一个可循环利用的屏障。 它的作用就是会让所有线程都等待完成后才会继续下一步行动。 现实生活中我们经常会遇到这样的情景，在进行某个活动前需要等待人全部都齐了才开始。例如吃饭时要等全家人都上座了才动筷子，旅游时要等全部人都到齐了才出发，比赛时要等运动员都上场后才开始。 在JUC包中为我们提供了一个同步工具类能够很好的模拟这类场景，它就是CyclicBarrier类。利用CyclicBarrier类可以实现一组线程相互等待，当所有线程都到达某个屏障点后再进行后续的操作。 CyclicBarrier字面意思是“可重复使用的栅栏”，CyclicBarrier相比 CountDownLatch来说，要简单很多，其源码没有什么高深的地方，它是 ReentrantLock 和 Condition 的组合使用。 看如下示意图，CyclicBarrier 和 CountDownLatch 是不是很像，只是 CyclicBarrier 可以有不止一个栅栏，因为它的栅栏（Barrier）可以重复使用（Cyclic）。 就好比以前的那种客车一样，当第一轮车坐满之后发车，然后接着等第二辆车坐满之后在发车。 构造方法 // parties表示屏障拦截的线程数量，每个线程调用 await 方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。 public CyclicBarrier(int parties) // 用于在线程到达屏障时，优先执行 barrierAction，方便处理更复杂的业务场景(该线程的执行时机是在到达屏障之后再执行) public CyclicBarrier(int parties, Runnable barrierAction) 常用方法 //屏障 指定数量的线程全部调用await()方法时，这些线程不再阻塞 // BrokenBarrierException 表示栅栏已经被破坏，破坏的原因可能是其中一个线程 await() 时被中断或者超时 public int await() throws InterruptedException, BrokenBarrierException public int await(long timeout, TimeUnit unit) throws InterruptedException, B rokenBarrierException, TimeoutException //循环 通过reset()方法可以进行重置 public void reset() 使用案例 import java.util.concurrent.CyclicBarrier; /** * CyclicBarrier(回环栅栏)允许一组线程互相等待，直到到达某个公共屏障点 (Common Barrier Point) * CountDownLatch 用于等待countDown事件，而栅栏用于等待其他线程。 */ public class CyclicBarrierTest { public static void main(String[] args) { CyclicBarrier cyclicBarrier = new CyclicBarrier(3); for (int i = 0; i &lt; 5; i++) { new Thread(new Runnable() { @Override public void run() { try { System.out.println(Thread.currentThread().getName() + &quot;开始等待其他线程&quot;); cyclicBarrier.await(); System.out.println(Thread.currentThread().getName() + &quot;开始执行&quot;); //TODO 模拟业务处理 Thread.sleep(5000); System.out.println(Thread.currentThread().getName() + &quot;执行完毕&quot;); } catch (Exception e) { e.printStackTrace(); } } }).start(); } } } 应用场景 可以用于多线程计算数据，最后合并计算结果的场景。 import java.util.Set; import java.util.concurrent.*; public class CyclicBarrierTest2 { //保存每个学生的平均成绩 private ConcurrentHashMap&lt;String, Integer&gt; map = new ConcurrentHashMap&lt;String, Integer&gt;(); private ExecutorService threadPool = Executors.newFixedThreadPool(3); private CyclicBarrier cb = new CyclicBarrier(3, () -&gt; { int result = 0; Set&lt;String&gt; set = map.keySet(); for (String s : set) { result += map.get(s); } System.out.println(&quot;三人平均成绩为:&quot; + (result / 3) + &quot;分&quot;); }); public void count() { for (int i = 0; i &lt; 3; i++) { threadPool.execute(new Runnable() { @Override public void run() { //获取学生平均成绩 int score = (int) (Math.random() * 40 + 60); map.put(Thread.currentThread().getName(), score); System.out.println(Thread.currentThread().getName() + &quot;同学的平均成绩为：&quot; + score); try { //执行完运行await(),等待所有学生平均成绩都计算完毕 cb.await(); } catch (InterruptedException | BrokenBarrierException e) { e.printStackTrace(); } } }); } } public static void main(String[] args) { CyclicBarrierTest2 cb = new CyclicBarrierTest2(); cb.count(); } } 测试结果： Semaphore Semaphore，俗称信号量，基于 AbstractQueuedSynchronizer 实现。使用 Semaphore 可以控制同时访问资源的线程个数。 比如：停车场入口立着的那个显示屏，每有一辆车进入停车场显示屏就会显示剩余车位减 1，每有一辆车从停车场出去，显示屏上显示的剩余车辆就会加 1，当显示屏上的剩余车位为 0 时，停车场入口的栏杆就不会再打开，车辆就无法进入停车场了，直到有一辆车从停车场出去为止。 比如：在学生时代都去餐厅打过饭，假如有 3 个窗口可以打饭，同一时刻也只能有 3 名同学打饭。第 4 个人来了之后就必须在外面等着，只要有打饭的同学好了，就可以去相应的窗口了 。 构造方法 //创建具有给定的许可数和非公平的公平设置的 Semaphore。 Semaphore(int permits) //创建具有给定的许可数和给定的公平设置的 Semaphore。 Semaphore(int permits, boolean fair) permits 表示许可证的数量（资源数），就好比一个学生可以占用 3 个打饭窗口。 fair 表示公平性，如果这个设为 true 的话，下次执行的线程会是等待最久的线程。 常用方法 public void acquire() throws InterruptedException public boolean tryAcquire() public void release() public int availablePermits() public final int getQueueLength() public final boolean hasQueuedThreads() protected void reducePermits(int reduction) protected Collection&lt;Thread&gt; getQueuedThreads() acquire()：表示阻塞并获取许可。 tryAcquire()：方法在没有许可的情况下会立即返回 false，要获取许可的线程不会阻塞。 release()：表示释放许可。 int availablePermits()：返回此信号量中当前可用的许可证数。 int getQueueLength()：返回正在等待获取许可证的线程数。 boolean hasQueuedThreads()：是否有线程正在等待获取许可证。 void reducePermit(int reduction)：减少 reduction 个许可证。 Collection getQueuedThreads()：返回所有等待获取许可证的线程集合。 使用案例 我们可以模拟车站买票，假如车站有 3 个窗口售票，那么同一时刻每个窗口只能存在一个人买票，其他人则等待前面的人完成后才可以去买票。 import java.util.concurrent.Semaphore; public class SemaphoreTest { public static void main(String[] args) { // 3 个窗口 Semaphore windows = new Semaphore(3); // 模拟 5 个人购票 for (int i = 0; i &lt; 5; i++) { new Thread(new Runnable() { @Override public void run() { // 占用窗口，加锁 try { windows.acquire(); System.out.println(Thread.currentThread().getName() + &quot;：开始购票&quot;); // 买票 Thread.sleep(5000); System.out.println(Thread.currentThread().getName() + &quot;：购票成功&quot;); } catch (InterruptedException e) { e.printStackTrace(); } finally { // 释放许可，释放窗口 windows.release(); } } }, &quot;Thread&quot; + i).start(); } } } 测试结果如下： 很明显可以看到当前面 3 个线程购票成功之后，剩余的线程再开始购票。 应用场景 可以用于做流量控制，特别是公用资源有限的应用场景。 如我们实现一个同时只能处理 5 个请求的限流器。 import java.util.concurrent.LinkedBlockingDeque; import java.util.concurrent.Semaphore; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; public class SemaphoneTest2 { /** * 实现一个同时只能处理5个请求的限流器 */ private static Semaphore semaphore = new Semaphore(5); /** * 定义一个线程池 * 0 */ private static ThreadPoolExecutor executor = new ThreadPoolExecutor(10, 50, 1 , TimeUnit.SECONDS, new LinkedBlockingDeque&lt;&gt;(200)); /** * 模拟执行方法 */ public static void exec() { try { semaphore.acquire(1); // 模拟真实方法执行 System.out.println(&quot;执行exec方法&quot;); Thread.sleep(2000); } catch (Exception e) { e.printStackTrace(); } finally { semaphore.release(1); } } public static void main(String[] args) throws InterruptedException { { for (;;) { Thread.sleep(100); // 模拟请求以10个/s的速度 executor.execute(() -&gt; exec()); } } } } 总结 1、CountDownLatch、CyclicBarrier、Semaphore的区别 CountDownLatch 和 CyclicBarrier 都能够实现线程之间的等待，只不过它们侧重点不同： CountDownLatch 一般用于某个线程 A 等待若干个其他线程执行完任务之后，它才执行； 而CyclicBarrier一般用于一组线程互相等待至某个状态，然后这一组线程再同时执行； 另外，CountDownLatch是不能够重用的，而 CyclicBarrier 是可以重用的（reset）。 Semaphore和锁有点类似，它一般用于控制对某组资源的访问权限。 2、CountDownLatch 与 Thread.join 的区别 CountDownLatch 的作用就是允许一个或多个线程等待其他线程完成操作，看起来有点类似 join() 方法，但其提供了比 join() 更加灵活的API。 CountDownLatch 可以手动控制在n个线程里调用 n 次 countDown()方法使计数器进行减一操作，也可以在一个线程里调用 n 次执行减一操作。 而 join() 的实现原理是不停检查 join 线程是否存活，如果 join 线程存活则让当前线程永远等待。所以两者之间相对来说还是 CountDownLatch 使用起来较为灵活。 3、CyclicBarrier 与 CountDownLatch 区别 CountDownLatch的计数器只能使用一次，而CyclicBarrier的计数器可以使用reset()方法重置。所以CyclicBarrier能处理更为复杂的业务场景，比如如果计算发生错误，可以重置计数器，并让线程们重新执行一次。 CyclicBarrier还提供getNumberWaiting(可以获得CyclicBarrier阻塞的线程数量)、isBroken(用来知道阻塞的线程是否被中断)等方法。 CountDownLatch会阻塞主线程，CyclicBarrier不会阻塞主线程，只会阻塞子线程。 CountDownLatch和CyclicBarrier都能够实现线程之间的等待，只不过它们侧重点不同。CountDownLatch一般用于一个或多个线程，等待其他线程执行完任务后，再执行。CyclicBarrier一般用于一组线程互相等待至某个状态，然后这一组线程再同时执行。 CyclicBarrier 还可以提供一个 barrierAction，合并多线程计算结果。 CyclicBarrier是通过ReentrantLock的&quot;独占锁&quot;和Conditon来实现一组线程的阻塞唤醒的，而CountDownLatch则是通过AQS的“共享锁”实现。 ","link":"https://tinaxiawuhao.github.io/post/bPYM5y-vS/"},{"title":"异步编程的 7 种实现方式","content":"早期的系统是同步的，容易理解，我们来看个例子 同步编程 当用户创建一笔电商交易订单时，要经历的业务逻辑流程还是很长的，每一步都要耗费一定的时间，那么整体的RT就会比较长。 于是，聪明的人们开始思考能不能将一些非核心业务从主流程中剥离出来，于是有了异步编程雏形。 异步编程是让程序并发运行的一种手段。它允许多个事件同时发生，当程序调用需要长时间运行的方法时，它不会阻塞当前的执行流程，程序可以继续运行。 核心思路：采用多线程优化性能，将串行操作变成并行操作。异步模式设计的程序可以显著减少线程等待，从而在高吞吐量场景中，极大提升系统的整体性能，显著降低时延。 接下来，我们来讲下异步有哪些编程实现方式 一、线程 Thread 直接继承 Thread类 是创建异步线程最简单的方式。 首先，创建Thread子类，普通类或匿名内部类方式；然后创建子类实例；最后通过start()方法启动线程。 public class AsyncThread extends Thread{ @Override public void run() { System.out.println(&quot;当前线程名称:&quot; + this.getName() + &quot;, 执行线程名称:&quot; + Thread.currentThread().getName() + &quot;-hello&quot;); } } public static void main(String[] args) { // 模拟业务流程 // ....... // 创建异步线程 AsyncThread asyncThread = new AsyncThread(); // 启动异步线程 asyncThread.start(); } 当然如果每次都创建一个 Thread线程，频繁的创建、销毁，浪费系统资源。我们可以采用线程池 @Bean(name = &quot;executorService&quot;) public ExecutorService downloadExecutorService() { return new ThreadPoolExecutor(20, 40, 60, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(2000), new ThreadFactoryBuilder().setNameFormat(&quot;defaultExecutorService-%d&quot;).build(), (r, executor) -&gt; log.error(&quot;defaultExecutor pool is full! &quot;)); } 将业务逻辑封装到 Runnable 或 Callable 中，交由 线程池 来执行 二、Future 上述方式虽然达到了多线程并行处理，但有些业务不仅仅要执行过程，还要获取执行结果。 Java 从1.5版本开始，提供了 Callable 和 Future，可以在任务执行完毕之后得到任务执行结果。 当然也提供了其他功能，如：取消任务、查询任务是否完成等 Future类位于java.util.concurrent包下，接口定义： public interface Future&lt;V&gt; { boolean cancel(boolean mayInterruptIfRunning); boolean isCancelled(); boolean isDone(); V get() throws InterruptedException, ExecutionException; V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; } 方法描述： cancel()：取消任务，如果取消任务成功返回true，如果取消任务失败则返回false isCancelled()：表示任务是否被取消成功，如果在任务正常完成前被取消成功，则返回 true isDone()：表示任务是否已经完成，如果完成，返回true get()：获取执行结果，这个方法会产生阻塞，会一直等到任务执行完毕才返回 get(long timeout, TimeUnit unit)：用来获取执行结果，如果在指定时间内，还没获取到结果，就直接返回null 代码示例： public class CallableAndFuture { public static ExecutorService executorService = new ThreadPoolExecutor(4, 40, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(1024), new ThreadFactoryBuilder() .setNameFormat(&quot;demo-pool-%d&quot;).build(), new ThreadPoolExecutor.AbortPolicy()); static class MyCallable implements Callable&lt;String&gt; { @Override public String call() throws Exception { return &quot;异步处理，Callable 返回结果&quot;; } } public static void main(String[] args) { Future&lt;String&gt; future = executorService.submit(new MyCallable()); try { System.out.println(future.get()); } catch (Exception e) { // nodo } finally { executorService.shutdown(); } } } Future 表示一个可能还没有完成的异步任务的结果，通过 get 方法获取执行结果，该方法会阻塞直到任务返回结果。 三、FutureTask FutureTask 实现了 RunnableFuture 接口，则 RunnableFuture 接口继承了 Runnable 接口和 Future 接口，所以可以将 FutureTask 对象作为任务提交给 ThreadPoolExecutor 去执行，也可以直接被 Thread 执行；又因为实现了 Future 接口，所以也能用来获得任务的执行结果。 FutureTask 构造函数： public FutureTask(Callable&lt;V&gt; callable) public FutureTask(Runnable runnable, V result) FutureTask 常用来封装 Callable 和 Runnable，可以作为一个任务提交到线程池中执行。除了作为一个独立的类之外，也提供了一些功能性函数供我们创建自定义 task 类使用。 FutureTask 线程安全由CAS来保证。 ExecutorService executor = Executors.newCachedThreadPool(); // FutureTask包装callbale任务，再交给线程池执行 FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;&gt;(() -&gt; { System.out.println(&quot;子线程开始计算：&quot;); Integer sum = 0; for (int i = 1; i &lt;= 100; i++) sum += i; return sum; }); // 线程池执行任务， 运行结果在 futureTask 对象里面 executor.submit(futureTask); try { System.out.println(&quot;task运行结果计算的总和为：&quot; + futureTask.get()); } catch (Exception e) { e.printStackTrace(); } executor.shutdown(); Callable 和 Future 的区别：Callable 用于产生结果，Future 用于获取结果 如果是对多个任务多次自由串行、或并行组合，涉及多个线程之间同步阻塞获取结果，Future 代码实现会比较繁琐，需要我们手动处理各个交叉点，很容易出错。 四、异步框架 CompletableFuture Future 类通过 get() 方法阻塞等待获取异步执行的运行结果，性能比较差。 JDK1.8 中，Java 提供了 CompletableFuture 类，它是基于异步函数式编程。相对阻塞式等待返回结果，CompletableFuture 可以通过回调的方式来处理计算结果，实现了异步非阻塞，性能更优。 优点： 异步任务结束时，会自动回调某个对象的方法 异步任务出错时，会自动回调某个对象的方法 主线程设置好回调后，不再关心异步任务的执行 泡茶示例： (内容摘自：极客时间的《Java 并发编程实战》) //任务1：洗水壶-&gt;烧开水 CompletableFuture&lt;Void&gt; f1 = CompletableFuture.runAsync(() -&gt; { System.out.println(&quot;T1:洗水壶...&quot;); sleep(1, TimeUnit.SECONDS); System.out.println(&quot;T1:烧开水...&quot;); sleep(15, TimeUnit.SECONDS); }); //任务2：洗茶壶-&gt;洗茶杯-&gt;拿茶叶 CompletableFuture&lt;String&gt; f2 = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;T2:洗茶壶...&quot;); sleep(1, TimeUnit.SECONDS); System.out.println(&quot;T2:洗茶杯...&quot;); sleep(2, TimeUnit.SECONDS); System.out.println(&quot;T2:拿茶叶...&quot;); sleep(1, TimeUnit.SECONDS); return &quot;龙井&quot;; }); //任务3：任务1和任务2完成后执行：泡茶 CompletableFuture&lt;String&gt; f3 = f1.thenCombine(f2, (__, tf) -&gt; { System.out.println(&quot;T1:拿到茶叶:&quot; + tf); System.out.println(&quot;T1:泡茶...&quot;); return &quot;上茶:&quot; + tf; }); //等待任务3执行结果 System.out.println(f3.join()); } CompletableFuture 提供了非常丰富的API，大约有50种处理串行，并行，组合以及处理错误的方法。 五、 SpringBoot 注解 @Async 除了硬编码的异步编程处理方式，SpringBoot 框架还提供了 注解式 解决方案，以 方法体 为边界，方法体内部的代码逻辑全部按异步方式执行。 首先，使用 @EnableAsync 启用异步注解 @SpringBootApplication @EnableAsync public class StartApplication { public static void main(String[] args) { SpringApplication.run(StartApplication.class, args); } } 自定义线程池： @Configuration @Slf4j public class ThreadPoolConfiguration { @Bean(name = &quot;defaultThreadPoolExecutor&quot;, destroyMethod = &quot;shutdown&quot;) public ThreadPoolExecutor systemCheckPoolExecutorService() { return new ThreadPoolExecutor(3, 10, 60, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(10000), new ThreadFactoryBuilder().setNameFormat(&quot;default-executor-%d&quot;).build(), (r, executor) -&gt; log.error(&quot;system pool is full! &quot;)); } } 在异步处理的方法上添加注解 @Async ，当对 execute 方法 调用时，通过自定义的线程池 defaultThreadPoolExecutor 异步化执行 execute 方法 @Service public class AsyncServiceImpl implements AsyncService { @Async(&quot;defaultThreadPoolExecutor&quot;) public Boolean execute(Integer num) { System.out.println(&quot;线程：&quot; + Thread.currentThread().getName() + &quot; , 任务：&quot; + num); return true; } } 用 @Async 注解标记的方法，称为异步方法。在spring boot应用中使用 @Async 很简单： 调用异步方法类上或者启动类加上注解 @EnableAsync 在需要被异步调用的方法外加上 @Async 所使用的 @Async 注解方法的类对象应该是Spring容器管理的bean对象； 六、Spring ApplicationEvent 事件 事件机制在一些大型项目中被经常使用，Spring 专门提供了一套事件机制的接口，满足了架构原则上的解耦。 ApplicationContext 通过 ApplicationEvent 类和 ApplicationListener 接口进行事件处理。如果将实现 ApplicationListener 接口的 bean 注入到上下文中，则每次使用 ApplicationContext 发布 ApplicationEvent 时，都会通知该 bean。本质上，这是标准的观察者设计模式。 ApplicationEvent 是由 Spring 提供的所有 Event 类的基类 首先，自定义业务事件子类，继承自 ApplicationEvent，通过泛型注入业务模型参数类。相当于 MQ 的消息体。 public class OrderEvent extends AbstractGenericEvent&lt;OrderModel&gt; { public OrderEvent(OrderModel source) { super(source); } } 然后，编写事件监听器。ApplicationListener 接口是由 Spring 提供的事件订阅者必须实现的接口，我们需要定义一个子类，继承 ApplicationListener。相当于 MQ 的消费端 @Component public class OrderEventListener implements ApplicationListener&lt;OrderEvent&gt; { @Override public void onApplicationEvent(OrderEvent event) { System.out.println(&quot;【OrderEventListener】监听器处理！&quot; + JSON.toJSONString(event.getSource())); } } 最后，发布事件，把某个事件告诉所有与这个事件相关的监听器。相当于 MQ 的生产端。 OrderModel orderModel = new OrderModel(); orderModel.setOrderId((long) i); orderModel.setBuyerName(&quot;Tom-&quot; + i); orderModel.setSellerName(&quot;judy-&quot; + i); orderModel.setAmount(100L); // 发布Spring事件通知 SpringUtils.getApplicationContext().publishEvent(new OrderEvent(orderModel)); 加个餐： [消费端]线程：http-nio-8090-exec-1，消费事件 {&quot;amount&quot;:100.0,&quot;buyerName&quot;:&quot;Tom-1&quot;,&quot;orderId&quot;:1,&quot;sellerName&quot;:&quot;judy-1&quot;} [生产端]线程：http-nio-8090-exec-1，发布事件 1 [消费端]线程：http-nio-8090-exec-1，消费事件 {&quot;amount&quot;:100.0,&quot;buyerName&quot;:&quot;Tom-2&quot;,&quot;orderId&quot;:2,&quot;sellerName&quot;:&quot;judy-2&quot;} [生产端]线程：http-nio-8090-exec-1，发布事件 2 [消费端]线程：http-nio-8090-exec-1，消费事件 {&quot;amount&quot;:100.0,&quot;buyerName&quot;:&quot;Tom-3&quot;,&quot;orderId&quot;:3,&quot;sellerName&quot;:&quot;judy-3&quot;} [生产端]线程：http-nio-8090-exec-1，发布事件 3 上面是跑了个demo的运行结果，我们发现无论生产端还是消费端，使用了同一个线程 http-nio-8090-exec-1，Spring 框架的事件机制默认是同步阻塞的。只是在代码规范方面做了解耦，有较好的扩展性，但底层还是采用同步调用方式。 那么问题来了，如果想实现异步调用，如何处理？ 我们需要手动创建一个 SimpleApplicationEventMulticaster，并设置 TaskExecutor，此时所有的消费事件采用异步线程执行。 @Component public class SpringConfiguration { @Bean public SimpleApplicationEventMulticaster applicationEventMulticaster(@Qualifier(&quot;defaultThreadPoolExecutor&quot;) ThreadPoolExecutor defaultThreadPoolExecutor) { SimpleApplicationEventMulticaster simpleApplicationEventMulticaster = new SimpleApplicationEventMulticaster(); simpleApplicationEventMulticaster.setTaskExecutor(defaultThreadPoolExecutor); return simpleApplicationEventMulticaster; } } 我们看下改造后的运行结果： [生产端]线程：http-nio-8090-exec-1，发布事件 1 [生产端]线程：http-nio-8090-exec-1，发布事件 2 [生产端]线程：http-nio-8090-exec-1，发布事件 3 [消费端]线程：default-executor-1，消费事件 {&quot;amount&quot;:100.0,&quot;buyerName&quot;:&quot;Tom-2&quot;,&quot;orderId&quot;:2,&quot;sellerName&quot;:&quot;judy-2&quot;} [消费端]线程：default-executor-2，消费事件 {&quot;amount&quot;:100.0,&quot;buyerName&quot;:&quot;Tom-1&quot;,&quot;orderId&quot;:1,&quot;sellerName&quot;:&quot;judy-1&quot;} [消费端]线程：default-executor-0，消费事件 {&quot;amount&quot;:100.0,&quot;buyerName&quot;:&quot;Tom-3&quot;,&quot;orderId&quot;:3,&quot;sellerName&quot;:&quot;judy-3&quot;} SimpleApplicationEventMulticaster 这个我们自己实例化的 Bean 与系统默认的加载顺序如何？会不会有冲突？ 查了下 Spring 源码，处理逻辑在 AbstractApplicationContext#initApplicationEventMulticaster 方法中，通过 beanFactory 查找是否有自定义的 Bean，如果没有，容器会自己 new 一个 SimpleApplicationEventMulticaster 对象注入到容器中。 代码地址：https://github.com/aalansehaiyang/wx-project 七、消息队列 异步架构是互联网系统中一种典型架构模式，与同步架构相对应。而消息队列天生就是这种异步架构，具有超高吞吐量和超低时延。 消息队列异步架构的主要角色包括消息生产者、消息队列和消息消费者。 消息生产者就是主应用程序，生产者将调用请求封装成消息发送给消息队列。 消息队列的职责就是缓冲消息，等待消费者消费。根据消费方式又分为点对点模式和发布订阅模式两种。 消息消费者，用来从消息队列中拉取、消费消息，完成业务逻辑处理。 当然市面上消息队列框架非常多，常见的有RabbitMQ、Kafka、RocketMQ、ActiveMQ 和 Pulsar 等 不同的消息队列的功能特性会略有不同，但整体架构类似，这里就不展开了。 我们只需要记住一个关键点，借助消息队列这个中间件可以高效的实现异步编程。 ","link":"https://tinaxiawuhao.github.io/post/9hE3Lg3SU/"},{"title":"RabbitMQ","content":"思维导图： 1. 消息队列 1.1 消息队列模式 消息队列目前主要 2 种模式，分别为“点对点模式”和“发布/订阅模式”。 1.1.1 点对点模式 一个具体的消息只能由一个消费者消费，多个生产者可以向同一个消息队列发送消息，但是一个消息在被一个消息者处理的时候，这个消息在队列上会被锁住或者被移除并且其他消费者无法处理该消息。 需要额外注意的是，如果消费者处理一个消息失败了，消息系统一般会把这个消息放回队列，这样其他消费者可以继续处理。 1.1.2 发布/订阅模式 单个消息可以被多个订阅者并发的获取和处理。一般来说，订阅有两种类型： 临时（ephemeral）订阅：这种订阅只有在消费者启动并且运行的时候才存在。一旦消费者退出，相应的订阅以及尚未处理的消息就会丢失。 持久（durable）订阅：这种订阅会一直存在，除非主动去删除。消费者退出后，消息系统会继续维护该订阅，并且后续消息可以被继续处理。 1.2 衡量标准 对消息队列进行技术选型时，需要通过以下指标衡量你所选择的消息队列，是否可以满足你的需求： 消息顺序：发送到队列的消息，消费时是否可以保证消费的顺序，比如A先下单，B后下单，应该是A先去扣库存，B再去扣，顺序不能反。 消息路由：根据路由规则，只订阅匹配路由规则的消息，比如有A/B两者规则的消息，消费者可以只订阅A消息，B消息不会消费。 消息可靠性：是否会存在丢消息的情况，比如有A/B两个消息，最后只有B消息能消费，A消息丢失。 消息时序：主要包括“消息存活时间”和“延迟/预定的消息”，“消息存活时间”表示生产者可以对消息设置TTL，如果超过该TTL，消息会自动消失；“延迟/预定的消息”指的是可以延迟或者预订消费消息，比如延时5分钟，那么消息会5分钟后才能让消费者消费，时间未到的话，是不能消费的。 消息留存：消息消费成功后，是否还会继续保留在消息队列。 容错性：当一条消息消费失败后，是否有一些机制，保证这条消息是一种能成功，比如异步第三方退款消息，需要保证这条消息消费掉，才能确定给用户退款成功，所以必须保证这条消息消费成功的准确性。 伸缩：当消息队列性能有问题，比如消费太慢，是否可以快速支持库容；当消费队列过多，浪费系统资源，是否可以支持缩容。 吞吐量：支持的最高并发数。 2. RabbitMQ 原理初探 RabbitMQ 2007 年发布，是使用 Erlang 语言开发的开源消息队列系统，基于 AMQP 协议来实现。 2.1 基本概念 提到RabbitMQ，就不得不提AMQP协议。AMQP协议是具有现代特征的二进制协议。是一个提供统一消息服务的应用层标准高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。 先了解一下AMQP协议中间的几个重要概念： Server：接收客户端的连接，实现AMQP实体服务。 Connection：连接，应用程序与Server的网络连接，TCP连接。 Channel：信道，消息读写等操作在信道中进行。客户端可以建立多个信道，每个信道代表一个会话任务。 Message：消息，应用程序和服务器之间传送的数据，消息可以非常简单，也可以很复杂。由Properties和Body组成。Properties为外包装，可以对消息进行修饰，比如消息的优先级、延迟等高级特性；Body就是消息体内容。 Virtual Host：虚拟主机，用于逻辑隔离。一个虚拟主机里面可以有若干个Exchange和Queue，同一个虚拟主机里面不能有相同名称的Exchange或Queue。 Exchange：交换器，接收消息，按照路由规则将消息路由到一个或者多个队列。如果路由不到，或者返回给生产者，或者直接丢弃。RabbitMQ常用的交换器常用类型有direct、topic、fanout、headers四种，后面详细介绍。 Binding：绑定，交换器和消息队列之间的虚拟连接，绑定中可以包含一个或者多个RoutingKey。 RoutingKey：路由键，生产者将消息发送给交换器的时候，会发送一个RoutingKey，用来指定路由规则，这样交换器就知道把消息发送到哪个队列。路由键通常为一个“.”分割的字符串，例如“com.rabbitmq”。 Queue：消息队列，用来保存消息，供消费者消费。 2.2 工作原理 AMQP 协议模型由三部分组成：生产者、消费者和服务端，执行流程如下： 生产者是连接到 Server，建立一个连接，开启一个信道。 生产者声明交换器和队列，设置相关属性，并通过路由键将交换器和队列进行绑定。 消费者也需要进行建立连接，开启信道等操作，便于接收消息。 生产者发送消息，发送到服务端中的虚拟主机。 虚拟主机中的交换器根据路由键选择路由规则，发送到不同的消息队列中。 订阅了消息队列的消费者就可以获取到消息，进行消费。 2.3 常用交换器 RabbitMQ常用的交换器类型有direct、topic、fanout、headers四种： Direct Exchange：见文知意，直连交换机意思是此交换机需要绑定一个队列，要求该消息与一个特定的路由键完全匹配。简单点说就是一对一的，点对点的发送。 Fanout Exchange：这种类型的交换机需要将队列绑定到交换机上。一个发送到交换机的消息都会被转发到与该交换机绑定的所有队列上。很像子网广播，每台子网内的主机都获得了一份复制的消息。简单点说就是发布订阅。 Topic Exchange：直接翻译的话叫做主题交换机，如果从用法上面翻译可能叫通配符交换机会更加贴切。这种交换机是使用通配符去匹配，路由到对应的队列。通配符有两种：&quot;*&quot; 、 &quot;#&quot;。需要注意的是通配符前面必须要加上&quot;.&quot;符号。 *符号：有且只匹配一个词。比如 a.*可以匹配到&quot;a.b&quot;、&quot;a.c&quot;，但是匹配不了&quot;a.b.c&quot;。 #符号：匹配一个或多个词。比如&quot;rabbit.#&quot;既可以匹配到&quot;rabbit.a.b&quot;、&quot;rabbit.a&quot;，也可以匹配到&quot;rabbit.a.b.c&quot;。 Headers Exchange：这种交换机用的相对没这么多。它跟上面三种有点区别，它的路由不是用routingKey进行路由匹配，而是在匹配请求头中所带的键值进行路由。创建队列需要设置绑定的头部信息，有两种模式：全部匹配和部分匹配。如上图所示，交换机会根据生产者发送过来的头部信息携带的键值去匹配队列绑定的键值，路由到对应的队列。 2.4 消费原理 我们先看几个基本概念： broker：每个节点运行的服务程序，功能为维护该节点的队列的增删以及转发队列操作请求。 master queue：每个队列都分为一个主队列和若干个镜像队列。 mirror queue：镜像队列，作为master queue的备份。在master queue所在节点挂掉之后，系统把mirror queue提升为master queue，负责处理客户端队列操作请求。注意，mirror queue只做镜像，设计目的不是为了承担客户端读写压力。 集群中有两个节点，每个节点上有一个broker，每个broker负责本机上队列的维护，并且borker之间可以互相通信。集群中有两个队列A和B，每个队列都分为master queue和mirror queue（备份）。那么队列上的生产消费怎么实现的呢？ 对于消费队列，如下图有两个consumer消费队列A，这两个consumer连在了集群的不同机器上。RabbitMQ集群中的任何一个节点都拥有集群上所有队列的元信息，所以连接到集群中的任何一个节点都可以，主要区别在于有的consumer连在master queue所在节点，有的连在非master queue节点上。 因为mirror queue要和master queue保持一致，故需要同步机制，正因为一致性的限制，导致所有的读写操作都必须都操作在master queue上（想想，为啥读也要从master queue中读？和数据库读写分离是不一样的），然后由master节点同步操作到mirror queue所在的节点。即使consumer连接到了非master queue节点，该consumer的操作也会被路由到master queue所在的节点上，这样才能进行消费。 对于生成队列，原理和消费一样，如果连接到非 master queue 节点，则路由过去。 所以，到这里小伙伴们就可以看到 RabbitMQ的不足：由于master queue单节点，导致性能瓶颈，吞吐量受限。虽然为了提高性能，内部使用了Erlang这个语言实现，但是终究摆脱不了架构设计上的致命缺陷。 2.5 高级特性 2.5.1 过期时间 Time To Live，也就是生存时间，是一条消息在队列中的最大存活时间，单位是毫秒，下面看看RabbitMQ过期时间特性： RabbitMQ可以对消息和队列设置TTL。 RabbitMQ支持设置消息的过期时间，在消息发送的时候可以进行指定，每条消息的过期时间可以不同。 RabbitMQ支持设置队列的过期时间，从消息入队列开始计算，直到超过了队列的超时时间配置，那么消息会变成死信，自动清除。 如果两种方式一起使用，则过期时间以两者中较小的那个数值为准。 当然也可以不设置TTL，不设置表示消息不会过期；如果设置为0，则表示除非此时可以直接将消息投递到消费者，否则该消息将被立即丢弃。 2.5.2 消息确认 为了保证消息从队列可靠地到达消费者，RabbitMQ提供了消息确认机制。 消费者订阅队列的时候，可以指定autoAck参数，当autoAck为true的时候，RabbitMQ采用自动确认模式，RabbitMQ自动把发送出去的消息设置为确认，然后从内存或者硬盘中删除，而不管消费者是否真正消费到了这些消息。 当autoAck为false的时候，RabbitMQ会等待消费者回复的确认信号，收到确认信号之后才从内存或者磁盘中删除消息。 消息确认机制是RabbitMQ消息可靠性投递的基础，只要设置autoAck参数为false，消费者就有足够的时间处理消息，不用担心处理消息的过程中消费者进程挂掉后消息丢失的问题。 2.5.3 持久化 消息的可靠性是RabbitMQ的一大特色，那么RabbitMQ是如何保证消息可靠性的呢？答案就是消息持久化。持久化可以防止在异常情况下丢失数据。RabbitMQ的持久化分为三个部分：交换器持久化、队列持久化和消息的持久化。 交换器持久化可以通过在声明队列时将durable参数设置为true。如果交换器不设置持久化，那么在RabbitMQ服务重启之后，相关的交换器元数据会丢失，不过消息不会丢失，只是不能将消息发送到这个交换器了。 队列的持久化能保证其本身的元数据不会因异常情况而丢失，但是不能保证内部所存储的消息不会丢失。要确保消息不会丢失，需要将其设置为持久化。队列的持久化可以通过在声明队列时将durable参数设置为true。 设置了队列和消息的持久化，当RabbitMQ服务重启之后，消息依然存在。如果只设置队列持久化或者消息持久化，重启之后消息都会消失。 当然，也可以将所有的消息都设置为持久化，但是这样做会影响RabbitMQ的性能，因为磁盘的写入速度比内存的写入要慢得多。 对于可靠性不是那么高的消息可以不采用持久化处理以提高整体的吞吐量。鱼和熊掌不可兼得，关键在于选择和取舍。在实际中，需要根据实际情况在可靠性和吞吐量之间做一个权衡。 2.5.4 死信队列 当消息在一个队列中变成死信之后，他能被重新发送到另一个交换器中，这个交换器成为死信交换器，与该交换器绑定的队列称为死信队列。 消息变成死信有下面几种情况： 消息被拒绝。 消息过期 队列达到最大长度 DLX也是一个正常的交换器，和一般的交换器没有区别，他能在任何的队列上面被指定，实际上就是设置某个队列的属性。当这个队列中有死信的时候，RabbitMQ会自动将这个消息重新发送到设置的交换器上，进而被路由到另一个队列，我们可以监听这个队列中消息做相应的处理。 死信队列有什么用？当发生异常的时候，消息不能够被消费者正常消费，被加入到了死信队列中。后续的程序可以根据死信队列中的内容分析当时发生的异常，进而改善和优化系统。 2.5.5 延迟队列 一般的队列，消息一旦进入队列就会被消费者立即消费。延迟队列就是进入该队列的消息会被消费者延迟消费，延迟队列中存储的对象是的延迟消息，“延迟消息”是指当消息被发送以后，等待特定的时间后，消费者才能拿到这个消息进行消费。 延迟队列用于需要延迟工作的场景。最常见的使用场景：淘宝或者天猫我们都使用过，用户在下单之后通常有30分钟的时间进行支付，如果这30分钟之内没有支付成功，那么订单就会自动取消。 除了延迟消费，延迟队列的典型应用场景还有延迟重试。比如消费者从队列里面消费消息失败了，可以延迟一段时间以后进行重试。 2.6 特性分析 这里才是内容的重点，不仅需要知道Rabbit的特性，还需要知道支持这些特性的原因： 消息路由（支持）：RabbitMQ可以通过不同的交换器支持不同种类的消息路由； 消息有序（不支持）：当消费消息时，如果消费失败，消息会被放回队列，然后重新消费，这样会导致消息无序； 消息时序（非常好）：通过延时队列，可以指定消息的延时时间，过期时间TTL等； 容错处理（非常好）：通过交付重试和死信交换器（DLX）来处理消息处理故障； 伸缩（一般）：伸缩其实没有非常智能，因为即使伸缩了，master queue还是只有一个，负载还是只有这一个master queue去抗，所以我理解RabbitMQ的伸缩很弱（个人理解）。 持久化（不太好）：没有消费的消息，可以支持持久化，这个是为了保证机器宕机时消息可以恢复，但是消费过的消息，就会被马上删除，因为RabbitMQ设计时，就不是为了去存储历史数据的。 消息回溯（不支持）：因为消息不支持永久保存，所以自然就不支持回溯。 高吞吐（中等）：因为所有的请求的执行，最后都是在master queue，它的这个设计，导致单机性能达不到十万级的标准。 3. RabbitMQ环境搭建 因为我用的是Mac，所以直接可以参考官网： https://www.rabbitmq.com/install-homebrew.html 需要注意的是，一定需要先执行： brew update 然后再执行： brew install rabbitmq 之前没有执行brew update，直接执行brew install rabbitmq时，会报各种各样奇怪的错误，其中“403 Forbidde”居多。 但是在执行“brew install rabbitmq”，会自动安装其它的程序，如果你使用源码安装Rabbitmq，因为启动该服务依赖erlang环境，所以你还需手动安装erlang，但是目前官方已经一键给你搞定，会自动安装Rabbitmq依赖的所有程序，是不是很棒！ 最后执行成功的输出如下： 启动服务： # 启动方式1：后台启动 brew services start rabbitmq # 启动方式2：当前窗口启动 cd /usr/local/Cellar/rabbitmq/3.8.19 rabbitmq-server 在浏览器输入： http://localhost:15672/ 会出现RabbitMQ后台管理界面（用户名和密码都为guest）： 通过brew安装，一行命令搞定，真香！ 4. RabbitMQ测试 4.1 添加账号 首先得启动mq ## 添加账号 ./rabbitmqctl add_user admin admin ## 添加访问权限 ./rabbitmqctl set_permissions -p &quot;/&quot; admin &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; ## 设置超级权限 ./rabbitmqctl set_user_tags admin administrator 4.2 编码实测 因为代码中引入了java 8的特性，pom引入依赖： &lt;dependency&gt; &lt;groupId&gt;com.rabbitmq&lt;/groupId&gt; &lt;artifactId&gt;amqp-client&lt;/artifactId&gt; &lt;version&gt;5.5.1&lt;/version&gt; &lt;/dependency&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;8&lt;/source&gt; &lt;target&gt;8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; 开始写代码： public class RabbitMqTest { //消息队列名称 private final static String QUEUE_NAME = &quot;hello&quot;; @Test public void send() throws java.io.IOException, TimeoutException { //创建连接工程 ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;127.0.0.1&quot;); factory.setPort(5672); factory.setUsername(&quot;admin&quot;); factory.setPassword(&quot;admin&quot;); //创建连接 Connection connection = factory.newConnection(); //创建消息通道 Channel channel = connection.createChannel(); //生成一个消息队列 channel.queueDeclare(QUEUE_NAME, true, false, false, null); for (int i = 0; i &lt; 10; i++) { String message = &quot;Hello World RabbitMQ count: &quot; + i; //发布消息，第一个参数表示路由（Exchange名称），为&quot;&quot;则表示使用默认消息路由 channel.basicPublish(&quot;&quot;, QUEUE_NAME, null, message.getBytes()); System.out.println(&quot; [x] Sent '&quot; + message + &quot;'&quot;); } //关闭消息通道和连接 channel.close(); connection.close(); } @Test public void consumer() throws java.io.IOException, TimeoutException { //创建连接工厂 ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;127.0.0.1&quot;); factory.setPort(5672); factory.setUsername(&quot;admin&quot;); factory.setPassword(&quot;admin&quot;); //创建连接 Connection connection = factory.newConnection(); //创建消息信道 final Channel channel = connection.createChannel(); //消息队列 channel.queueDeclare(QUEUE_NAME, true, false, false, null); System.out.println(&quot;[*] Waiting for message. To exist press CTRL+C&quot;); DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; { String message = new String(delivery.getBody(), &quot;UTF-8&quot;); System.out.println(&quot; [x] Received '&quot; + message + &quot;'&quot;); }; channel.basicConsume(QUEUE_NAME, true, deliverCallback, consumerTag -&gt; {}); } } 执行send()后控制台输出： [x] Sent 'Hello World RabbitMQ count: 0' [x] Sent 'Hello World RabbitMQ count: 1' [x] Sent 'Hello World RabbitMQ count: 2' [x] Sent 'Hello World RabbitMQ count: 3' [x] Sent 'Hello World RabbitMQ count: 4' [x] Sent 'Hello World RabbitMQ count: 5' [x] Sent 'Hello World RabbitMQ count: 6' [x] Sent 'Hello World RabbitMQ count: 7' [x] Sent 'Hello World RabbitMQ count: 8' [x] Sent 'Hello World RabbitMQ count: 9' 执行consumer()后： 示例中的代码讲解，可以直接参考官网：https://www.rabbitmq.com/tutorials/tutorial-one-java.html 5. 基本使用姿势 5.1 公共代码封装 封装工厂类： public class RabbitUtil { public static ConnectionFactory getConnectionFactory() { //创建连接工程，下面给出的是默认的case ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;127.0.0.1&quot;); factory.setPort(5672); factory.setUsername(&quot;admin&quot;); factory.setPassword(&quot;admin&quot;); factory.setVirtualHost(&quot;/&quot;); return factory; } } 封装生成者： public class MsgProducer { public static void publishMsg(String exchange, BuiltinExchangeType exchangeType, String toutingKey, String message) throws IOException, TimeoutException { ConnectionFactory factory = RabbitUtil.getConnectionFactory(); //创建连接 Connection connection = factory.newConnection(); //创建消息通道 Channel channel = connection.createChannel(); // 声明exchange中的消息为可持久化，不自动删除 channel.exchangeDeclare(exchange, exchangeType, true, false, null); // 发布消息 channel.basicPublish(exchange, toutingKey, null, message.getBytes()); System.out.println(&quot;Sent '&quot; + message + &quot;'&quot;); channel.close(); connection.close(); } } 封装消费者： public class MsgConsumer { public static void consumerMsg(String exchange, String queue, String routingKey) throws IOException, TimeoutException { ConnectionFactory factory = RabbitUtil.getConnectionFactory(); //创建连接 Connection connection = factory.newConnection(); //创建消息信道 final Channel channel = connection.createChannel(); //消息队列 channel.queueDeclare(queue, true, false, false, null); //绑定队列到交换机 channel.queueBind(queue, exchange, routingKey); System.out.println(&quot;[*] Waiting for message. To exist press CTRL+C&quot;); Consumer consumer = new DefaultConsumer(channel) { @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { String message = new String(body, &quot;UTF-8&quot;); try { System.out.println(&quot; [x] Received '&quot; + message); } finally { System.out.println(&quot; [x] Done&quot;); channel.basicAck(envelope.getDeliveryTag(), false); } } }; // 取消自动ack channel.basicConsume(queue, false, consumer); } } 5.2 Direct方式 5.2.1 Direct示例 生产者： public class DirectProducer { private static final String EXCHANGE_NAME = &quot;direct.exchange&quot;; public void publishMsg(String routingKey, String msg) { try { MsgProducer.publishMsg(EXCHANGE_NAME, BuiltinExchangeType.DIRECT, routingKey, msg); } catch (Exception e) { e.printStackTrace(); } } public static void main(String[] args) throws InterruptedException { DirectProducer directProducer = new DirectProducer(); String[] routingKey = new String[]{&quot;aaa&quot;, &quot;bbb&quot;, &quot;ccc&quot;}; String msg = &quot;hello &gt;&gt;&gt; &quot;; for (int i = 0; i &lt; 10; i++) { directProducer.publishMsg(routingKey[i % 3], msg + i); } System.out.println(&quot;----over-------&quot;); Thread.sleep(1000 * 60 * 100); } } 执行生产者，往消息队列中放入10条消息，其中key分别为“aaa”、“bbb”和“ccc”，分别放入qa、qb、qc三个队列： 下面是qa队列的信息： 消费者： public class DirectConsumer { private static final String exchangeName = &quot;direct.exchange&quot;; public void msgConsumer(String queueName, String routingKey) { try { MsgConsumer.consumerMsg(exchangeName, queueName, routingKey); } catch (IOException e) { e.printStackTrace(); } catch (TimeoutException e) { e.printStackTrace(); } } public static void main(String[] args) throws InterruptedException { DirectConsumer consumer = new DirectConsumer(); String[] routingKey = new String[]{&quot;aaa&quot;, &quot;bbb&quot;, &quot;ccc&quot;}; String[] queueNames = new String[]{&quot;qa&quot;, &quot;qb&quot;, &quot;qc&quot;}; for (int i = 0; i &lt; 3; i++) { consumer.msgConsumer(queueNames[i], routingKey[i]); } Thread.sleep(1000 * 60 * 100); } } 执行后的输出： [*] Waiting for message. To exist press CTRL+C [x] Received 'hello &gt;&gt;&gt; 0 [x] Done [x] Received 'hello &gt;&gt;&gt; 3 [x] Done [x] Received 'hello &gt;&gt;&gt; 6 [x] Done [x] Received 'hello &gt;&gt;&gt; 9 [x] Done [*] Waiting for message. To exist press CTRL+C [x] Received 'hello &gt;&gt;&gt; 1 [x] Done [x] Received 'hello &gt;&gt;&gt; 4 [x] Done [x] Received 'hello &gt;&gt;&gt; 7 [x] Done [*] Waiting for message. To exist press CTRL+C [x] Received 'hello &gt;&gt;&gt; 2 [x] Done [x] Received 'hello &gt;&gt;&gt; 5 [x] Done [x] Received 'hello &gt;&gt;&gt; 8 [x] Done 可以看到，分别从qa、qb、qc中将不同的key的数据消费掉。 5.2.2 问题探讨 有个疑问：这个队列的名称qa、qb和qc是RabbitMQ自动生成的么，我们可以指定队列名称么？ 我做了个简单的实验，我把消费者代码修改了一下： public static void main(String[] args) throws InterruptedException { DirectConsumer consumer = new DirectConsumer(); String[] routingKey = new String[]{&quot;aaa&quot;, &quot;bbb&quot;, &quot;ccc&quot;}; String[] queueNames = new String[]{&quot;qa&quot;, &quot;qb&quot;, &quot;qc1&quot;}; // 将qc修改为qc1 for (int i = 0; i &lt; 3; i++) { consumer.msgConsumer(queueNames[i], routingKey[i]); } Thread.sleep(1000 * 60 * 100); } 执行后如下图所示： 我们可以发现，多了一个qc1，所以可以判断这个界面中的queues，是消费者执行时，会将消费者指定的队列名称和direct.exchange绑定，绑定的依据就是key。 当我们把队列中的数据全部消费掉，然后重新执行生成者后，会发现qc和qc1中都有3条待消费的数据，因为绑定的key都是“ccc”，所以两者的数据是一样的： 绑定关系如下： 注意：当没有Queue绑定到Exchange时，往Exchange中写入的消息也不会重新分发到之后绑定的queue上。 思考：不执行消费者，看不到这个Queres中信息，我其实可以把这个界面理解为消费者信息界面。不过感觉还是怪怪的，这个queues如果是消费者信息，就不应该叫queues，我理解queues应该是RabbitMQ中实际存放数据的queues，难道是我理解错了？ 5.3 Fanout方式（指定队列） 生产者封装： public class FanoutProducer { private static final String EXCHANGE_NAME = &quot;fanout.exchange&quot;; public void publishMsg(String routingKey, String msg) { try { MsgProducer.publishMsg(EXCHANGE_NAME, BuiltinExchangeType.FANOUT, routingKey, msg); } catch (Exception e) { e.printStackTrace(); } } public static void main(String[] args) { FanoutProducer directProducer = new FanoutProducer(); String msg = &quot;hello &gt;&gt;&gt; &quot;; for (int i = 0; i &lt; 10; i++) { directProducer.publishMsg(&quot;&quot;, msg + i); } } } 消费者： public class FanoutConsumer { private static final String EXCHANGE_NAME = &quot;fanout.exchange&quot;; public void msgConsumer(String queueName, String routingKey) { try { MsgConsumer.consumerMsg(EXCHANGE_NAME, queueName, routingKey); } catch (IOException e) { e.printStackTrace(); } catch (TimeoutException e) { e.printStackTrace(); } } public static void main(String[] args) { FanoutConsumer consumer = new FanoutConsumer(); String[] queueNames = new String[]{&quot;qa-2&quot;, &quot;qb-2&quot;, &quot;qc-2&quot;}; for (int i = 0; i &lt; 3; i++) { consumer.msgConsumer(queueNames[i], &quot;&quot;); } } } 执行生成者，结果如下： 我们发现，生产者生产的10条数据，在每个消费者中都可以消费，这个是和Direct不同的地方，但是使用Fanout方式时，有几个点需要注意一下： 生产者的routkey可以为空，因为生产者的所有数据，会下放到每一个队列，所以不会通过routkey去路由； 消费者需要指定queues，因为消费者需要绑定到指定的queues才能消费。 这幅图就画出了Fanout的精髓之处，exchange会和所有的queue进行绑定，不区分路由，消费者需要绑定指定的queue才能发起消费。 注意：往队列塞数据时，可能通过界面看不到消息个数的增加，可能是你之前已经开启了消费进程，导致增加的消息马上被消费了。 5.4 Fanout方式（随机获取队列） 上面我们是指定了队列，这个方式其实很不友好，比如对于Fanout，我其实根本无需关心队列的名字，如果还指定对应队列进行消费，感觉这个很冗余，所以我们这里就采用随机获取队列名字的方式，下面代码直接Copy官网。 生成者封装： public static void publishMsgV2(String exchange, BuiltinExchangeType exchangeType, String message) throws IOException, TimeoutException { ConnectionFactory factory = RabbitUtil.getConnectionFactory(); //创建连接 Connection connection = factory.newConnection(); //创建消息通道 Channel channel = connection.createChannel(); // 声明exchange中的消息 channel.exchangeDeclare(exchange, exchangeType); // 发布消息 channel.basicPublish(exchange, &quot;&quot;, null, message.getBytes(&quot;UTF-8&quot;)); System.out.println(&quot;Sent '&quot; + message + &quot;'&quot;); channel.close(); connection.close(); } 消费者封装： public static void consumerMsgV2(String exchange) throws IOException, TimeoutException { ConnectionFactory factory = RabbitUtil.getConnectionFactory(); Connection connection = factory.newConnection(); final Channel channel = connection.createChannel(); channel.exchangeDeclare(exchange, &quot;fanout&quot;); String queueName = channel.queueDeclare().getQueue(); channel.queueBind(queueName, exchange, &quot;&quot;); System.out.println(&quot; [*] Waiting for messages. To exit press CTRL+C&quot;); DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; { String message = new String(delivery.getBody(), &quot;UTF-8&quot;); System.out.println(&quot; [x] Received '&quot; + message + &quot;'&quot;); }; channel.basicConsume(queueName, true, deliverCallback, consumerTag -&gt; { }); } 生产者： public class FanoutProducer { private static final String EXCHANGE_NAME = &quot;fanout.exchange.v2&quot;; public void publishMsg(String msg) { try { MsgProducer.publishMsgV2(EXCHANGE_NAME, BuiltinExchangeType.FANOUT, msg); } catch (Exception e) { e.printStackTrace(); } } public static void main(String[] args) { FanoutProducer directProducer = new FanoutProducer(); String msg = &quot;hello &gt;&gt;&gt; &quot;; for (int i = 0; i &lt; 10000; i++) { directProducer.publishMsg(msg + i); } } } 消费者： public class FanoutConsumer { private static final String EXCHANGE_NAME = &quot;fanout.exchange.v2&quot;; public void msgConsumer() { try { MsgConsumer.consumerMsgV2(EXCHANGE_NAME); } catch (IOException e) { e.printStackTrace(); } catch (TimeoutException e) { e.printStackTrace(); } } public static void main(String[] args) { FanoutConsumer consumer = new FanoutConsumer(); for (int i = 0; i &lt; 3; i++) { consumer.msgConsumer(); } } } 执行后，管理界面如下： 5.5 Topic方式 代码详见官网：https://www.rabbitmq.com/tutorials/tutorial-five-java.html 更多方式，请直接查看官网：https://www.rabbitmq.com/getstarted.html 6. RabbitMQ 进阶 6.1 durable 和 autoDeleted 在定义Queue时，可以指定这两个参数： /** * Declare an exchange. * @see com.rabbitmq.client.AMQP.Exchange.Declare * @see com.rabbitmq.client.AMQP.Exchange.DeclareOk * @param exchange the name of the exchange * @param type the exchange type * @param durable true if we are declaring a durable exchange (the exchange will survive a server restart) * @param autoDelete true if the server should delete the exchange when it is no longer in use * @param arguments other properties (construction arguments) for the exchange * @return a declaration-confirm method to indicate the exchange was successfully declared * @throws java.io.IOException if an error is encountered */ Exchange.DeclareOk exchangeDeclare(String exchange, BuiltinExchangeType type, boolean durable, boolean autoDelete, Map&lt;String, Object&gt; arguments) throws IOException; /** * Declare a queue * @see com.rabbitmq.client.AMQP.Queue.Declare * @see com.rabbitmq.client.AMQP.Queue.DeclareOk * @param queue the name of the queue * @param durable true if we are declaring a durable queue (the queue will survive a server restart) * @param exclusive true if we are declaring an exclusive queue (restricted to this connection) * @param autoDelete true if we are declaring an autodelete queue (server will delete it when no longer in use) * @param arguments other properties (construction arguments) for the queue * @return a declaration-confirm method to indicate the queue was successfully declared * @throws java.io.IOException if an error is encountered */ Queue.DeclareOk queueDeclare(String queue, boolean durable, boolean exclusive, boolean autoDelete, Map&lt;String, Object&gt; arguments) throws IOException; 6.1.1 durable 持久化，保证RabbitMQ在退出或者crash等异常情况下数据没有丢失，需要将queue，exchange和Message都持久化。 若是将queue的持久化标识durable设置为true，则代表是一个持久的队列，那么在服务重启之后，会重新读取之前被持久化的queue。 虽然队列可以被持久化，但是里面的消息是否为持久化，还要看消息的持久化设置。即重启queue，但是queue里面还没有发出去的消息，那队列里面还存在该消息么？这个取决于该消息的设置。 6.1.2 autoDeleted 自动删除，如果该队列没有任何订阅的消费者的话，该队列会被自动删除。这种队列适用于临时队列。 当一个Queue被设置为自动删除时，当消费者断掉之后，queue会被删除，这个主要针对的是一些不是特别重要的数据，不希望出现消息积累的情况。 6.1.3 小节 当一个Queue已经声明好了之后，不能更新durable或者autoDelted值；当需要修改时，需要先删除再重新声明 消费的Queue声明应该和投递的Queue声明的 durable,autoDelted属性一致，否则会报错 对于重要的数据，一般设置 durable=true, autoDeleted=false 对于设置 autoDeleted=true 的队列，当没有消费者之后，队列会自动被删除 6.4 ACK 执行一个任务可能需要花费几秒钟，你可能会担心如果一个消费者在执行任务过程中挂掉了。一旦RabbitMQ将消息分发给了消费者，就会从内存中删除。在这种情况下，如果正在执行任务的消费者宕机，会丢失正在处理的消息和分发给这个消费者但尚未处理的消息。 但是，我们不想丢失任何任务，如果有一个消费者挂掉了，那么我们应该将分发给它的任务交付给另一个消费者去处理。 为了确保消息不会丢失，RabbitMQ支持消息应答。消费者发送一个消息应答，告诉RabbitMQ这个消息已经接收并且处理完毕了。RabbitMQ就可以删除它了。 因此手动ACK的常见手段： // 接收消息之后，主动ack/nak Consumer consumer = new DefaultConsumer(channel) { @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { String message = new String(body, &quot;UTF-8&quot;); try { System.out.println(&quot; [ &quot; + queue + &quot; ] Received '&quot; + message); channel.basicAck(envelope.getDeliveryTag(), false); } catch (Exception e) { channel.basicNack(envelope.getDeliveryTag(), false, true); } } }; // 取消自动ack channel.basicConsume(queue, false, consumer); ","link":"https://tinaxiawuhao.github.io/post/olBu0Tj_j/"},{"title":"ExecutorCompletionService","content":"Future Future模式是多线程设计常用的一种设计模式。Future模式可以理解成：我有一个任务，提交给了Future，Future替我完成这个任务。期间我自己可以去做任何想做的事情。一段时间之后，我就便可以从Future那儿取出结果。 Future提供了三种功能： 判断任务是否完成 能够中断任务 能够获取任务执行的结果 向线程池中提交任务的submit方法不是阻塞方法，而Future.get方法是一个阻塞方法，当submit提交多个任务时，只有所有任务都完成后，才能使用get按照任务的提交顺序得到返回结果，所以一般需要使用future.isDone先判断任务是否全部执行完成，完成后再使用future.get得到结果。（也可以用get (long timeout, TimeUnit unit)方法可以设置超时时间，防止无限时间的等待） 三段式的编程：1.启动多线程任务2.处理其他事3.收集多线程任务结果，Future虽然可以实现获取异步执行结果的需求，但是它没有提供通知的机制，要么使用阻塞，在future.get()的地方等待future返回的结果，这时又变成同步操作；要么使用isDone()轮询地判断Future是否完成，这样会耗费CPU的资源。 解决方法：CompletionService和CompletableFuture（按照任务完成的先后顺序获取任务的结果） ExecutorService 创建线程池，多线程功能调用 public static void test1() throws Exception { ExecutorService executorService = Executors.newCachedThreadPool(); ArrayList&lt;Future&lt;String&gt;&gt; futureArrayList = new ArrayList&lt;&gt;(); System.out.println(&quot;公司让你通知大家聚餐 你开车去接人&quot;); Future&lt;String&gt; future10 = executorService.submit(() -&gt; { System.out.println(&quot;总裁：我在家上大号 我最近拉肚子比较慢 要蹲1个小时才能出来 你等会来接我吧&quot;); TimeUnit.SECONDS.sleep(10); System.out.println(&quot;总裁：1小时了 我上完大号了。你来接吧&quot;); return &quot;总裁上完大号了&quot;; }); futureArrayList.add(future10); Future&lt;String&gt; future6 = executorService.submit(() -&gt; { System.out.println(&quot;中层管理：我在家上大号 要蹲10分钟就可以出来 你等会来接我吧&quot;); TimeUnit.SECONDS.sleep(6); System.out.println(&quot;中层管理：10分钟 我上完大号了。你来接吧&quot;); return &quot;中层管理上完大号了&quot;; }); futureArrayList.add(future6); Future&lt;String&gt; future3 = executorService.submit(() -&gt; { System.out.println(&quot;研发：我在家上大号 我比较快 要蹲3分钟就可以出来 你等会来接我吧&quot;); TimeUnit.SECONDS.sleep(3); System.out.println(&quot;研发：3分钟 我上完大号了。你来接吧&quot;); return &quot;研发上完大号了&quot;; }); futureArrayList.add(future3); TimeUnit.SECONDS.sleep(1); System.out.println(&quot;都通知完了,等着接吧。&quot;); try { for (Future&lt;String&gt; future : futureArrayList) { String returnStr = future.get(); System.out.println(returnStr + &quot;，你去接他&quot;); } } catch (Exception e) { e.printStackTrace(); }finally { executorService.shutdown(); } } 结果 公司让你通知大家聚餐 你开车去接人 研发：我在家上大号 我比较快 要蹲3分钟就可以出来 你等会来接我吧 总裁：我在家上大号 我最近拉肚子比较慢 要蹲1个小时才能出来 你等会来接我吧 中层管理：我在家上大号 要蹲10分钟就可以出来 你等会来接我吧 都通知完了,等着接吧。 研发：3分钟 我上完大号了。你来接吧 研发上完大号了，你去接他 中层管理：10分钟 我上完大号了。你来接吧 总裁：1小时了 我上完大号了。你来接吧 总裁上完大号了，你去接他 中层管理上完大号了，你去接他 ExecutorCompletionService public static void test2() throws Exception { ExecutorService executorService = Executors.newCachedThreadPool(); ExecutorCompletionService&lt;String&gt; completionService = new ExecutorCompletionService&lt;&gt;(executorService); System.out.println(&quot;公司让你通知大家聚餐 你开车去接人&quot;); completionService.submit(() -&gt; { System.out.println(&quot;总裁：我在家上大号 我最近拉肚子比较慢 要蹲1个小时才能出来 你等会来接我吧&quot;); TimeUnit.SECONDS.sleep(10); System.out.println(&quot;总裁：1小时了 我上完大号了。你来接吧&quot;); return &quot;总裁上完大号了&quot;; }); completionService.submit(() -&gt; { System.out.println(&quot;中层管理：我在家上大号 要蹲10分钟就可以出来 你等会来接我吧&quot;); TimeUnit.SECONDS.sleep(6); System.out.println(&quot;中层管理：10分钟 我上完大号了。你来接吧&quot;); return &quot;中层管理上完大号了&quot;; }); completionService.submit(() -&gt; { System.out.println(&quot;研发：我在家上大号 我比较快 要蹲3分钟就可以出来 你等会来接我吧&quot;); TimeUnit.SECONDS.sleep(3); System.out.println(&quot;研发：3分钟 我上完大号了。你来接吧&quot;); return &quot;研发上完大号了&quot;; }); TimeUnit.SECONDS.sleep(1); System.out.println(&quot;都通知完了,等着接吧。&quot;); //提交了3个异步任务） try { for (int i = 0; i &lt; 3; i++) { String returnStr = completionService.take().get(); System.out.println(returnStr + &quot;，你去接他&quot;); } Thread.currentThread().join(); } catch (Exception e) { e.printStackTrace(); }finally { executorService.shutdown(); } } 结果 公司让你通知大家聚餐 你开车去接人 总裁：我在家上大号 我最近拉肚子比较慢 要蹲1个小时才能出来 你等会来接我吧 研发：我在家上大号 我比较快 要蹲3分钟就可以出来 你等会来接我吧 中层管理：我在家上大号 要蹲10分钟就可以出来 你等会来接我吧 都通知完了,等着接吧。 研发：3分钟 我上完大号了。你来接吧 研发上完大号了，你去接他 中层管理：10分钟 我上完大号了。你来接吧 中层管理上完大号了，你去接他 总裁：1小时了 我上完大号了。你来接吧 总裁上完大号了，你去接他 源码分析 completionService是JUC里的线程池ExecutorCompletionService 初始化线程池 同步初始化一个完成队列completionQueue public ExecutorCompletionService(Executor executor) { if (executor == null) throw new NullPointerException(); this.executor = executor; this.aes = (executor instanceof AbstractExecutorService) ? (AbstractExecutorService) executor : null; this.completionQueue = new LinkedBlockingQueue&lt;Future&lt;V&gt;&gt;(); } 执行submit方法 线程池执行的时候传入了自己的内部类QueueingFuture public Future&lt;V&gt; submit(Callable&lt;V&gt; task) { if (task == null) throw new NullPointerException(); RunnableFuture&lt;V&gt; f = newTaskFor(task); executor.execute(new QueueingFuture&lt;V&gt;(f, completionQueue)); return f; } QueueingFuture构造函数，需要关注这里重写了done方法，向阻塞队列里添加task private static class QueueingFuture&lt;V&gt; extends FutureTask&lt;Void&gt; { QueueingFuture(RunnableFuture&lt;V&gt; task, BlockingQueue&lt;Future&lt;V&gt;&gt; completionQueue) { super(task, null); this.task = task; this.completionQueue = completionQueue; } private final Future&lt;V&gt; task; private final BlockingQueue&lt;Future&lt;V&gt;&gt; completionQueue; protected void done() { completionQueue.add(task); } //往completionQueue里面存值 } 执行FutureTask的run方法 public void run() { if (state != NEW || !RUNNER.compareAndSet(this, null, Thread.currentThread())) return; try { Callable&lt;V&gt; c = callable; if (c != null &amp;&amp; state == NEW) { V result; boolean ran; try { result = c.call(); ran = true; } catch (Throwable ex) { result = null; ran = false; setException(ex); } if (ran) set(result); //设置值 } } finally { // runner must be non-null until state is settled to // prevent concurrent calls to run() runner = null; // state must be re-read after nulling runner to prevent // leaked interrupts int s = state; if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); } } set方法 protected void set(V v) { if (STATE.compareAndSet(this, NEW, COMPLETING)) { outcome = v; STATE.setRelease(this, NORMAL); // final state finishCompletion(); } } 在finishCompletion方法中执行了done方法 private void finishCompletion() { // assert state &gt; COMPLETING; for (WaitNode q; (q = waiters) != null;) { if (WAITERS.weakCompareAndSet(this, q, null)) { for (;;) { Thread t = q.thread; if (t != null) { q.thread = null; LockSupport.unpark(t); } WaitNode next = q.next; if (next == null) break; q.next = null; // unlink to help gc q = next; } break; } } done(); //执行done方法-completionQueue.add(task); callable = null; // to reduce footprint } 所以每次线程完成任务，都会往completionQueue中新增一条记录。completionQueue中的记录需要通过别的方式取：ExecutorCompletionService.take()。 ExecutorCompletionService的实际用法应该是这样的：CompletionService一边执行任务，一边处理完成的任务结果，这样可以将执行的任务与处理任务隔离开来进行处理，使用submit执行任务，使用take获取已完成的任务，先完成的任务结果先加入队列。获取的结果，跟提交给线程池的任务是无关联的。 Future&lt;String&gt; submit = completionService.submit(() -&gt; { return &quot;假装有返回&quot;; }); submit.get();//直接获取返回值 如上如果我们在使用过程中由于需要的是当前线程任务的返回值，所以没有调用take方法，而是通过get方法获取到当前线程调用的返回值，会导致task任务在队列中一直增加，从而造成OOM ","link":"https://tinaxiawuhao.github.io/post/wT9gsiZx3/"},{"title":"消息队列选型","content":"消息队列 常用的消息队列主要这 4 种，分别为 Kafka、RabbitMQ、RocketMQ 和 ActiveMQ，主要介绍前三，不BB，上思维导图！ 消息队列基础 什么是消息队列？ 消息队列是在消息的传输过程中保存消息的容器，用于接收消息并以文件的方式存储，一个消息队列可以被一个也可以被多个消费者消费，包含以下 3 元素： Producer：消息生产者，负责产生和发送消息到 Broker； Broker：消息处理中心，负责消息存储、确认、重试等，一般其中会包含多个 Queue； Consumer：消息消费者，负责从 Broker 中获取消息，并进行相应处理。 消息队列模式 点对点模式：多个生产者可以向同一个消息队列发送消息，一个具体的消息只能由一个消费者消费。 发布/订阅模式：单个消息可以被多个订阅者并发的获取和处理。 消息队列应用场景 应用解耦：消息队列减少了服务之间的耦合性，不同的服务可以通过消息队列进行通信，而不用关心彼此的实现细节。 异步处理：消息队列本身是异步的，它允许接收者在消息发送很长时间后再取回消息。 流量削锋：当上下游系统处理能力存在差距的时候，利用消息队列做一个通用的”载体”，在下游有能力处理的时候，再进行分发与处理。 日志处理：日志处理是指将消息队列用在日志处理中，比如 Kafka 的应用，解决大量日志传输的问题。 消息通讯：消息队列一般都内置了高效的通信机制，因此也可以用在纯的消息通讯，比如实现点对点消息队列，或者聊天室等。 消息广播：如果没有消息队列，每当一个新的业务方接入，我们都要接入一次新接口。有了消息队列，我们只需要关心消息是否送达了队列，至于谁希望订阅，是下游的事情，无疑极大地减少了开发和联调的工作量。 常用消息队列 由于官方社区现在对 ActiveMQ 5.x 维护越来越少，较少在大规模吞吐的场景中使用，所以我们主要讲解 Kafka、RabbitMQ 和 RocketMQ。 Kafka Apache Kafka 最初由 LinkedIn 公司基于独特的设计实现为一个分布式的提交日志系统，之后成为 Apache 项目的一部分，号称大数据的杀手锏，在数据采集、传输、存储的过程中发挥着举足轻重的作用。 它是一个分布式的，支持多分区、多副本，基于 Zookeeper 的分布式消息流平台，它同时也是一款开源的基于发布订阅模式的消息引擎系统。 重要概念 主题（Topic）：消息的种类称为主题，可以说一个主题代表了一类消息，相当于是对消息进行分类，主题就像是数据库中的表。 分区（partition）：主题可以被分为若干个分区，同一个主题中的分区可以不在一个机器上，有可能会部署在多个机器上，由此来实现 kafka 的伸缩性。 批次：为了提高效率， 消息会分批次写入 Kafka，批次就代指的是一组消息。 消费者群组（Consumer Group）：消费者群组指的就是由一个或多个消费者组成的群体。 Broker: 一个独立的 Kafka 服务器就被称为 broker，broker 接收来自生产者的消息，为消息设置偏移量，并提交消息到磁盘保存。 Broker 集群：broker 集群由一个或多个 broker 组成。 重平衡（Rebalance）：消费者组内某个消费者实例挂掉后，其他消费者实例自动重新分配订阅主题分区的过程。 Kafka 架构 一个典型的 Kafka 集群中包含 Producer、broker、Consumer Group、Zookeeper 集群。 Kafka 通过 Zookeeper 管理集群配置，选举 leader，以及在 Consumer Group 发生变化时进行 rebalance。Producer 使用 push 模式将消息发布到 broker，Consumer 使用 pull 模式从 broker 订阅并消费消息。 Kafka 工作原理 消息经过序列化后，通过不同的分区策略，找到对应的分区。 相同主题和分区的消息，会被存放在同一个批次里，然后由一个独立的线程负责把它们发到 Kafka Broker 上。 分区的策略包括顺序轮询、随机轮询和 key hash 这 3 种方式，那什么是分区呢？ 分区是 Kafka 读写数据的最小粒度，比如主题 A 有 15 条消息，有 5 个分区，如果采用顺序轮询的方式，15 条消息会顺序分配给这 5 个分区，后续消费的时候，也是按照分区粒度消费。 由于分区可以部署在多个不同的机器上，所以可以通过分区实现 Kafka 的伸缩性，比如主题 A 的 5 个分区，分别部署在 5 台机器上，如果下线一台，分区就变为 4。 Kafka 消费是通过消费群组完成，同一个消费者群组，一个消费者可以消费多个分区，但是一个分区，只能被一个消费者消费。 如果消费者增加，会触发 Rebalance，也就是分区和消费者需要重新配对。 不同的消费群组互不干涉，比如下图的 2 个消费群组，可以分别消费这 4 个分区的消息，互不影响。 更多知识，详见Kafka 架构深入 RocketMQ RocketMQ 是阿里开源的消息中间件，它是纯 Java 开发，具有高性能、高可靠、高实时、适合大规模分布式系统应用的特点。 RocketMQ 思路起源于 Kafka，但并不是 Kafka 的一个 Copy，它对消息的可靠传输及事务性做了优化，目前在阿里集团被广泛应用于交易、充值、流计算、消息推送、日志流式处理、binlog 分发等场景。 重要概念 Name 服务器（NameServer）：充当注册中心，类似 Kafka 中的 Zookeeper。 Broker: 一个独立的 RocketMQ 服务器就被称为 broker，broker 接收来自生产者的消息，为消息设置偏移量。 主题（Topic）：消息的第一级类型，一条消息必须有一个 Topic。 子主题（Tag）：消息的第二级类型，同一业务模块不同目的的消息就可以用相同 Topic 和不同的 Tag 来标识。 分组（Group）：一个组可以订阅多个 Topic，包括生产者组（Producer Group）和消费者组（Consumer Group）。 队列（Queue）：可以类比 Kafka 的分区 Partition。 RocketMQ 工作原理 RockerMQ 中的消息模型就是按照主题模型所实现的，包括 Producer Group、Topic、Consumer Group 三个角色。 为了提高并发能力，一个 Topic 包含多个 Queue，生产者组根据主题将消息放入对应的 Topic，下图是采用轮询的方式找到里面的 Queue。 RockerMQ 中的消费群组和 Queue，可以类比 Kafka 中的消费群组和 Partition：不同的消费者组互不干扰，一个 Queue 只能被一个消费者消费，一个消费者可以消费多个 Queue。 消费 Queue 的过程中，通过偏移量记录消费的位置。 RocketMQ 架构 RocketMQ 技术架构中有四大角色 NameServer、Broker、Producer 和 Consumer，下面主要介绍 Broker。 Broker 用于存放 Queue，一个 Broker 可以配置多个 Topic，一个 Topic 中存在多个 Queue。 如果某个 Topic 消息量很大，应该给它多配置几个 Queue，并且尽量多分布在不同 broker 上，以减轻某个 broker 的压力。Topic 消息量都比较均匀的情况下，如果某个 broker 上的队列越多，则该 broker 压力越大。 简单提一下，Broker 通过集群部署，并且提供了 master/slave 的结构，slave 定时从 master 同步数据（同步刷盘或者异步刷盘），如果 master 宕机，则 slave 提供消费服务，但是不能写入消息。 看到这里，大家应该可以发现，RocketMQ 的设计和 Kafka 真的很像！ 更多知识，详见 RabbitMQ RabbitMQ 2007 年发布，是使用 Erlang 语言开发的开源消息队列系统，基于 AMQP 协议来实现。 AMQP 的主要特征是面向消息、队列、路由、可靠性、安全。AMQP 协议更多用在企业系统内，对数据一致性、稳定性和可靠性要求很高的场景，对性能和吞吐量的要求还在其次。 重要概念 信道（Channel）：消息读写等操作在信道中进行，客户端可以建立多个信道，每个信道代表一个会话任务。 交换器（Exchange）：接收消息，按照路由规则将消息路由到一个或者多个队列；如果路由不到，或者返回给生产者，或者直接丢弃。 路由键（RoutingKey）：生产者将消息发送给交换器的时候，会发送一个 RoutingKey，用来指定路由规则，这样交换器就知道把消息发送到哪个队列。 绑定（Binding）：交换器和消息队列之间的虚拟连接，绑定中可以包含一个或者多个 RoutingKey。 RabbitMQ 工作原理 AMQP 协议模型由三部分组成：生产者、消费者和服务端，执行流程如下： 生产者是连接到 Server，建立一个连接，开启一个信道。 生产者声明交换器和队列，设置相关属性，并通过路由键将交换器和队列进行绑定。 消费者也需要进行建立连接，开启信道等操作，便于接收消息。 生产者发送消息，发送到服务端中的虚拟主机。 虚拟主机中的交换器根据路由键选择路由规则，发送到不同的消息队列中。 订阅了消息队列的消费者就可以获取到消息，进行消费。 常用交换器 RabbitMQ 常用的交换器类型有 direct、topic、fanout、headers 四种 具体的使用方法，可以参考官网： 官网入口：https://www.rabbitmq.com/getstarted.html 更多知识，详见 消息队列对比&amp;选型 消息队列对比 Kafka 优点： 高吞吐、低延迟：Kafka 最大的特点就是收发消息非常快，Kafka 每秒可以处理几十万条消息，它的最低延迟只有几毫秒； 高伸缩性：每个主题（topic）包含多个分区（partition），主题中的分区可以分布在不同的主机（broker）中； 高稳定性：Kafka 是分布式的，一个数据多个副本，某个节点宕机，Kafka 集群能够正常工作； 持久性、可靠性、可回溯：Kafka 能够允许数据的持久化存储，消息被持久化到磁盘，并支持数据备份防止数据丢失，支持消息回溯； 消息有序：通过控制能够保证所有消息被消费且仅被消费一次； 有优秀的第三方 Kafka Web 管理界面 Kafka-Manager，在日志领域比较成熟，被多家公司和多个开源项目使用。 缺点： Kafka 单机超过 64 个队列/分区，Load 会发生明显的飙高现象，队列越多，load 越高，发送消息响应时间变长； 不支持消息路由，不支持延迟发送，不支持消息重试； 社区更新较慢。 RocketMQ 优点： 高吞吐：借鉴 Kafka 的设计，单一队列百万消息的堆积能力； 高伸缩性：灵活的分布式横向扩展部署架构，整体架构其实和 kafka 很像； 高容错性：通过ACK机制，保证消息一定能正常消费； 持久化、可回溯：消息可以持久化到磁盘中，支持消息回溯； 消息有序：在一个队列中可靠的先进先出（FIFO）和严格的顺序传递； 支持发布/订阅和点对点消息模型，支持拉、推两种消息模式； 提供 docker 镜像用于隔离测试和云集群部署，提供配置、指标和监控等功能丰富的 Dashboard。 缺点： 不支持消息路由，支持的客户端语言不多，目前是 java 及 c++，其中 c++ 不成熟； 部分支持消息有序：需要将同一类的消息 hash 到同一个队列 Queue 中，才能支持消息的顺序，如果同一类消息散落到不同的 Queue中，就不能支持消息的顺序。 社区活跃度一般。 RabbitMQ 优点： 支持几乎所有最受欢迎的编程语言：Java，C，C ++，C＃，Ruby，Perl，Python，PHP等等； 支持消息路由：RabbitMQ 可以通过不同的交换器支持不同种类的消息路由； 消息时序：通过延时队列，可以指定消息的延时时间，过期时间TTL等； 支持容错处理：通过交付重试和死信交换器（DLX）来处理消息处理故障； 提供了一个易用的用户界面，使得用户可以监控和管理消息 Broker； 社区活跃度高。 缺点： Erlang 开发，很难去看懂源码，不利于做二次开发和维护，基本只能依赖于开源社区的快速维护和修复 bug； RabbitMQ 吞吐量会低一些，这是因为他做的实现机制比较重； 不支持消息有序、持久化不好、不支持消息回溯、伸缩性一般。 消息队列选型 Kafka：追求高吞吐量，一开始的目的就是用于日志收集和传输，适合产生大量数据的互联网服务的数据收集业务，大型公司建议可以选用，如果有日志采集功能，肯定是首选 kafka。 RocketMQ：天生为金融互联网领域而生，对于可靠性要求很高的场景，尤其是电商里面的订单扣款，以及业务削峰，在大量交易涌入时，后端可能无法及时处理的情况。RocketMQ 在稳定性上可能更值得信赖，这些业务场景在阿里双 11 已经经历了多次考验，如果你的业务有上述并发场景，建议可以选择 RocketMQ。 RabbitMQ：结合 erlang 语言本身的并发优势，性能较好，社区活跃度也比较高，但是不利于做二次开发和维护，不过 RabbitMQ 的社区十分活跃，可以解决开发过程中遇到的 bug。如果你的数据量没有那么大，小公司优先选择功能比较完备的 RabbitMQ。 ActiveMQ：官方社区现在对 ActiveMQ 5.x 维护越来越少，较少在大规模吞吐的场景中使用。 ","link":"https://tinaxiawuhao.github.io/post/DyzAzkEVU/"},{"title":"easyExcel","content":"读Excel 注解 使用注解很简单，只要在对应的实体类上面加上注解即可。 ExcelProperty 用于匹配excel和实体类的匹配,参数如下： 名称 默认值 描述 value 空 用于匹配excel中的头，必须全匹配,如果有多行头，会匹配最后一行头 order Integer.MAX_VALUE 优先级高于value，会根据order的顺序来匹配实体和excel中数据的顺序 index -1 优先级高于value和order，会根据index直接指定到excel中具体的哪一列 converter 自动选择 指定当前字段用什么转换器，默认会自动选择。读的情况下只要实现com.alibaba.excel.converters.Converter#convertToJavaData(com.alibaba.excel.converters.ReadConverterContext&lt;?&gt;) 方法即可 ExcelIgnore 默认所有字段都会和excel去匹配，加了这个注解会忽略该字段 ExcelIgnoreUnannotated 默认不加ExcelProperty 的注解的都会参与读写，加了不会参与 DateTimeFormat 日期转换，用String去接收excel日期格式的数据会调用这个注解,参数如下： 名称 默认值 描述 value 空 参照java.text.SimpleDateFormat书写即可 use1904windowing 自动选择 excel中时间是存储1900年起的一个双精度浮点数，但是有时候默认开始日期是1904，所以设置这个值改成默认1904年开始 NumberFormat 数字转换，用String去接收excel数字格式的数据会调用这个注解。 名称 默认值 描述 value 空 参照java.text.DecimalFormat书写即可 roundingMode RoundingMode.HALF_UP 格式化的时候设置舍入模式 参数 概念介绍 ReadWorkbook 可以理解成一个excel ReadSheet 理解成一个excel里面的一个表单 通用参数 ReadWorkbook,ReadSheet 都会有的参数，如果为空，默认使用上级。 名称 默认值 描述 converter 空 默认加载了很多转换器，这里可以加入不支持的字段 readListener 空 可以注册多个监听器，读取excel的时候会不断的回调监听器中的方法 headRowNumber 1 excel中头的行数，默认1行 head 空 与clazz二选一。读取文件头对应的列表，会根据列表匹配数据，建议使用class clazz 空 与head二选一。读取文件的头对应的class，也可以使用注解。如果两个都不指定，则会读取全部数据 autoTrim true 会对头、读取数据等进行自动trim use1904windowing false excel中时间是存储1900年起的一个双精度浮点数，但是有时候默认开始日期是1904，所以设置这个值改成默认1904年开始 useScientificFormat false 数字转文本的时候在较大的数值的是否是否采用科学计数法 ReadWorkbook 设置方法如下，找不到参数的看下通用参数里面是否存在。 EasyExcel.read(fileName, DemoData.class, new DemoDataListener()) // 在 read 方法之后， 在 sheet方法之前都是设置ReadWorkbook的参数 .sheet() .doRead(); 名称 默认值 描述 excelType 空 当前excel的类型,支持XLS、XLSX、CSV inputStream 空 与file二选一。读取文件的流，如果接收到的是流就只用，不用流建议使用file参数。因为使用了inputStream easyexcel会帮忙创建临时文件，最终还是file file 空 与inputStream二选一。读取文件的文件。 mandatoryUseInputStream false 强制使用 inputStream 来创建对象，性能会变差，但是不会创建临文件。 charset Charset#defaultCharset 只有csv文件有用，读取文件的时候使用的编码 autoCloseStream true 自动关闭读取的流。 readCache 空 默认小于5M用 内存，超过5M会使用 EhCache,这里不建议使用这个参数。 readCacheSelector SimpleReadCacheSelector 用于选择什么时候用内存去存储临时数据，什么时候用磁盘存储临时数据 ignoreEmptyRow true 忽略空的行 password 空 读取文件的密码 xlsxSAXParserFactoryName 空 指定sax读取使用的class的名称，例如：com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl useDefaultListener true @since 2.1.4 默认会加入ModelBuildEventListener 来帮忙转换成传入class的对象，设置成false后将不会协助转换对象，自定义的监听器会接收到Map&lt;Integer,CellData&gt;对象，如果还想继续接听到class对象，请调用readListener方法，加入自定义的beforeListener、 ModelBuildEventListener、 自定义的afterListener即可。 extraReadSet 空 额外需要读取内容的set，默认不读取这些数据 ReadSheet 设置方法如下，找不到参数的看下通用参数里面是否存在。 EasyExcel.read(fileName, DemoData.class, new DemoDataListener()) .sheet() // 在 sheet 方法之后， 在 doRead方法之前都是设置ReadSheet的参数 .doRead(); 名称 默认值 描述 sheetNo 0 需要读取Sheet的编码，建议使用这个来指定读取哪个Sheet sheetName 空 根据名字去匹配Sheet 写Excel 注解 使用注解很简单，只要在对应的实体类上面加上注解即可。 ExcelProperty 用于匹配excel和实体类的匹配,参数如下： 名称 默认值 描述 value 空 用于匹配excel中的头，必须全匹配,如果有多行头，会匹配最后一行头 order Integer.MAX_VALUE 优先级高于value，会根据order的顺序来匹配实体和excel中数据的顺序 index -1 优先级高于value和order，会根据index直接指定到excel中具体的哪一列 converter 自动选择 指定当前字段用什么转换器，默认会自动选择。写的情况下只要实现com.alibaba.excel.converters.Converter#convertToExcelData(com.alibaba.excel.converters.WriteConverterContext&lt;T&gt;) 方法即可 ExcelIgnore 默认所有字段都会和excel去匹配，加了这个注解会忽略该字段 ExcelIgnoreUnannotated 默认不加ExcelProperty 的注解的都会参与读写，加了不会参与 DateTimeFormat 日期转换，用String去接收excel日期格式的数据会调用这个注解,参数如下： 名称 默认值 描述 value 空 参照java.text.SimpleDateFormat书写即可 use1904windowing 自动选择 excel中时间是存储1900年起的一个双精度浮点数，但是有时候默认开始日期是1904，所以设置这个值改成默认1904年开始 NumberFormat 数字转换，用String去接收excel数字格式的数据会调用这个注解。 名称 默认值 描述 value 空 参照java.text.DecimalFormat书写即可 roundingMode RoundingMode.HALF_UP 格式化的时候设置舍入模式 参数 概念介绍 WriteWorkbook 可以理解成一个excel WriteSheet 理解成一个excel里面的一个表单 WriteTable 一个表单里面如果有多个实际用的表格，则可以用WriteTable 通用参数 WriteWorkbook,WriteSheet ,WriteTable都会有的参数，如果为空，默认使用上级。 名称 默认值 描述 converter 空 默认加载了很多转换器，这里可以加入不支持的字段 writeHandler 空 写的处理器。可以实现WorkbookWriteHandler,SheetWriteHandler,RowWriteHandler,CellWriteHandler，在写入excel的不同阶段会调用 relativeHeadRowIndex 0 写入到excel和上面空开几行 head 空 与clazz二选一。读取文件头对应的列表，会根据列表匹配数据，建议使用class clazz 空 与head二选一。读取文件的头对应的class，也可以使用注解。如果两个都不指定，则会读取全部数据 autoTrim true 会对头、读取数据等进行自动trim use1904windowing false excel中时间是存储1900年起的一个双精度浮点数，但是有时候默认开始日期是1904，所以设置这个值改成默认1904年开始 useScientificFormat false 数字转文本的时候在较大的数值的是否是否采用科学计数法 needHead true 是否需要写入头到excel useDefaultStyle true 是否使用默认的样式 automaticMergeHead true 自动合并头，头中相同的字段上下左右都会去尝试匹配 excludeColumnIndexes 空 需要排除对象中的index的数据 excludeColumnFieldNames 空 需要排除对象中的字段的数据 includeColumnIndexes 空 只要导出对象中的index的数据 includeColumnFieldNames 空 只要导出对象中的字段的数据 WriteWorkbook 设置方法如下，找不到参数的看下通用参数里面是否存在。 EasyExcel.write(fileName, DemoData.class) // 在 write 方法之后， 在 sheet方法之前都是设置WriteWorkbook的参数 .sheet(&quot;模板&quot;) .doWrite(() -&gt; { // 分页查询数据 return data(); }); 名称 默认值 描述 excelType 空 当前excel的类型,支持XLS、XLSX、CSV outputStream 空 与file二选一。写入文件的流 file 空 与outputStream二选一。写入的文件 templateInputStream 空 模板的文件流 templateFile 空 模板文件 charset Charset#defaultCharset 只有csv文件有用，写入文件的时候使用的编码 autoCloseStream true 自动关闭写入的流。 password 空 读取文件的密码 inMemory false 是否在内存处理，默认会生成临时文件以节约内存。内存模式效率会更好，但是容易OOM writeExcelOnException false 写入过程中抛出异常了，是否尝试把数据写入到excel WriteSheet 设置方法如下，找不到参数的看下通用参数里面是否存在。 EasyExcel.write(fileName, DemoData.class) .sheet(&quot;模板&quot;) // 在 sheet 方法之后， 在 doWrite方法之前都是设置WriteSheet的参数 .doWrite(() -&gt; { // 分页查询数据 return data(); }); 名称 默认值 描述 sheetNo 0 需要写入的编码 sheetName 空 需要些的Sheet名称，默认同sheetNo WriteTable 设置方法如下，找不到参数的看下通用参数里面是否存在。 EasyExcel.write(fileName, DemoData.class) .sheet(&quot;模板&quot;) .table() // 在 table 方法之后， 在 doWrite方法之前都是设置WriteTable的参数 .doWrite(() -&gt; { // 分页查询数据 return data(); }); 名称 默认值 描述 tableNo 0 需要写入的编码 &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;easyexcel&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;/dependency&gt; WebTest import java.io.IOException; import java.net.URLEncoder; import java.util.Date; import java.util.List; import java.util.Map; import javax.servlet.http.HttpServletResponse; import com.alibaba.excel.EasyExcel; import com.alibaba.excel.util.ListUtils; import com.alibaba.excel.util.MapUtils; import com.alibaba.fastjson.JSON; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.PostMapping; import org.springframework.web.bind.annotation.ResponseBody; import org.springframework.web.multipart.MultipartFile; /** * web读写案例 * **/ @Controller public class WebTest { @Autowired private UploadDAO uploadDAO; /** * 文件下载（失败了会返回一个有部分数据的Excel） * &lt;p&gt; * 1. 创建excel对应的实体对象 参照{@link DownloadData} * &lt;p&gt; * 2. 设置返回的 参数 * &lt;p&gt; * 3. 直接写，这里注意，finish的时候会自动关闭OutputStream,当然你外面再关闭流问题不大 */ @GetMapping(&quot;download&quot;) public void download(HttpServletResponse response) throws IOException { // 这里注意 有同学反应使用swagger 会导致各种问题，请直接用浏览器或者用postman response.setContentType(&quot;application/vnd.openxmlformats-officedocument.spreadsheetml.sheet&quot;); response.setCharacterEncoding(&quot;utf-8&quot;); // 这里URLEncoder.encode可以防止中文乱码 当然和easyexcel没有关系 String fileName = URLEncoder.encode(&quot;测试&quot;, &quot;UTF-8&quot;).replaceAll(&quot;\\\\+&quot;, &quot;%20&quot;); response.setHeader(&quot;Content-disposition&quot;, &quot;attachment;filename*=utf-8''&quot; + fileName + &quot;.xlsx&quot;); EasyExcel.write(response.getOutputStream(), DownloadData.class).sheet(&quot;模板&quot;).doWrite(data()); } /** * 文件下载并且失败的时候返回json（默认失败了会返回一个有部分数据的Excel） * * @since 2.1.1 */ @GetMapping(&quot;downloadFailedUsingJson&quot;) public void downloadFailedUsingJson(HttpServletResponse response) throws IOException { // 这里注意 有同学反应使用swagger 会导致各种问题，请直接用浏览器或者用postman try { response.setContentType(&quot;application/vnd.openxmlformats-officedocument.spreadsheetml.sheet&quot;); response.setCharacterEncoding(&quot;utf-8&quot;); // 这里URLEncoder.encode可以防止中文乱码 当然和easyexcel没有关系 String fileName = URLEncoder.encode(&quot;测试&quot;, &quot;UTF-8&quot;).replaceAll(&quot;\\\\+&quot;, &quot;%20&quot;); response.setHeader(&quot;Content-disposition&quot;, &quot;attachment;filename*=utf-8''&quot; + fileName + &quot;.xlsx&quot;); // 这里需要设置不关闭流 EasyExcel.write(response.getOutputStream(), DownloadData.class).autoCloseStream(Boolean.FALSE).sheet(&quot;模板&quot;) .doWrite(data()); } catch (Exception e) { // 重置response response.reset(); response.setContentType(&quot;application/json&quot;); response.setCharacterEncoding(&quot;utf-8&quot;); Map&lt;String, String&gt; map = MapUtils.newHashMap(); map.put(&quot;status&quot;, &quot;failure&quot;); map.put(&quot;message&quot;, &quot;下载文件失败&quot; + e.getMessage()); response.getWriter().println(JSON.toJSONString(map)); } } /** * 文件上传 * &lt;p&gt; * 1. 创建excel对应的实体对象 参照{@link UploadData} * &lt;p&gt; * 2. 由于默认一行行的读取excel，所以需要创建excel一行一行的回调监听器，参照{@link UploadDataListener} * &lt;p&gt; * 3. 直接读即可 */ @PostMapping(&quot;upload&quot;) @ResponseBody public String upload(MultipartFile file) throws IOException { EasyExcel.read(file.getInputStream(), UploadData.class, new UploadDataListener(uploadDAO)).sheet().doRead(); return &quot;success&quot;; } private List&lt;DownloadData&gt; data() { List&lt;DownloadData&gt; list = ListUtils.newArrayList(); for (int i = 0; i &lt; 10; i++) { DownloadData data = new DownloadData(); data.setString(&quot;字符串&quot; + 0); data.setDate(new Date()); data.setDoubleData(0.56); list.add(data); } return list; } } DownloadData import java.util.Date; import com.alibaba.excel.annotation.ExcelProperty; import lombok.EqualsAndHashCode; import lombok.Getter; import lombok.Setter; /** * 基础数据类 * **/ @Getter @Setter @EqualsAndHashCode public class DownloadData { @ExcelProperty(&quot;字符串标题&quot;) private String string; @ExcelProperty(&quot;日期标题&quot;) private Date date; @ExcelProperty(&quot;数字标题&quot;) private Double doubleData; } UploadDAO import java.util.List; import org.springframework.stereotype.Repository; /** * 假设这个是你的DAO存储。当然还要这个类让spring管理，当然你不用需要存储，也不需要这个类。 * **/ @Repository public class UploadDAO { public void save(List&lt;UploadData&gt; list) { // 如果是mybatis,尽量别直接调用多次insert,自己写一个mapper里面新增一个方法batchInsert,所有数据一次性插入 } } UploadData import java.util.Date; import lombok.EqualsAndHashCode; import lombok.Getter; import lombok.Setter; /** * 基础数据类 * **/ @Getter @Setter @EqualsAndHashCode public class UploadData { private String string; private Date date; private Double doubleData; } UploadDataListener import java.util.List; import com.alibaba.excel.context.AnalysisContext; import com.alibaba.excel.read.listener.ReadListener; import com.alibaba.excel.util.ListUtils; import com.alibaba.fastjson.JSON; import lombok.extern.slf4j.Slf4j; /** * 模板的读取类 * */ // 有个很重要的点 DemoDataListener 不能被spring管理，要每次读取excel都要new,然后里面用到spring可以构造方法传进去 @Slf4j public class UploadDataListener implements ReadListener&lt;UploadData&gt; { /** * 每隔5条存储数据库，实际使用中可以100条，然后清理list ，方便内存回收 */ private static final int BATCH_COUNT = 5; private List&lt;UploadData&gt; cachedDataList = ListUtils.newArrayListWithExpectedSize(BATCH_COUNT); /** * 假设这个是一个DAO，当然有业务逻辑这个也可以是一个service。当然如果不用存储这个对象没用。 */ private UploadDAO uploadDAO; public UploadDataListener() { // 这里是demo，所以随便new一个。实际使用如果到了spring,请使用下面的有参构造函数 uploadDAO = new UploadDAO(); } /** * 如果使用了spring,请使用这个构造方法。每次创建Listener的时候需要把spring管理的类传进来 * * @param uploadDAO */ public UploadDataListener(UploadDAO uploadDAO) { this.uploadDAO = uploadDAO; } /** * 这个每一条数据解析都会来调用 * * @param data one row value. Is is same as {@link AnalysisContext#readRowHolder()} * @param context */ @Override public void invoke(UploadData data, AnalysisContext context) { log.info(&quot;解析到一条数据:{}&quot;, JSON.toJSONString(data)); cachedDataList.add(data); // 达到BATCH_COUNT了，需要去存储一次数据库，防止数据几万条数据在内存，容易OOM if (cachedDataList.size() &gt;= BATCH_COUNT) { saveData(); // 存储完成清理 list cachedDataList = ListUtils.newArrayListWithExpectedSize(BATCH_COUNT); } } /** * 所有数据解析完成了 都会来调用 * * @param context */ @Override public void doAfterAllAnalysed(AnalysisContext context) { // 这里也要保存数据，确保最后遗留的数据也存储到数据库 saveData(); log.info(&quot;所有数据解析完成！&quot;); } /** * 加上存储数据库 */ private void saveData() { log.info(&quot;{}条数据，开始存储数据库！&quot;, cachedDataList.size()); uploadDAO.save(cachedDataList); log.info(&quot;存储数据库成功！&quot;); } } ExcelUtil import com.alibaba.excel.EasyExcelFactory; import com.alibaba.excel.ExcelWriter; import com.alibaba.excel.context.AnalysisContext; import com.alibaba.excel.event.AnalysisEventListener; import com.alibaba.excel.metadata.BaseRowModel; import com.alibaba.excel.metadata.Sheet; import lombok.Data; import lombok.Getter; import lombok.Setter; import lombok.extern.slf4j.Slf4j; import org.springframework.util.CollectionUtils; import org.springframework.util.StringUtils; import java.io.*; import java.util.ArrayList; import java.util.Collections; import java.util.List; @Slf4j public class ExcelUtil { private static Sheet initSheet; static { initSheet = new Sheet(1, 0); initSheet.setSheetName(&quot;sheet&quot;); //设置自适应宽度 initSheet.setAutoWidth(Boolean.TRUE); } /** * 读取少于1000行数据 * @param filePath 文件绝对路径 * @return */ public static List&lt;Object&gt; readLessThan1000Row(String filePath){ return readLessThan1000RowBySheet(filePath,null); } /** * 读小于1000行数据, 带样式 * filePath 文件绝对路径 * initSheet ： * sheetNo: sheet页码，默认为1 * headLineMun: 从第几行开始读取数据，默认为0, 表示从第一行开始读取 * clazz: 返回数据List&lt;Object&gt; 中Object的类名 */ public static List&lt;Object&gt; readLessThan1000RowBySheet(String filePath, Sheet sheet){ if(!StringUtils.hasText(filePath)){ return null; } sheet = sheet != null ? sheet : initSheet; InputStream fileStream = null; try { fileStream = new FileInputStream(filePath); return EasyExcelFactory.read(fileStream, sheet); } catch (FileNotFoundException e) { log.info(&quot;找不到文件或文件路径错误, 文件：{}&quot;, filePath); }finally { try { if(fileStream != null){ fileStream.close(); } } catch (IOException e) { log.info(&quot;excel文件读取失败, 失败原因：{}&quot;, e); } } return null; } /** * 读大于1000行数据 * @param filePath 文件觉得路径 * @return */ public static List&lt;Object&gt; readMoreThan1000Row(String filePath){ return readMoreThan1000RowBySheet(filePath,null); } /** * 读大于1000行数据, 带样式 * @param filePath 文件觉得路径 * @return */ public static List&lt;Object&gt; readMoreThan1000RowBySheet(String filePath, Sheet sheet){ if(!StringUtils.hasText(filePath)){ return null; } sheet = sheet != null ? sheet : initSheet; InputStream fileStream = null; try { fileStream = new FileInputStream(filePath); ExcelListener excelListener = new ExcelListener(); EasyExcelFactory.readBySax(fileStream, sheet, excelListener); return excelListener.getDatas(); } catch (FileNotFoundException e) { log.error(&quot;找不到文件或文件路径错误, 文件：{}&quot;, filePath); }finally { try { if(fileStream != null){ fileStream.close(); } } catch (IOException e) { log.error(&quot;excel文件读取失败, 失败原因：{}&quot;, e); } } return null; } /** * 生成excle * @param filePath 绝对路径, 如：/home/chenmingjian/Downloads/aaa.xlsx * @param data 数据源 * @param head 表头 */ public static void writeBySimple(String filePath, List&lt;List&lt;Object&gt;&gt; data, List&lt;String&gt; head){ writeSimpleBySheet(filePath,data,head,null); } /** * 生成excle * @param filePath 绝对路径, 如：/home/chenmingjian/Downloads/aaa.xlsx * @param data 数据源 * @param sheet excle页面样式 * @param head 表头 */ public static void writeSimpleBySheet(String filePath, List&lt;List&lt;Object&gt;&gt; data, List&lt;String&gt; head, Sheet sheet){ sheet = (sheet != null) ? sheet : initSheet; if(head != null){ List&lt;List&lt;String&gt;&gt; list = new ArrayList&lt;&gt;(); head.forEach(h -&gt; list.add(Collections.singletonList(h))); sheet.setHead(list); } OutputStream outputStream = null; ExcelWriter writer = null; try { outputStream = new FileOutputStream(filePath); writer = EasyExcelFactory.getWriter(outputStream); writer.write1(data,sheet); } catch (FileNotFoundException e) { log.error(&quot;找不到文件或文件路径错误, 文件：{}&quot;, filePath); }finally { try { if(writer != null){ writer.finish(); } if(outputStream != null){ outputStream.close(); } } catch (IOException e) { log.error(&quot;excel文件导出失败, 失败原因：{}&quot;, e); } } } /** * 生成excle * @param filePath 绝对路径, 如：/home/chenmingjian/Downloads/aaa.xlsx * @param data 数据源 */ public static void writeWithTemplate(String filePath, List&lt;? extends BaseRowModel&gt; data){ writeWithTemplateAndSheet(filePath,data,null); } /** * 生成excle * @param filePath 绝对路径, 如：/home/chenmingjian/Downloads/aaa.xlsx * @param data 数据源 * @param sheet excle页面样式 */ public static void writeWithTemplateAndSheet(String filePath, List&lt;? extends BaseRowModel&gt; data, Sheet sheet){ if(CollectionUtils.isEmpty(data)){ return; } sheet = (sheet != null) ? sheet : initSheet; sheet.setClazz(data.get(0).getClass()); OutputStream outputStream = null; ExcelWriter writer = null; try { outputStream = new FileOutputStream(filePath); writer = EasyExcelFactory.getWriter(outputStream); writer.write(data,sheet); } catch (FileNotFoundException e) { log.error(&quot;找不到文件或文件路径错误, 文件：{}&quot;, filePath); }finally { try { if(writer != null){ writer.finish(); } if(outputStream != null){ outputStream.close(); } } catch (IOException e) { log.error(&quot;excel文件导出失败, 失败原因：{}&quot;, e); } } } /** * 生成多Sheet的excle * @param filePath 绝对路径, 如：/home/chenmingjian/Downloads/aaa.xlsx * @param multipleSheelPropetys */ public static void writeWithMultipleSheel(String filePath,List&lt;MultipleSheelPropety&gt; multipleSheelPropetys){ if(CollectionUtils.isEmpty(multipleSheelPropetys)){ return; } OutputStream outputStream = null; ExcelWriter writer = null; try { outputStream = new FileOutputStream(filePath); writer = EasyExcelFactory.getWriter(outputStream); for (MultipleSheelPropety multipleSheelPropety : multipleSheelPropetys) { Sheet sheet = multipleSheelPropety.getSheet() != null ? multipleSheelPropety.getSheet() : initSheet; if(!CollectionUtils.isEmpty(multipleSheelPropety.getData())){ sheet.setClazz(multipleSheelPropety.getData().get(0).getClass()); } writer.write(multipleSheelPropety.getData(), sheet); } } catch (FileNotFoundException e) { log.error(&quot;找不到文件或文件路径错误, 文件：{}&quot;, filePath); }finally { try { if(writer != null){ writer.finish(); } if(outputStream != null){ outputStream.close(); } } catch (IOException e) { log.error(&quot;excel文件导出失败, 失败原因：{}&quot;, e); } } } /*********************匿名内部类开始，可以提取出去******************************/ @Data public static class MultipleSheelPropety{ private List&lt;? extends BaseRowModel&gt; data; private Sheet sheet; } /** * 解析监听器， * 每解析一行会回调invoke()方法。 * 整个excel解析结束会执行doAfterAllAnalysed()方法 */ @Getter @Setter public static class ExcelListener extends AnalysisEventListener { private List&lt;Object&gt; datas = new ArrayList&lt;&gt;(); /** * 逐行解析 * object : 当前行的数据 */ @Override public void invoke(Object object, AnalysisContext context) { //当前行 // context.getCurrentRowNum() if (object != null) { datas.add(object); } } /** * 解析完所有数据后会调用该方法 */ @Override public void doAfterAllAnalysed(AnalysisContext context) { //解析结束销毁不用的资源 } } /************************匿名内部类结束，可以提取出去***************************/ } ","link":"https://tinaxiawuhao.github.io/post/T8lb37RDN/"},{"title":"ES6新语法","content":"const 与 let 变量 使用var带来的麻烦: function getClothing(isCold) { if (isCold) { var freezing = 'Grab a jacket!'; } else { var hot = 'It's a shorts kind of day.'; console.log(freezing); } } 运行getClothing(false)后输出的是undefined,这是因为执行function函数之前,所有变量都会被提升, 提升到函数作用域顶部. let与const声明的变量解决了这种问题,因为他们是块级作用域, 在代码块(用{}表示)中使用let或const声明变量, 该变量会陷入暂时性死区直到该变量的声明被处理. function getClothing(isCold) { if (isCold) { const freezing = 'Grab a jacket!'; } else { const hot = 'It's a shorts kind of day.'; console.log(freezing); } } 运行getClothing(false)后输出的是ReferenceError: freezing is not defined,因为 freezing 没有在 else 语句、函数作用域或全局作用域内声明，所以抛出 ReferenceError。 关于使用let与const规则: 使用let声明的变量可以重新赋值,但是不能在同一作用域内重新声明 使用const声明的变量必须赋值初始化,但是不能在同一作用域类重新声明也无法重新赋值. 模板字面量 在ES6之前,将字符串连接到一起的方法是+或者concat()方法,如 const student = { name: 'Richard Kalehoff', guardian: 'Mr. Kalehoff' }; const teacher = { name: 'Mrs. Wilson', room: 'N231' } let message = student.name + ' please see ' + teacher.name + ' in ' + teacher.room + ' to pick up your report card.'; 模板字面量本质上是包含嵌入式表达式的字符串字面量. 模板字面量用倒引号 ( `` )（而不是单引号 ( '' ) 或双引号( &quot;&quot; )）表示，可以包含用 ${expression} 表示的占位符 let message = `${student.name} please see ${teacher.name} in ${teacher.room} to pick up your report card.`; 解构 在ES6中,可以使用解构从数组和对象提取值并赋值给独特的变量 解构数组的值: const point = [10, 25, -34]; const [x, y, z] = point; console.log(x, y, z); Prints: 10 25 -34 []表示被解构的数组, x,y,z表示要将数组中的值存储在其中的变量, 在解构数组是, 还可以忽略值, 例如const[x,,z]=point,忽略y坐标. 解构对象中的值: const gemstone = { type: 'quartz', color: 'rose', karat: 21.29 }; const {type, color, karat} = gemstone; console.log(type, color, karat); 花括号 { } 表示被解构的对象，type、color 和 karat 表示要将对象中的属性存储到其中的变量 对象字面量简写法 let type = 'quartz'; let color = 'rose'; let carat = 21.29; const gemstone = { type: type, color: color, carat: carat }; console.log(gemstone); 使用和所分配的变量名称相同的名称初始化对象时如果属性名称和所分配的变量名称一样，那么就可以从对象属性中删掉这些重复的变量名称。 let type = 'quartz'; let color = 'rose'; let carat = 21.29; const gemstone = {type,color,carat}; console.log(gemstone); 简写方法的名称: const gemstone = { type, color, carat, calculateWorth: function() { // 将根据类型(type)，颜色(color)和克拉(carat)计算宝石(gemstone)的价值 } }; 匿名函数被分配给属性 calculateWorth，但是真的需要 function 关键字吗？在 ES6 中不需要！ let gemstone = { type, color, carat, calculateWorth() { ... } }; for...of循环 for...of循环是最新添加到 JavaScript 循环系列中的循环。 它结合了其兄弟循环形式 for 循环和 for...in 循环的优势，可以循环任何可迭代（也就是遵守可迭代协议）类型的数据。默认情况下，包含以下数据类型：String、Array、Map 和 Set，注意不包含 Object 数据类型（即 {}）。默认情况下，对象不可迭代。 for循环 const digits = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]; for (let i = 0; i &lt; digits.length; i++) { console.log(digits[i]); } for 循环的最大缺点是需要跟踪计数器和退出条件。 虽然 for 循环在循环数组时的确具有优势，但是某些数据结构不是数组，因此并非始终适合使用 loop 循环。 for...in循环 const digits = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]; for (const index in digits) { console.log(digits[index]); } 依然需要使用 index 来访问数组的值 当你需要向数组中添加额外的方法（或另一个对象）时，for...in 循环会带来很大的麻烦。因为 for...in 循环循环访问所有可枚举的属性，意味着如果向数组的原型中添加任何其他属性，这些属性也会出现在循环中。 Array.prototype.decimalfy = function() { for (let i = 0; i &lt; this.length; i++) { this[i] = this[i].toFixed(2); } }; const digits = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]; for (const index in digits) { console.log(digits[index]); } forEach 循环 forEach 循环是另一种形式的 JavaScript 循环。但是，forEach() 实际上是数组方法，因此只能用在数组中。也无法停止或退出 forEach 循环。如果希望你的循环中出现这种行为，则需要使用基本的 for 循环。 for...of循环 for...of 循环用于循环访问任何可迭代的数据类型。 for...of 循环的编写方式和 for...in 循环的基本一样，只是将 in 替换为 of，可以忽略索引。 const digits = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]; for (const digit of digits) { console.log(digit); } 建议使用复数对象名称来表示多个值的集合。这样，循环该集合时，可以使用名称的单数版本来表示集合中的单个值。例如，for (const button of buttons) {…}。 for...of 循环还具有其他优势，解决了 for 和 for...in 循环的不足之处。你可以随时停止或退出 for...of 循环。 for (const digit of digits) { if (digit % 2 === 0) { continue; } console.log(digit); } 不用担心向对象中添加新的属性。for...of 循环将只循环访问对象中的值。 Array.prototype.decimalfy = function() { for (i = 0; i &lt; this.length; i++) { this[i] = this[i].toFixed(2); } }; const digits = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]; for (const digit of digits) { console.log(digit); } 展开运算符 展开运算符（用三个连续的点 (...) 表示）是 ES6 中的新概念，使你能够将字面量对象展开为多个元素 const books = [&quot;Don Quixote&quot;, &quot;The Hobbit&quot;, &quot;Alice in Wonderland&quot;, &quot;Tale of Two Cities&quot;]; console.log(...books); Prints: Don Quixote The Hobbit Alice in Wonderland Tale of Two Cities 展开运算符的一个用途是结合数组。 如果你需要结合多个数组，在有展开运算符之前，必须使用 Array的 concat() 方法。 const fruits = [&quot;apples&quot;, &quot;bananas&quot;, &quot;pears&quot;]; const vegetables = [&quot;corn&quot;, &quot;potatoes&quot;, &quot;carrots&quot;]; const produce = fruits.concat(vegetables); console.log(produce); Prints: [&quot;apples&quot;, &quot;bananas&quot;, &quot;pears&quot;, &quot;corn&quot;, &quot;potatoes&quot;, &quot;carrots&quot;] 使用展开符来结合数组 const fruits = [&quot;apples&quot;, &quot;bananas&quot;, &quot;pears&quot;]; const vegetables = [&quot;corn&quot;, &quot;potatoes&quot;, &quot;carrots&quot;]; const produce = [...fruits,...vegetables]; console.log(produce); 剩余参数(可变参数) 使用展开运算符将数组展开为多个元素, 使用剩余参数可以将多个元素绑定到一个数组中. 剩余参数也用三个连续的点 ( ... ) 表示，使你能够将不定数量的元素表示为数组. 用途1: 将变量赋数组值时: const order = [20.17, 18.67, 1.50, &quot;cheese&quot;, &quot;eggs&quot;, &quot;milk&quot;, &quot;bread&quot;]; const [total, subtotal, tax, ...items] = order; console.log(total, subtotal, tax, items); 用途2: 可变参数函数 对于参数不固定的函数,ES6之前是使用参数对象(arguments)处理: function sum() { let total = 0; for(const argument of arguments) { total += argument; } return total; } 在ES6中使用剩余参数运算符则更为简洁,可读性提高: function sum(...nums) { let total = 0; for(const num of nums) { total += num; } return total; } ES6箭头函数 ES6之前,使用普通函数把其中每个名字转换为大写形式： const upperizedNames = ['Farrin', 'Kagure', 'Asser'].map(function(name) { return name.toUpperCase(); }); 箭头函数表示: const upperizedNames = ['Farrin', 'Kagure', 'Asser'].map( name =&gt; name.toUpperCase() ); 普通函数可以是函数声明或者函数表达式, 但是箭头函数始终都是表达式, 全程是箭头函数表达式, 因此因此仅在表达式有效时才能使用，包括： 存储在变量中， 当做参数传递给函数， 存储在对象的属性中。 const greet = name =&gt; `Hello ${name}!`; 可以如下调用: greet('Asser'); 如果函数的参数只有一个,不需要使用()包起来,但是只有一个或者多个, 则必须需要将参数列表放在圆括号内: // 空参数列表需要括号 const sayHi = () =&gt; console.log('Hello Udacity Student!'); // 多个参数需要括号 const orderIceCream = (flavor, cone) =&gt; console.log(`Here's your ${flavor} ice cream in a ${cone} cone.`); orderIceCream('chocolate', 'waffle'); 一般箭头函数都只有一个表达式作为函数主题: const upperizedNames = ['Farrin', 'Kagure', 'Asser'].map( name =&gt; name.toUpperCase() ); 这种函数表达式形式称为简写主体语法: 1,在函数主体周围没有花括号, 2,自动返回表达式 3,但是如果箭头函数的主体内需要多行代码, 则需要使用常规主体语法: 它将函数主体放在花括号内 需要使用 return 语句来返回内容。 const upperizedNames = ['Farrin', 'Kagure', 'Asser'].map( name =&gt; { name = name.toUpperCase(); return `${name} has ${name.length} characters in their name`; }); javascript标准函数this new 对象 const mySundae = new Sundae('Chocolate', ['Sprinkles', 'Hot Fudge']); sundae这个构造函数内的this的值是实例对象, 因为他使用new被调用. 指定的对象 const result = obj1.printName.call(obj2); 函数使用call/apply被调用,this的值指向指定的obj2,因为call()第一个参数明确设置this的指向 上下文对象 data.teleport(); 函数是对象的方法, this指向就是那个对象,此处this就是指向data. 全局对象或 undefined teleport(); 此处是this指向全局对象,在严格模式下,指向undefined. javascript中this是很复杂的概念, 要详细判断this,请参考this豁然开朗 箭头函数和this 对于普通函数, this的值基于函数如何被调用, 对于箭头函数,this的值基于函数周围的上下文, 换句话说,this的值和函数外面的this的值是一样的. function IceCream() { this.scoops = 0; } // 为 IceCream 添加 addScoop 方法 IceCream.prototype.addScoop = function() { setTimeout(function() { this.scoops++; console.log('scoop added!'); console.log(this.scoops); // undefined+1=NaN console.log(dessert.scoops); //0 }, 500); }; const dessert = new IceCream(); dessert.addScoop(); 传递给 setTimeout() 的函数被调用时没用到 new、call() 或 apply()，也没用到上下文对象。意味着函数内的 this 的值是全局对象，不是 dessert 对象。实际上发生的情况是，创建了新的 scoops 变量（默认值为 undefined），然后递增（undefined + 1 结果为 NaN）; 解决此问题的方式之一是使用闭包(closure): // 构造函数 function IceCream() { this.scoops = 0; } // 为 IceCream 添加 addScoop 方法 IceCream.prototype.addScoop = function() { const cone = this; // 设置 `this` 给 `cone`变量 setTimeout(function() { cone.scoops++; // 引用`cone`变量 console.log('scoop added!'); console.log(dessert.scoops);//1 }, 0.5); }; const dessert = new IceCream(); dessert.addScoop(); 箭头函数的作用正是如此, 将setTimeOut()的函数改为剪头函数: // 构造函数 function IceCream() { this.scoops = 0; } // 为 IceCream 添加 addScoop 方法 IceCream.prototype.addScoop = function() { setTimeout(() =&gt; { // 一个箭头函数被传递给setTimeout this.scoops++; console.log('scoop added!'); console.log(dessert.scoops);//1 }, 0.5); }; const dessert = new IceCream(); dessert.addScoop(); 默认参数函数 function greet(name, greeting) { name = (typeof name !== 'undefined') ? name : 'Student'; greeting = (typeof greeting !== 'undefined') ? greeting : 'Welcome'; return `${greeting} ${name}!`; } greet(); // Welcome Student! greet('James'); // Welcome James! greet('Richard', 'Howdy'); // Howdy Richard! greet() 函数中混乱的前两行的作用是什么？它们的作用是当所需的参数未提供时，为函数提供默认的值。但是看起来很麻烦, ES6引入一种新的方式创建默认值, 他叫默认函数参数: function greet(name = 'Student', greeting = 'Welcome') { return `${greeting} ${name}!`; } greet(); // Welcome Student! greet('James'); // Welcome James! greet('Richard', 'Howdy'); // Howdy Richard! 默认值与解构 默认值与解构数组 function createGrid([width = 5, height = 5]) { return `Generates a ${width} x ${height} grid`; } createGrid([]); // Generates a 5 x 5 grid createGrid([2]); // Generates a 2 x 5 grid createGrid([2, 3]); // Generates a 2 x 3 grid createGrid([undefined, 3]); // Generates a 5 x 3 grid createGrid() 函数预期传入的是数组。它通过解构将数组中的第一项设为 width，第二项设为 height。如果数组为空，或者只有一项，那么就会使用默认参数，并将缺失的参数设为默认值 5。 但是存在一个问题: createGrid(); // throws an error Uncaught TypeError: Cannot read property 'Symbol(Symbol.iterator)' of undefined 出现错误，因为 createGrid() 预期传入的是数组，然后对其进行解构。因为函数被调用时没有传入数组，所以出现问题。但是，我们可以使用默认的函数参数！ function createGrid([width = 5, height = 5] = []) { return `Generating a grid of ${width} by ${height}`; } createGrid(); // Generates a 5 x 5 grid Returns: Generates a 5 x 5 grid 默认值与解构函数 就像使用数组默认值解构数组一样，函数可以让对象成为一个默认参数，并使用对象解构： function createSundae({scoops = 1, toppings = ['Hot Fudge']}={}) { const scoopText = scoops === 1 ? 'scoop' : 'scoops'; return `Your sundae has ${scoops} ${scoopText} with ${toppings.join(' and ')} toppings.`; } createSundae({}); // Your sundae has 1 scoop with Hot Fudge toppings. createSundae({scoops: 2}); // Your sundae has 2 scoops with Hot Fudge toppings. createSundae({scoops: 2, toppings: ['Sprinkles']}); // Your sundae has 2 scoops with Sprinkles toppings. createSundae({toppings: ['Cookie Dough']}); // Your sundae has 1 scoop with Cookie Dough toppings. createSundae(); // Your sundae has 1 scoop with Hot Fudge toppings. 数组默认值与对象默认值 默认函数参数只是个简单的添加内容，但是却带来很多便利！与数组默认值相比，对象默认值具备的一个优势是能够处理跳过的选项。看看下面的代码： function createSundae({scoops = 1, toppings = ['Hot Fudge']} = {}) { … } 在 createSundae() 函数使用对象默认值进行解构时，如果你想使用 scoops 的默认值，但是更改 toppings，那么只需使用 toppings 传入一个对象： createSundae({toppings: ['Hot Fudge', 'Sprinkles', 'Caramel']}); 将上述示例与使用数组默认值进行解构的同一函数相对比。 function createSundae([scoops = 1, toppings = ['Hot Fudge']] = []) { … } 对于这个函数，如果想使用 scoops 的默认数量，但是更改 toppings，则必须以这种奇怪的方式调用你的函数： createSundae([undefined, ['Hot Fudge', 'Sprinkles', 'Caramel']]); 因为数组是基于位置的，我们需要传入 undefined 以跳过第一个参数（并使用默认值）来到达第二个参数。 Javascript类 ES5创建类: function Plane(numEngines) { this.numEngines = numEngines; this.enginesActive = false; } // 由所有实例 &quot;继承&quot; 的方法 Plane.prototype.startEngines = function () { console.log('starting engines...'); this.enginesActive = true; }; ES6创建类 ES6类只是一个语法糖,原型继续实际上在底层隐藏起来, 与传统类机制语言有些区别. class Plane { //constructor方法虽然在类中,但不是原型上的方法,只是用来生成实例的. constructor(numEngines) { this.numEngines = numEngines; this.enginesActive = false; } //原型上的方法, 由所有实例对象共享. startEngines() { console.log('starting engines…'); this.enginesActive = true; } } console.log(typeof Plane); //function javascript中类其实只是function, 方法之间不能使用,,不用逗号区分属性和方法. 静态方法 要添加静态方法，请在方法名称前面加上关键字 static class Plane { constructor(numEngines) { this.numEngines = numEngines; this.enginesActive = false; } static badWeather(planes) { for (plane of planes) { plane.enginesActive = false; } } startEngines() { console.log('starting engines…'); this.enginesActive = true; } } 关键字class带来其他基于类的语言的很多思想,但是没有向javascript中添加此功能 javascript类实际上还是原型继承 创建javascript类的新实例时必须使用new关键字 super 和 extends 使用新的super和extends关键字扩展类: class Tree { constructor(size = '10', leaves = {spring: 'green', summer: 'green', fall: 'orange', winter: null}) { this.size = size; this.leaves = leaves; this.leafColor = null; } changeSeason(season) { this.leafColor = this.leaves[season]; if (season === 'spring') { this.size += 1; } } } class Maple extends Tree { constructor(syrupQty = 15, size, leaves) { super(size, leaves); //super用作函数 this.syrupQty = syrupQty; } changeSeason(season) { super.changeSeason(season);//super用作对象 if (season === 'spring') { this.syrupQty += 1; } } gatherSyrup() { this.syrupQty -= 3; } } 使用ES5编写同样功能的类: function Tree(size, leaves) { this.size = size || 10; this.leaves = leaves || {spring: 'green', summer: 'green', fall: 'orange', winter: null}; this.leafColor; } Tree.prototype.changeSeason = function(season) { this.leafColor = this.leaves[season]; if (season === 'spring') { this.size += 1; } } function Maple (syrupQty, size, leaves) { Tree.call(this, size, leaves); this.syrupQty = syrupQty || 15; } Maple.prototype = Object.create(Tree.prototype); Maple.prototype.constructor = Maple; Maple.prototype.changeSeason = function(season) { Tree.prototype.changeSeason.call(this, season); if (season === 'spring') { this.syrupQty += 1; } } Maple.prototype.gatherSyrup = function() { this.syrupQty -= 3; } super 必须在 this 之前被调用 在子类构造函数中，在使用 this 之前，必须先调用超级类。 class Apple {} class GrannySmith extends Apple { constructor(tartnessLevel, energy) { this.tartnessLevel = tartnessLevel; // 在 'super' 之前会抛出一个错误！ super(energy); } } ","link":"https://tinaxiawuhao.github.io/post/MKTk7DnaM/"},{"title":"springBoot参数和业务校验","content":"为什么需要参数校验 在日常的接口开发中，为了防止非法参数对业务造成影响，经常需要对接口的参数做校验，例如登录的时候需要校验用户名密码是否为空，创建用户的时候需要校验邮件、手机号码格式是否准确。靠代码对接口参数一个个校验的话就太繁琐了，代码可读性极差。 Validator框架就是为了解决开发人员在开发的时候少写代码，提升开发效率 Validator校验框架遵循了JSR-303验证规范（参数校验规范）, JSR是Java Specification Requests的缩写。 接下来我们看看在SpringbBoot中如何集成参数校验框架。 SpringBoot中集成参数校验 第一步，引入依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-validation&lt;/artifactId&gt; &lt;/dependency&gt; 注：从springboot-2.3开始，校验包被独立成了一个starter组件，所以需要引入validation和web，而springboot-2.3之前的版本只需要引入 web 依赖就可以了。 第二步，定义要参数校验的实体类 @Data public class ValidVO { private String id; @Length(min = 6,max = 12,message = &quot;appId长度必须位于6到12之间&quot;) private String appId; @NotBlank(message = &quot;名字为必填项&quot;) private String name; @Email(message = &quot;请填写正确的邮箱地址&quot;) private String email; private String sex; @NotEmpty(message = &quot;级别不能为空&quot;) private String level; } 在实际开发中对于需要校验的字段都需要设置对应的业务提示，即message属性。 常见的约束注解如下： 注解 功能 @AssertFalse 可以为null,如果不为null的话必须为false @AssertTrue 可以为null,如果不为null的话必须为true @DecimalMax 设置不能超过最大值 @DecimalMin 设置不能超过最小值 @Digits 设置必须是数字且数字整数的位数和小数的位数必须在指定范围内 @Future 日期必须在当前日期的未来 @Past 日期必须在当前日期的过去 @Max 最大不得超过此最大值 @Min 最大不得小于此最小值 @NotNull 不能为null，可以是空 @Null 必须为null @Pattern 必须满足指定的正则表达式 @Size 集合、数组、map等的size()值必须在指定范围内 @Email 必须是email格式 @Length 长度必须在指定范围内 @NotBlank 字符串不能为null,字符串trim()后也不能等于“” @NotEmpty 不能为null，集合、数组、map等size()不能为0；字符串trim()后可以等于“” @Range 值必须在指定范围内 @URL 必须是一个URL 注：此表格只是简单的对注解功能的说明，并没有对每一个注解的属性进行说明；可详见源码。 第三步，定义校验类进行测试 @RestController @Slf4j @Validated public class ValidController { @ApiOperation(&quot;RequestBody校验&quot;) @PostMapping(&quot;/valid/test1&quot;) public String test1(@Validated @RequestBody ValidVO validVO){ log.info(&quot;validEntity is {}&quot;, validVO); return &quot;test1 valid success&quot;; } @ApiOperation(&quot;Form校验&quot;) @PostMapping(value = &quot;/valid/test2&quot;) public String test2(@Validated ValidVO validVO){ log.info(&quot;validEntity is {}&quot;, validVO); return &quot;test2 valid success&quot;; } @ApiOperation(&quot;单参数校验&quot;) @PostMapping(value = &quot;/valid/test3&quot;) public String test3(@Email String email){ log.info(&quot;email is {}&quot;, email); return &quot;email valid success&quot;; } } 这里我们先定义三个方法test1，test2，test3，test1使用了@RequestBody注解，用于接受前端发送的json数据，test2模拟表单提交，test3模拟单参数提交。注意，当使用单参数校验时需要在Controller上加上@Validated注解，否则不生效。 第四步，体验效果 调用test1方法，提示的是org.springframework.web.bind.MethodArgumentNotValidException异常 POST http://localhost:8080/valid/test1 Content-Type: application/json { &quot;id&quot;: 1, &quot;level&quot;: &quot;12&quot;, &quot;email&quot;: &quot;47693899&quot;, &quot;appId&quot;: &quot;ab1c&quot; } { &quot;status&quot;: 500, &quot;message&quot;: &quot;Validation failed for argument [0] in public java.lang.String com.jianzh5.blog.valid.ValidController.test1(com.jianzh5.blog.valid.ValidVO) with 3 errors: [Field error in object 'validVO' on field 'email': rejected value [47693899]; codes [Email.validVO.email,Email.email,Email.java.lang.String,Email]; arguments [org.springframework.context.support.DefaultMessageSourceResolvable: codes [validVO.email,email]; arguments []; default message [email],[Ljavax.validation.constraints.Pattern$Flag;@26139123,.*]; default message [不是一个合法的电子邮件地址]]...&quot;, &quot;data&quot;: null, &quot;timestamp&quot;: 1628239624332 } 调用test2方法，提示的是org.springframework.validation.BindException异常 POST http://localhost:8080/valid/test2 Content-Type: application/x-www-form-urlencoded id=1&amp;level=12&amp;email=476938977&amp;appId=ab1c { &quot;status&quot;: 500, &quot;message&quot;: &quot;org.springframework.validation.BeanPropertyBindingResult: 3 errors\\nField error in object 'validVO' on field 'name': rejected value [null]; codes [NotBlank.validVO.name,NotBlank.name,NotBlank.java.lang.String,NotBlank]; arguments [org.springframework.context.support.DefaultMessageSourceResolvable: codes [validVO.name,name]; arguments []; default message [name]]; default message [名字为必填项]...&quot;, &quot;data&quot;: null, &quot;timestamp&quot;: 1628239301951 } 调用test3方法，提示的是javax.validation.ConstraintViolationException异常 POST http://localhost:8080/valid/test3 Content-Type: application/x-www-form-urlencoded email=476938977 { &quot;status&quot;: 500, &quot;message&quot;: &quot;test3.email: 不是一个合法的电子邮件地址&quot;, &quot;data&quot;: null, &quot;timestamp&quot;: 1628239281022 } 通过加入Validator校验框架可以帮助我们自动实现参数的校验。 参数异常加入全局异常处理器 Validator校验框架返回的错误提示太臃肿了，不便于阅读，为了方便前端提示，我们需要将其简化一下。 创建RestExceptionHandler，单独拦截参数校验的三个异常：javax.validation.ConstraintViolationException，org.springframework.validation.BindException，org.springframework.web.bind.MethodArgumentNotValidException，代码如下： @ExceptionHandler(value = {BindException.class, ValidationException.class, MethodArgumentNotValidException.class}) public ResponseEntity&lt;ResultData&lt;String&gt;&gt; handleValidatedException(Exception e) { ResultData&lt;String&gt; resp = null; if (e instanceof MethodArgumentNotValidException) { // BeanValidation exception MethodArgumentNotValidException ex = (MethodArgumentNotValidException) e; resp = ResultData.fail(HttpStatus.BAD_REQUEST.value(), ex.getBindingResult().getAllErrors().stream() .map(ObjectError::getDefaultMessage) .collect(Collectors.joining(&quot;; &quot;)) ); } else if (e instanceof ConstraintViolationException) { // BeanValidation GET simple param ConstraintViolationException ex = (ConstraintViolationException) e; resp = ResultData.fail(HttpStatus.BAD_REQUEST.value(), ex.getConstraintViolations().stream() .map(ConstraintViolation::getMessage) .collect(Collectors.joining(&quot;; &quot;)) ); } else if (e instanceof BindException) { // BeanValidation GET object param BindException ex = (BindException) e; resp = ResultData.fail(HttpStatus.BAD_REQUEST.value(), ex.getAllErrors().stream() .map(ObjectError::getDefaultMessage) .collect(Collectors.joining(&quot;; &quot;)) ); } return new ResponseEntity&lt;&gt;(resp,HttpStatus.BAD_REQUEST); } 体验效果 POST http://localhost:8080/valid/test1 Content-Type: application/json { &quot;id&quot;: 1, &quot;level&quot;: &quot;12&quot;, &quot;email&quot;: &quot;47693899&quot;, &quot;appId&quot;: &quot;ab1c&quot; } { &quot;status&quot;: 400, &quot;message&quot;: &quot;名字为必填项; 不是一个合法的电子邮件地址; appId长度必须位于6到12之间&quot;, &quot;data&quot;: null, &quot;timestamp&quot;: 1628435116680 } 是不是感觉清爽多了？ 自定义参数校验 虽然Spring Validation 提供的注解基本上够用，但是面对复杂的定义，我们还是需要自己定义相关注解来实现自动校验。 比如上面实体类中的sex性别属性，只允许前端传递传 M，F 这2个枚举值，如何实现呢？ 第一步，创建自定义注解 @Target({METHOD, FIELD, ANNOTATION_TYPE, CONSTRUCTOR, PARAMETER, TYPE_USE}) @Retention(RUNTIME) @Repeatable(EnumString.List.class) @Documented @Constraint(validatedBy = EnumStringValidator.class)//标明由哪个类执行校验逻辑 public @interface EnumString { String message() default &quot;value not in enum values.&quot;; Class&lt;?&gt;[] groups() default {}; Class&lt;? extends Payload&gt;[] payload() default {}; /** * @return date must in this value array */ String[] value(); /** * Defines several {@link EnumString} annotations on the same element. * * @see EnumString */ @Target({METHOD, FIELD, ANNOTATION_TYPE, CONSTRUCTOR, PARAMETER, TYPE_USE}) @Retention(RUNTIME) @Documented @interface List { EnumString[] value(); } } 第二步，自定义校验逻辑 public class EnumStringValidator implements ConstraintValidator&lt;EnumString, String&gt; { private List&lt;String&gt; enumStringList; @Override public void initialize(EnumString constraintAnnotation) { enumStringList = Arrays.asList(constraintAnnotation.value()); } @Override public boolean isValid(String value, ConstraintValidatorContext context) { if(value == null){ return true; } return enumStringList.contains(value); } } 第三步，在字段上增加注解 @ApiModelProperty(value = &quot;性别&quot;) @EnumString(value = {&quot;F&quot;,&quot;M&quot;}, message=&quot;性别只允许为F或M&quot;) private String sex; 第四步，体验效果 POST http://localhost:8080/valid/test2 Content-Type: application/x-www-form-urlencoded id=1&amp;name=javadaily&amp;level=12&amp;email=476938977@qq.com&amp;appId=ab1cdddd&amp;sex=N { &quot;status&quot;: 400, &quot;message&quot;: &quot;性别只允许为F或M&quot;, &quot;data&quot;: null, &quot;timestamp&quot;: 1628435243723 } 分组校验 一个VO对象在新增的时候某些字段为必填，在更新的时候又非必填。如上面的ValidVO中 id 和 appId 属性在新增操作时都是非必填，而在编辑操作时都为必填，name在新增操作时为必填，面对这种场景你会怎么处理呢？ 在实际开发中我见到很多同学都是建立两个VO对象，ValidCreateVO，ValidEditVO来处理这种场景，这样确实也能实现效果，但是会造成类膨胀，而且极其容易被开发老鸟们嘲笑。 其实Validator校验框架已经考虑到了这种场景并且提供了解决方案，就是分组校验，只不过很多同学不知道而已。要使用分组校验，只需要三个步骤： 第一步：定义分组接口 public interface ValidGroup extends Default { interface Crud extends ValidGroup{ interface Create extends Crud{ } interface Update extends Crud{ } interface Query extends Crud{ } interface Delete extends Crud{ } } } 这里我们定义一个分组接口ValidGroup让其继承javax.validation.groups.Default，再在分组接口中定义出多个不同的操作类型，Create，Update，Query，Delete。至于为什么需要继承Default我们稍后再说。 第二步，在模型中给参数分配分组 @Data @ApiModel(value = &quot;参数校验类&quot;) public class ValidVO { @ApiModelProperty(&quot;ID&quot;) @Null(groups = ValidGroup.Crud.Create.class) @NotNull(groups = ValidGroup.Crud.Update.class, message = &quot;应用ID不能为空&quot;) private String id; @Null(groups = ValidGroup.Crud.Create.class) @NotNull(groups = ValidGroup.Crud.Update.class, message = &quot;应用ID不能为空&quot;) @ApiModelProperty(value = &quot;应用ID&quot;,example = &quot;cloud&quot;) private String appId; @ApiModelProperty(value = &quot;名字&quot;) @NotBlank(groups = ValidGroup.Crud.Create.class,message = &quot;名字为必填项&quot;) private String name; @ApiModelProperty(value = &quot;邮箱&quot;) @Email(message = &quot;请填写正取的邮箱地址&quot;) privte String email; ... } 给参数指定分组，对于未指定分组的则使用的是默认分组。 第三步，给需要参数校验的方法指定分组 @RestController @Api(&quot;参数校验&quot;) @Slf4j @Validated public class ValidController { @ApiOperation(&quot;新增&quot;) @PostMapping(value = &quot;/valid/add&quot;) public String add(@Validated(value = ValidGroup.Crud.Create.class) ValidVO validVO){ log.info(&quot;validEntity is {}&quot;, validVO); return &quot;test3 valid success&quot;; } @ApiOperation(&quot;更新&quot;) @PostMapping(value = &quot;/valid/update&quot;) public String update(@Validated(value = ValidGroup.Crud.Update.class) ValidVO validVO){ log.info(&quot;validEntity is {}&quot;, validVO); return &quot;test4 valid success&quot;; } } 这里我们通过value属性给add()和update()方法分别指定Create和Update分组。 第四步，体验效果 POST http://localhost:8080/valid/add Content-Type: application/x-www-form-urlencoded name=javadaily&amp;level=12&amp;email=476938977@qq.com&amp;sex=F 在Create时我们没有传递id和appId参数，校验通过。 当我们使用同样的参数调用update方法时则提示参数校验错误。 { &quot;status&quot;: 400, &quot;message&quot;: &quot;ID不能为空; 应用ID不能为空&quot;, &quot;data&quot;: null, &quot;timestamp&quot;: 1628492514313 } 由于email属于默认分组，而我们的分组接口ValidGroup已经继承了Default分组，所以也是可以对email字段作参数校验的。如： POST http://localhost:8080/valid/add Content-Type: application/x-www-form-urlencoded name=javadaily&amp;level=12&amp;email=476938977&amp;sex=F { &quot;status&quot;: 400, &quot;message&quot;: &quot;请填写正取的邮箱地址&quot;, &quot;data&quot;: null, &quot;timestamp&quot;: 1628492637305 } 当然如果你的ValidGroup没有继承Default分组，那在代码属性上就需要加上@Validated(value = {ValidGroup.Crud.Create.class, Default.class}才能让email字段的校验生效。 业务规则校验 业务规则校验指接口需要满足某些特定的业务规则，举个例子：业务系统的用户需要保证其唯一性，用户属性不能与其他用户产生冲突，不允许与数据库中任何已有用户的用户名称、手机号码、邮箱产生重复。 这就要求在创建用户时需要校验用户名称、手机号码、邮箱是否被注册；编辑用户时不能将信息修改成已有用户的属性。 95%的程序员当面对这种业务规则校验时往往选择写在service逻辑中，常见的代码逻辑如下： public void create(User user) { Account account = accountDao.queryByUserNameOrPhoneOrEmail(user.getName(),user.getPhone(),user.getEmail()); if (account != null) { throw new IllegalArgumentException(&quot;用户已存在，请重新输入&quot;); } } 最优雅的实现方法应该是参考 Bean Validation 的标准方式，借助自定义校验注解完成业务规则校验。 接下来我们通过上面提到的用户接口案例，通过自定义注解完成业务规则校验。 代码实战 需求很容易理解，注册新用户时，应约束不与任何已有用户的关键信息重复；而修改自己的信息时，只能与自己的信息重复，不允许修改成已有用户的信息。 这些约束规则不仅仅为这两个方法服务，它们可能会在用户资源中的其他入口被使用到，乃至在其他分层的代码中被使用到，在 Bean 上做校验就能全部覆盖上述这些使用场景。 自定义注解 首先我们需要创建两个自定义注解，用于业务规则校验： UniqueUser:表示一个用户是唯一的，唯一性包含：用户名，手机号码、邮箱 @Documented @Retention(RUNTIME) @Target({FIELD, METHOD, PARAMETER, TYPE}) @Constraint(validatedBy = UserValidation.UniqueUserValidator.class) public @interface UniqueUser { String message() default &quot;用户名、手机号码、邮箱不允许与现存用户重复&quot;; Class&lt;?&gt;[] groups() default {}; Class&lt;? extends Payload&gt;[] payload() default {}; } NotConflictUser:表示一个用户的信息是无冲突的，无冲突是指该用户的敏感信息与其他用户不重合 @Documented @Retention(RUNTIME) @Target({FIELD, METHOD, PARAMETER, TYPE}) @Constraint(validatedBy = UserValidation.NotConflictUserValidator.class) public @interface NotConflictUser { String message() default &quot;用户名称、邮箱、手机号码与现存用户产生重复&quot;; Class&lt;?&gt;[] groups() default {}; Class&lt;? extends Payload&gt;[] payload() default {}; } 实现业务校验规则 想让自定义验证注解生效，需要实现 ConstraintValidator 接口。接口的第一个参数是 自定义注解类型，第二个参数是 被注解字段的类，因为需要校验多个参数，我们直接传入用户对象。需要提到的一点是 ConstraintValidator 接口的实现类无需添加 @Component 它在启动的时候就已经被加载到容器中了。 @Slf4j public class UserValidation&lt;T extends Annotation&gt; implements ConstraintValidator&lt;T, User&gt; { protected Predicate&lt;User&gt; predicate = c -&gt; true; @Resource protected UserRepository userRepository; @Override public boolean isValid(User user, ConstraintValidatorContext constraintValidatorContext) { return userRepository == null || predicate.test(user); } /** * 校验用户是否唯一 * 即判断数据库是否存在当前新用户的信息，如用户名，手机，邮箱 */ public static class UniqueUserValidator extends UserValidation&lt;UniqueUser&gt;{ @Override public void initialize(UniqueUser uniqueUser) { predicate = c -&gt; !userRepository.existsByUserNameOrEmailOrTelphone(c.getUserName(),c.getEmail(),c.getTelphone()); } } /** * 校验是否与其他用户冲突 * 将用户名、邮件、电话改成与现有完全不重复的，或者只与自己重复的，就不算冲突 */ public static class NotConflictUserValidator extends UserValidation&lt;NotConflictUser&gt;{ @Override public void initialize(NotConflictUser notConflictUser) { predicate = c -&gt; { log.info(&quot;user detail is {}&quot;,c); Collection&lt;User&gt; collection = userRepository.findByUserNameOrEmailOrTelphone(c.getUserName(), c.getEmail(), c.getTelphone()); // 将用户名、邮件、电话改成与现有完全不重复的，或者只与自己重复的，就不算冲突 return collection.isEmpty() || (collection.size() == 1 &amp;&amp; collection.iterator().next().getId().equals(c.getId())); }; } } } 这里使用Predicate函数式接口对业务规则进行判断。 使用 @RestController @RequestMapping(&quot;/senior/user&quot;) @Slf4j @Validated public class UserController { @Autowired private UserRepository userRepository; @PostMapping public User createUser(@UniqueUser @Valid User user){ User savedUser = userRepository.save(user); log.info(&quot;save user id is {}&quot;,savedUser.getId()); return savedUser; } @SneakyThrows @PutMapping public User updateUser(@NotConflictUser @Valid @RequestBody User user){ User editUser = userRepository.save(user); log.info(&quot;update user is {}&quot;,editUser); return editUser; } } 使用很简单，只需要在方法上加入自定义注解即可，业务逻辑中不需要添加任何业务规则的代码。 测试 调用接口后出现如下错误，说明业务规则校验生效。 { &quot;status&quot;: 400, &quot;message&quot;: &quot;用户名、手机号码、邮箱不允许与现存用户重复&quot;, &quot;data&quot;: null, &quot;timestamp&quot;: 1644309081037 } 小结 通过上面几步操作，业务校验便和业务逻辑就完全分离开来，在需要校验时用@Validated注解自动触发，或者通过代码手动触发执行，可根据你们项目的要求，将这些注解应用于控制器、服务层、持久层等任何层次的代码之中。 这种方式比任何业务规则校验的方法都优雅，推荐大家在项目中使用。在开发时可以将不带业务含义的格式校验注解放到 Bean 的类定义之上，将带业务逻辑的校验放到 Bean 的类定义的外面。这两者的区别是放在类定义中的注解能够自动运行，而放到类外面则需要像前面代码那样，明确标出注解时才会运行。 ","link":"https://tinaxiawuhao.github.io/post/d4XUQj9K0/"},{"title":"WebFlux与WebMVC区别","content":"在构建响应式 Web 服务上，Spring 5 中引入了全新的编程框架，那就是 Spring WebFlux。作为一款新型的 Web 服务开发框架，它与传统的 WebMVC 相比具体有哪些优势呢？ Spring WebFlux 的应用场景 WebFlux 用于构建响应式 Web 服务。在详细介绍 WebFlux 之前，我们先梳理一下这个新框架的应用场景，了解应用场景才能帮助我们对所要采用的技术体系做出正确的选择。 微服务架构的兴起为 WebFlux 的应用提供了一个很好的场景。我们知道在一个微服务系统中，存在数十乃至数百个独立的微服务，它们相互通信以完成复杂的业务流程。这个过程势必会涉及大量的 I/O 操作，尤其是阻塞式 I/O 操作会整体增加系统的延迟并降低吞吐量。如果能够在复杂的流程中集成非阻塞、异步通信机制，我们就可以高效处理跨服务之间的网络请求。针对这种场景，WebFlux 是一种非常有效的解决方案。 从 WebMVC 到 WebFlux 接下来，我们将讨论 WebMVC 与 WebFlux 之间的差别，而这些差别实际上正是体现在从 WebMVC 到 WebFlux 的演进过程中。让我们先从传统的 Spring WebMVC 技术栈开始说起。 Spring WebMVC技术栈 一般而言，Web 请求处理机制都会使用“管道-过滤器（Pipe-Filter）”架构模式，而 Spring WebMVC 作为一种处理 Web 请求的典型实现方案，同样使用了 Servlet 中的过滤器链（FilterChain）来对请求进行拦截，如下图所示。 我们知道 WebMVC 运行在 Servlet 容器上，这些容器常用的包括 Tomcat、JBoss 等。当 HTTP 请求通过 Servlet 容器时就会被转换为一个 ServletRequest 对象，而最终返回一个 ServletResponse 对象，FilterChain 的定义如下所示。 public interface FilterChain { public void doFilter (ServletRequest request, ServletResponse response ) throws IOException, ServletException; } 当 ServletRequest 通过过滤器链中所包含的一系列过滤器之后，最终就会到达作为前端控制器的 DispatcherServlet。DispatcherServlet 是 WebMVC 的核心组件，扩展了 Servlet 对象，并持有一组 HandlerMapping 和 HandlerAdapter。 当 ServletRequest 请求到达时，DispatcherServlet 负责搜索 HandlerMapping 实例并使用合适的 HandlerAdapter 对其进行适配。其中，HandlerMapping 的作用是根据当前请求找到对应的处理器 Handler，它只定义了一个方法，如下所示。 public interface HandlerMapping { //找到与请求对应的 Handler，封装为一个 HandlerExecutionChain 返回 HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception; } 而 HandlerAdapter 根据给定的 HttpServletRequest 和 HttpServletResponse 对象真正调用给定的 Handler，核心方法如下所示。 public interface HandlerAdapter { //针对给定的请求/响应对象调用目标 Handler ModelAndView handle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception; } 在执行过程中，DispatcherServlet 会在应用上下文中搜索所有 HandlerMapping。日常开发过程中，最常用的 HandlerMapping 包含 BeanNameUrlHandlerMapping 和 RequestMappingHandlerMapping，前者负责检测所有 Controller 并根据请求 URL 的匹配规则映射到具体的 Controller 实例上，而后者基于 @RequestMapping 注解来找到目标 Controller。 如果我们使用了 RequestMappingHandlerMapping，那么对应的 HandlerAdapter 就是 RequestMappingHandlerAdapter，它负责将传入的 ServletRequest 绑定到添加了 @RequestMapping 注解的控制器方法上，从而实现对请求的正确响应。同时， HandlerAdapter 还提供请求验证和响应转换等辅助性功能，使得 Spring WebMVC 框架在日常 Web 开发中非常实用。 作为总结，我梳理了 Spring WebMVC 的整体架构，如下图所示。 一直以来，Spring WebMVC 是我们开发 Web 服务的主流框架。但要注意的是，尽管 Servlet 本身在新版本中提供了异步非阻塞的通信机制，但 Spring WebMVC 在实现上并不允许在整个请求生命周期中都采用非阻塞式的操作方式。因此，Spring 在尽量沿用原有的开发模式以及 API 设计上提供了支持异步非阻塞的 Spring WebFlux 框架。 Spring WebFlux 技术栈 介绍完 Spring WebMVC，我们来说说 Spring WebFlux。事实上，前面介绍的 HandlerMapping、HandlerAdapter 等组件在 WebFlux 里都有同名的响应式版本，这是 WebFlux 的一种设计理念，即在既有设计的基础上，提供新的实现版本，只对部分需要增强和弱化的地方做了调整。 我们先来看第一个需要调整的地方，显然，我们应该替换掉原有的 Servlet API 以便融入响应式流。因此，在 WebFlux 中，代表请求和响应的是全新的 ServerHttpRequest 和 ServerHttpResponse 对象。 同样，WebFlux 中同样提供了一个过滤器链 WebFilterChain，定义如下。 public interface WebFilterChain { Mono&lt;Void&gt; filter(ServerWebExchange exchange); } 这里的 ServerWebExchange 相当于一个上下文容器，保存了 ServerHttpRequest、ServerHttpResponse 以及一些框架运行时状态信息。 在 WebFlux 中，和 WebMVC 中的 DispatcherServlet 相对应的组件是 DispatcherHandler。与 DispatcherServlet 类似，DispatcherHandler 同样使用了一套响应式版本的 HandlerMapping 和 HandlerAdapter 完成对请求的处理。请注意，这两个接口是定义在 org.springframework.web.reactive 包中，而不是在原有的 org.springframework.web 包中。响应式版本的 HandlerMapping 接口定义如下，可以看到这里返回的是一个 Mono 对象，从而启用了响应式行为模式。 public interface HandlerMapping { Mono&lt;Object&gt; getHandler(ServerWebExchange exchange); } 同样，我们找到响应式版本的 HandlerAdapter，如下所示。 public interface HandlerAdapter { Mono&lt;HandlerResult&gt; handle(ServerWebExchange exchange, Object handler); } 对比非响应式版本的 HandlerAdapter，这里的 ServerWebExchange 中同时包含了 ServerHttpRequest 和 ServerHttpResponse 对象，而 HandlerResult 则代表了处理结果。相比 WebMVC 中 ModelAndView 这种比较模糊的返回结果，HandlerResult 更加直接和明确。 在 WebFlux 中，同样实现了响应式版本的 RequestMappingHandlerMapping 和 RequestMappingHandlerAdapter，因此我们仍然可以采用注解的方法来构建 Controller。另一方面，WebFlux 中还提供了 RouterFunctionMapping 和 HandlerFunctionAdapter 组合，专门用来提供基于函数式编程的开发模式。这样 Spring WebFlux 的整体架构图就演变成这样。 请注意，在处理 HTTP 请求上，我们需要使用支持异步非阻塞的响应式服务器引擎，常见的包括 Netty、Undertow 以及支持 Servlet 3.1 及以上版本的 Servlet 容器。 对比 WebFlux 和 WebMVC 的处理模型 现在我们已经明确了 WebMVC 到 WebFlux 的演进过程，但你可能会问，新的 WebFlux 要比传统 WebMVC 好在哪里呢？从两者的处理模型上入手可以帮助你很好地理解这个问题，我们一起来看一下。 WebFlux 和 Web MVC 中的处理模型 通过前面的讨论你已经知道 Servlet 是阻塞式的，所以 WebMVC 建立在阻塞 I/O 之上，我们来分析这种模型下线程处理请求的过程。假设有一个工作线程会处理来自客户端的请求，所有请求构成一个请求队列，并由一个线程按顺序进行处理。针对一个请求，线程需要执行两部分工作，首先是接受请求，然后再对其进行处理，如下图所示。 在前面的示例中，正如你可能注意到的，工作线程的实际处理时间远小于花费在阻塞操作上的时间。这意味着工作线程会被 I/O 读取或写入数据这一操作所阻塞。从这个简单的图中，我们可以得出结论，线程效率低下。同时，因为所有请求是排队的，相当于一个请求队列，所以接受请求和处理请求这两部分操作实际上是可以共享等待时间的。 相比之下，WebFlux 构建在非阻塞 API 之上，这意味着没有操作需要与 I/O 阻塞线程进行交互。接受和处理请求的效率很高，如下图所示。 将上图中所展示的异步非阻塞请求处理与前面的阻塞过程进行比较，我们会注意到，现在没有在读取请求数据时发生等待，工作线程高效接受新连接。然后，提供了非阻塞 I/O 机制的底层操作系统会告诉我们请求数据是否已经接收完成，并且处理器可以在不阻塞的情况下进行处理。 类似的，写入响应结果时同样不需要阻塞，操作系统会在准备好将一部分数据非阻塞地写入 I/O 时通知我们。这样，我们就拥有了最佳的 CPU 利用率。 前面的示例展示了 WebFlux 比 WebMVC 更有效地利用一个工作线程，因此可以在相同的时间内处理更多的请求。那么，如果是在多线程的场景下会发生什么呢？我们来看下面这张图。 从上图中可以看出，多线程模型允许更快地处理排队请求，能够同时接受、处理和响应几乎相同数量的请求。当然，我们明白多线程技术有利有弊。当处理用户请求涉及太多的线程实例时，相互之间就需要协调资源，这是由于它们之间的不一致性会导致性能下降。 ","link":"https://tinaxiawuhao.github.io/post/OoSJ8O7Jf/"},{"title":"Spring Boot 自带 Buff 工具类","content":"断言 断言是一个逻辑判断，用于检查不应该发生的情况 Assert 关键字在 JDK1.4 中引入，可通过 JVM 参数-enableassertions开启 SpringBoot 中提供了 Assert 断言工具类，通常用于数据合法性检查 // 要求参数 object 必须为非空（Not Null），否则抛出异常，不予放行 // 参数 message 参数用于定制异常信息。 void notNull(Object object, String message) // 要求参数必须空（Null），否则抛出异常，不予『放行』。 // 和 notNull() 方法断言规则相反 void isNull(Object object, String message) // 要求参数必须为真（True），否则抛出异常，不予『放行』。 void isTrue(boolean expression, String message) // 要求参数（List/Set）必须非空（Not Empty），否则抛出异常，不予放行 void notEmpty(Collection collection, String message) // 要求参数（String）必须有长度（即，Not Empty），否则抛出异常，不予放行 void hasLength(String text, String message) // 要求参数（String）必须有内容（即，Not Blank），否则抛出异常，不予放行 void hasText(String text, String message) // 要求参数是指定类型的实例，否则抛出异常，不予放行 void isInstanceOf(Class type, Object obj, String message) // 要求参数 `subType` 必须是参数 superType 的子类或实现类，否则抛出异常，不予放行 void isAssignable(Class superType, Class subType, String message) 对象、数组、集合 ObjectUtils 获取对象的基本信息 // 获取对象的类名。参数为 null 时，返回字符串：&quot;null&quot; String nullSafeClassName(Object obj) // 参数为 null 时，返回 0 int nullSafeHashCode(Object object) // 参数为 null 时，返回字符串：&quot;null&quot; String nullSafeToString(boolean[] array) // 获取对象 HashCode（十六进制形式字符串）。参数为 null 时，返回 0 String getIdentityHexString(Object obj) // 获取对象的类名和 HashCode。 参数为 null 时，返回字符串：&quot;&quot; String identityToString(Object obj) // 相当于 toString()方法，但参数为 null 时，返回字符串：&quot;&quot; String getDisplayString(Object obj) 判断工具 // 判断数组是否为空 boolean isEmpty(Object[] array) // 判断参数对象是否是数组 boolean isArray(Object obj) // 判断数组中是否包含指定元素 boolean containsElement(Object[] array, Object element) // 相等，或同为 null时，返回 true boolean nullSafeEquals(Object o1, Object o2) /* 判断参数对象是否为空，判断标准为： Optional: Optional.empty() Array: length == 0 CharSequence: length == 0 Collection: Collection.isEmpty() Map: Map.isEmpty() */ boolean isEmpty(Object obj) 其他工具方法 // 向参数数组的末尾追加新元素，并返回一个新数组 &lt;A, O extends A&gt; A[] addObjectToArray(A[] array, O obj) // 原生基础类型数组 --&gt; 包装类数组 Object[] toObjectArray(Object source) StringUtils 字符串判断工具 // 判断字符串是否为 null，或 &quot;&quot;。注意，包含空白符的字符串为非空 boolean isEmpty(Object str) // 判断字符串是否是以指定内容结束。忽略大小写 boolean endsWithIgnoreCase(String str, String suffix) // 判断字符串是否已指定内容开头。忽略大小写 boolean startsWithIgnoreCase(String str, String prefix) // 是否包含空白符 boolean containsWhitespace(String str) // 判断字符串非空且长度不为 0，即，Not Empty boolean hasLength(CharSequence str) // 判断字符串是否包含实际内容，即非仅包含空白符，也就是 Not Blank boolean hasText(CharSequence str) // 判断字符串指定索引处是否包含一个子串。 boolean substringMatch(CharSequence str, int index, CharSequence substring) // 计算一个字符串中指定子串的出现次数 int countOccurrencesOf(String str, String sub) 字符串操作工具 // 查找并替换指定子串 String replace(String inString, String oldPattern, String newPattern) // 去除尾部的特定字符 String trimTrailingCharacter(String str, char trailingCharacter) // 去除头部的特定字符 String trimLeadingCharacter(String str, char leadingCharacter) // 去除头部的空白符 String trimLeadingWhitespace(String str) // 去除头部的空白符 String trimTrailingWhitespace(String str) // 去除头部和尾部的空白符 String trimWhitespace(String str) // 删除开头、结尾和中间的空白符 String trimAllWhitespace(String str) // 删除指定子串 String delete(String inString, String pattern) // 删除指定字符（可以是多个） String deleteAny(String inString, String charsToDelete) // 对数组的每一项执行 trim() 方法 String[] trimArrayElements(String[] array) // 将 URL 字符串进行解码 String uriDecode(String source, Charset charset) 路径相关工具方法 // 解析路径字符串，优化其中的 “..” String cleanPath(String path) // 解析路径字符串，解析出文件名部分 String getFilename(String path) // 解析路径字符串，解析出文件后缀名 String getFilenameExtension(String path) // 比较两个两个字符串，判断是否是同一个路径。会自动处理路径中的 “..” boolean pathEquals(String path1, String path2) // 删除文件路径名中的后缀部分 String stripFilenameExtension(String path) // 以 “. 作为分隔符，获取其最后一部分 String unqualify(String qualifiedName) // 以指定字符作为分隔符，获取其最后一部分 String unqualify(String qualifiedName, char separator) CollectionUtils 集合判断工具 // 判断 List/Set 是否为空 boolean isEmpty(Collection&lt;?&gt; collection) // 判断 Map 是否为空 boolean isEmpty(Map&lt;?,?&gt; map) // 判断 List/Set 中是否包含某个对象 boolean containsInstance(Collection&lt;?&gt; collection, Object element) // 以迭代器的方式，判断 List/Set 中是否包含某个对象 boolean contains(Iterator&lt;?&gt; iterator, Object element) // 判断 List/Set 是否包含某些对象中的任意一个 boolean containsAny(Collection&lt;?&gt; source, Collection&lt;?&gt; candidates) // 判断 List/Set 中的每个元素是否唯一。即 List/Set 中不存在重复元素 boolean hasUniqueObject(Collection&lt;?&gt; collection) 集合操作工具 // 将 Array 中的元素都添加到 List/Set 中 &lt;E&gt; void mergeArrayIntoCollection(Object array, Collection&lt;E&gt; collection) // 将 Properties 中的键值对都添加到 Map 中 &lt;K,V&gt; void mergePropertiesIntoMap(Properties props, Map&lt;K,V&gt; map) // 返回 List 中最后一个元素 &lt;T&gt; T lastElement(List&lt;T&gt; list) // 返回 Set 中最后一个元素 &lt;T&gt; T lastElement(Set&lt;T&gt; set) // 返回参数 candidates 中第一个存在于参数 source 中的元素 &lt;E&gt; E findFirstMatch(Collection&lt;?&gt; source, Collection&lt;E&gt; candidates) // 返回 List/Set 中指定类型的元素。 &lt;T&gt; T findValueOfType(Collection&lt;?&gt; collection, Class&lt;T&gt; type) // 返回 List/Set 中指定类型的元素。如果第一种类型未找到，则查找第二种类型，以此类推 Object findValueOfType(Collection&lt;?&gt; collection, Class&lt;?&gt;[] types) // 返回 List/Set 中元素的类型 Class&lt;?&gt; findCommonElementType(Collection&lt;?&gt; collection) 文件、资源、IO 流 FileCopyUtils 输入 // 从文件中读入到字节数组中 byte[] copyToByteArray(File in) // 从输入流中读入到字节数组中 byte[] copyToByteArray(InputStream in) // 从输入流中读入到字符串中 String copyToString(Reader in) 输出 // 从字节数组到文件 void copy(byte[] in, File out) // 从文件到文件 int copy(File in, File out) // 从字节数组到输出流 void copy(byte[] in, OutputStream out) // 从输入流到输出流 int copy(InputStream in, OutputStream out) // 从输入流到输出流 int copy(Reader in, Writer out) // 从字符串到输出流 void copy(String in, Writer out) ResourceUtils 从资源路径获取文件 // 判断字符串是否是一个合法的 URL 字符串。 static boolean isUrl(String resourceLocation) // 获取 URL static URL getURL(String resourceLocation) // 获取文件（在 JAR 包内无法正常使用，需要是一个独立的文件） static File getFile(String resourceLocation) Resource // 文件系统资源 D:\\... FileSystemResource // URL 资源，如 file://... http://... UrlResource // 类路径下的资源，classpth:... ClassPathResource // Web 容器上下文中的资源（jar 包、war 包） ServletContextResource 复制代码 // 判断资源是否存在 boolean exists() // 从资源中获得 File 对象 File getFile() // 从资源中获得 URI 对象 URI getURI() // 从资源中获得 URI 对象 URL getURL() // 获得资源的 InputStream InputStream getInputStream() // 获得资源的描述信息 String getDescription() StreamUtils 输入 void copy(byte[] in, OutputStream out) int copy(InputStream in, OutputStream out) void copy(String in, Charset charset, OutputStream out) long copyRange(InputStream in, OutputStream out, long start, long end) 输出 byte[] copyToByteArray(InputStream in) String copyToString(InputStream in, Charset charset) // 舍弃输入流中的内容 int drain(InputStream in) 反射、AOP ReflectionUtils 获取方法 // 在类中查找指定方法 Method findMethod(Class&lt;?&gt; clazz, String name) // 同上，额外提供方法参数类型作查找条件 Method findMethod(Class&lt;?&gt; clazz, String name, Class&lt;?&gt;... paramTypes) // 获得类中所有方法，包括继承而来的 Method[] getAllDeclaredMethods(Class&lt;?&gt; leafClass) // 在类中查找指定构造方法 Constructor&lt;T&gt; accessibleConstructor(Class&lt;T&gt; clazz, Class&lt;?&gt;... parameterTypes) // 是否是 equals() 方法 boolean isEqualsMethod(Method method) // 是否是 hashCode() 方法 boolean isHashCodeMethod(Method method) // 是否是 toString() 方法 boolean isToStringMethod(Method method) // 是否是从 Object 类继承而来的方法 boolean isObjectMethod(Method method) // 检查一个方法是否声明抛出指定异常 boolean declaresException(Method method, Class&lt;?&gt; exceptionType) 执行方法 // 执行方法 Object invokeMethod(Method method, Object target) // 同上，提供方法参数 Object invokeMethod(Method method, Object target, Object... args) // 取消 Java 权限检查。以便后续执行该私有方法 void makeAccessible(Method method) // 取消 Java 权限检查。以便后续执行私有构造方法 void makeAccessible(Constructor&lt;?&gt; ctor) 获取字段 // 在类中查找指定属性 Field findField(Class&lt;?&gt; clazz, String name) // 同上，多提供了属性的类型 Field findField(Class&lt;?&gt; clazz, String name, Class&lt;?&gt; type) // 是否为一个 &quot;public static final&quot; 属性 boolean isPublicStaticFinal(Field field) 设置字段 // 获取 target 对象的 field 属性值 Object getField(Field field, Object target) // 设置 target 对象的 field 属性值，值为 value void setField(Field field, Object target, Object value) // 同类对象属性对等赋值 void shallowCopyFieldState(Object src, Object dest) // 取消 Java 的权限控制检查。以便后续读写该私有属性 void makeAccessible(Field field) // 对类的每个属性执行 callback void doWithFields(Class&lt;?&gt; clazz, ReflectionUtils.FieldCallback fc) // 同上，多了个属性过滤功能。 void doWithFields(Class&lt;?&gt; clazz, ReflectionUtils.FieldCallback fc, ReflectionUtils.FieldFilter ff) // 同上，但不包括继承而来的属性 void doWithLocalFields(Class&lt;?&gt; clazz, ReflectionUtils.FieldCallback fc) AopUtils 判断代理类型 // 判断是不是 Spring 代理对象 boolean isAopProxy() // 判断是不是 jdk 动态代理对象 isJdkDynamicProxy() // 判断是不是 CGLIB 代理对象 boolean isCglibProxy() 获取被代理对象的 class // 获取被代理的目标 class Class&lt;?&gt; getTargetClass() AopContext 获取当前对象的代理对象 Object currentProxy() ","link":"https://tinaxiawuhao.github.io/post/I1j63ouUN/"},{"title":"nginx.conf中文详解","content":"#定义Nginx运行的用户和用户组 user www www; #nginx进程数，建议设置为等于CPU总核心数。 worker_processes 8; #全局错误日志定义类型，[ debug | info | notice | warn | error | crit ] error_log /usr/local/nginx/logs/error.log info; #进程pid文件 pid /usr/local/nginx/logs/nginx.pid; #指定进程可以打开的最大描述符：数目 #工作模式与连接数上限 #这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致。 #现在在linux 2.6内核下开启文件打开数为65535，worker_rlimit_nofile就相应应该填写65535。 #这是因为nginx调度时分配请求到进程并不是那么的均衡，所以假如填写10240，总并发量达到3-4万时就有进程可能超过10240了，这时会返回502错误。 worker_rlimit_nofile 65535; events{ ​ #参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型 ​ #是Linux 2.6以上版本内核中的高性能网络I/O模型，linux建议epoll，如果跑在FreeBSD上面，就用kqueue模型。 ​ #补充说明： ​ #与apache相类，nginx针对不同的操作系统，有不同的事件模型 ​ #A）标准事件模型 ​ #Select、poll属于标准事件模型，如果当前系统不存在更有效的方法，nginx会选择select或poll ​ #B）高效事件模型 ​ #Kqueue：使用于FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0 和 MacOS X.使用双处理器的MacOS X系统使用kqueue可能会造成内核崩溃。 ​ #Epoll：使用于Linux内核2.6版本及以后的系统。 ​ #/dev/poll：使用于Solaris 7 11/99+，HP/UX 11.22+ (eventport)，IRIX 6.5.15+ 和 Tru64 UNIX 5.1A+。 ​ #Eventport：使用于Solaris 10。 为了防止出现内核崩溃的问题， 有必要安装安全补丁。 ​ use epoll; ​ #单个进程最大连接数（最大连接数=连接数*进程数） ​ #根据硬件调整，和前面工作进程配合起来用，尽量大，但是别把cpu跑到100%就行。每个进程允许的最多连接数，理论上每台nginx服务器的最大连接数为。 ​ worker_connections 65535; ​ #keepalive超时时间。 ​ keepalive_timeout 60; ​ #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求头的大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。 ​ #分页大小可以用命令getconf PAGESIZE 取得。 ​ #[root@web001 ~]# getconf PAGESIZE ​ #4096 ​ #但也有client_header_buffer_size超过4k的情况，但是client_header_buffer_size该值必须设置为“系统分页大小”的整倍数。 ​ client_header_buffer_size 4k; ​ #这个将为打开文件指定缓存，默认是没有启用的，max指定缓存数量，建议和打开文件数一致，inactive是指经过多长时间文件没被请求后删除缓存。 ​ open_file_cache max=65535 inactive=60s; ​ #这个是指多长时间检查一次缓存的有效信息。 ​ #语法:open_file_cache_valid time 默认值:open_file_cache_valid 60 使用字段:http, server, location 这个指令指定了何时需要检查open_file_cache中缓存项目的有效信息. ​ open_file_cache_valid 80s; ​ #open_file_cache指令中的inactive参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive时间内一次没被使用，它将被移除。 ​ #语法:open_file_cache_min_uses number 默认值:open_file_cache_min_uses 1 使用字段:http, server, location 这个指令指定了在open_file_cache指令无效的参数中一定的时间范围内可以使用的最小文件数,如果使用更大的值,文件描述符在cache中总是打开状态. ​ open_file_cache_min_uses 1; ​ #语法:open_file_cache_errors on | off 默认值:open_file_cache_errors off 使用字段:http, server, location 这个指令指定是否在搜索一个文件时记录cache错误. ​ open_file_cache_errors on; } #设定http服务器，利用它的反向代理功能提供负载均衡支持 http{ ​ #文件扩展名与文件类型映射表 ​ include mime.types; ​ #默认文件类型 ​ default_type application/octet-stream; ​ #默认编码 ​ #charset utf-8; ​ #服务器名字的hash表大小 ​ #保存服务器名字的hash表是由指令server_names_hash_max_size 和server_names_hash_bucket_size所控制的。参数hash bucket size总是等于hash表的大小，并且是一路处理器缓存大小的倍数。在减少了在内存中的存取次数后，使在处理器中加速查找hash表键值成为可能。如果hash bucket size等于一路处理器缓存的大小，那么在查找键的时候，最坏的情况下在内存中查找的次数为2。第一次是确定存储单元的地址，第二次是在存储单元中查找键 值。因此，如果Nginx给出需要增大hash max size 或 hash bucket size的提示，那么首要的是增大前一个参数的大小. ​ server_names_hash_bucket_size 128; ​ #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求的头部大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。分页大小可以用命令getconf PAGESIZE取得。 ​ client_header_buffer_size 32k; ​ #客户请求头缓冲大小。nginx默认会用client_header_buffer_size这个buffer来读取header值，如果header过大，它会使用large_client_header_buffers来读取。 ​ large_client_header_buffers 4 64k; ​ #设定通过nginx上传文件的大小 ​ client_max_body_size 8m; ​ #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。 ​ #sendfile指令指定 nginx 是否调用sendfile 函数（zero copy 方式）来输出文件，对于普通应用，必须设为on。如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络IO处理速度，降低系统uptime。 ​ sendfile on; ​ #开启目录列表访问，合适下载服务器，默认关闭。 ​ autoindex on; ​ #此选项允许或禁止使用socke的TCP_CORK的选项，此选项仅在使用sendfile的时候使用 ​ tcp_nopush on; ​ tcp_nodelay on; ​ #长连接超时时间，单位是秒 ​ keepalive_timeout 120; ​ #FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。 ​ fastcgi_connect_timeout 300; ​ fastcgi_send_timeout 300; ​ fastcgi_read_timeout 300; ​ fastcgi_buffer_size 64k; ​ fastcgi_buffers 4 64k; ​ fastcgi_busy_buffers_size 128k; ​ fastcgi_temp_file_write_size 128k; ​ #gzip模块设置 ​ gzip on; #开启gzip压缩输出 ​ gzip_min_length 1k; #最小压缩文件大小 ​ gzip_buffers 4 16k; #压缩缓冲区 ​ gzip_http_version 1.0; #压缩版本（默认1.1，前端如果是squid2.5请使用1.0） ​ gzip_comp_level 2; #压缩等级 ​ gzip_types text/plain application/x-javascript text/css application/xml; #压缩类型，默认就已经包含textml，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。 ​ gzip_vary on; ​ #开启限制IP连接数的时候需要使用 ​ #limit_zone crawler $binary_remote_addr 10m; ​ #负载均衡配置 ​ upstream jh.w3cschool.cn { ​ #upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。 ​ server 192.168.80.121:80 weight=3; ​ server 192.168.80.122:80 weight=2; ​ server 192.168.80.123:80 weight=3; ​ #nginx的upstream目前支持4种方式的分配 ​ #1、轮询（默认） ​ #每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 ​ #2、weight ​ #指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 ​ #例如： ​ #upstream bakend { ​ # server 192.168.0.14 weight=10; ​ # server 192.168.0.15 weight=10; ​ #} ​ #2、ip_hash ​ #每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 ​ #例如： ​ #upstream bakend { ​ # ip_hash; ​ # server 192.168.0.14:88; ​ # server 192.168.0.15:80; ​ #} ​ #3、fair（第三方） ​ #按后端服务器的响应时间来分配请求，响应时间短的优先分配。 ​ #upstream backend { ​ # server server1; ​ # server server2; ​ # fair; ​ #} ​ #4、url_hash（第三方） ​ #按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 ​ #例：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法 ​ #upstream backend { ​ # server squid1:3128; ​ # server squid2:3128; ​ # hash $request_uri; ​ # hash_method crc32; ​ #} ​ #tips: ​ #upstream bakend{#定义负载均衡设备的Ip及设备状态}{ ​ # ip_hash; ​ # server 127.0.0.1:9090 down; ​ # server 127.0.0.1:8080 weight=2; ​ # server 127.0.0.1:6060; ​ # server 127.0.0.1:7070 backup; ​ #} ​ #在需要使用负载均衡的server中增加 proxy_pass http://bakend/; ​ #每个设备的状态设置为: ​ #1.down表示单前的server暂时不参与负载 ​ #2.weight为weight越大，负载的权重就越大。 ​ #3.max_fails：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream模块定义的错误 ​ #4.fail_timeout:max_fails次失败后，暂停的时间。 ​ #5.backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。 ​ #nginx支持同时设置多组的负载均衡，用来给不用的server来使用。 ​ #client_body_in_file_only设置为On 可以讲client post过来的数据记录到文件中用来做debug ​ #client_body_temp_path设置记录文件的目录 可以设置最多3层目录 ​ #location对URL进行匹配.可以进行重定向或者进行新的代理 负载均衡 ​ } ​ #虚拟主机的配置 ​ server{ ​ #监听端口 ​ listen 80; ​ #域名可以有多个，用空格隔开 ​ server_name www.w3cschool.cn w3cschool.cn; ​ index index.html index.htm index.php; ​ root /data/www/w3cschool; ​ #对******进行负载均衡 ​ location ~ .*.(php|php5)?${ ​ fastcgi_pass 127.0.0.1:9000; ​ fastcgi_index index.php; ​ include fastcgi.conf; ​ } ​ #图片缓存时间设置 ​ location ~ .*.(gif|jpg|jpeg|png|bmp|swf)${ ​ expires 10d; ​ } ​ #JS和CSS缓存时间设置 ​ location ~ .*.(js|css)?${ ​ expires 1h; ​ } ​ #日志格式设定 ​ #$remote_addr与$http_x_forwarded_for用以记录客户端的ip地址； ​ #$remote_user：用来记录客户端用户名称； ​ #$time_local： 用来记录访问时间与时区； ​ #$request： 用来记录请求的url与http协议； ​ #$status： 用来记录请求状态；成功是200， ​ #$body_bytes_sent ：记录发送给客户端文件主体内容大小； ​ #$http_referer：用来记录从那个页面链接访问过来的； ​ #$http_user_agent：记录客户浏览器的相关信息； ​ #通常web服务器放在反向代理的后面，这样就不能获取到客户的IP地址了，通过$remote_add拿到的IP地址是反向代理服务器的iP地址。反向代理服务器在转发请求的http头信息中，可以增加x_forwarded_for信息，用以记录原有客户端的IP地址和原来客户端的请求的服务器地址。 ​ log_format access '$remote_addr - $remote_user [$time_local] &quot;$request&quot; ' ​ '$status $body_bytes_sent &quot;$http_referer&quot; ' ​ '&quot;$http_user_agent&quot; $http_x_forwarded_for'; ​ #定义本虚拟主机的访问日志 ​ access_log /usr/local/nginx/logs/host.access.log main; ​ access_log /usr/local/nginx/logs/host.access.404.log log404; ​ #对 &quot;/&quot; 启用反向代理 ​ location / { ​ proxy_pass http://127.0.0.1:88; ​ proxy_redirect off; ​ proxy_set_header X-Real-IP $remote_addr; ​ #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP ​ proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; ​ #以下是一些反向代理的配置，可选。 ​ proxy_set_header Host $host; ​ #允许客户端请求的最大单文件字节数 ​ client_max_body_size 10m; ​ #缓冲区代理缓冲用户端请求的最大字节数， ​ #如果把它设置为比较大的数值，例如256k，那么，无论使用firefox还是IE浏览器，来提交任意小于256k的图片，都很正常。如果注释该指令，使用默认的client_body_buffer_size设置，也就是操作系统页面大小的两倍，8k或者16k，问题就出现了。 ​ #无论使用firefox4.0还是IE8.0，提交一个比较大，200k左右的图片，都返回500 Internal Server Error错误 ​ client_body_buffer_size 128k; ​ #表示使nginx阻止HTTP应答代码为400或者更高的应答。 ​ proxy_intercept_errors on; ​ #后端服务器连接的超时时间_发起握手等候响应超时时间 ​ #nginx跟后端服务器连接超时时间(代理连接超时) ​ proxy_connect_timeout 90; ​ #后端服务器数据回传时间(代理发送超时) ​ #后端服务器数据回传时间_就是在规定时间之内后端服务器必须传完所有的数据 ​ proxy_send_timeout 90; ​ #连接成功后，后端服务器响应时间(代理接收超时) ​ #连接成功后_等候后端服务器响应时间_其实已经进入后端的排队之中等候处理（也可以说是后端服务器处理请求的时间） ​ proxy_read_timeout 90; ​ #设置代理服务器（nginx）保存用户头信息的缓冲区大小 ​ #设置从被代理服务器读取的第一部分应答的缓冲区大小，通常情况下这部分应答中包含一个小的应答头，默认情况下这个值的大小为指令proxy_buffers中指定的一个缓冲区的大小，不过可以将其设置为更小 ​ proxy_buffer_size 4k; ​ #proxy_buffers缓冲区，网页平均在32k以下的设置 ​ #设置用于读取应答（来自被代理服务器）的缓冲区数目和大小，默认情况也为分页大小，根据操作系统的不同可能是4k或者8k ​ proxy_buffers 4 32k; ​ #高负荷下缓冲大小（proxy_buffers*2） ​ proxy_busy_buffers_size 64k; ​ #设置在写入proxy_temp_path时数据的大小，预防一个工作进程在传递文件时阻塞太长 ​ #设定缓存文件夹大小，大于这个值，将从upstream服务器传 ​ proxy_temp_file_write_size 64k; ​ } ​ #设定查看Nginx状态的地址 ​ location /NginxStatus { ​ stub_status on; ​ access_log on; ​ auth_basic &quot;NginxStatus&quot;; ​ auth_basic_user_file confpasswd; ​ #htpasswd文件的内容可以用apache提供的htpasswd工具来产生。 ​ } ​ #本地动静分离反向代理配置 ​ #所有jsp的页面均交由tomcat或resin处理 ​ location ~ .(jsp|jspx|do)?$ { ​ proxy_set_header Host $host; ​ proxy_set_header X-Real-IP $remote_addr; ​ proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; ​ proxy_pass http://127.0.0.1:8080; ​ } ​ #所有静态文件由nginx直接读取不经过tomcat或resin ​ location ~ .*.(htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt| ​ pdf|xls|mp3|wma)${ ​ expires 15d; ​ } ​ location ~ .*.(js|css)?${ ​ expires 1h; ​ } ​ } } ","link":"https://tinaxiawuhao.github.io/post/yRJDaVAsJ/"},{"title":"docker 宿主机定时清除容器的运行日志","content":"一般docker容器都是最小化安装，不仅如此系统定时器相关的服务也不存在，自己去安装也很麻烦，故此直接使用宿主机的定时器即可。 一、在容器中编写清除日志脚本 这一部分不论你是把定时器加在宿主机或者是容器都必须要去做的 ； 网上随意一搜就可以看到如下的删除模板： find 对应目录 -mtime +天数 -name &quot;文件名&quot; -exec rm -rf {} \\; 因为本人的日志目录层级比较深 所以改良了如下： 1. -- /opt/auto-del-log.sh 2. \\#!/bin/sh 3. find /home/schedule_log/ -mtime -5 -type f -iname &quot;*.log&quot; -exec rm -rf {} \\; 一定记得加可执行权限 chmod +777 /opt/auto-del-log.sh 后面经过验证 其实效果是一样的! 重点就是你要去验证你的脚本有无效！ 你可以这样直接输入验证 1. find /home/schedule_log/ -type f -iname &quot;*.log&quot; 2. 或者 3. find /home/schedule_log/ -name &quot;*.log&quot; 如果能查出你想删除的文件那么后面就可以开始套模板了。 -mtime：标准语句写法； +30：查找30天前的文件，这里用数字代表天数； &quot;*.log&quot;：希望查找的数据类型，&quot;*.jpg&quot;表示查找扩展名为jpg的所有文件，&quot;*&quot;表示查找所有文件，这个可以灵活运用，举一反三； -exec：固定写法； rm -rf：强制删除文件，包括目录； {} \\; ：固定写法，一对大括号+空格+\\+; 二、宿主机加入定时器 使用docker exec 命令校验之前写的脚本是否有效 如下： docker exec -it tomcat8002 /opt/auto-del-log.sh tomcat8002 ： 容器名称或者ID /opt/auto-del-log.sh :脚本在容器中的位置 如果此命令有效那么就可以编辑定时器了 本人采用的是centos7 具体可以参看网上介绍的挺全的一篇博客如下： centos7 linux定时任务详解 crontab -e 02 4 * * * docker exec -it tomcat8002 /opt/auto-del-log.sh 接下来就OK啦！ ","link":"https://tinaxiawuhao.github.io/post/KW_02xshc/"},{"title":"docker安装elasticsearch","content":"docker search elasticsearch 选择一个版本，拉取镜像 docker pull elasticsearch:2.4.4 查看镜像 docker images 通过镜像，启动一个容器，并将9200和9300端口映射到本机 docker run -d -p 9200:9200 -p 9300:9300 -e ES_JAVA_OPTS=&quot;-Xms512m -Xmx512m&quot; --name elasticsearch elasticsearch:2.4.4 查看已启动容器 docker ps 验证是否安装成功？访问： http://localhost:9200/ 安装插件，先进入容器： docker exec -it 4d34fbf944a5 /bin/bash 进入容器bin目录，并执行安装插件命令： cd bin ls plugin install mobz/elasticsearch-head /**（低版本执行命令有所不同）**/ plugin -install mobz/elasticsearch-head 访问： http://localhost:9200/_plugin/head/ 插件安装成功 ","link":"https://tinaxiawuhao.github.io/post/2K9TL16e1/"},{"title":"使用docker搭建FastDFS文件系统","content":"1.首先下载FastDFS文件系统的docker镜像 查询镜像 [root@localhost /]# docker search fastdfs 安装镜像 [root@localhost ~]# docker pull season/fastdfs [root@localhost ~]# docker images 2.使用docker镜像构建tracker容器（跟踪服务器，起到调度的作用）： 创建tracker容器 [root@localhost /]# docker run -ti -d --name trakcer -v ~/tracker_data:/fastdfs/tracker/data --net=host season/fastdfs tracker Tracker服务器的端口默认是22122，你可以查看是否启用端口 [root@localhost /]# netstat -aon | grep 22122 3.使用docker镜像构建storage容器（存储服务器，提供容量和备份服务）： docker run -tid --name storage -v ~/storage_data:/fastdfs/storage/data -v ~/store_path:/fastdfs/store_path --net=host -e TRACKER_SERVER:192.168.115.130:22122 -e GROUP_NAME=group1 season/fastdfs storage 4.此时两个服务都以启动，进行服务的配置。 进入storage容器，到storage的配置文件中配置http访问的端口，配置文件在fdfs_conf目录下的storage.conf。 [root@localhost /]# docker exec -it storage bash root@localhost:/# cd fdfs_conf root@localhost:/fdfs_conf# more storage.conf 往下拉，你会发现storage容器的ip不是你linux的ip，如下： 接下来，退出storage容器，并将配置文件拷贝一份出来： [root@localhost ~]# docker cp storage:/fdfs_conf/storage.conf ~/ [root@localhost ~]# vi ~/storage.conf 将修改后的配置文件拷贝到storagee的配置目录下： [root@localhost ~]# docker cp ~/storage.conf storage:/fdfs_conf/ 重新启动storage容器 [root@localhost ~]# docker stop storage [root@localhost ~]# docker start storage 查看tracker容器和storage容器的关联 [root@localhost ~]# docker exec -it storage bash root@localhost:/# cd fdfs_conf root@localhost:/fdfs_conf# fdfs_monitor storage.conf 5.在docker模拟客户端上传文件到storage容器 开启一个客户端 [root@localhost 00]# docker run -tid --name fdfs_sh --net=host season/fastdfs sh 更改配置文件，因为之前已经改过一次了，所以现在直接拷贝 [root@localhost 00]# docker cp ~/storage.conf fdfs_sh:/fdfs_conf/ 创建一个txt文件 [root@localhost 00]# docker exec -it fdfs_sh bash root@localhost:/# echo hello&gt;a.txt 进入fdfs_conf目录，并将文件上传到storage容器 root@localhost:/# cd fdfs_conf root@localhost:/fdfs_conf# fdfs_upload_file storage.conf /a.txt /a.txt：指要上传的文件 上传之后，根据返回的路径去找a.txt 退出去查看上传的txt文件 [root@localhost ~]# cd ~/store_path/data/00/00 [root@localhost 00]# ls 查看是否和输入的值是否相同 [root@localhost 00]# more wKhzg1wGsieAL-3RAAAABncc3SA337.txt ","link":"https://tinaxiawuhao.github.io/post/yIe_yl-14/"},{"title":"Dockerfile详解","content":"Dockerfile详解 环境介绍 Dockerfile中所用的所有文件一定要和Dockerfile文件在同一级父目录下，可以为Dockerfile父目录的子目录 Dockerfile中相对路径默认都是Dockerfile所在的目录 Dockerfile中一定要惜字如金，能写到一行的指令，一定要写到一行，原因是分层构建，联合挂载这个特性。 Dockerfile中每一条指令被视为一层 Dockerfile中指明大写（约定俗成） 指令介绍 FROM 功能为指定基础镜像，并且必须是第一条指令。 如果不以任何镜像为基础，那么写法为：FROM scratch。 同时意味着接下来所写的指令将作为镜像的第一层开始 语法： FROM &lt;image&gt; FROM &lt;image&gt;:&lt;tag&gt; FROM &lt;image&gt;:&lt;digest&gt; 三种写法，其中&lt;tag&gt;和&lt;digest&gt; 是可选项，如果没有选择，那么默认值为latest MAINTAINER 指定作者 语法： MAINTAINER &lt;name&gt; 新版docker中使用LABEL指明 LABEL 功能是为镜像指定标签 语法： LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ... 一个Dockerfile种可以有多个LABEL，如下： LABEL &quot;com.example.vendor&quot;=&quot;ACME Incorporated&quot; LABEL com.example.label-with-value=&quot;foo&quot; LABEL version=&quot;1.0&quot; LABEL description=&quot;This text illustrates that label-values can span multiple lines.&quot; 但是并不建议这样写，最好就写成一行，如太长需要换行的话则使用\\符号 如下： LABEL multi.label1=&quot;value1&quot; multi.label2=&quot;value2&quot; other=&quot;value3&quot; 说明：LABEL会继承基础镜像种的LABEL，如遇到key相同，则值覆盖 ADD 一个复制命令，把文件复制到镜像中。 如果把虚拟机与容器想象成两台linux服务器的话，那么这个命令就类似于scp，只是scp需要加用户名和密码的权限验证，而ADD不用。 语法如下： 1. ADD &lt;src&gt;... &lt;dest&gt; 2. ADD [&quot;&lt;src&gt;&quot;,... &quot;&lt;dest&gt;&quot;] 路径的填写可以是容器内的绝对路径，也可以是相对于工作目录的相对路径，推荐写成绝对路径 可以是一个本地文件或者是一个本地压缩文件，还可以是一个url 如果把写成一个url，那么ADD就类似于wget命令 示例 ADD test relativeDir/ ADD test /relativeDir ADD http://example.com/foobar/ 注意事项 src为一个目录的时候，会自动把目录下的文件复制过去，目录本身不会复制 如果src为多个文件，dest一定要是一个目录 COPY 看这个名字就知道，又是一个复制命令 语法如下： COPY &lt;src&gt;... &lt;dest&gt; COPY [&quot;&lt;src&gt;&quot;,... &quot;&lt;dest&gt;&quot;] 与ADD的区别 COPY的只能是本地文件，其他用法一致 EXPOSE 功能为暴漏容器运行时的监听端口给外部 但是EXPOSE并不会使容器访问主机的端口 如果想使得容器与主机的端口有映射关系，必须在容器启动的时候加上 -P参数 语法： EXPOSE &lt;port&gt;/&lt;tcp/udp&gt; ENV 功能为设置环境变量 语法有两种 ENV &lt;key&gt; &lt;value&gt; ENV &lt;key&gt;=&lt;value&gt; ... 两者的区别就是第一种是一次设置一个，第二种是一次设置多个 在Dockerfile中使用变量的方式 $varname ${varname} ${varname:-default value} $(varname:+default value} 第一种和第二种相同 第三种表示当变量不存在使用-号后面的值 第四种表示当变量存在时使用+号后面的值（当然不存在也是使用后面的值） RUN 功能为运行指定的命令 RUN命令有两种格式 1. RUN &lt;command&gt; 2. RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] 第一种后边直接跟shell命令 在linux操作系统上默认 /bin/sh -c 在windows操作系统上默认 cmd /S /C 第二种是类似于函数调用。 可将executable理解成为可执行文件，后面就是两个参数。 CMD 功能为容器启动时默认命令或参数 语法有三种写法 CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] CMD [&quot;param1&quot;,&quot;param2&quot;] CMD command param1 param2 第三种比较好理解了，就时shell这种执行方式和写法 第一种和第二种其实都是可执行文件加上参数的形式 举例说明两种写法： CMD [ &quot;sh&quot;, &quot;-c&quot;, &quot;echo $HOME&quot; CMD [ &quot;echo&quot;, &quot;$HOME&quot; ] 补充细节：这里边包括参数的一定要用双引号，就是&quot;,不能是单引号。千万不能写成单引号。 原因是参数传递后，docker解析的是一个JSON array RUN&amp;&amp;CMD 不要把RUN和CMD搞混了。 RUN是构件容器时就运行的命令以及提交运行结果 CMD是容器启动时执行的命令，在构件时并不运行，构件时紧紧指定了这个命令到底是个什么样子 ENTRYPOINT 功能是：容器启动时运行得启动命令 语法如下： ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] ENTRYPOINT command param1 param2 如果从上到下看到这里的话，那么你应该对这两种语法很熟悉啦。 第二种就是写shell 第一种就是可执行文件加参数 与CMD比较说明（这俩命令太像了，而且还可以配合使用）： 相同点： 只能写一条，如果写了多条，那么只有最后一条生效 容器启动时才运行，运行时机相同 不同点： ENTRYPOINT不会被运行的command覆盖，而CMD则会被覆盖 如果我们在Dockerfile种同时写了ENTRYPOINT和CMD，并且CMD指令不是一个完整的可执行命令，那么CMD指定的内容将会作为ENTRYPOINT的参数 如下： FROM ubuntu ENTRYPOINT [&quot;top&quot;, &quot;-b&quot;] CMD [&quot;-c&quot;] 如果我们在Dockerfile种同时写了ENTRYPOINT和CMD，并且CMD是一个完整的指令，那么它们两个会互相覆盖，谁在最后谁生效 如下： FROM ubuntu ENTRYPOINT [&quot;top&quot;, &quot;-b&quot;] CMD ls -al 那么将执行ls -al ,top -b不会执行。 Docker官方使用一张表格来展示了ENTRYPOINT 和 CMD不同组合的执行情况 VOLUME 可实现挂载功能，可以将宿主机目录挂载到容器中 说的这里大家都懂了，可用专用的文件存储当作Docker容器的数据存储部分 语法如下： VOLUME [&quot;/data&quot;] 说明： [&quot;/data&quot;]可以是一个JsonArray ，也可以是多个值。所以如下几种写法都是正确的 VOLUME [&quot;/var/log/&quot;] VOLUME /var/log VOLUME /var/log /var/db 一般的使用场景为需要持久化存储数据时 容器使用的是AUFS，这种文件系统不能持久化数据，当容器关闭后，所有的更改都会丢失。 USER 设置启动容器的用户，可以是用户名或UID，所以，只有下面的两种写法是正确的 USER daemo USER UID 注意：如果设置了容器以daemon用户去运行，那么RUN, CMD 和 ENTRYPOINT 都会以这个用户去运行, 使用这个命令一定要确认容器中拥有这个用户，并且拥有足够权限 WORKDIR 设置工作目录 语法： WORKDIR /path/to/workdir 设置工作目录，对RUN,CMD,ENTRYPOINT,COPY,ADD生效。如果不存在则会创建，也可以设置多次。 如： WORKDIR /a WORKDIR b WORKDIR c RUN pwd pwd执行的结果是/a/b/c WORKDIR也可以解析环境变量 如： ENV DIRPATH /path WORKDIR $DIRPATH/$DIRNAME RUN pwd pwd的执行结果是/path/$DIRNAME ARG 设置变量命令 语法： ARG &lt;name&gt;[=&lt;default value&gt;] 设置变量命令，ARG命令定义了一个变量，在docker build创建镜像的时候，使用 --build-arg =来指定参数 如果用户在build镜像时指定了一个参数没有定义在Dockerfile种，那么将有一个Warning 提示如下： [Warning] One or more build-args [foo] were not consumed. 我们可以定义一个或多个参数，如下： FROM busybox ARG user1 ARG buildno 也可以给参数一个默认值： FROM busybox ARG user1=someuser ARG buildno=1 如果我们给了ARG定义的参数默认值，那么当build镜像时没有指定参数值，将会使用这个默认值 ONBUILD 语法： ONBUILD [INSTRUCTION] 这个命令只对当前镜像的子镜像生效。 比如当前镜像为A，在Dockerfile种添加： ONBUILD RUN ls -al 这个 ls -al 命令不会在A镜像构建或启动的时候执行 此时有一个镜像B是基于A镜像构建的，那么这个ls -al 命令会在B镜像构建的时候被执行。 STOPSIGNAL 语法： STOPSIGNAL signal STOPSIGNAL命令是的作用是当容器停止时给系统发送什么样的指令，默认是15 HEALTHCHECK 容器健康状况检查命令 语法有两种： HEALTHCHECK [OPTIONS] CMD command HEALTHCHECK NONE 第一个的功能是在容器内部运行一个命令来检查容器的健康状况 第二个的功能是在基础镜像中取消健康检查命令 [OPTIONS]的选项支持以下三中选项： –interval=DURATION 两次检查默认的时间间隔为30秒 –timeout=DURATION 健康检查命令运行超时时长，默认30秒 –retries=N 当连续失败指定次数后，则容器被认为是不健康的，状态为unhealthy，默认次数是3 注意： HEALTHCHECK命令只能出现一次，如果出现了多次，只有最后一个生效。 CMD后边的命令的返回值决定了本次健康检查是否成功，具体的返回值如下： 0: success - 表示容器是健康的 1: unhealthy - 表示容器已经不能工作了 2: reserved - 保留值 例子： HEALTHCHECK --interval=5m --timeout=3s CMD curl -f http://localhost/ || exit 1 健康检查命令是：curl -f http://localhost/ || exit 1 两次检查的间隔时间是5秒 命令超时时间为3秒 ","link":"https://tinaxiawuhao.github.io/post/jw7hb2R8R/"},{"title":"Docker ","content":"Docker 学习目标： 掌握Docker基础知识，能够理解Docker镜像与容器的概念 完成Docker安装与启动 掌握Docker镜像与容器相关命令 掌握Tomcat Nginx 等软件的常用应用的安装 掌握docker迁移与备份相关命令 能够运用Dockerfile编写创建容器的脚本 能够搭建与使用docker私有仓库 1 Docker简介 1.1 什么是虚拟化 ​ 在计算机中，虚拟化（英语：Virtualization）是一种资源管理技术，是将计算机的各种实体资源，如服务器、网络、内存及存储等，予以抽象、转换后呈现出来，打破实体结构间的不可切割的障碍，使用户可以比原本的组态更好的方式来应用这些资源。这些资源的新虚拟部份是不受现有资源的架设方式，地域或物理组态所限制。一般所指的虚拟化资源包括计算能力和资料存储。 ​ 在实际的生产环境中，虚拟化技术主要用来解决高性能的物理硬件产能过剩和老的旧的硬件产能过低的重组重用，透明化底层物理硬件，从而最大化的利用物理硬件 对资源充分利用 ​ 虚拟化技术种类很多，例如：软件虚拟化、硬件虚拟化、内存虚拟化、网络虚拟化(vip)、桌面虚拟化、服务虚拟化、虚拟机等等。 1.2 什么是Docker ​ Docker 是一个开源项目，诞生于 2013 年初，最初是 dotCloud 公司内部的一个业余项目。它基于 Google 公司推出的 Go 语言实现。 项目后来加入了 Linux 基金会，遵从了 Apache 2.0 协议，项目代码在 GitHub 上进行维护。 ​ ​ Docker 自开源后受到广泛的关注和讨论，以至于 dotCloud 公司后来都改名为 Docker Inc。Redhat 已经在其 RHEL6.5 中集中支持 Docker；Google 也在其 PaaS 产品中广泛应用。 ​ Docker 项目的目标是实现轻量级的操作系统虚拟化解决方案。 Docker 的基础是 Linux 容器（LXC）等技术。 ​ 在 LXC 的基础上 Docker 进行了进一步的封装，让用户不需要去关心容器的管理，使得操作更为简便。用户操作 Docker 的容器就像操作一个快速轻量级的虚拟机一样简单。 为什么选择Docker? （1）上手快。 ​ 用户只需要几分钟，就可以把自己的程序“Docker化”。Docker依赖于“写时复制”（copy-on-write）模型，使修改应用程序也非常迅速，可以说达到“随心所致，代码即改”的境界。 随后，就可以创建容器来运行应用程序了。大多数Docker容器只需要不到1秒中即可启动。由于去除了管理程序的开销，Docker容器拥有很高的性能，同时同一台宿主机中也可以运行更多的容器，使用户尽可能的充分利用系统资源。 （2）职责的逻辑分类 ​ 使用Docker，开发人员只需要关心容器中运行的应用程序，而运维人员只需要关心如何管理容器。Docker设计的目的就是要加强开发人员写代码的开发环境与应用程序要部署的生产环境一致性。从而降低那种“开发时一切正常，肯定是运维的问题（测试环境都是正常的，上线后出了问题就归结为肯定是运维的问题）” （3）快速高效的开发生命周期 ​ Docker的目标之一就是缩短代码从开发、测试到部署、上线运行的周期，让你的应用程序具备可移植性，易于构建，并易于协作。（通俗一点说，Docker就像一个盒子，里面可以装很多物件，如果需要这些物件的可以直接将该大盒子拿走，而不需要从该盒子中一件件的取。） （4）鼓励使用面向服务的架构 ​ Docker还鼓励面向服务的体系结构和微服务架构。Docker推荐单个容器只运行一个应用程序或进程，这样就形成了一个分布式的应用程序模型，在这种模型下，应用程序或者服务都可以表示为一系列内部互联的容器，从而使分布式部署应用程序，扩展或调试应用程序都变得非常简单，同时也提高了程序的内省性。（当然，可以在一个容器中运行多个应用程序） 1.3 容器与虚拟机比较 ​ 下面的图片比较了 Docker 和传统虚拟化方式的不同之处，可见容器是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统，而传统方式则是在硬件层面实现。 与传统的虚拟机相比，Docker优势体现为启动速度快、占用体积小。 1.4 Docker 组件 1.4.1 Docker服务器与客户端 ​ Docker是一个客户端-服务器（C/S）架构程序。Docker客户端只需要向Docker服务器或者守护进程发出请求，服务器或者守护进程将完成所有工作并返回结果。Docker提供了一个命令行工具Docker以及一整套RESTful API。你可以在同一台宿主机上运行Docker守护进程和客户端，也可以从本地的Docker客户端连接到运行在另一台宿主机上的远程Docker守护进程。 1.4.2 Docker镜像与容器 ​ 镜像是构建Docker的基石。用户基于镜像来运行自己的容器。镜像也是Docker生命周期中的“构建”部分。镜像是基于联合文件系统的一种层式结构，由一系列指令一步一步构建出来。例如： 添加一个文件； 执行一个命令； 打开一个窗口。 也可以将镜像当作容器的“源代码”。镜像体积很小，非常“便携”，易于分享、存储和更新。 ​ Docker可以帮助你构建和部署容器，你只需要把自己的应用程序或者服务打包放进容器即可。容器是基于镜像启动起来的，容器中可以运行一个或多个进程。我们可以认为，镜像是Docker生命周期中的构建或者打包阶段，而容器则是启动或者执行阶段。 容器基于镜像启动，一旦容器启动完成后，我们就可以登录到容器中安装自己需要的软件或者服务。 所以Docker容器就是： ​ 一个镜像格式； ​ 一些列标准操作； ​ 一个执行环境。 ​ Docker借鉴了标准集装箱的概念。标准集装箱将货物运往世界各地，Docker将这个模型运用到自己的设计中，唯一不同的是：集装箱运输货物，而Docker运输软件。 和集装箱一样，Docker在执行上述操作时，并不关心容器中到底装了什么，它不管是web服务器，还是数据库，或者是应用程序服务器什么的。所有的容器都按照相同的方式将内容“装载”进去。 Docker也不关心你要把容器运到何方：我们可以在自己的笔记本中构建容器，上传到Registry，然后下载到一个物理的或者虚拟的服务器来测试，在把容器部署到具体的主机中。像标准集装箱一样，Docker容器方便替换，可以叠加，易于分发，并且尽量通用。 1.4.3 Registry（注册中心） ​ Docker用Registry来保存用户构建的镜像。Registry分为公共和私有两种。Docker公司运营公共的Registry叫做Docker Hub。用户可以在Docker Hub注册账号，分享并保存自己的镜像（说明：在Docker Hub下载镜像巨慢，可以自己构建私有的Registry）。 ​ https://hub.docker.com/ 2 Docker安装与启动 2.1 安装Docker ​ Docker官方建议在Ubuntu中安装，因为Docker是基于Ubuntu发布的，而且一般Docker出现的问题Ubuntu是最先更新或者打补丁的。在很多版本的CentOS中是不支持更新最新的一些补丁包的。 ​ 由于我们学习的环境都使用的是CentOS，因此这里我们将Docker安装到CentOS上。注意：这里建议安装在CentOS7.x以上的版本，在CentOS6.x的版本中，安装前需要安装其他很多的环境而且Docker很多补丁不支持更新。 ​ 请直接挂载课程配套的Centos7.x镜像 （1）yum 包更新到最新 sudo yum update （2）安装需要的软件包， yum-util 提供yum-config-manager功能，另外两个是devicemapper驱动依赖的 sudo yum install -y yum-utils device-mapper-persistent-data lvm2 （3）设置yum源为阿里云 sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo （4）安装docker sudo yum install docker-ce （5）安装后查看docker版本 docker -v 2.2 设置ustc的镜像 ustc是老牌的linux镜像服务提供者了，还在遥远的ubuntu 5.04版本的时候就在用。ustc的docker镜像加速器速度很快。ustc docker mirror的优势之一就是不需要注册，是真正的公共服务。 https://lug.ustc.edu.cn/wiki/mirrors/help/docker 编辑该文件： vi /etc/docker/daemon.json 在该文件中输入如下内容： { &quot;registry-mirrors&quot;: [&quot;https://docker.mirrors.ustc.edu.cn&quot;] } 2.3 Docker的启动与停止 systemctl命令是系统服务管理器指令 启动docker： systemctl start docker 停止docker： systemctl stop docker 重启docker： systemctl restart docker 查看docker状态： systemctl status docker 开机启动： systemctl enable docker 查看docker概要信息 docker info 查看docker帮助文档 docker --help 3 常用命令 3.1 镜像相关命令 3.1.1 查看镜像 docker images REPOSITORY：镜像名称 TAG：镜像标签 IMAGE ID：镜像ID CREATED：镜像的创建日期（不是获取该镜像的日期） SIZE：镜像大小 这些镜像都是存储在Docker宿主机的/var/lib/docker目录下 3.1.2 搜索镜像 如果你需要从网络中查找需要的镜像，可以通过以下命令搜索 docker search 镜像名称 NAME：仓库名称 DESCRIPTION：镜像描述 STARS：用户评价，反应一个镜像的受欢迎程度 OFFICIAL：是否官方 AUTOMATED：自动构建，表示该镜像由Docker Hub自动构建流程创建的 3.1.3 拉取镜像 拉取镜像就是从中央仓库中下载镜像到本地 docker pull 镜像名称 例如，我要下载centos7镜像 docker pull centos:7 Docker采用联合文件系统，不同镜像的相同文件无需再次下载： 4 删除镜像 按镜像ID删除镜像 docker rmi 镜像ID 删除所有镜像 docker rmi `docker images -q` 3.2 容器相关命令 3.2.1 查看容器 查看正在运行的容器 docker ps 查看所有容器 docker ps –a 查看最后一次运行的容器 docker ps –l 查看停止的容器 docker ps -f status=exited 3.2.2 创建与启动容器 创建容器常用的参数说明： 创建容器命令：docker run -i：表示运行容器 -t：表示容器启动后会进入其命令行。加入这两个参数后，容器创建就能登录进去。即分配一个伪终端。 --name :为创建的容器命名。 -v：表示目录映射关系（前者是宿主机目录，后者是映射到宿主机上的目录），可以使用多个－v做多个目录或文件映射。注意：最好做目录映射，在宿主机上做修改，然后共享到容器上。 -d：在run后面加上-d参数,则会创建一个守护式容器在后台运行（这样创建容器后不会自动登录容器，如果只加-i -t两个参数，创建后就会自动进去容器）。 -p：表示端口映射，前者是宿主机端口，后者是容器内的映射端口。可以使用多个-p做多个端口映射 （1）交互式方式创建容器 docker run -it --name=容器名称 镜像名称:标签 /bin/bash 这时我们通过ps命令查看，发现可以看到启动的容器，状态为启动状态 退出当前容器 exit （2）守护式方式创建容器： docker run -di --name=容器名称 镜像名称:标签 登录守护式容器方式： docker exec -it 容器名称 (或者容器ID) /bin/bash Exit # 从容器中退回主机 CTRL+Q+P # 容器不停止退出 3.2.3 停止与启动容器 停止容器： docker stop 容器名称（或者容器ID） 启动容器： docker start 容器名称（或者容器ID） 3.2.4 文件拷贝 如果我们需要将文件拷贝到容器内可以使用cp命令 docker cp 需要拷贝的文件或目录 容器名称:容器目录 也可以将文件从容器内拷贝出来 docker cp 容器名称:容器目录 需要拷贝的文件或目录 3.2.5 目录挂载 我们可以在创建容器的时候，将宿主机的目录与容器内的目录进行映射，这样我们就可以通过修改宿主机某个目录的文件从而去影响容器。 创建容器 添加-v参数 后边为 宿主机目录:容器目录，例如： docker run -di -v /usr/local/myhtml:/usr/local/myhtml --name=mycentos3 centos:7 如果你共享的是多级的目录，可能会出现权限不足的提示。 这是因为CentOS7中的安全模块selinux把权限禁掉了，我们需要添加参数 --privileged=true 来解决挂载的目录没有权限的问题 匿名挂载 docker run -d -v 容器内目录 镜像名/id # 匿名挂载 匿名挂载后，使用docker volume ls命令查看所有挂载的卷： 每一个VOLUME NAME对应一个挂载的卷，由于挂载时未指定主机目录，因此无法直接找到目录。 具名挂载 docker run -d -v 卷名：容器内目录 镜像名/id # 具名挂载 可以发现挂载的卷：volume01，并通过docker volume inspect 卷名 命令找到主机内目录： 所有docker容器内的卷，在未指定主机内目录时，都在：/var/lib/docker/volumes/卷名/_data 下，可通过具名挂载可以方便的找到卷，因此广泛使用这种方式进行挂载。 数据卷容器 docker run -it --name container02 --volumes from container01 镜像名/id # 将两个容器进行挂载 3.2.6 查看容器IP地址 我们可以通过以下命令查看容器运行的各种数据 docker inspect 容器名称（容器ID） 也可以直接执行下面的命令直接输出IP地址 docker inspect --format='{{.NetworkSettings.IPAddress}}' 容器名称（容器ID） 3.2.7 删除容器 删除指定的容器： docker rm 容器名称（容器ID） 3.2.8 其他命令 docker start/restart/stop/kill 容器名/id docker logs -tf --tail 显示的日志条数 容器名/id # 查看日志 docker top 容器名/id # 查看容器中的进程信息 docker inspect 容器名/id # 查看镜像的元数据 docker exec -it 容器名/id /bin/bash # 通常容器以后台方式运行，需要进入其中修改配置：进入容器后开启一个新终端 docker attach 容器名/id # 进入容器正在执行的终端 docker cp 容器名/id:容器内路径 主机文件路径 # 从容器内拷贝文件到主机上 4 Docker镜像详解 UnionFS（联合文件系统） 联合文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下。联合文件系统是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。 特性：一次同时加载多个文件系统，但从外面看起来只能看到一个文件系统。联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录。 镜像加载原理 Docker的镜像实际由一层一层的文件系统组成： bootfs（boot file system）主要包含bootloader和kernel。bootloader主要是引导加载kernel，完成后整个内核就都在内存中了。此时内存的使用权已由bootfs转交给内核，系统卸载bootfs。可以被不同的Linux发行版公用。 rootfs（root file system），包含典型Linux系统中的/dev，/proc，/bin，/etc等标准目录和文件。rootfs就是各种不同操作系统发行版（Ubuntu，Centos等）。因为底层直接用Host的kernel，rootfs只包含最基本的命令，工具和程序就可以了。 分层理解 所有的Docker镜像都起始于一个基础镜像层，当进行修改或增加新的内容时，就会在当前镜像层之上，创建新的容器层。 容器在启动时会在镜像最外层上建立一层可读写的容器层（R/W），而镜像层是只读的（R/O）。 docker commit -m=&quot;描述信息&quot; -a=&quot;作者&quot; 容器id 目标镜像名:[tag] # 编辑容器后提交容器成为一个新镜像 4 应用部署 4.1 MySQL部署 （1）拉取mysql镜像 docker pull centos/mysql-57-centos7 （2）创建容器 docker run -di --name=tensquare_mysql -p 33306:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql -p 代表端口映射，格式为 宿主机映射端口:容器运行端口 -e 代表添加环境变量 MYSQL_ROOT_PASSWORD 是root用户的登陆密码 （3）远程登录mysql 连接宿主机的IP ,指定端口为33306 4.2 tomcat部署 （1）拉取镜像 docker pull tomcat:7-jre7 （2）创建容器 创建容器 -p表示地址映射 docker run -di --name=mytomcat -p 9000:8080 -v /usr/local/webapps:/usr/local/tomcat/webapps tomcat:7-jre7 4.3 Nginx部署 （1）拉取镜像 docker pull nginx （2）创建Nginx容器 docker run -di --name=mynginx -p 80:80 nginx 4.4 Redis部署 （1）拉取镜像 docker pull redis （2）创建容器 docker run -di --name=myredis -p 6379:6379 redis 4.5 Redis集群部署 # 创建网卡 docker network create redis --subnet 172.38.0.0/16 # 通过脚本创建六个redis配置 for port in $(seq 1 6);\\ do \\ mkdir -p /mydata/redis/node-${port}/conf touch /mydata/redis/node-${port}/conf/redis.conf cat &lt;&lt; EOF &gt;&gt; /mydata/redis/node-${port}/conf/redis.conf port 6379 bind 0.0.0.0 cluster-enabled yes cluster-config-file nodes.conf cluster-node-timeout 5000 cluster-announce-ip 172.38.0.1${port} cluster-announce-port 6379 cluster-announce-bus-port 16379 appendonly yes EOF done # 通过脚本运行六个redis for port in $(seq 1 6);\\ docker run -p 637${port}:6379 -p 1667${port}:16379 --name redis-${port} \\ -v /mydata/redis/node-${port}/data:/data \\ -v /mydata/redis/node-${port}/conf/redis.conf:/etc/redis/redis.conf \\ -d --net redis --ip 172.38.0.1${port} redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.conf docker exec -it redis-1 /bin/sh #redis默认没有bash redis-cli --cluster create 172.38.0.11:6379 172.38.0.12:6379 172.38.0.13:6379 172.38.0.14:6379 172.38.0.15:6379 172.38.0.16:6379 --cluster-replicas 1 5 迁移与备份 5.1 容器保存为镜像 我们可以通过以下命令将容器保存为镜像 docker commit mynginx mynginx_i 5.2 镜像备份 我们可以通过以下命令将镜像保存为tar 文件 docker save -o mynginx.tar mynginx_i 5.3 镜像恢复与迁移 首先我们先删除掉mynginx_img镜像 然后执行此命令进行恢复 docker load -i mynginx.tar -i 输入的文件 执行后再次查看镜像，可以看到镜像已经恢复 6 Dockerfile 6.1 什么是Dockerfile Dockerfile是由一系列命令和参数构成的脚本，这些命令应用于基础镜像并最终创建一个新的镜像。 1、对于开发人员：可以为开发团队提供一个完全一致的开发环境； 2、对于测试人员：可以直接拿开发时所构建的镜像或者通过Dockerfile文件构建一个新的镜像开始工作了； 3、对于运维人员：在部署时，可以实现应用的无缝移植。 6.2 常用命令 命令 作用 FROM image_name:tag 定义了使用哪个基础镜像启动构建流程 MAINTAINER user_name 声明镜像的创建者 ENV key value 设置环境变量 (可以写多条) RUN command 是Dockerfile的核心部分(可以写多条) ADD source_dir/file dest_dir/file 将宿主机的文件复制到容器内，如果是一个压缩文件，将会在复制后自动解压 COPY source_dir/file dest_dir/file 和ADD相似，但是如果有压缩文件并不能解压 WORKDIR path_dir 设置工作目录 6.3 使用脚本创建镜像 步骤： （1）创建目录 mkdir –p /usr/local/dockerjdk8 （2）下载jdk-8u171-linux-x64.tar.gz并上传到服务器（虚拟机）中的/usr/local/dockerjdk8目录 （3）创建文件Dockerfile vi Dockerfile #依赖镜像名称和ID FROM centos:7 #指定镜像创建者信息 MAINTAINER ITCAST #切换工作目录 WORKDIR /usr RUN mkdir /usr/local/java #ADD 是相对路径jar,把java添加到容器中 ADD jdk-8u171-linux-x64.tar.gz /usr/local/java/ #配置java环境变量 ENV JAVA_HOME /usr/local/java/jdk1.8.0_171 ENV JRE_HOME $JAVA_HOME/jre ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATH ENV PATH $JAVA_HOME/bin:$PATH （4）执行命令构建镜像 docker build -t='jdk1.8' . 注意后边的空格和点，不要省略 （5）查看镜像是否建立完成 docker images 7 Docker私有仓库 7.1 私有仓库搭建与配置 （1）拉取私有仓库镜像（此步省略） docker pull registry （2）启动私有仓库容器 docker run -di --name=registry -p 5000:5000 registry （3）打开浏览器 输入地址http://192.168.184.141:5000/v2/_catalog看到{&quot;repositories&quot;:[]} 表示私有仓库搭建成功并且内容为空 （4）修改daemon.json vi /etc/docker/daemon.json 添加以下内容，保存退出。 {&quot;insecure-registries&quot;:[&quot;192.168.184.141:5000&quot;]} 此步用于让 docker信任私有仓库地址 （5）重启docker 服务 systemctl restart docker 7.2 镜像上传至私有仓库 （1）标记此镜像为私有仓库的镜像 docker tag jdk1.8 192.168.184.141:5000/jdk1.8 （2）再次启动私服容器 docker start registry （3）上传标记的镜像 docker push 192.168.184.141:5000/jdk1.8 8 Docker网络 8.1 理解Doker0 通过命令ip addr查看本地ip地址，我们发现除了本机回环地址和埃里远的内网地址外，还多了一个网卡：Docker0，这是Docker服务启动后自动生成的。 而如果进入一个正在后台运行的tomcat容器，同样使用ip addr命令，发现容器得到了一个新的网络：12: eth@if13，ip地址：172.17.0.2。这是Docker在容器启动时为其分配的。 思考一个问题：此时我们的linux主机可以ping通容器内部（172.17.0.2）吗？（注意与容器暴露端口相区分) linux可以ping通docker容器内部，因为docker0的ip地址为172.17.0.1，容器为172.17.0.2。 原理：我们每启动一个docker容器，docker就会给容器分配一个默认的可用ip，我们只要安装了docker，就会有一个网卡docker0(bridge)。网卡采用桥接模式，并使用veth-pair技术（veth-pair就是一堆虚拟设备接口，成对出现，一段连着协议，一段彼此相连，充当一个桥梁。）。 这时我们退出容器，回到主机再次观察主机的ip地址： 我们惊奇地发现了一个新网络13: vethda1df4b@if12，对应容器内网络地址的12: eth@if13。 容器和容器之间是可以互相ping通的：容器1→Docker0→容器2 docker中的所有网络接口都是虚拟的 ，转发效率高。删除容器后，对应的网桥也随之删除。 8.2 --link 若编写一个微服务并连接数据库，如果数据库ip改变，如何根据容器名而不是ip访问容器？显然，直接使用容器名是无法ping通容器内部的： 这时我们可以在容器启动命令中加入一个选项：--link，使得我们可以根据容器名来访问容器。 docker run -d -P --link 容器名/id 镜像名/id 然而反向就不可以ping通，这是因为--link的本质是把需要连接的容器名/id写入启动容器的配置文件中，即增加了一个ip和容器名/id的映射： 目前已经不建议使用这种方式。 8.3 自定义网络 我们使用命令： docker network ls # 查看所有的docker网络 docker中的网络模式有： bridge：桥接（docker默认）/ none：不配置网络 / host：和宿主机共享网络 docker run 命令默认带有一个参数--net bridge，此处的bridge指的就是docker0。如果我们不想使用docker0，那如何创建一个新的网络呢？ docker network create --driver 网络模式 --subnet 子网ip --gateway 网关 网络名 我们不仅在docker network ls命令下发现了这个新创建的网络newnet，还可以使用docker network inspect命令查看其详细信息，包括了我们创建时定义的子网ip和网关： 只要两个容器启动时都通过 --net，选用了同一个已创建的网络，不同容器间即可通过ip地址或容器名/id连通: 8.4 网络连通 对于建立在不同网络下(docker0, newnet)的两个容器tomcat01和tomcat02，他们的网段不同，因此是无法彼此ping通容器内部的： 这时我们需要通过docker network connect命令打通容器与网络之间的连接： docker network connect 网络名 容器名/id 这个功能类似于将一个容器赋予多个ip地址，同样可以用docker network inspect命令查看网络连通后，该网络的变化： 原本newnet网络中只含有tomcat02，现在增加了tomcat01，因此可以连通。 ","link":"https://tinaxiawuhao.github.io/post/VSWRPm3xo/"},{"title":"Git 操作命令清单","content":"git工作流程图 下面是常用 的Git 命令清单。几个专用名词的译名如下： Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 一、新建代码库 # 在当前目录新建一个Git代码库 $ git init # 新建一个目录，将其初始化为Git代码库 $ git init [project-name] # 下载一个项目和它的整个代码历史 $ git clone [url] 二、配置 Git的设置文件为.gitconfig，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。 # 显示当前的Git配置 $ git config --list # 编辑Git配置文件 $ git config -e [--global] # 设置提交代码时的用户信息 $ git config [--global] user.name &quot;[name]&quot; $ git config [--global] user.email &quot;[email address]&quot; 三、增加/删除文件 # 添加指定文件到暂存区 $ git add [file1] [file2] ... # 添加指定目录到暂存区，包括子目录 $ git add [dir] # 添加当前目录的所有文件到暂存区 $ git add . # 添加每个变化前，都会要求确认 # 对于同一个文件的多处变化，可以实现分次提交 $ git add -p # 删除工作区文件，并且将这次删除放入暂存区 $ git rm [file1] [file2] ... # 停止追踪指定文件，但该文件会保留在工作区 $ git rm --cached [file] # 改名文件，并且将这个改名放入暂存区 $ git mv [file-original] [file-renamed] 四、代码提交 # 提交暂存区到仓库区 $ git commit -m [message] # 提交暂存区的指定文件到仓库区 $ git commit [file1] [file2] ... -m [message] # 提交工作区自上次commit之后的变化，直接到仓库区 $ git commit -a # 提交时显示所有diff信息 $ git commit -v # 使用一次新的commit，替代上一次提交 # 如果代码没有任何新变化，则用来改写上一次commit的提交信息 $ git commit --amend -m [message] # 重做上一次commit，并包括指定文件的新变化 $ git commit --amend [file1] [file2] ... 五、分支 # 列出所有本地分支 $ git branch # 列出所有远程分支 $ git branch -r # 列出所有本地分支和远程分支 $ git branch -a # 新建一个分支，但依然停留在当前分支 $ git branch [branch-name] # 新建一个分支，并切换到该分支 $ git checkout -b [branch] # 新建一个分支，指向指定commit $ git branch [branch] [commit] # 新建一个分支，与指定的远程分支建立追踪关系 $ git branch --track [branch] [remote-branch] # 切换到指定分支，并更新工作区 $ git checkout [branch-name] # 切换到上一个分支 $ git checkout - # 建立追踪关系，在现有分支与指定的远程分支之间 $ git branch --set-upstream [branch] [remote-branch] # 合并指定分支到当前分支 $ git merge [branch] # 选择一个commit，合并进当前分支 $ git cherry-pick [commit] # 删除分支 $ git branch -d [branch-name] # 删除远程分支 $ git push origin --delete [branch-name] $ git branch -dr [remote/branch] 六、标签 # 列出所有tag $ git tag # 新建一个tag在当前commit $ git tag [tag] # 新建一个tag在指定commit $ git tag [tag] [commit] # 删除本地tag $ git tag -d [tag] # 删除远程tag $ git push origin :refs/tags/[tagName] # 查看tag信息 $ git show [tag] # 提交指定tag $ git push [remote] [tag] # 提交所有tag $ git push [remote] --tags # 新建一个分支，指向某个tag $ git checkout -b [branch] [tag] 七、查看信息 # 显示有变更的文件 $ git status # 显示当前分支的版本历史 $ git log # 显示commit历史，以及每次commit发生变更的文件 $ git log --stat # 搜索提交历史，根据关键词 $ git log -S [keyword] # 显示某个commit之后的所有变动，每个commit占据一行 $ git log [tag] HEAD --pretty=format:%s # 显示某个commit之后的所有变动，其&quot;提交说明&quot;必须符合搜索条件 $ git log [tag] HEAD --grep feature # 显示某个文件的版本历史，包括文件改名 $ git log --follow [file] $ git whatchanged [file] # 显示指定文件相关的每一次diff $ git log -p [file] # 显示过去5次提交 $ git log -5 --pretty --oneline # 显示所有提交过的用户，按提交次数排序 $ git shortlog -sn # 显示指定文件是什么人在什么时间修改过 $ git blame [file] # 显示暂存区和工作区的差异 $ git diff # 显示暂存区和上一个commit的差异 $ git diff --cached [file] # 显示工作区与当前分支最新commit之间的差异 $ git diff HEAD # 显示两次提交之间的差异 $ git diff [first-branch]...[second-branch] # 显示今天你写了多少行代码 $ git diff --shortstat &quot;@{0 day ago}&quot; # 显示某次提交的元数据和内容变化 $ git show [commit] # 显示某次提交发生变化的文件 $ git show --name-only [commit] # 显示某次提交时，某个文件的内容 $ git show [commit]:[filename] # 显示当前分支的最近几次提交 $ git reflog 八、远程同步 # 下载远程仓库的所有变动 $ git fetch [remote] # 显示所有远程仓库 $ git remote -v # 显示某个远程仓库的信息 $ git remote show [remote] # 增加一个新的远程仓库，并命名 $ git remote add [shortname] [url] # 取回远程仓库的变化，并与本地分支合并 $ git pull [remote] [branch] # 上传本地指定分支到远程仓库 $ git push [remote] [branch] # 强行推送当前分支到远程仓库，即使有冲突 $ git push [remote] --force # 推送所有分支到远程仓库 $ git push [remote] --all 九、撤销 # 恢复暂存区的指定文件到工作区 $ git checkout [file] # 恢复某个commit的指定文件到暂存区和工作区 $ git checkout [commit] [file] # 恢复暂存区的所有文件到工作区 $ git checkout . # 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变 $ git reset [file] # 重置暂存区与工作区，与上一次commit保持一致 $ git reset --hard # 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变 $ git reset [commit] # 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致 $ git reset --hard [commit] # 重置当前HEAD为指定commit，但保持暂存区和工作区不变 $ git reset --keep [commit] # 新建一个commit，用来撤销指定commit # 后者的所有变化都将被前者抵消，并且应用到当前分支 $ git revert [commit] # 暂时将未提交的变化移除，稍后再移入 $ git stash $ git stash pop 十、其他 # 生成一个可供发布的压缩包 $ git archive ","link":"https://tinaxiawuhao.github.io/post/awhwcpAEp/"},{"title":"jar包内类动态加载","content":"实体对象 public class CmptDef { private String idCmptDef; //组件在DB里的唯一ID号 private String name;//组件的唯一名称，只能是字母、数字、下划线组成，大于1小雨50，通常为Jar的名字 private CmptCategory cmptCategory; //种类 private String fullQualifiedName; //组件入口类全限定名 private String extraParas; //组件额外参数 private String formUrl; //组件自定义表单 private Float version = 1.0f; //组件版本 private CmptExecutePos executePos = CmptExecutePos.PRE; //组件在网关中执行的位置 private Integer priority = 100; //组件在相同位置的执行优先级，数字越小越高 private Integer timeout = 1000;//组件执行超时时间，单位毫秒 private String description; //组件描述 private Boolean defaultVersion; //是否是默认有效版本 private CmptStatus cmptStatus = CmptStatus.editing; // 组件状态 private String remarks; // 组件发布时需要填写备注信息 private String code; // 组件code，同一个组件的多个版本的code是一样的 private Date releaseTime; // 组件发布时间 private CustomFormCode customFormCode; // 自定义表单里类型 private String cmptType; //组件类型（特殊组件|普通组件|默认组件） } /** * 组件类型。 */ public enum CmptCategory { AUTHENTICATION, //认证 AUTHORIZATION, //鉴权 FLOW_MANAGEMENT, //流量管理 REQUEST_COUNT_MANAGEMENT, //请求次数管理 CACHE, //缓存 ROUTER, //路由 TRANSFORM, //数据转换 LOGGER, //日志 OTHER//其它 } /** * 组件执行的位置。 */ public enum CmptExecutePos { PRE, //调用上游服务前 ROUTING, //调用中 AFTER //调用后 } public enum CmptStatus { editing, //编辑中 published, //已经发布 offline //已经下线 } /** * 组件风格。 */ public enum CustomFormCode { RESTFUL_FORM, SQL_FORM, DUBBO_FORM, OTHER, MQ_FORM, WEBSERVICE_FORM } 方法接口 public interface ICmptService { /** * 根据组件类名和版本从缓存中获取组件,如果不存在则尝试动态加载组件类，实例化并刷新缓存后再获取，还是不存在则返回null； * 另外配置更新会有单独的线程刷新缓存。 * * @return */ ICmpt getCmptInstance(final CmptDef cmptDef); /** * 刷新API关联的组件配置信息 * * @param apis * @param ignoreRefreshTime * @return */ boolean refreshCmptInstanceCache(List&lt;Api&gt; apis, boolean ignoreRefreshTime); /** * 删除组件实例缓存 * * @param fullQualifiedName * @param code */ void removeCmpt(final String fullQualifiedName, final Float version, final String code); } 方法实现 @Service public class CmptServiceImpl implements ICmptService { private static Logger logger = LoggerFactory.getLogger(CmptServiceImpl.class); @Value(&quot;${cmpt.dynamicLoadCmptClass}&quot;) private boolean dynamicLoadCmptClass; @Autowired private ICmptDefService cmptDefService; @Override public synchronized ICmpt getCmptInstance(final CmptDef cmptDef) { ICmpt cmpt = CmptInstanceHolder.getInstance().getCmpt(cmptDef.getFullQualifiedName(), cmptDef.getVersion(), cmptDef.getCode()); if (null == cmpt) { if (logger.isDebugEnabled()) { logger.debug(&quot;反射拿到实例&gt;&gt;&gt; &quot; + cmptDef.getFullQualifiedName()); } if (this.dynamicLoadCmptClass) { cmpt = CmptClassLoaderUtil.newInstance(cmptDef); } else { try { Class clazz = Class.forName(cmptDef.getFullQualifiedName()); cmpt = (ICmpt) clazz.newInstance(); } catch (Exception e) { e.printStackTrace(); } } if (null != cmpt) { if (cmpt instanceof AbstractCmpt) { //设置版本号 ((AbstractCmpt) cmpt).setVersion(cmptDef.getVersion()); ((AbstractCmpt) cmpt).setIdCmptDef(cmptDef.getIdCmptDef()); } CmptInstanceHolder.getInstance().addEntry(cmpt, cmptDef.getFullQualifiedName(), cmptDef.getVersion(), cmptDef.getCode()); } } else { if (logger.isDebugEnabled()) { logger.debug(&quot;缓存拿到实例&gt;&gt;&gt; &quot; + cmptDef.getFullQualifiedName()); } } if (null != cmpt) { ApplicationContext context = SpringContextHolder.getContext(); if (null != context) { ZkClient zkClient = context.getBean(ZkClient.class); if (null != zkClient) { BaseListener listener = zkClient.getListener(); listener.addObserver((AbstractCmpt) cmpt); } } } return cmpt; } @Override public boolean refreshCmptInstanceCache(List&lt;Api&gt; apis, boolean ignoreRefreshTime) { return false; } @Override public synchronized void removeCmpt(final String fullQualifiedName, final Float version, final String code) { ICmpt cmpt = CmptInstanceHolder.getInstance().getCmpt(fullQualifiedName, version, code); if (null != cmpt) { ApplicationContext context = SpringContextHolder.getContext(); if (null != context) { ZkClient zkClient = context.getBean(ZkClient.class); if (null != zkClient) { BaseListener listener = zkClient.getListener(); listener.deleteObserver((AbstractCmpt) cmpt); } } cmpt.destroy(); CmptDef cmptDef = CmptDefHolder.getInstance().getCmptDef(fullQualifiedName, version, code); if (null == cmptDef) { cmptDef = new CmptDef(); cmptDef.setFullQualifiedName(fullQualifiedName); cmptDef.setVersion(version); cmptDef.setCode(code); } //删除引用实体 CmptInstanceHolder.getInstance().removeEntry(fullQualifiedName, version, code); cmpt = null; String jarPath = CmptClassLoaderUtil.getJarPath(cmptDef); if (logger.isDebugEnabled()) { logger.debug(&quot;尝试卸载jar包: &quot; + jarPath); } CmptClassLoaderManager.unLoadJar(jarPath); } CmptInstanceHolder.getInstance().removeEntry(fullQualifiedName, version, code); } } 工具类 /** * 组件配置缓存持有者 */ public class CmptInstanceHolder { private static CmptInstanceHolder cmptInstanceHolder = new CmptInstanceHolder(); //API所关联的组件配置缓存 key: fullQualifiedName value: ICmpt private Map&lt;String, ICmpt&gt; cmpts = new ConcurrentHashMap&lt;&gt;(); private CmptInstanceHolder() { } public static CmptInstanceHolder getInstance() { return CmptInstanceHolder.cmptInstanceHolder; } /** * 根据类名加版本从缓存中获取组件实例,如果不存在直接返回null； * * @return */ public ICmpt getCmpt(final String fullQualifiedName, final Float version, String code) { final String key = buildKey(fullQualifiedName, version, code); return this.cmpts.get(key); } /** * 添加对象 */ public void addEntry(final ICmpt cmpt, final String fullQualifiedName, final Float version, final String code) { if (cmpt == null || StringUtils.isEmpty(fullQualifiedName) || StringUtils.isEmpty(code)) { return; } final String key = buildKey(fullQualifiedName, version, code); removeEntry(fullQualifiedName, version, code); this.cmpts.put(key, cmpt); } private String buildKey(final String fullQualifiedName, Float version, final String code) { return fullQualifiedName + &quot;.&quot; + version + code; } public void removeEntry(final String fullQualifiedName, final Float version, final String code) { ICmpt remove = this.cmpts.remove(buildKey(fullQualifiedName, version, code)); if (null != remove) { remove.destroy(); } } /** * 获取所有的组件实例 * @return */ public Map&lt;String, ICmpt&gt; getAllCmpts(){ return this.cmpts; } } public class CmptClassLoaderUtil { private static Logger logger = LoggerFactory.getLogger(CmptClassLoaderUtil.class); public static ICmpt newInstance(final CmptDef cmptDef) { ICmpt cmpt = null; try { final String jarPath = getJarPath(cmptDef); logger.info(&quot;尝试载入jar包,jar包路径: &quot; + jarPath); //加载依赖jar CmptClassLoader cmptClassLoader = CmptClassLoaderManager.loadJar(cmptDef.getIdCmptDef(), jarPath, true); // 创建实例 if (null != cmptClassLoader) { cmpt = LoadClassUtil.newObject(cmptDef, ICmpt.class, cmptClassLoader); } else { logger.error(&quot;加载组件jar包失败! jarPath: &quot; + jarPath); } } catch (Exception e) { logger.error(&quot;组件类加载失败，请检查类名和版本是否正确。ClassName=&quot; + cmptDef.getFullQualifiedName() + &quot;, Version=&quot; + cmptDef.getVersion()); e.printStackTrace(); } return cmpt; } public static String getJarPath(final CmptDef cmptDef) { StringBuffer sb = new StringBuffer(); sb.append(AppConfigUtil.getValue(&quot;app.home&quot;)); sb.append(AppConfigUtil.getValue(&quot;cmpt.location&quot;)); sb.append(&quot;/&quot;); //开发中关闭多级目录,不好部署 sb.append(cmptDef.getCode()); sb.append(&quot;/&quot;); sb.append(cmptDef.getVersion()); sb.append(&quot;/&quot;); String[] split = cmptDef.getFullQualifiedName().split(&quot;\\\\.&quot;); sb.append(split[split.length - 1]); sb.append(&quot;_&quot;); sb.append(cmptDef.getVersion()); sb.append(&quot;.jar&quot;); if (logger.isDebugEnabled()) { logger.debug(&quot;构建jar包路径: &quot; + sb.toString()); } return sb.toString(); } } #linux版home目录 app.home=/work/sharestore #组件资源存放路径，&lt;home&gt;/cmpt/&lt;code&gt;/&lt;version&gt;/&lt;name&gt;_&lt;version&gt;.jar,html cmpt.location=/cmpt /** * 类加载器管理, 加载, 卸载jar包 */ public class CmptClassLoaderManager { private static final Logger logger = Logger.getLogger(CmptClassLoaderManager.class); private static Map&lt;String, CmptClassLoader&gt; classLoaderMap = new ConcurrentHashMap&lt;&gt;(); private CmptClassLoaderManager() { } /** * 载入Jar包, 判断是否重新载入Jar包 * * @param fileName * @param isReloadJar * @return */ public static CmptClassLoader loadJar(String IdCmptDef, String fileName, boolean isReloadJar) { fileName = FileUtil.fixFileName(fileName); CmptClassLoader cmptClassLoader = classLoaderMap.get(IdCmptDef); if (isReloadJar || null == cmptClassLoader) { if (logger.isDebugEnabled()) { logger.debug(&quot;从文件载入jar组件&quot;); } return loadJar(IdCmptDef, fileName); } else { if (logger.isDebugEnabled()) { logger.debug(&quot;从缓存载入jar组件&quot;); } return cmptClassLoader; } } /** * 载入Jar包, 以新Jar包载入 * * @param fileName * @return 返回一个类加载器 */ private static CmptClassLoader loadJar(String IdCmptDef, String fileName) { CmptClassLoader loader; try { boolean exists = new File(fileName).exists(); if (exists) { if (null == classLoaderMap.get(IdCmptDef) || unLoadJar(IdCmptDef)) { loader = new CmptClassLoader(); boolean loadJar = loader.addURL(fileName); if (loadJar) { classLoaderMap.put(IdCmptDef, loader); return loader; } else { loader.close(); } } } else { throw new IllegalArgumentException(&quot;传入参数错误,文件不存在! file: &quot; + fileName); } } catch (Exception e) { logger.error(&quot;&quot;,e); } return null; } public static boolean unLoadJar(String IdCmptDef) { boolean unLoadJar = false; if (logger.isDebugEnabled()) { logger.debug(&quot;请求卸载jar包: &quot; + IdCmptDef); } try { CmptClassLoader loader = classLoaderMap.remove(IdCmptDef); if (null != loader) { loader.close(); } unLoadJar = true; } catch (Exception e) { logger.error(&quot;&quot;,e); } return unLoadJar; } /** * 获取组件定义对应的实例的类加载器 * * @param idCmptDef * @return */ public static ClassLoader getCmptClassLoader(String idCmptDef) { return classLoaderMap.get(idCmptDef); } } public class FileUtil { public static String fixFileName(String fileName) { if (null == fileName) fileName = &quot;&quot;; fileName = fileName.replaceAll(&quot;\\\\\\\\&quot;, &quot;/&quot;); fileName = fileName.replaceAll(&quot;//&quot;, &quot;/&quot;); return fileName; } } /** * 类加载器终极版, 加载, 卸载jar包 */ public class CmptClassLoader extends URLClassLoader { private static final Logger logger = Logger.getLogger(CmptClassLoader.class); CmptClassLoader(URL[] urls, ClassLoader parent) { super(urls, parent); } CmptClassLoader(URL[] urls) { super(urls); } CmptClassLoader(URL[] urls, ClassLoader parent, URLStreamHandlerFactory factory) { super(urls, parent, factory); } CmptClassLoader() { super(new URL[]{}, findParentClassLoader()); } /** * 载入Jar * * @param fileName * @return */ boolean addURL(String fileName) { try { URL url = new File(fileName).toURL(); URLClassPath ucp = this.getUCP(); ucp.addURL(url); } catch (Exception e) { e.printStackTrace(); return false; } return true; } /** * 定位当前父类加载器 * * @return */ private static ClassLoader findParentClassLoader() { ClassLoader parent = logger.getClass().getClassLoader(); if (parent == null) { throw new RuntimeException(&quot;无法获取当前父加载器!&quot;); } return parent; } private URLClassPath getUCP() { URLClassPath ucp = null; try { Field declaredField = URLClassLoader.class.getDeclaredField(&quot;ucp&quot;); declaredField.setAccessible(true); Object o = declaredField.get(this); ucp = (URLClassPath) o; } catch (IllegalAccessException e) { e.printStackTrace(); } catch (NoSuchFieldException e) { e.printStackTrace(); } return ucp; } @Override protected void finalize() throws Throwable { super.finalize(); this.close(); } } /** * 类加载器工具类 */ public class LoadClassUtil { private final static LoadClassUtil LOAD_JAR_UTIL = new LoadClassUtil(); private static Logger logger = LoggerFactory.getLogger(LoadClassUtil.class); private LoadClassUtil() { } /** * 载入jar包 * 将jar包路径添加到系统类加载器扫描类和资源的文件列表里 * * @param fileName jar绝对路径 * @return */ public static boolean loadJar(String fileName) { try { if (strNotNull(fileName) &amp;&amp; fileExists(fileName)) {//(ClassLoader要与当前程序同一个loader) getMethod().invoke(LOAD_JAR_UTIL.getClass().getClassLoader(), getURL(fileName));//添加路径URL //getMethod().invoke(Launcher.getLauncher().getClassLoader(), getURL(fileName));//添加路径URL return true; } else { throw new IllegalArgumentException(&quot;传入参数错误,文件或不存在! file: &quot; + fileName); } } catch (Exception e) { logger.error(&quot;&quot;,e); } return false; } /** * 判断字符串不为空 * * @param str * @return */ private static boolean strNotNull(String str) { return null != str &amp;&amp; !str.equals(&quot;&quot;); } private static boolean fileExists(String fileName) { return new File(fileName).exists(); } /** * 拿到添加扫描类和资源的路径URL的方法 * * @return * @throws NoSuchMethodException */ private static Method getMethod() throws NoSuchMethodException { Method method = URLClassLoader.class.getDeclaredMethod(&quot;addURL&quot;, URL.class); // 破解方法的访问权限 method.setAccessible(true); return method; } /** * 得到一个URL * * @param fileName * @return * @throws MalformedURLException */ private static URL getURL(String fileName) throws MalformedURLException { return new File(fileName).toURI().toURL(); } /** * 创建对象实例 * * @param className 全限定名 * @return */ public static Object newObject(String className) { try { return Class.forName(className).newInstance(); } catch (Exception e) { logger.error(&quot;&quot;,e); } return null; } /** * 创建转换类型后的对象实例 * * @param className 全限定名 * @param tClass 返回类型 * @param &lt;T&gt; * @return */ public static &lt;T&gt; T newObject(String className, Class&lt;T&gt; tClass) { try { return (T) Class.forName(className).newInstance(); } catch (Exception e) { logger.error(&quot;&quot;,e); } return null; } /** * 创建转换类型后的对象实例 * * @param cmptDef 组件定义-&gt;全限定名 * @param tClass 返回类型 * @param loader 类加载器 * @param &lt;T&gt; * @return */ public static &lt;T&gt; T newObject(CmptDef cmptDef, Class&lt;T&gt; tClass, ClassLoader loader) { try { String className = cmptDef.getFullQualifiedName(); Object newInstance = Class.forName(className, true, loader).newInstance(); return (T) newInstance; } catch (Exception e) { String jarPath = CmptClassLoaderUtil.getJarPath(cmptDef); logger.error(&quot;创建组件实例失败! className=&quot; + cmptDef.getFullQualifiedName() + &quot; jarPath=&quot; + jarPath); logger.error(&quot;创建出错组件:&quot; + cmptDef.getName() + cmptDef.getVersion() + &quot; ,文件信息:&quot; + FileUtil.fileInfo(jarPath)); try { Class&lt;?&gt; aClass = loader.loadClass(cmptDef.getFullQualifiedName()); } catch (ClassNotFoundException e1) { e1.printStackTrace(); logger.error(&quot;类未载入:&quot; + cmptDef.getFullQualifiedName()); } } return null; } /** * 执行对象的方法 * * @param o 实例对象 * @param methodName 方法名称 * @param args 形参 * @return */ public static Object invokeMethod(Object o, String methodName, Object... args) { try { Object invoke; if (null == args || args.length == 0) {//有缺陷,如果形参是任意Object,但传入参数是null,无法得到形参类型,得到有参的方法. Method method = o.getClass().getMethod(methodName); invoke = method.invoke(o); } else { Method method = o.getClass().getMethod(methodName, args.getClass()); invoke = method.invoke(o, args); } return invoke; } catch (Exception e) { logger.error(&quot;&quot;,e); } return null; } } 组件实现公共接口类 public interface ICmpt { /** * 组件执行入口 * * @param request * @param config，组件实例的参数配置 * @param actionNode, 当前执行的流程节点 * @param procContextDTO, 流程引擎实例上下文 * @return */ CmptResult execute(CmptRequest request, Map&lt;String, FieldDTO&gt; config, ActionNode actionNode, ProcContextDTO procContextDTO); /** * 销毁组件持有的特殊资源，比如线程。 */ void destroy(); } ","link":"https://tinaxiawuhao.github.io/post/E2kaihQ5a/"},{"title":"mybatis级联查询","content":"ModelMapper.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace=&quot;com.haier.biz.mapper.ModelMapper&quot;&gt; &lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;com.haier.biz.entity.Model&quot;&gt; &lt;id column=&quot;model_id&quot; jdbcType=&quot;BIGINT&quot; property=&quot;modelId&quot;/&gt; &lt;result column=&quot;equipment_model_mark&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;equipmentModelMark&quot;/&gt; &lt;result column=&quot;equipment_name&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;equipmentName&quot;/&gt; &lt;result column=&quot;specification_model&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;specificationModel&quot;/&gt; &lt;result column=&quot;model_description&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;modelDescription&quot;/&gt; &lt;result column=&quot;model_sort_key&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;modelSortKey&quot;/&gt; &lt;result column=&quot;model_classification_label&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;modelClassificationLabel&quot;/&gt; &lt;result column=&quot;tenant_product_id&quot; jdbcType=&quot;BIGINT&quot; property=&quot;tenantProductId&quot;/&gt; &lt;result column=&quot;tenant_product_name&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;tenantProductName&quot;/&gt; &lt;result column=&quot;manufacturer&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;manufacturer&quot;/&gt; &lt;result column=&quot;create_by&quot; jdbcType=&quot;BIGINT&quot; property=&quot;createBy&quot;/&gt; &lt;result column=&quot;create_time&quot; jdbcType=&quot;TIMESTAMP&quot; property=&quot;createTime&quot;/&gt; &lt;result column=&quot;update_by&quot; jdbcType=&quot;BIGINT&quot; property=&quot;updateBy&quot;/&gt; &lt;result column=&quot;update_time&quot; jdbcType=&quot;TIMESTAMP&quot; property=&quot;updateTime&quot;/&gt; &lt;/resultMap&gt; &lt;!--级联查询--&gt; &lt;resultMap id=&quot;BaseResultInstanceMap&quot; type=&quot;com.haier.biz.entity.DTO.ModelDTO&quot; extends=&quot;BaseResultMap&quot;&gt; &lt;result column=&quot;publishNumber&quot; jdbcType=&quot;BIGINT&quot; property=&quot;publishNumber&quot;/&gt; &lt;result column=&quot;customer_id&quot; jdbcType=&quot;BIGINT&quot; property=&quot;customerId&quot;/&gt; &lt;result column=&quot;topic&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;topic&quot;/&gt; &lt;!--property来源ModelDTO属性，column来源于selectInstancelList查询--&gt; &lt;!--查询单条--&gt; &lt;association property=&quot;equipmentPicture&quot; column=&quot;equipment_model_mark&quot; select=&quot;getEquipmentPicture&quot;&gt;&lt;/association&gt; &lt;association property=&quot;instanceNumber&quot; column=&quot;{equipmentModelMark=equipment_model_mark,customerId=customer_id}&quot; select=&quot;getCustomerInstanceNumber&quot;&gt;&lt;/association&gt; &lt;!--查询列表--&gt; &lt;collection property=&quot;equipmentPictures&quot; column=&quot;equipment_model_mark&quot; select=&quot;getEquipmentPictures&quot;&gt;&lt;/collection&gt; &lt;/resultMap&gt; &lt;select id=&quot;selectInstancelList&quot; parameterType=&quot;com.haier.biz.entity.VO.InstanceSelectVo&quot; resultMap=&quot;BaseResultInstanceMap&quot;&gt; SELECT distinct model.model_id, equipment_model_mark, equipment_name, specification_model, model_description, model_sort_key, model_classification_label, tenant_product_id, tenant_product_name, manufacturer, model.create_by, model.create_time, model.update_by, model.update_time, 0 as publishNumber, relation_model_customer.customer_id, relation_model_customer.topic FROM model LEFT JOIN relation_model_customer on relation_model_customer.model_id=model.model_id &lt;where&gt; &lt;if test=&quot;customerId != null&quot;&gt; and relation_model_customer.customer_id = #{customerId} &lt;/if&gt; &lt;if test=&quot;modelSortKey != null and modelSortKey!=''&quot;&gt; and `model`.model_sort_key = #{modelSortKey} &lt;/if&gt; &lt;if test=&quot;equipmentName != null and equipmentName!=''&quot;&gt; and `model`.equipment_name like CONCAT('%', #{equipmentName,jdbcType=VARCHAR},'%') &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; &lt;select id=&quot;getEquipmentPicture&quot; parameterType=&quot;java.lang.String&quot; resultType=&quot;java.lang.String&quot;&gt; SELECT model_instance_file_associate.file_object_key as equipmentPicture from model_instance_file_associate where model_instance_file_associate.file_type=4 and model_instance_file_associate.type=0 and model_instance_file_associate.del_flag=0 and model_instance_file_associate.model_or_instance_mark = #{equipmentModelMark,jdbcType=VARCHAR} limit 1 &lt;/select&gt; &lt;select id=&quot;getCustomerInstanceNumber&quot; parameterType=&quot;java.util.Map&quot; resultType=&quot;java.lang.Long&quot;&gt; SELECT count(*) from model_instance_associate left join `instance` on model_instance_associate.instance_mark=`instance`.instance_mark where model_instance_associate.equipment_model_mark = #{equipmentModelMark,jdbcType=VARCHAR} and `instance`.customer_id=#{customerId} &lt;/select&gt; &lt;select id=&quot;getEquipmentPictures&quot; parameterType=&quot;java.lang.String&quot; resultType=&quot;java.lang.String&quot;&gt; SELECT model_instance_file_associate.file_object_key as equipmentPictures from model_instance_file_associate where model_instance_file_associate.file_type=4 and model_instance_file_associate.type=0 and model_instance_file_associate.del_flag=0 and model_instance_file_associate.model_or_instance_mark = #{equipmentModelMark,jdbcType=VARCHAR} &lt;/select&gt; &lt;/mapper&gt; ModelMapper @Repository public interface ModelMapper extends BaseMapper&lt;Model&gt; { List&lt;ModelDTO&gt; selectInstancelList(InstanceSelectVo instanceSelectVo); List&lt;String&gt; getEquipmentPictures(@Param(&quot;equipmentModelMark&quot;) String equipmentModelMark); } ModelDTO @Data @EqualsAndHashCode(callSuper = false) @Accessors(chain = true) @AllArgsConstructor @NoArgsConstructor public class ModelDTO extends Model { @ApiModelProperty(value = &quot;产品(模型)图片(取第一张)&quot;) private String equipmentPicture; @ApiModelProperty(value = &quot;产品(模型)图片&quot;) private List&lt;String&gt; equipmentPictures; @ApiModelProperty(value = &quot;发布客户数&quot;) private Long publishNumber; @ApiModelProperty(value = &quot;实例设备数&quot;) private Long instanceNumber; @ApiModelProperty(value = &quot;使用方Id&quot;) private Long customerId; @ApiModelProperty(value = &quot;实例设备运行情况&quot;) private List&lt;NumberDTO&gt; equipmentList; @ApiModelProperty(value = &quot;kafka实时推送的topic&quot;) private String topic; } ","link":"https://tinaxiawuhao.github.io/post/hIydIZSN1/"},{"title":"mysql 简单导入 clickhouse","content":"数据迁移需要从 mysql 导入 clickhouse, clickhouse 自身支持的三种方式 。 create table engin mysql CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2], ... INDEX index_name1 expr1 TYPE type1(...) GRANULARITY value1, INDEX index_name2 expr2 TYPE type2(...) GRANULARITY value2 ) ENGINE = MySQL('host:port', 'database', 'table', 'user', 'password'[, replace_query, 'on_duplicate_clause']); 官方文档: https://clickhouse.yandex/docs/en/operations/table_engines/mysql/ 注意，实际数据存储在远端 mysql 数据库中，可以理解成外表。 可以通过在 mysql 增删数据进行验证。 insert into select from -- 先建表 CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = engine -- 导入数据 INSERT INTO [db.]table [(c1, c2, c3)] select 列或者* from mysql('host:port', 'db', 'table_name', 'user', 'password') 可以自定义列类型，列数，使用 clickhouse 函数对数据进行处理，比如 select toDate(xx) from mysql(&quot;host:port&quot;,&quot;db&quot;,&quot;table_name&quot;,&quot;user_name&quot;,&quot;password&quot;) create table as select from CREATE TABLE [IF NOT EXISTS] [db.]table_name ENGINE =Log AS SELECT * FROM mysql('host:port', 'db', 'article_clientuser_sum', 'user', 'password') 网友文章: http://jackpgao.github.io/2018/02/04/ClickHouse-Use-MySQL-Data/ 不支持自定义列，参考资料里的博主写的 ENGIN=MergeTree 测试失败。 可以理解成 create table 和 insert into select 的组合 ","link":"https://tinaxiawuhao.github.io/post/BHjbgcFm-/"},{"title":"Thingsboard源码编译","content":"环境安装 开发环境要求： Jdk 1.11 版本 Postgresql 9 以上 Node.js Npm Maven 3.6 以上 Git 工具 Idea 开发工具 Redis JDK 下载安装 JDK 官方下载地址： Java Downloads | Oracle JDK 版本选择 JDK11，我本地环境是 Windos10 64 位，所以选择 jdk-11.0.13-windows-x64.exe 下载好了之后直接默认安装就行 免安装版本 下载jdk11 http://openjdk.java.net/install/index.html 这个页面大部分都是linux系统的； 然后我们点击jdk.java.net ，接着我们选择下载Java SE 11； 下好了后，我们得到这样的文件：openjdk-11+28_windows-x64_bin.zip，解压后得到：jdk-11这样的文件夹；将该文件夹放到你习惯的地方； 配置环境变量 步骤 1： 在 JAVA_HOME 中增加 JDK 的安装地址：C:\\Program Files\\Java\\jdk1.8.0_221 步骤 2： 在 CLASSPATH 中增加 JDK 的安装地址中的文件：.;%JAVA_HOME%\\lib;%JAVA_HOME%\\lib\\dt.jar;%JAVA_HOME%\\lib\\tools.jar 步骤 3： 在 Path 中增加 JDK 的地址：%JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; 步骤 4 输入以下命令 java -version 如果能出现以下的提示信息，就算安装成功了 安装 IDEA 参考：IDEA 安装教程 安装 Maven 步骤 1：下载 maven，进入地址：http://maven.apache.org/download.cgi 步骤 2：下载到本地 步骤 3：配置环境变量 增加 MAVEN_HOME，即 maven 的地址：D:\\tb\\apache-maven-3.6.1-bin，请注意，如果直接解压，有可能会有两个 apache-maven-3.6.1-bin MAVEN_OPTS，参数是 -Xms128m -Xmx1024m 修改 Path，增加 Maven 的地址%MAVEN_HOME%\\bin; 测试 Maven 安装，打开命令行工具。使用命令 mvn -v，如果能出现以下提示，即安装成功 Nodejs 安装 步骤 1：下载 Nodejs 安装包，Nodejs 官网地址：https://nodejs.org/en/download/ 步骤 2：安装完成后，使用命令查看 Nodejs 是否已经安装完成，能出现以下提示说明已经安装成功 ! 安装 git 步骤 1：下载 git 安装包，git 官网地址是：https://git-scm.com/download/win 步骤 2：安装完成后，使用命令行测试 git 安装 npm 全局依赖 步骤 1：使用管理员 CMD 命令行，执行下面命令 #npm 环境读取环境变量包 npm install -g cross-env #webpack打包工具 npm install -g webpack 安装 redis Redis 安装参考：https://www.iotschool.com/wiki/redis 环境安装到此结束，接下来是通过 Git 拉取代码。 克隆 thingsboard 代码 确定代码存放位置 在本地创建代码存放位置的文件目录，然后进入当前目录点击鼠标右键，选择 Git Bash Here 输入 git 命令克隆源代码 git clone https://github.com/thingsboard/thingsboard.git 耐心等待一段时间后，看到以下界面就算下载成功 切换 git 分支 默认下载的代码是 master 主分支的，我们开发需要切换到最新版本的分支。 查看项目源码的所有分支，下载源码后，需要进入到 thingsboard 文件夹 发现最新发布的版本是 2.4，所以我这里选择 2.4，当然你可以根据自己的情况进行分支选择 输入命令以下，即可切换至 2.4 的分支 git checkout release-2.4 看到下图这样，即切换成成功 准备工作 外网连接 因为 TB 在编译过程中需要依赖很多国外的包，那么需要外网才能连接，有连接外网支持，可以到社区求助：https://www.iotschool.com/topics/node8 设置 Maven 为淘宝镜像 工程是基于 Maven 管理，直接通过 idea open，之后会自动下载各种依赖包。依赖包的默认存储地址为：C:\\Users\\用户名.m2\\repository，内容如下： $tree ~/.m2 -L 2 /home/jay/.m2 └── repository ├── antlr ├── aopalliance ├── asm ├── backport-util-concurrent ├── ch ... 一般情况下，使用官方镜像更新依赖包，网速不稳定，可将 Maven 镜像源设置为淘宝的，在 maven 安装包目录下找到 settings.xml 设置 大概位置截图： 把 settings.xml 里面内容设置成以下： &lt;mirrors&gt; &lt;mirror&gt; &lt;!--This sends everything else to /public --&gt; &lt;id&gt;aliyun_nexus&lt;/id&gt; &lt;mirrorOf&gt;*,!maven_nexus_201&lt;/mirrorOf&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; 不会设置的，可以参考这个文件：https://cdn.iotschool.com/iotschool/settings.xml thingsboard QQ 群也有这个资源：121202538 设置 npm 为淘宝镜像 同上，网速不好 npm 过程中也会下载失败，这是导致很多同学 thingsboard 编译失败的主要原因，所以我们在进行编译之前，也将 npm 替换为淘宝镜像： npm install -g mirror-config-china --registry=http://registry.npm.taobao.org #使用淘宝镜像 npm config get registry #查询当前镜像 npm config rm registry #删除自定义镜像，使用官方镜像 npm info express 设置 IDEA 管理员启动 我本地开发环境编译项目使用 IDEA 工具进行编译，所以需要设置管理员启动，这样才有所有的权限执行编译命令。 步骤 1：点击 IDEA 图标右键，选择属性。 步骤 2：点击兼容性 - 更改所有用户设置 - 以管理员身份运行此程序 开始编译 编译项目跟网速有关，最好连接上外网进行编译，一般 5~30 分钟都有可能，超过 30 分钟要检查你的网络。 清理项目编译文件 使用 IDEA Maven 工具进行清理 输入编译命令开始编译 在 IDEA 控制台（左下方）Terminal 输入以下命令进行编译： mvn clean install -DskipTests 等一段时间后，看到下面这张图就算编译成功，如果没有编译成功请按照本教程最后的常见问题进行排查，一般都是网络问题。如果还有问题，请到社区thingsboard 专题中提问。 常见问题 pom包pkg.name等标签未定义 &lt;properties&gt; &lt;pkg.name&gt;thingsboard&lt;/pkg.name&gt; &lt;main.dir&gt;${basedir}&lt;/main.dir&gt; &lt;pkg.type&gt;java&lt;/pkg.type&gt; &lt;pkg.mainClass&gt;org.thingsboard.server.ThingsboardServerApplication&lt;/pkg.mainClass&gt; &lt;pkg.copyInstallScripts&gt;true&lt;/pkg.copyInstallScripts&gt; &lt;main.dir&gt;${basedir}&lt;/main.dir&gt; &lt;pkg.disabled&gt;true&lt;/pkg.disabled&gt; &lt;pkg.process-resources.phase&gt;none&lt;/pkg.process-resources.phase&gt; &lt;pkg.package.phase&gt;none&lt;/pkg.package.phase&gt; &lt;pkg.user&gt;thingsboard&lt;/pkg.user&gt; &lt;pkg.implementationTitle&gt;${project.name}&lt;/pkg.implementationTitle&gt; &lt;pkg.unixLogFolder&gt;/var/log/${pkg.name}&lt;/pkg.unixLogFolder&gt; &lt;pkg.installFolder&gt;/usr/share/${pkg.name}&lt;/pkg.installFolder&gt; &lt;/properties&gt; 缓存导致编译失败 每次编译失败进行二次编译时，要清理缓存，并杀死遗留进程 步骤 1：执行下面命令，杀死遗留进程 taskkill /f /im java.exe 步骤 2：使用 IDEA Maven 工具进行清理 温馨提示：要进行二次编译前，最好重启一次电脑！ Server UI 编译失败 [ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.0:npm (npm install) on project ui: Failed to run task: 'npm install' failed. (error code 1) -&gt; [Help 1] 如果遇到这个问题，可从以下几个原因进行分析： 原因 1：node、npm 版本号问题 本地环境安装的 node、npm 版本号与源码中 pom.xml 文件配置的版本号不一致。 解决方案： 步骤 1：使用 node -v、npm -v 查看安装的 node 和 npm 版本号 步骤 2：修改源码中 pom.xml 文件中的版本号 &lt;configuration&gt; &lt;nodeVersion&gt;v12.13.1&lt;/nodeVersion&gt; &lt;npmVersion&gt;6.12.1&lt;/npmVersion&gt; &lt;/configuration&gt; 需要修改的文件有三处，位置如下： 原因 2：node-sass 下载失败 编译 Server UI 时，会下载 node-sass 依赖，如果因为网络原因没有下载成功，也会编译失败。如果你是按照本本教材一步一步来的，应该不会有问题，上面准备工作中，将 npm 镜像源切换为淘宝，那么下载会很快的。 [INFO] Downloading binary from https://github.com/sass/node-sass/releases/download/v4.12.0/win32-x64-72_binding.node [ERROR] Cannot download &quot;https://github.com/sass/node-sass/releases/download/v4.12.0/win32-x64-72_binding.node&quot;: [ERROR] [ERROR] ESOCKETTIMEDOUT [ERROR] [ERROR] Hint: If github.com is not accessible in your location [ERROR] try setting a proxy via HTTP_PROXY, e.g. [ERROR] [ERROR] export HTTP_PROXY=http://example.com:1234 [ERROR] [ERROR] or configure npm proxy via [ERROR] [ERROR] npm config set proxy http://example.com:8080 [INFO] [INFO] &gt; node-sass@4.12.0 postinstall F:\\workspace\\thingsboard\\thingsboard\\ui\\node_modules\\node-sass [INFO] &gt; node scripts/build.js [INFO] 解决方案：切换镜像源为淘宝 解决方案：重启电脑，清理缓存 原因 3：Thingsboard 3.0 版本编译遇到的问题 亲测：2.4 版本也可以通过这种方式来解决 Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.7.5:npm (npm install) on project ui-ngx: Failed to run task: 'npm install' failed. org.apache.commons.exec.ExecuteException: Process exited with an error: -4048 (Exit value: -4048) -&gt; [Help 1] 解决方案：https://www.iotschool.com/topics/84 原因 4：二次编译导致残留进程 报错： [ERROR] Failed to execute goal org.apache.maven.plugins:maven-clean-plugin:2.5:clean (default-clean) on project ui: Failed to clean project: Failed to delete F:\\workspace\\thingsboard\\thingsboard\\ui\\target\\node\\node.exe -&gt; [Help 1] Server Tool 编译失败 [ERROR] Failed to execute goal on project tools: Could not resolve dependencies for project org.thingsboard:tools:jar:2.4.3: Failed to collect dependencies at org.eclipse.paho:org.eclipse.paho.client.mqttv3:jar:1.1.0: Failed to read artifact descriptor for org.eclipse.paho:org.eclipse.paho.clien t.mqttv3:jar:1.1.0: Could not transfer artifact org.eclipse.paho:org.eclipse.paho.client.mqttv3:pom:1.1.0 from/to aliyun_nexus (http://maven.aliyun.com/nexus/content/groups/public/): Failed to transfer file http://maven.aliyun.com/nexus/content/groups/public/org/eclipse/paho/org.eclipse.paho.cli ent.mqttv3/1.1.0/org.eclipse.paho.client.mqttv3-1.1.0.pom with status code 502 -&gt; [Help 1] 一般由于网络原因，IoTSchool 小编至少编译了 3 次才成功，每次编译都重启电脑，并清理环境。 解决方案：如果使用的是 mvn clean install -DskipTests 命令进行编译，那么就多尝试几次，每次编译前，要清理环境。 参考：https://github.com/thingsboard/performance-tests/issues/10 JavaScript Executor 编译失败 JavaScript Executor Microservice 编译失败 [ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.0:npm (npm install) on project js-executor: Failed to run task: 'npm install' failed. (error code 2) -&gt; [Help 1] [ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch. [ERROR] Re-run Maven using the -X switch to enable full debug logging. [ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles: [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException [ERROR] [ERROR] After correcting the problems, you can resume the build with the command [ERROR] mvn &lt;goals&gt; -rf :js-executor 原因：本地缓存缺少 fetched-v10.15.3-linux-x64 和 fetched-v10.15.3-win-x64 这两个文件。 解决方案： 步骤 1：下载这两个文件到本地，下载后记得重命名，下载地址：https://github.com/zeit/pkg-fetch/releases 步骤 2: 将下载的两个文件放到：放到：C:\\Users\\你的用户名 \\ .pkg-cache\\v2.6。并将名字分别修改为：fetched-v10.15.3-linux-x64 和 fetched-v10.15.3-win-x64 参考：https://github.com/thingsboard/thingsboard/issues/2084 License 检查不通过 [ERROR] Failed to execute goal com.mycila:license-maven-plugin:3.0:check (default) on project thingsboard: Some files do not have the expected license header -&gt; [Help 1] 解决方案：在根目录 pom.xml 中屏蔽 license-maven-plugin 搜索 license-maven-plugin，将整个 plugin 都注释掉 Web UI 编译失败 Web UI 编译失败请参考[Server UI 编译失败第一个原因](https://www.iotschool.com/wiki/tbinstall#Server Tool编译失败) maven:Could not resolve dependencies for project org.thingsboard:application: 错误信息 [ERROR] Failed to execute goal on project application: Could not resolve dependencies for project org.thingsboard:application:jar:2.4.1: The following artifacts could not be resolved: org.thingsboard.rule-engine:rule-engine-components:jar:2.4.1, org.thingsboard:dao:jar:2.4.1: Could not find artifact org.thingsboard.rule-engine:rule-engine-components:jar:2.4.1 in jenkins (http://repo.jenkins-ci.org/releases) -&gt; [Help 1] 解决方案：根目录下去 maven 编译，不要在每个单独编译，否则不能自动解决依赖，如果你已经在子模块进行了编译，请回到根目录先 clean 一下，再重新编译。 maven:Failed to delete tb-http-transport.rpm 错误信息： [ERROR] Failed to execute goal org.apache.maven.plugins:maven-clean-plugin:2.5:clean (default-clean) on project http: Failed to clean project: Failed to delete D:\\my_project\\thingsboard\\transport\\http\\target\\tb-http-transport.rpm -&gt; [Help 1] 解决方案：第一次编译失败，再次编译可能会提示该错误，可以手动到报错路径删除，如果提示文件正在使用，需要在任务管理器杀死 java 进程后再手动删除。 npm:npm:cb() never called! 错误信息： npm ERR! cb() never called! npm ERR! This is an error with npm itself. Please report this error at: npm ERR! &lt;https://npm.community&gt; npm ERR! A complete log of this run can be found in: npm ERR! C:\\Users\\yuren\\AppData\\Roaming\\npm-cache\\_logs\\2019-11-06T10_55_28_258Z-debug.log 解决方案： 尝试 npm cache clean --force 后再次 npm install 无果； 尝试更换淘宝镜像源后再次 npm install 无果； 怀疑有些包下载需要翻墙，全局代理翻墙后问题依然存在； 参考网上关闭所有代理后问题依然存在； 通过 log 日志分析最后一个解包报错的地方，屏蔽需要的 material-design-icons，新 modules rxjs 仍然报错； extract material-design-icons@3.0.1 extracted to node_modules\\.staging\\material-design-icons-61b4d55e (72881ms) extract rxjs@6.5.2 extracted to node_modules\\.staging\\rxjs-e901ba4c (24280ms) 参考 npm ERR cb() never called 执行 npm install --no-package-lock 之后提示 npm ERR! path git，添加 git 到环境变量后正常。 npm:npm ERR! path git 错误信息 npm ERR! path git npm ERR! code ENOENT npm ERR! errno ENOENT npm ERR! syscall spawn git npm ERR! enoent Error while executing: npm ERR! enoent undefined ls-remote -h -t git://github.com/fabiobiondi/angular- 解决方案：添加 git 到环境变量。 No compiler is provided in this environment 错误信息： [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3. 1:compile (default-compile) on project netty-mqtt: Compilation failure [ERROR] No compiler is provided in this environment. Perhaps you are running on a JRE rather than a JDK? 需要在环境变量中设置 java，包含%JAVA_HOME%bin;%JAVA_HOME%lib; Failed to execute goal org.thingsboard:gradle-maven-plugin:1.0.10:invoke (default) on project http: org.gradle.tooling.BuildException: Could not execute build using Gradle distribution 'https://services.gradle.org/distributions/gradle-6.3-bin.zip'. Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.1:compile (default-compile) on project rest-client: Compilation failure An unknown compilation problem occurred 这个问题主要是jdk版本跟项目不一致导致的，如果项目的版本是大于(不含！)3.2.1，则需要JDK11，反之JDK8 ","link":"https://tinaxiawuhao.github.io/post/DseKIIHw3/"},{"title":"ClickHouse-8分片集群","content":"ClickHouse-分片集群 副本虽然能够提高数据的可用性，降低丢失风险，但是每台服务器实际上必须容纳全量数据，对数据的横向扩容没有解决。 要解决数据水平切分的问题，需要引入分片的概念。通过分片把一份完整的数据进行切分，不同的分片分布到不同的节点上，再通过 Distributed 表引擎把数据拼接起来一同使用。 Distributed 表引擎本身不存储数据，有点类似于 MyCat 之于 MySql，成为一种中间件，通过分布式逻辑表来写入、分发、路由来操作多台节点不同分片的分布式数据。 注意：ClickHouse 的集群是表级别的，实际企业中，大部分做了高可用，但是没有用分片，避免降低查询性能以及操作集群的复杂性。 1.集群写入流程（3 分片 2 副本共 6 个节点） internal_replication:内部副本同步 true：由分片自己同步 false：由distribute表同步，压力大 2.集群读取流程（3 分片 2 副本共 6 个节点） 3 分片 2 副本共 6 个节点集群配置（供参考） 配置的位置还是在之前的/etc/clickhouse-server/config.d/metrika.xml，内容如下 注：也可以不创建外部文件，直接在 config.xml 的&lt;remote_servers&gt;中指定 &lt;yandex&gt; &lt;remote_servers&gt; &lt;fz_cluster&gt; &lt;!-- 集群名称--&gt; &lt;shard&gt; &lt;!--集群的第一个分片--&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;!--该分片的第一个副本--&gt; &lt;replica&gt; &lt;host&gt;node01&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;!--该分片的第二个副本--&gt; &lt;replica&gt; &lt;host&gt;node02&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;!--集群的第二个分片--&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!--该分片的第一个副本--&gt; &lt;host&gt;node03&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;!--该分片的第二个副本--&gt; &lt;host&gt;node04&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;!--集群的第三个分片--&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!--该分片的第一个副本--&gt; &lt;host&gt;node05&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;!--该分片的第二个副本--&gt; &lt;host&gt;node06&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/fz_cluster&gt; &lt;/remote_servers&gt; &lt;/yandex&gt; 4 .配置三节点版本集群及副本 4.1 集群及副本规划（2 个分片，只有第一个分片有副本） 4.2 配置步骤 1）在 Node01 的/etc/clickhouse-server/config.d 目录下创建 metrika-shard.xml 文件 vim /etc/clickhouse-server/config.d/metrika-shard.xml 注：也可以不创建外部文件，直接在 config.xml 的&lt;remote_servers&gt;中指定 &lt;?xml version=&quot;1.0&quot;?&gt; &lt;yandex&gt; &lt;remote_servers&gt; &lt;gmall_cluster&gt; &lt;!-- 集群名称--&gt; &lt;shard&gt; &lt;!--集群的第一个分片--&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!--该分片的第一个副本--&gt; &lt;host&gt;Node01&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;1234qwer&lt;/password&gt; &lt;/replica&gt; &lt;replica&gt; &lt;!--该分片的第二个副本--&gt; &lt;host&gt;Node02&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;1234qwer&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;!--集群的第二个分片--&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!--该分片的第一个副本--&gt; &lt;host&gt;Node03&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;1234qwer&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/gmall_cluster&gt; &lt;/remote_servers&gt; &lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt;Node01&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;2&quot;&gt; &lt;host&gt;Node02&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;3&quot;&gt; &lt;host&gt;Node03&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper-servers&gt; &lt;macros&gt; &lt;shard&gt;01&lt;/shard&gt; &lt;!--不同机器放的分片数不一样--&gt; &lt;replica&gt;rep_1_1&lt;/replica&gt; &lt;!--不同机器放的副本数不一样--&gt; &lt;/macros&gt; &lt;/yandex&gt; 2）将 Node01 的 metrika-shard.xml 同步到 Node02 和 Node03 scp /etc/clickhouse-server/config.d/metrika-shard.xml root@Node02:/etc/clickhouse-server/config.d/ scp /etc/clickhouse-server/config.d/metrika-shard.xml root@Node03:/etc/clickhouse-server/config.d/ 3）修改 Node02 和 Node03 中 metrika-shard.xml 宏的配置 （1）Node02 &lt;macros&gt; &lt;shard&gt;01&lt;/shard&gt; &lt;!--不同机器放的分片数不一样--&gt; &lt;replica&gt;rep_1_2&lt;/replica&gt; &lt;!--不同机器放的副本数不一样--&gt; &lt;/macros&gt;1.2.3.4. （2）Node03 &lt;macros&gt; &lt;shard&gt;02&lt;/shard&gt; &lt;!--不同机器放的分片数不一样--&gt; &lt;replica&gt;rep_2_1&lt;/replica&gt; &lt;!--不同机器放的副本数不一样--&gt; &lt;/macros&gt;1.2.3.4. 4）在 Node01 上修改/etc/clickhouse-server/config.xml vim /etc/clickhouse-server/config.xml &lt;zookeeper incl=&quot;zookeeper-servers&quot; optional=&quot;true&quot; /&gt; &lt;include_from&gt;/etc/clickhouse-server/config.d/metrika-shard.xml&lt;/include_from&gt; 5）同步/etc/clickhouse-server/config.xml 到 Node02 和 Node03 scp /etc/clickhouse-server/config.xml root@Node02:/etc/clickhouse-server/ scp /etc/clickhouse-server/config.xml root@Node03:/etc/clickhouse-server/ 6）重启三台服务器上的 ClickHouse 服务 sudo clickhouse restart 查看集群 superset-BI :) show clusters; SHOW CLUSTERS Query id: 391735d2-bf74-43f5-aa86-b6d203c357cd ┌─cluster─────────────────────────────────────────┐ │ gmall_cluster │ │ test_cluster_one_shard_three_replicas_localhost │ │ test_cluster_two_shards │ │ test_cluster_two_shards_internal_replication │ │ test_cluster_two_shards_localhost │ │ test_shard_localhost │ │ test_shard_localhost_secure │ │ test_unavailable_shard │ └─────────────────────────────────────────────────┘ 8 rows in set. Elapsed: 0.002 sec. 7）在 Node01 上执行建表语句 ➢ 会自动同步到 Node02 和 Node03 上 ➢ 集群名字要和配置文件中的一致 ➢ 分片和副本名称从配置文件的宏定义中获取 create table st_fz_order_mt_01 on cluster gmall_cluster ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime ) engine =ReplicatedMergeTree('/clickhouse/tables/{shard}/st_fz_order_mt_01','{replica}') partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id); create table st_fz_order_mt_01 on cluster gmall_cluster ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime ) engine =ReplicatedMergeTree('/clickhouse/tables/{shard}/st_fz_order_mt_01','{replica}') partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id); CREATE TABLE st_fz_order_mt_01 ON CLUSTER gmall_cluster ( `id` UInt32, `sku_id` String, `total_amount` Decimal(16, 2), `create_time` Datetime ) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/st_fz_order_mt_01', '{replica}') PARTITION BY toYYYYMMDD(create_time) PRIMARY KEY id ORDER BY (id, sku_id) 在Node02和Node03上查看表是否创建成功 show tables; 8）在 Node02 上创建 Distribute 分布式表 create table st_fz_order_mt_all2 on cluster gmall_cluster ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime )engine = Distributed(gmall_cluster,default, st_fz_order_mt_01,hiveHash(sku_id)); CREATE TABLE st_fz_order_mt_all2 ON CLUSTER gmall_cluster ( `id` UInt32, `sku_id` String, `total_amount` Decimal(16, 2), `create_time` Datetime ) ENGINE = Distributed(gmall_cluster, default, st_fz_order_mt_01, hiveHash(sku_id)) 参数含义： Distributed（集群名称，库名，本地表名，分片键） 分片键必须是整型数字，所以用 hiveHash 函数转换，也可以 rand() 9）在 Node01 上插入测试数据 insert into st_order_mt_all2 values (201,'sku_001',1000.00,'2020-06-01 12:00:00') , (202,'sku_002',2000.00,'2020-06-01 12:00:00'), (203,'sku_004',2500.00,'2020-06-01 12:00:00'), (204,'sku_002',2000.00,'2020-06-01 12:00:00'), (205,'sku_003',600.00,'2020-06-02 12:00:00');1.2.3.4.5.6. 10）通过查询分布式表和本地表观察输出结果 （1）分布式表 select * From st_fz_order_mt_all2; Query id: d8b676e9-c119-4483-8ca2-f0b5cd150a61 ┌──id─┬─sku_id──┬─total_amount─┬─────────create_time─┐ │ 202 │ sku_002 │ 2000 │ 2020-06-01 12:00:00 │ │ 203 │ sku_004 │ 2500 │ 2020-06-01 12:00:00 │ │ 204 │ sku_002 │ 2000 │ 2020-06-01 12:00:00 │ └─────┴─────────┴──────────────┴─────────────────────┘ ┌──id─┬─sku_id──┬─total_amount─┬─────────create_time─┐ │ 205 │ sku_003 │ 600 │ 2020-06-02 12:00:00 │ └─────┴─────────┴──────────────┴─────────────────────┘ ┌──id─┬─sku_id──┬─total_amount─┬─────────create_time─┐ │ 201 │ sku_001 │ 1000 │ 2020-06-01 12:00:00 │ └─────┴─────────┴──────────────┴─────────────────────┘ （2）本地表 Node1: SELECT * FROM st_fz_order_mt_01 Query id: ddcb5176-e443-4253-9877-57fec8f57311 ┌──id─┬─sku_id──┬─total_amount─┬─────────create_time─┐ │ 202 │ sku_002 │ 2000 │ 2020-06-01 12:00:00 │ │ 203 │ sku_004 │ 2500 │ 2020-06-01 12:00:00 │ │ 204 │ sku_002 │ 2000 │ 2020-06-01 12:00:00 │ └─────┴─────────┴──────────────┴─────────────────────┘ 3 rows in set. Elapsed: 0.002 sec. Node3: SELECT * FROM st_fz_order_mt_01 Query id: 7a336004-7040-4098-948e-1e7c5d983edb ┌──id─┬─sku_id──┬─total_amount─┬─────────create_time─┐ │ 205 │ sku_003 │ 600 │ 2020-06-02 12:00:00 │ └─────┴─────────┴──────────────┴─────────────────────┘ ┌──id─┬─sku_id──┬─total_amount─┬─────────create_time─┐ │ 201 │ sku_001 │ 1000 │ 2020-06-01 12:00:00 │ └─────┴─────────┴──────────────┴─────────────────────┘ 2 rows in set. Elapsed: 0.002 sec. 可以看到数据分布在Node1和Node3两个节点上。 ","link":"https://tinaxiawuhao.github.io/post/vuilLU8qj/"},{"title":"ClickHouse-7副本","content":"1 副本写入流程 2 配置步骤 启动 zookeeper 集群 在 hadoop102 的/etc/clickhouse-server/config.d 目录下创建一个名为 metrika.xml 的配置文件,内容如下： 注：也可以不创建外部文件，直接在 config.xml 中指定 &lt;?xml version=&quot;1.0&quot;?&gt; &lt;yandex&gt; &lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt;hadoop102&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;2&quot;&gt; &lt;host&gt;hadoop103&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;3&quot;&gt; &lt;host&gt;hadoop104&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper-servers&gt; &lt;/yandex&gt; 同步到 hadoop103 和 hadoop104 上 sudo /home/atguigu/bin/xsync /etc/clickhouse-server/config.d/metrika.xml 在 hadoop102 的/etc/clickhouse-server/config.xml 中增加 &lt;zookeeper incl=&quot;zookeeper-servers&quot; optional=&quot;true&quot; /&gt; &lt;include_from&gt;/etc/clickhouse-server/config.d/metrika.xml&lt;/include_from&gt; 同步到 hadoop103 和 hadoop104 上 sudo /home/atguigu/bin/xsync /etc/clickhouse-server/config.xml 分别在 hadoop102 和 hadoop103 上启动 ClickHouse 服务 注意：因为修改了配置文件，如果以前启动了服务需要重启 sudo clickhouse restart 注意：我们演示副本操作只需要在 hadoop102 和 hadoop103 两台服务器即可，上面的 操作，我们 hadoop104 可以你不用同步，我们这里为了保证集群中资源的一致性，做了同 步。 在 hadoop102 和 hadoop103 上分别建表 副本只能同步数据，不能同步表结构，所以我们需要在每台机器上自己手动建表 create table t_order_rep2 ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime ) engine = ReplicatedMergeTree('/clickhouse/table/01/t_order_rep','rep_102') partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id); ReplicatedMergeTree(‘/clickhouse/table/01/t_order_rep’,‘rep_102’)中， 第一个参数是分片的 zk_path ， 一般按照：/clickhouse/table/{shard}/{table_name} 的格式写，如果只有一个分片就写 01 即可。 第二个参数是副本名称，相同的分片副本名称不能相同。 在 hadoop102 上执行 insert 语句 insert into t_order_rep2 values (101,'sku_001',1000.00,'2020-06-01 12:00:00'), (102,'sku_002',2000.00,'2020-06-01 12:00:00'), (103,'sku_004',2500.00,'2020-06-01 12:00:00'), (104,'sku_002',2000.00,'2020-06-01 12:00:00'), (105,'sku_003',600.00,'2020-06-02 12:00:00'); 在 hadoop103 上执行 select，可以查询出结果，说明副本配置正确 ","link":"https://tinaxiawuhao.github.io/post/bps7a3f5p/"},{"title":"ClickHouse-6sql操作","content":"1 Insert 基本与标准 SQL（MySQL）基本一致 标准 insert into [table_name] values(…),(….) 从表到表的插入 insert into [table_name] select a,b,c from [table_name_2] 2 Update 和 Delete ClickHouse 提供了 Delete 和 Update 的能力，这类操作被称为 Mutation 查询，它可以看做 Alter 的一种。 虽然可以实现修改和删除，但是和一般的 OLTP 数据库不一样，Mutation 语句是一种很“重”的操作，而且不支持事务。 “重”的原因主要是每次修改或者删除都会导致放弃目标数据的原有分区，重建新分区。所以尽量做批量的变更，不要进行频繁小数据的操作。 删除操作 alter table t_order_smt delete where sku_id ='sku_001'; 修改操作 alter table t_order_smt update total_amount=toDecimal32(2000.00,2) where id =102; 由于操作比较“重”，所以 Mutation 语句分两步执行，同步执行的部分其实只是进行新增数据新增分区和并把旧分区打上逻辑上的失效标记。直到触发分区合并的时候，才会删除旧数据释放磁盘空间，一般不会开放这样的功能给用户，由管理员完成。 3 查询操作 ClickHouse 基本上与标准 SQL 差别不大 支持子查询 支持 CTE(Common Table Expression 公用表表达式 with 子句) 支持各种 JOIN，但是 JOIN 操作无法使用缓存, 尽量避免使用 JOIN，所以即使是两次相同的 JOIN 语句，ClickHouse 也会视为两条新 SQL 窗口函数 不支持自定义函数 GROUP BY 操作增加了 with rollup \\ with cube \\ with total 用来计算小计和总计。 插入数据 alter table t_order_mt delete where 1=1; insert into t_order_mt values (101,'sku_001',1000.00,'2020-06-01 12:00:00'), (101,'sku_002',2000.00,'2020-06-01 12:00:00'), (103,'sku_004',2500.00,'2020-06-01 12:00:00'), (104,'sku_002',2000.00,'2020-06-01 12:00:00'), (105,'sku_003',600.00,'2020-06-02 12:00:00'), (106,'sku_001',1000.00,'2020-06-04 12:00:00'), (107,'sku_002',2000.00,'2020-06-04 12:00:00'), (108,'sku_004',2500.00,'2020-06-04 12:00:00'), (109,'sku_002',2000.00,'2020-06-04 12:00:00'), (110,'sku_003',600.00,'2020-06-01 12:00:00'); with rollup：上卷，从右至左去掉维度进行小计 select id , sku_id,sum(total_amount) from t_order_mt group by id,sku_id with rollup; with cube : 多维分析， 从右至左去掉维度进行小计，再从左至右去掉维度进行小计 select id , sku_id,sum(total_amount) from t_order_mt group by id,sku_id with cube; with totals: 只计算合计 select id , sku_id,sum(total_amount) from t_order_mt group by id,sku_id with totals; 4 alter 操作 同 MySQL 的修改字段基本一致 新增字段 alter table tableName add column newcolname String after col1; 修改字段类型 alter table tableName modify column newcolname String; 删除字段 alter table tableName drop column newcolname; 5 导出数据 这个用的比较少 clickhouse-client --query &quot;select * from t_order_mt where create_time='2020-06-01 12:00:00'&quot; --format CSVWithNames&gt; /opt/module/data/rs1.csv 更多支持格式参照：https://clickhouse.com/docs/en/interfaces/formats/ ","link":"https://tinaxiawuhao.github.io/post/3wkSNwWQy/"},{"title":"Clickhouse-5数据分区","content":"MergeTree 数据分区规则 创建按照月份为分区条件的表 tab_partition CREATE TABLE tab_partition(`dt` Date, `v` UInt8) ENGINE = MergeTree PARTITION BY toYYYYMM(dt) ORDER BY v; insert into tab_partition(dt,v) values ('2020-02-11',1),('2020-02-13',2); insert into tab_partition(dt,v) values ('2020-04-11',3),('2020-04-13',4); insert into tab_partition(dt,v) values ('2020-09-11',5),('2020-09-10',6); insert into tab_partition(dt,v) values ('2020-10-12',7),('2020-10-09',8); insert into tab_partition(dt,v) values ('2020-02-14',9),('2020-02-15',10); insert into tab_partition(dt,v) values ('2020-02-11',23),('2020-02-13',45); MergeTree 存储引擎在写入数据之后生成对应的分区文件为： MergeTree 的分区目录是在写入数据的过程中被创建出来，每 insert 一次，就会创建一批次分区目录。也就是说如果仅创建表结构，是不会创建分区目录的，因为木有数据。 MergeTree 数据分区目录命名规则其规则为：PartitionID_MinBlockNum_MaxBlockNum_Level 比如 202002_4_4_0 其中 202002 是分区ID ，4_4 对应的是 最小的数据块编号和最大的数据块编号，最后的 _0 表示目前分区合并的层级。 各部分的含义及命名规则如下： PartitionID：该值由 insert 数据时分区键的值来决定。分区键支持使用任何一个或者多个字段组合表达式，针对取值数据类型的不同，分区 ID 的生成逻辑目前有四种规则： 不指定分区键：如果建表时未指定分区键，则分区 ID 默认使用 all，所有数据都被写入 all 分区中。 整型字段：如果分区键取值是整型字段，并且无法转换为 YYYYMMDD 的格式，则会按照该整型字段的字符形式输出，作为分区 ID 取值。 日期类型：如果分区键属于日期格式，或可以转换为 YYYYMMDD 格式的整型，则按照 YYYYMMDD 格式化后的字符形式输出，作为分区 ID 取值。 其他类型：如果使用其他类似 Float、String 等类型作为分区键，会通过对其插入数据的 128 位 Hash 值作为分区 ID 的取值。 MinBlockNum 和 MaxBlockNum：BlockNum 是一个整型的自增长型编号，该编号在单张 MergeTree 表中从 1 开始全局累加，当有新的分区目录创建后，该值就加 1，对新的分区目录来讲，MinBlockNum 和 MaxBlockNum 取值相同。例如上面示例数据为 202002_1_1_0 202002_1_5_1，但当分区目录进行合并后，取值规则会发生变化，MinBlockNum 取同一分区所欲目录中最新的 MinBlockNum 值。MaxBlockNum 取同一分区内所有目录中的最大值。 Level：表示合并的层级。相当于某个分区被合并的次数，它不是以表全局累加，而是以分区为单位，初始创建的分区，初始值为 0，相同分区 ID 发生合并动作时，在相应分区内累计加 1。 MergeTree 数据分区合并规则 随着数据的写入 MergeTree 存储引擎会很多分区目录。如果分区目录数太多怎么办？因为 Clickhouse 的 MergeTree 存储引擎是基于 LSM 实现的。MergeTree 可以通过分区合并将属于相同分区的多个目录合并为一个新的目录（官方描述在 10 到 15 分钟内会进行合并，也可直接执行 optimize 语句），已经存在的旧目录（也即 system.parts 表中 activie 为 0 的分区）在之后某个时刻通过后台任务删除（默认 8 分钟）。 分区合并 我们回顾之前创建的表的分区目录 # ls 202002_1_1_0 202004_2_2_0 202009_3_3_0 202002_4_4_0 202002_5_5_0 手工触发分区合并 qabb-qa-ch00 :) optimize table tab_partition; OPTIMIZE TABLE tab_partition Ok. 0 rows in set. Elapsed: 0.003 sec. qabb-qa-ch00 :) select partition,name,part_type, active from system.parts where table ='tab_partition'; ┌─partition─┬─name─────────┬─part_type─┬─active─┐ │ 202002 │ 202002_1_1_0 │ Wide │ 0 │ │ 202002 │ 202002_1_5_1 │ Wide │ 1 │ │ 202002 │ 202002_4_4_0 │ Wide │ 0 │ │ 202002 │ 202002_5_5_0 │ Wide │ 0 │ │ 202004 │ 202004_2_2_0 │ Wide │ 1 │ │ 202009 │ 202009_3_3_0 │ Wide │ 1 │ └───────────┴──────────────┴───────────┴────────┘ 6 rows in set. Elapsed: 0.003 sec. 其中 active 为 1 表示经过合并之后的最新分区，为 0 则表示旧分区，查询时会自动过滤 active=0 的分区。 我们通过分区 202002 最新的分区目录 202002_1_5_1 看到合并分区新目录的命名规则如下： PartitionID：分区 ID 保持不变 MinBlockNum：取同一个分区内所有目录中最小的 MinBlockNum 值 MaxBlockNUm：取同一个分区内所有目录中最大的 MaxBlockNum 值 Level：取同一个分区内最大 Level 值并加 1 合并之后的目录结构如下： ","link":"https://tinaxiawuhao.github.io/post/hle1ZXFqv/"},{"title":"ClickHouse-4表引擎","content":"1 表引擎的使用 表引擎是 ClickHouse 的一大特色。可以说，表引擎决定了如何存储表的数据。包括： 数据的存储方式和位置，写到哪里以及从哪里读取数据。(默认是在安装路径下的 data 路径) 支持哪些查询以及如何支持。（有些语法只有在特定的引擎下才能用） 并发数据访问。 索引的使用（如果存在）。 是否可以执行多线程请求。 数据复制参数。 表引擎的使用方式就是必须显式在创建表时定义该表使用的引擎，以及引擎使用的相关参数。 特别注意：引擎的名称大小写敏感, 驼峰命名 2 TinyLog 以列文件的形式保存在磁盘上，不支持索引，没有并发控制。一般保存少量数据的小表，生产环境上作用有限。可以用于平时练习测试用。 如： create table t_tinylog ( id String, name String) engine=TinyLog; 3 Memory 内存引擎，数据以未压缩的原始形式直接保存在内存当中，服务器重启数据就会消失。读写操作不会相互阻塞，不支持索引。简单查询下有非常非常高的性能表现（超过 10G/s）。 一般用到它的地方不多，除了用来测试，就是在需要非常高的性能，同时数据量又不太大（上限大概 1 亿行）的场景。 4 MergeTree ClickHouse 中最强大的表引擎当属 MergeTree（合并树）引擎及该系列（*MergeTree）中的其他引擎，支持索引和分区，地位可以相当于 innodb 之于 Mysql。而且基于 MergeTree，还衍生除了很多小弟，也是非常有特色的引擎。 建表语句 create table t_order_mt( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime ) engine =MergeTree partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id); 插入数据 insert into t_order_mt values (101,'sku_001',1000.00,'2020-06-01 12:00:00') , (102,'sku_002',2000.00,'2020-06-01 11:00:00'), (102,'sku_004',2500.00,'2020-06-01 12:00:00'), (102,'sku_002',2000.00,'2020-06-01 13:00:00'), (102,'sku_002',12000.00,'2020-06-01 13:00:00'), (102,'sku_002',600.00,'2020-06-02 12:00:00'); MergeTree 其实还有很多参数(绝大多数用默认值即可)，但是三个参数是更加重要的，也涉及了关于 MergeTree 的很多概念。 4.1 partition by 分区(可选) 作用 学过 hive 的应该都不陌生，分区的目的主要是降低扫描的范围，优化查询速度，如果不填,只会使用一个分区 —— all。 分区目录 MergeTree 是以列文件+索引文件+表定义文件组成的，但是如果设定了分区那么这些文件就会保存到不同的分区目录中。目录保存在本地磁盘 并行 分区后，面对涉及跨分区的查询统计，ClickHouse 会以分区为单位并行处理， 即一个线程处理一个分区内的数据。 数据写入与分区合并 任何一个批次的数据写入都会产生一个临时分区，不会纳入任何一个已有的分区。写入后的某个时刻（大概 10-15 分钟后），ClickHouse 会自动执行合并操作（等不及也可以手动通过 optimize 执行），把临时分区的数据，合并到已有分区中。 optimize table xxxx final; 例如 再次执行上面的插入操作 insert into t_order_mt values (101,'sku_001',1000.00,'2020-06-01 12:00:00') , (102,'sku_002',2000.00,'2020-06-01 11:00:00'), (102,'sku_004',2500.00,'2020-06-01 12:00:00'), (102,'sku_002',2000.00,'2020-06-01 13:00:00'), (102,'sku_002',12000.00,'2020-06-01 13:00:00'), (102,'sku_002',600.00,'2020-06-02 12:00:00'); 查看数据并没有纳入任何分区 手动 optimize 之后 optimize table t_order_mt final; 4.2 primary key 主键(可选) ClickHouse 中的主键，和其他数据库不太一样，它只提供了数据的一级索引，但是却不是唯一约束。这就意味着是可以存在相同 primary key 的数据的。 主键的设定主要依据是查询语句中的 where 条件。 根据条件通过对主键进行某种形式的二分查找，能够定位到对应的 index granularity,避免了全表扫描。 index granularity： 直接翻译的话就是索引粒度，指在稀疏索引中两个相邻索引对应数据的间隔。ClickHouse 中的 MergeTree 默认是 8192。官方不建议修改这个值，除非该列存在大量重复值，比如在一个分区中几万行才有一个不同数据。 稀疏索引： 稀疏索引的好处就是可以用很少的索引数据，定位更多的数据，代价就是只能定位到索引粒度的第一行，然后再进行进行一点扫描。 4.3 order by（必选） order by 设定了分区内的数据按照哪些字段顺序进行有序保存。 order by 是 MergeTree 中唯一一个必填项，甚至比 primary key 还重要，因为当用户不设置主键的情况，很多处理会依照 order by 的字段进行处理（比如后面会讲的去重和汇总）。 要求：主键必须是 order by 字段的前缀字段。有点类似 SQL 索引中的最左前缀。 比如 order by 字段是 (id,sku_id) 那么主键必须是 id 或者(id,sku_id) 4.4 二级索引(跳数索引) 目前在 ClickHouse 的官网上二级索引的功能在 v20.1.2.4 之前是被标注为实验性的，在这个版本之后默认是开启的。 老版本使用二级索引前需要增加设置 是否允许使用实验性的二级索引（v20.1.2.4 开始，这个参数已被删除，默认开启） set allow_experimental_data_skipping_indices=1; 创建测试表 create table t_order_mt2( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime, INDEX a total_amount TYPE minmax GRANULARITY 5 ) engine =MergeTree partition by toYYYYMMDD(create_time) primary key (id) order by (id, sku_id); 最主要的是需要加上这一句：INDEX a total_amount TYPE minmax GRANULARITY 5 其中： INDEX a： 定义一个索引，并命名为 a total_amount： 索引字段名称 TYPE minmax： 定义类型， 保存最大最小值 GRANULARITY N：是设定二级索引对于一级索引粒度的粒度。一级索引会记录每个分区的最大最小值，二级索引就是在一级索引的基础上，取每 N 个分区合在一起的最大最小值。 插入数据 insert into t_order_mt2 values (101,'sku_001',1000.00,'2020-06-01 12:00:00') , (102,'sku_002',2000.00,'2020-06-01 11:00:00'), (102,'sku_004',2500.00,'2020-06-01 12:00:00'), (102,'sku_002',2000.00,'2020-06-01 13:00:00'), (102,'sku_002',12000.00,'2020-06-01 13:00:00'), (102,'sku_002',600.00,'2020-06-02 12:00:00'); 对比效果 那么在使用下面语句进行测试，可以看出二级索引能够为非主键字段的查询发挥作用。 clickhouse-client --send_logs_level=trace &lt;&lt;&lt; 'select * from t_order_mt2 where total_amount &gt; toDecimal32(900., 2)'; 4.5 数据 TTL（数据存活时间） TTL 即 Time To Live，MergeTree 提供了可以管理数据表或者列的生命周期的功能。过期的数据不会立马处理，只是会做标记，等到合并时一起处理。 列级别 TTL （1）创建测试表 create table t_order_mt3( id UInt32, sku_id String, total_amount Decimal(16,2) TTL create_time+interval 10 SECOND, create_time Datetime ) engine =MergeTree partition by toYYYYMMDD(create_time) primary key (id) order by (id, sku_id); 在列后加上： TTL create_time+interval 10 SECOND 其中： TTL：定义一个过期时间 create_time：使用了当前表中的一个 Datetime 类型的列，TTL 中引用的字段不能是主键字段，且类型必须是 日期类型 ± interval 10 SECOND：需要增加/减少的 时间长度 时间单位 注：如果在建表时没有加 TTL 配置，需要在建表之后加，那么则需要使用如下语法： ALTER TABLE 表名 MODIFY COLUMN 列名 列数据类型 TTL d + INTERVAL 10 SECOND; （2）插入数据（注意：根据实际时间改变） insert into t_order_mt3 values (106,'sku_001',1000.00,'2020-06-12 22:52:30'), (107,'sku_002',2000.00,'2020-06-12 22:52:30'), (110,'sku_003',600.00,'2020-06-13 12:00:00'); （3）手动合并，查看效果,到期后，指定的字段数据归 0；如果执行合并后还是可以看到数据，可能是需要设置时区，或者需要重启。 表级 TTL 与列级相同，在建表时与建表之后都有对应的语法进行设置 （1）在建表时，写在 Order By 的后面 CREATE TABLE example_table ( d DateTime, a Int ) ENGINE = MergeTree PARTITION BY toYYYYMM(d) ORDER BY d TTL d + INTERVAL 1 MONTH [DELETE], d + INTERVAL 1 WEEK TO VOLUME 'aaa', d + INTERVAL 2 WEEK TO DISK 'bbb'; 该示例中的 [DELETE]、TO VOLUME……在下文&quot;注意&quot;中说明 （2）在建表之后，使用 alter alter table t_order_mt3 MODIFY TTL create_time + INTERVAL 10 SECOND; 表级的 TTL ，如果是按照表中字段值判断是否过期，那么不会整表删除，只会是某一行数据到期删某一行。 注意 涉及判断的字段必须是 Date 或者 Datetime 类型，推荐使用分区的日期字段。 能够使用的时间单位： SECOND MINUTE HOUR DAY WEEK MONTH QUARTER YEAR 上文提到的 [DELETE]、TO VOLUME…… 表示的是过期之后，需要做的操作。可选的配置如下 DELETE - 删除数据 （默认，不配置则删除）; RECOMPRESS codec_name - 使用codec_name重新压缩数据部分； TO DISK ‘aaa’ - 将数据移动到磁盘 aaa 上 TO VOLUME ‘bbb’ - 将数据移动到磁盘 bbb 上 GROUP BY - 聚合过期行 5 ReplacingMergeTree ReplacingMergeTree 是 MergeTree 的一个变种，它存储特性完全继承 MergeTree，只是多了一个去重的功能。 尽管 MergeTree 可以设置主键，但是 primary key 其实没有唯一约束的功能。如果你想处理掉重复的数据，可以借助这个 ReplacingMergeTree。去重按照order by的字段去重 去重时机 数据的去重只会在合并的过程中出现。合并会在未知的时间在后台进行，所以你无法预先作出计划。有一些数据可能仍未被处理。 去重范围 如果表经过了分区，去重只会在分区内部进行去重，不能执行跨分区的去重。 所以 ReplacingMergeTree 能力有限， ReplacingMergeTree 适用于在后台清除重复的数据以节省空间，但是它不保证没有重复的数据出现。 案例演示 （1）创建表 create table t_order_rmt( id UInt32, sku_id String, total_amount Decimal(16,2) , create_time Datetime ) engine =ReplacingMergeTree(create_time) partition by toYYYYMMDD(create_time) primary key (id) order by (id, sku_id); ReplacingMergeTree() 填入的参数为版本字段，重复数据保留版本字段值最大的。 如果不填版本字段，默认按照插入顺序保留最后一条。 （2）向表中插入数据 insert into t_order_rmt values (101,'sku_001',1000.00,'2020-06-01 12:00:00') , (102,'sku_002',2000.00,'2020-06-01 11:00:00'), (102,'sku_004',2500.00,'2020-06-01 12:00:00'), (102,'sku_002',2000.00,'2020-06-01 13:00:00'), (102,'sku_002',12000.00,'2020-06-01 13:00:00'), (102,'sku_002',600.00,'2020-06-02 12:00:00'); （3）执行第一次查询 select * from t_order_rmt; （4）手动合并 OPTIMIZE TABLE t_order_rmt FINAL; （5）再执行一次查询 select * from t_order_rmt; 通过测试得到结论 实际上是使用 order by 字段作为唯一键 去重不能跨分区 只有同一批插入（新版本）或合并分区时才会进行去重 认定重复的数据保留，版本字段值最大的 如果版本字段相同则按插入顺序保留最后一笔 6 SummingMergeTree 对于不查询明细，只关心以维度进行汇总聚合结果的场景。如果只使用普通的MergeTree的话，无论是存储空间的开销，还是查询时临时聚合的开销都比较大。 ClickHouse 为了这种场景，提供了一种能够“预聚合”的引擎 SummingMergeTree。该引擎聚合的依据依然是 order by 的字段，相当于按照 order by 的字段做了一次 Group By。 案例演示 （1）创建表 create table t_order_smt( id UInt32, sku_id String, total_amount Decimal(16,2) , create_time Datetime ) engine =SummingMergeTree(total_amount) partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id ); （2）插入数据 insert into t_order_smt values (101,'sku_001',1000.00,'2020-06-01 12:00:00'), (102,'sku_002',2000.00,'2020-06-01 11:00:00'), (102,'sku_004',2500.00,'2020-06-01 12:00:00'), (102,'sku_002',2000.00,'2020-06-01 13:00:00'), (102,'sku_002',12000.00,'2020-06-01 13:00:00'), (102,'sku_002',600.00,'2020-06-02 12:00:00'); （3）执行第一次查询 select * from t_order_smt; （4）手动合并 OPTIMIZE TABLE t_order_smt FINAL; （5）再执行一次查询 select * from t_order_smt; 通过结果可以得到以下结论 以 SummingMergeTree（）中指定的列作为汇总数据列 可以填写多列必须数字列，如果不填，以所有非维度列且为数字列的字段为汇总数据列 以 order by 的列为准，作为维度列 其他的列按插入顺序保留第一行 不在一个分区的数据不会被聚合 只有在同一批次插入(新版本)或分片合并时才会进行聚合 开发建议 设计聚合表的话，唯一键值、流水号可以去掉，所有字段全部是维度、度量或者时间戳。 问题 能不能直接执行以下 SQL 得到汇总值 select total_amount from XXX where province_name=’’ and create_date=’xxx’ 答： 不行，可能会包含一些还没来得及聚合的临时明细 如果要是获取汇总值，还是需要使用 sum 进行聚合，这样效率会有一定的提高，但本身 ClickHouse 是列式存储的，效率提升有限，不会特别明显。 select sum(total_amount) from province_name=’’ and create_date=‘xxx’ ","link":"https://tinaxiawuhao.github.io/post/XvNkKhwMg/"},{"title":"ClickHouse-3数据类型","content":"1. 整型 固定长度的整型，包括 有符号整型(有正有负) 或 无符号整型。 类比 Java 类型： CH类型 Java类型 整型范围（-2n-1~2n-1-1） Int8 - [-128 : 127] Byte Int16 - [-32768 : 32767] Short Int32 - [-2147483648 : 2147483647] Int Int64 - [-9223372036854775808 : 9223372036854775807] Long 无符号整型范围（0~2n-1） UInt8 - [0 : 255] UInt16 - [0 : 65535] UInt32 - [0 : 4294967295] UInt64 - [0 : 18446744073709551615] 后面的数组就代表位数 使用场景： 个数、数量、也可以存储型 id。 2. 浮点型 类比 Java 类型： CH类型 Java类型 Float32 float Float64 double 建议尽可能以整数形式存储数据。例如，将固定精度的数字转换为整数值，如时间用毫秒为单位表示，因为浮点型进行计算时可能引起四舍五入的误差。 使用场景：一般数据值比较小，不涉及大量的统计计算，精度要求不高的时候。比如保存商品的重量。 3. 布尔型 没有单独的类型来存储布尔值。可以使用 UInt8 类型，取值限制为 0 或 1。 4. Decimal 型 有符号的浮点数，可在加、减和乘法运算过程中保持精度。对于除法，最低有效数字会被丢弃（不四舍五入）。 有三种声明 (s 标识小数位)： Decimal32(s)， 即 一共有 8 位，其中小数部分占 s 位，相当于 Mysql - Decimal(9-s,s) Decimal64(s)， 即 一共有 18 位，其中小数部分占 s 位，相当于 Decimal(18-s,s) Decimal128(s)， 即 一共有 38位，其中小数部分占 s 位，相当于 Decimal(38-s,s) 使用场景： 一般金额字段、汇率、利率等字段为了保证小数点精度，都使用 Decimal进行存储。 5. 字符串 String - 对应 Mysql - Varchar 字符串可以任意长度的。它可以包含任意的字节集，包含空字节。 FixedString(N) 固定长度 N 的字符串，N 必须是严格的正自然数。当服务端读取长度小于 N 的字符串时候，通过在字符串末尾添加空字节来达到 N 字节长度。 当服务端读取长度大于 N 的字符串时候，将返回错误消息。 与 String 相比，极少会使用 FixedString，因为使用起来不是很方便。 使用场景：名称、文字描述、字符型编码。 固定长度的可以保存一些定长的内容，比如一些编码，性别等但是考虑到一定的变化风险，带来收益不够明显，所以定长字符串使用意义有限。 6. 枚举类型 包括 Enum8 和 Enum16 类型。Enum 保存 ‘string’= integer 的对应关系。 Enum8 用 ‘String’= Int8 对描述。 Enum16 用 ‘String’= Int16 对描述。 创建一个带有一个枚举 Enum8(‘hello’ = 1, ‘world’ = 2) 类型的列 CREATE TABLE t_enum ( x Enum8('hello' = 1, 'world' = 2) ) ENGINE = TinyLog; 这个 x 列只能存储类型定义中列出的值：‘hello’或’world’ INSERT INTO t_enum VALUES ('hello'), ('world'), ('hello'); 如果尝试保存任何其他值，ClickHouse 抛出异常 insert into t_enum values('a') 如果需要看到对应行的数值，则必须将 Enum 值转换为整数类型 SELECT CAST(x, 'Int8') FROM t_enum; 使用场景：对一些状态、类型的字段算是一种空间优化，也算是一种数据约束。但是实际使用中往往因为一些数据内容的变化增加一定的维护成本，甚至是数据丢失问题。所以谨慎使用。 7. 时间类型 目前 ClickHouse 有三种时间类型 Date 接受年-月-日的字符串比如 ‘2019-12-16’ Datetime 接受年-月-日 时:分:秒的字符串比如 ‘2019-12-16 20:50:10’ Datetime64 接受年-月-日 时:分:秒.亚秒的字符串比如‘2019-12-16 20:50:10.66’ 日期类型，用两个字节存储，表示从 1970-01-01 (无符号) 到当前的日期值。 8. 数组 Array(T)：由 T 类型元素组成的数组。 T 可以是任意类型，包含数组类型。 但不推荐使用多维数组，ClickHouse 对多维数组的支持有限。例如，不能在 MergeTree 表中存储多维数组。 创建数组方式 1，使用 array 函数array(T) SELECT array(1, 2) AS x, toTypeName(x) ; 创建数组方式 2：使用方括号[] SELECT [1, 2] AS x, toTypeName(x); 9. 其他 还有很多数据结构，可以参考官方文档：https://clickhouse.com/docs/zh/sql-reference/data-types/ ","link":"https://tinaxiawuhao.github.io/post/1h169zkFb/"},{"title":"Docker安装clickhouse","content":"ClickHouse 很多大厂都在用，本篇主要使用Docker进行安装 安装配置 创建目录并更改权限 mkdir -p /app/cloud/clickhouse/data mkdir -p /app/cloud/clickhouse/conf mkdir -p /app/cloud/clickhouse/log chmod -R 777 /app/cloud/clickhouse/data chmod -R 777 /app/cloud/clickhouse/conf chmod -R 777 /app/cloud/clickhouse/log 拉取镜像 docker pull yandex/clickhouse-server:20.3.5.21 docker pull yandex/clickhouse-client:20.3.5.21 查看 https://hub.docker.com/r/yandex/clickhouse-server/dockerfile 文件，EXPOSE 9000 8123 9009 了三个端口 创建临时容器 docker run --rm -d --name=clickhouse-server --ulimit nofile=262144:262144 -p 8123:8123 -p 9009:9009 -p 9000:9000 yandex/clickhouse-server:20.3.5.21 复制临时容器内配置文件到宿主机 docker cp clickhouse-server:/etc/clickhouse-server/config.xml D:/clickhouse/conf/config.xml docker cp clickhouse-server:/etc/clickhouse-server/users.xml D:/clickhouse/conf/users.xml 停掉临时容器 docker stop clickhouse-server 创建default账号密码 PASSWORD=$(base64 &lt; /dev/urandom | head -c8); echo &quot;$PASSWORD&quot;; echo -n &quot;$PASSWORD&quot; | sha256sum | tr -d '-' SEGByR98 211371f5bc54970907173acf6facb35f0acbc17913e1b71b814117667c01d96d 会输出明码和SHA256密码 创建root账号密码 PASSWORD=$(base64 &lt; /dev/urandom | head -c8); echo &quot;$PASSWORD&quot;; echo -n &quot;$PASSWORD&quot; | sha256sum | tr -d '-' 092j3AnV 35542ded44184b1b4b6cd621e052662578025b58b4187176a3ad2b9548c8356e 会输出明码和SHA256密码 修改 D:/clickhouse/conf/users.xml 把default账号设为只读权限，并设置密码yandex--&gt;users--&gt;default--&gt;profile节点设为 readonly 注释掉 yandex--&gt;users--&gt;default--&gt;password 节点 新增 yandex--&gt;users--&gt;default--&gt;password_sha256_hex 节点，填入生成的密码 修改default账号 &lt;?xml version=&quot;1.0&quot;?&gt; &lt;yandex&gt; &lt;users&gt; &lt;default&gt; &lt;password_sha256_hex&gt;211371f5bc54970907173acf6facb35f0acbc17913e1b71b814117667c01d96d&lt;/password_sha256_hex&gt; &lt;networks incl=&quot;networks&quot; replace=&quot;replace&quot;&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;readonly&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/default&gt; &lt;/users&gt; &lt;/yandex&gt; 新增root账号 &lt;?xml version=&quot;1.0&quot;?&gt; &lt;yandex&gt; &lt;users&gt; &lt;root&gt; &lt;password_sha256_hex&gt;35542ded44184b1b4b6cd621e052662578025b58b4187176a3ad2b9548c8356e&lt;/password_sha256_hex&gt; &lt;networks incl=&quot;networks&quot; replace=&quot;replace&quot;&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;default&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/root&gt; &lt;/users&gt; &lt;/yandex&gt; 创建容器 docker run -d --name=clickhouse-server -p 8123:8123 -p 9009:9009 -p 9000:9000 --ulimit nofile=262144:262144 -v D:/clickhouse/data:/var/lib/clickhouse:rw -v D:/clickhouse/conf/config.xml:/etc/clickhouse-server/config.xml -v D:/clickhouse/conf/users.xml:/etc/clickhouse-server/users.xml -v D:/clickhouse/log:/var/log/clickhouse-server:rw yandex/clickhouse-server 操作 docker exec -it docker-clickhouse /bin/bash 进入容器 clickhouse-client 进入clickhouse命令行 ","link":"https://tinaxiawuhao.github.io/post/BBgY7nkHa/"},{"title":"ClickHouse-2初识","content":"一、简介 1.1 ClickHouse 是什么？ ClickHouse 是 Yandex（俄罗斯最大的搜索引擎）开源的一个用于实时数据分析的基于列存储的数据库，其处理数据的速度比传统方法快 100-1000 倍。ClickHouse 的性能超过了目前市场上可比的面向列的 DBMS，每秒钟每台服务器每秒处理数亿至十亿多行和数十千兆字节的数据。 1.2 ClickHouse的一些特性： 快速：ClickHouse 会充分利用所有可用的硬件，以尽可能快地处理每个查询。单个查询的峰值处理性能超过每秒 2 TB（解压缩后，仅使用的列）。在分布式设置中，读取是在健康副本之间自动平衡的，以避免增加延迟。 容错：ClickHouse 支持多主机异步复制，并且可以跨多个数据中心进行部署。所有节点都相等，这可以避免出现单点故障。单个节点或整个数据中心的停机时间不会影响系统的读写可用性。 可伸缩：ClickHouse 可以在垂直和水平方向上很好地缩放。ClickHouse 易于调整以在具有数百或数千个节点的群集上或在单个服务器上，甚至在小型虚拟机上执行。当前，每个单节点安装的数据量超过数万亿行或数百兆兆字节。 易用：ClickHouse 简单易用，开箱即用。它简化了所有数据处理：将所有结构化数据吸收到系统中，并且立即可用于构建报告。SQL 允许表达期望的结果，而无需涉及某些 DBMS 中可以找到的任何自定义非标准 API。 充分利用硬件：ClickHouse 与具有相同的可用 I/O 吞吐量和 CPU 容量的传统的面向行的系统相比，其处理典型的分析查询要快两到三个数量级。列式存储格式允许在 RAM 中容纳更多热数据，从而缩短了响应时间。 提高 CPU 效率：向量化查询执行涉及相关的 SIMD 处理器指令和运行时代码生成。处理列中的数据会提高 CPU 行缓存的命中率。 优化磁盘访问：ClickHouse 可以最大程度地减少范围查询的次数，从而提高了使用旋转磁盘驱动器的效率，因为它可以保持连续存储数据。 最小化数据传输：ClickHouse 使公司无需使用专门针对高性能计算的专用网络即可管理其数据。 何时使用 ClickHouse： 用于分析结构良好且不可变的事件或日志流，建议将每个此类流放入具有预连接维度的单个宽表中。 何时不使用 ClickHouse： 不适合事务性工作负载（OLTP）、高价值的键值请求、Blob 或文档存储。 1.3 为什么 ClickHouse 速度这么快？ 首先我们了解一下 OLAP 场景的特点： 读多于写。 大宽表，读大量行但是少量列，结果集较小。 数据批量写入，且数据不更新或少更新。 针对分析类查询，通常只需要读取表的一小部分列。在列式数据库中你可以只读取你需要的数据。例如，如果只需要读取 100 列中的 5 列，这将帮助你最少减少 20 倍的 I/O 消耗。 由于数据总是打包成批量读取的，所以压缩是非常容易的。同时数据按列分别存储这也更容易压缩。这进一步降低了 I/O 的体积。由于 I/O 的降低，这将帮助更多的数据被系统缓存。 例如，查询《统计每个广告平台的记录数量》需要读取《广告平台 ID》这一列，它在未压缩的情况下需要 1 个字节进行存储。如果大部分流量不是来自广告平台，那么这一列至少可以以十倍的压缩率被压缩。当采用快速压缩算法，它的解压速度最少在十亿字节（未压缩数据）每秒。换句话说，这个查询可以在单个服务器上以每秒大约几十亿行的速度进行处理。这实际上是当前实现的速度。 ClickHouse 从 OLAP 场景需求出发，定制开发了一套全新的高效列式存储引擎 column-oriented 图片来源见水印相比于行式存储，列式存储在分析场景下有着许多优良的特性。 如前所述，分析场景中往往需要读大量行但是少数几个列。在行存模式下，数据按行连续存储，所有列的数据都存储在一个 block 中，不参与计算的列在 IO 时也要全部读出，读取操作被严重放大。而列存模式下，只需要读取参与计算的列即可，极大的减低了 IO cost，加速了查询。 同一列中的数据属于同一类型，压缩效果显著。列存往往有着高达十倍甚至更高的压缩比，节省了大量的存储空间，降低了存储成本。 更高的压缩比意味着更小的 data size，从磁盘中读取相应数据耗时更短。 自由的压缩算法选择。不同列的数据具有不同的数据类型，适用的压缩算法也就不尽相同。可以针对不同列类型，选择最合适的压缩算法。 高压缩比，意味着同等大小的内存能够存放更多数据，系统 cache 效果更好。 ","link":"https://tinaxiawuhao.github.io/post/GwxNFXsxh/"},{"title":"Clickhouse-1安装","content":"1.ClickHouse的安装 1.1 准备工作 下载RPM包: https://repo.yandex.ru/clickhouse/rpm/stable/x86_64/ 下载完毕如下: ​ 1.1.1 确定防火墙处于关闭状态 相关命令: sudo systemctl status firewalld sudo systemctl start firewalld sudo systemctl stop firewalld sudo systemctl restart firewalld 1.1.2 CentOS取消打开文件数限制 Ø 在 /etc/security/limits.conf文件的末尾加入以下内容 vim /etc/security/limits.conf * soft nofile 65536 * hard nofile 65536 * soft nproc 131072 * hard nproc 131072 Ø 在/etc/security/limits.d/20-nproc.conf文件的末尾加入以下内容 vim /etc/security/limits.d/20-nproc.conf * soft nofile 65536 * hard nofile 65536 * soft nproc 131072 * hard nproc 131072 Ø 执行同步操作(同步到集群其他机器) xsync /etc/security/limits.conf xsync /etc/security/limits.d/20-nproc.conf 1.1.3 安装依赖 sudo yum install -y libtool sudo yum install -y *unixODBC* 同样在集群其他机器上执行以上操作 1.1.4 CentOS取消SELINUX SELINUX(美国开源的linux安全增强功能) Ø 修改/etc/selinux/config中的SELINUX=disabled sudo vim /etc/selinux/config SELINUX=disabled Ø 执行同步操作 sudo /home/atguigu/bin/xsync /etc/selinux/config Ø 重启三台服务器 1.1.5 将安装文件同步到其他两个服务器 1.1.6 分别在三台机子上安装这4个rpm文件 sudo rpm -ivh *.rpm(提前把四个*.rpm放在一个目录下) sudo rpm -qa|grep clickhouse查看安装情况 1.1.7 修改配置文件 vim /etc/clickhouse-server/config.xml 把 &lt;listen_host&gt;::&lt;/listen_host&gt; 的注释打开，这样的话才能让ClickHouse被除本机以外的服务器访问 #分发配置文件 xsync /etc/clickhouse-server/config.xml 1.1.8 启动Server sudo systemctl start clickhouse-server 1.1.9 三台机器上关闭开机自启 systemctl disable clickhouse-server 1.1.10 使用client连接server clickhouse-client -m Clickhouse常用端口号: 第2章 副本 副本的目的主要是保障数据的高可用性，即使一台ClickHouse节点宕机，那么也可以从其他服务器获得相同的数据。 2.1 副本写入流程 2.2 配置步骤 Ø 启动zookeeper集群 Ø 注意:服务器的hostname需要改成对应的ck101、ck102、ck103 Ø 在/etc/clickhouse-server/config.d目录下创建一个名为metrika.xml的配置文件,内容如下： &lt;?xml version=&quot;1.0&quot;?&gt; &lt;yandex&gt; &lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt;ck101&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;2&quot;&gt; &lt;host&gt;ck102&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;3&quot;&gt; &lt;host&gt;ck103&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper-servers&gt; &lt;/yandex&gt; Ø 同步到另外两个机器上 xsync /etc/clickhouse-server/config.d/metrika.xml Ø 在 /etc/clickhouse-server/config.xml中增加 &lt;zookeeper incl=&quot;zookeeper-servers&quot; optional=&quot;true&quot; /&gt; &lt;include_from&gt;/etc/clickhouse-server/config.d/metrika.xml&lt;/include_from&gt; Ø 同步到另外两个机器上 xsync /etc/clickhouse-server/config.xml Ø 分别在另外两个机器上启动ClickHouse服务 注意：因为修改了配置文件，如果以前启动了服务需要重启 systemctl start clickhouse-server Ø 在另外两个机器上分别建表 副本只能同步数据，不能同步表结构，所以我们需要在每台机器上自己手动建表 Ck101 create table t_order_rep ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime ) engine =ReplicatedMergeTree('/clickhouse/table/01/t_order_rep','rep_102') partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id); Ck102 create table t_order_rep ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime ) engine =ReplicatedMergeTree('/clickhouse/table/01/t_order_rep','rep_103') partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id); 参数解释 ReplicatedMergeTree 中， 第一个参数是分片的zk_path一般按照： /clickhouse/table/{shard}/{table_name} 的格式写，如果只有一个分片就写01即可。 第二个参数是副本名称，相同的分片副本名称不能相同。 Ø 在ck101上执行insert语句 insert into t_order_rep values (101,'sku_001',1000.00,'2020-06-01 12:00:00'), (102,'sku_002',2000.00,'2020-06-01 12:00:00'), (103,'sku_004',2500.00,'2020-06-01 12:00:00'), (104,'sku_002',2000.00,'2020-06-01 12:00:00'), (105,'sku_003',600.00,'2020-06-02 12:00:00'); Ø 在ck102上执行select，可以查询出结果，说明副本配置正确 第3章 分片集群 副本虽然能够提高数据的可用性，降低丢失风险，但是每台服务器实际上必须容纳全量数据，对数据的横向扩容没有解决。 要解决数据水平切分的问题，需要引入分片的概念。通过分片把一份完整的数据进行切分，不同的分片分布到不同的节点上，再通过Distributed表引擎把数据拼接起来一同使用。 Distributed表引擎本身不存储数据，有点类似于MyCat之于MySql，成为一种中间件，通过分布式逻辑表来写入、分发、路由来操作多台节点不同分片的分布式数据。 注意：ClickHouse的集群是表级别的，实际企业中，大部分做了高可用，但是没有用分片，避免降低查询性能以及操作集群的复杂性。 3.1 集群写入流程（3分片2副本共6个节点） 3.2 集群读取流程（3分片2副本共6个节点） 3.3 配置三节点版本集群及副本 3.3.1 集群及副本规划（2个分片，只有第一个分片有副本） ck101 ck102 ck103 01 rep_1_1 01 rep_1_2 02 rep_2_1 3.3.2 配置步骤 (1) 在ck101的/etc/clickhouse-server/config.d目录下创建metrika-shard.xml文件 &lt;?xml version=&quot;1.0&quot;?&gt; &lt;yandex&gt; &lt;remote_servers&gt; &lt;my_cluster&gt; &lt;!-- 集群名称--&gt; &lt;shard&gt; &lt;!--集群的第一个分片--&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!--该分片的第一个副本--&gt; &lt;host&gt;ck101&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;!--该分片的第二个副本--&gt; &lt;host&gt;ck102&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;!--集群的第二个分片--&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!--该分片的第一个副本--&gt; &lt;host&gt;ck103&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/my_cluster&gt; &lt;/remote_servers&gt; &lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt; ck101&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;2&quot;&gt; &lt;host&gt; ck102&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;3&quot;&gt; &lt;host&gt; ck103&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper-servers&gt; &lt;macros&gt; &lt;shard&gt;01&lt;/shard&gt; &lt;!--不同机器放的分片数不一样--&gt; &lt;replica&gt;rep_1_1&lt;/replica&gt; &lt;!--不同机器放的副本数不一样--&gt; &lt;/macros&gt; &lt;/yandex&gt; (2) 将ck101的metrika-shard.xml同步到102和103 xsync /etc/clickhouse-server/config.d/metrika-shard.xml (3) 修改102和103中metrika-shard.xml宏的配置 Ø 102 vim /etc/clickhouse-server/config.d/metrika-shard.xml Ø 103 vim /etc/clickhouse-server/config.d/metrika-shard.xml (4) 在hadoop102上修改/etc/clickhouse-server/config.xml (5) 同步/etc/clickhouse-server/config.xml到103和104 sudo /home/atguigu/bin/xsync /etc/clickhouse-server/config.xml (6) 重启三台服务器上的ClickHouse服务 systemctl stop clickhouse-server systemctl start clickhouse-server systemctl status clickhouse-server (7) 在ck101上执行建表语句 Ø 会自动同步到ck102和ck103上 Ø 集群名字要和配置文件中的一致 Ø 分片和副本名称从配置文件的宏定义中获取 create table st_order_mt on cluster my_cluster ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime ) engine =ReplicatedMergeTree('/clickhouse/tables/{shard}/st_order_mt','{replica}') partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id); 可以到ck102和ck103上查看表是否创建成功 (8) 在ck101上创建Distribute 分布式表 create table st_order_mt_all on cluster my_cluster ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime )engine = Distributed(my_cluster,default, st_order_mt,hiveHash(sku_id)); 参数含义 ​ Distributed(集群名称，库名，本地表名，分片键) 分片键必须是整型数字，所以用hiveHash函数转换，也可以rand() (9) 在ck101上插入测试数据 insert into st_order_mt_all values (201,'sku_001',1000.00,'2020-06-01 12:00:00') , (202,'sku_002',2000.00,'2020-06-01 12:00:00'), (203,'sku_004',2500.00,'2020-06-01 12:00:00'), (204,'sku_002',2000.00,'2020-06-01 12:00:00'), (205,'sku_003',600.00,'2020-06-02 12:00:00'); (10) 通过查询分布式表和本地表观察输出结果 Ø 分布式表 SELECT * FROM st_order_mt_all; Ø 本地表 select * from st_order_mt; Ø 观察数据的分布 st_order_mt_all Ck101: st_order_mt Ck102: st_order_mt Ck103: st_order_mt 第4章 版本信息 Clickhouse: clickhouse-21.9.4.35 Zookeeper: zookeeper-3.4.6 ","link":"https://tinaxiawuhao.github.io/post/r2VmiGgor/"},{"title":"Kafka 架构深入","content":"3.1 Kafka 工作流程及文件存储机制 Kafka 中消息是以 topic 进行分类的， 生产者生产消息，消费者消费消息，都是面向 topic 的。 topic 是逻辑上的概念，而 partition 是物理上的概念，每个 partition 对应于一个 log 文件，该 log 文件中存储的就是 producer 生产的数据。 Producer 生产的数据会被不断追加到该 log 文件末端，且每条数据都有自己的 offset。 消费者组中的每个消费者， 都会实时记录自己消费到了哪个 offset，以便出错恢复时，从上次的位置继续消费 。 由于生产者生产的消息会不断追加到 log 文件末尾， 为防止 log 文件过大导致数据定位效率低下， Kafka 采取了分片和索引机制，将每个 partition 分为多个 segment。 每个 segment 对应两个文件——“.index”文件和“.log”文件。 这些文件位于一个文件夹下， 该文件夹的命名规则为： topic名称+分区序号。例如， first 这个 topic 有三个分区，则其对应的文件夹为 first-0,first-1,first-2。 00000000000000000000.index 00000000000000000000.log 00000000000000170410.index 00000000000000170410.log 00000000000000239430.index 00000000000000239430.log index 和 log 文件以当前 segment 的第一条消息的 offset 命名。下图为 index 文件和 log文件的结构示意图。 “.index”文件存储大量的索引信息，“.log”文件存储大量的数据，索引文件中的元数据指向对应数据文件中 message 的物理偏移地址。 3.2 Kafka 生产者 3.2.1 分区策略 分区的原因 （1） 方便在集群中扩展，每个 Partition 可以通过调整以适应它所在的机器，而一个 topic 又可以有多个 Partition 组成，因此整个集群就可以适应任意大小的数据了； （2） 可以提高并发，因为可以以 Partition 为单位读写了。 分区的原则 我们需要将 producer 发送的数据封装成一个 ProducerRecord 对象。 （1） 指明 partition 的情况下，直接将指明的值直接作为 partiton 值； （2）没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值； （3） 既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后 面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。 3.2.2 数据可靠性保证 为保证 producer 发送的数据，能可靠的发送到指定的 topic， topic 的每个 partition 收到 producer 发送的数据后， 都需要向 producer 发送 ack（acknowledgement 确认收到） ，如果 producer 收到 ack， 就会进行下一轮的发送，否则重新发送数据。 1） 副本数据同步策略 方案 优点 缺点 半数以上完成同步， 就发 送 ack 延迟低 选举新的 leader 时， 容忍 n 台 节点的故障，需要 2n+1 个副 本 全部完成同步，才发送 ack 选举新的 leader 时， 容忍 n 台 节点的故障，需要 n+1 个副 本 延迟高 Kafka 选择了第二种方案，原因如下： 同样为了容忍 n 台节点的故障，第一种方案需要 2n+1 个副本，而第二种方案只需要 n+1 个副本，而 Kafka 的每个分区都有大量的数据， 第一种方案会造成大量数据的冗余。 虽然第二种方案的网络延迟会比较高，但网络延迟对 Kafka 的影响较小。 2） ISR 采用第二种方案之后，设想以下情景： leader 收到数据，所有 follower 都开始同步数据， 但有一个 follower，因为某种故障，迟迟不能与 leader 进行同步，那 leader 就要一直等下去， 直到它完成同步，才能发送 ack。这个问题怎么解决呢？ Leader 维护了一个动态的 in-sync replica set (ISR)，意为和 leader 保持同步的 follower 集 合。当 ISR 中的 follower 完成数据的同步之后， leader 就会给 follower 发送 ack。如果 follower 长 时 间 未 向 leader 同 步 数 据 ， 则 该 follower 将 被 踢 出 ISR ， 该 时 间 阈 值 由replica.lag.time.max.ms 参数设定。 Leader 发生故障之后，就会从 ISR 中选举新的 leader。 3） ack 应答机制 对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失， 所以没必要等 ISR 中的 follower 全部接收成功。 所以 Kafka 为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡， 选择以下的配置。 acks 参数配置： acks： 0： producer 不等待 broker 的 ack，这一操作提供了一个最低的延迟， broker 一接收到还 没有写入磁盘就已经返回，当 broker 故障时有可能丢失数据； 1： producer 等待 broker 的 ack， partition 的 leader 落盘成功后返回 ack，如果在 follower 同步成功之前 leader 故障，那么将会丢失数据； -1（all） ： producer 等待 broker 的 ack， partition 的 leader 和 follower 全部落盘成功后才 返回 ack。但是如果在 follower 同步完成后， broker 发送 ack 之前， leader 发生故障，那么会 造成数据重复。 4） 故障处理细节 LEO：指的是每个副本最大的 offset； HW：指的是消费者能见到的最大的 offset， ISR 队列中最小的 LEO。 （1） follower 故障 follower 发生故障后会被临时踢出 ISR，待该 follower 恢复后， follower 会读取本地磁盘记录的上次的 HW，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步。等该 follower 的 LEO 大于等于该 Partition 的 HW，即 follower 追上 leader 之后，就可以重新加入 ISR 了。 （2） leader 故障 leader 发生故障之后，会从 ISR 中选出一个新的 leader，之后，为保证多个副本之间的 数据一致性， 其余的 follower 会先将各自的 log 文件高于 HW 的部分截掉，然后从新的 leader同步数据。 注意： 这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。 3.2.3 Exactly Once 语义 将服务器的 ACK 级别设置为-1，可以保证 Producer 到 Server 之间不会丢失数据，即 At Least Once 语义。相对的，将服务器 ACK 级别设置为 0，可以保证生产者每条消息只会被 发送一次，即 At Most Once 语义。 At Least Once 可以保证数据不丢失，但是不能保证数据不重复；相对的， At Most Once 可以保证数据不重复，但是不能保证数据不丢失。 但是，对于一些非常重要的信息，比如说 交易数据，下游数据消费者要求数据既不重复也不丢失，即 Exactly Once 语义。 在 0.11 版 本以前的 Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局 去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。 0.11 版本的 Kafka，引入了一项重大特性：幂等性。所谓的幂等性就是指 Producer 不论向 Server 发送多少次重复数据， Server 端都只会持久化一条。幂等性结合 At Least Once 语义，就构成了 Kafka 的 Exactly Once 语义。即： At Least Once + 幂等性 = Exactly Once 要启用幂等性，只需要将 Producer 的参数中 enable.idompotence 设置为 true 即可。 Kafka 的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的 Producer 在 初始化的时候会被分配一个 PID，发往同一 Partition 的消息会附带 Sequence Number。而 Broker 端会对&lt;PID, Partition, SeqNumber&gt;做缓存，当具有相同主键的消息提交时， Broker 只 会持久化一条。 但是 PID 重启就会变化，同时不同的 Partition 也具有不同主键，所以幂等性无法保证跨 分区跨会话的 Exactly Once。 3.3 Kafka 消费者 3.3.1 消费方式 consumer 采用 pull（拉） 模式从 broker 中读取数据。 push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker 决定的。 它的目标是尽可能以最快速度传递消息，但是这样很容易造成 consumer 来不及处理消息， 典型的表现就是拒绝服务以及网络拥塞。而 pull 模式则可以根据 consumer 的消费能力以适 当的速率消费消息。 pull 模式不足之处是，如果 kafka 没有数据，消费者可能会陷入循环中， 一直返回空数 据。 针对这一点， Kafka 的消费者在消费数据时会传入一个时长参数 timeout，如果当前没有 数据可供消费， consumer 会等待一段时间之后再返回，这段时长即为 timeout。 3.3.2 分区分配策略 一个 consumer group 中有多个 consumer，一个 topic 有多个 partition，所以必然会涉及 到 partition 的分配问题，即确定那个 partition 由哪个 consumer 来消费。 Kafka 有两种分配策略，一是 RoundRobin，一是 Range。 当消费者个数改变时，会用到分区分配策略 1） RoundRobin RoundRobinAssignor策略的原理是将消费组内所有消费者以及消费者所订阅的所有topic的partition按照字典序排序，然后通过轮询方式逐个将分区以此分配给每个消费者。RoundRobinAssignor策略对应的partition.assignment.strategy参数值为：org.apache.kafka.clients.consumer.RoundRobinAssignor。 使用RoundRobin策略有两个前提条件必须满足： 同一个消费者组里面的所有消费者的num.streams（消费者消费线程数）必须相等； 每个消费者订阅的主题必须相同。 所以这里假设前面提到的2个消费者的num.streams = 2。RoundRobin策略的工作原理：将所有主题的分区组成 TopicAndPartition 列表，然后对 TopicAndPartition 列表按照 hashCode 进行排序,在我们的例子里面，按照 hashCode 排序完的topic-partitions组依次为T1-5, T1-3, T1-0, T1-8, T1-2, T1-1, T1-4, T1-7, T1-6, T1-9，我们的消费者线程排序为C1-0, C1-1, C2-0, C2-1，最后分区分配的结果为： C1-0 将消费 T1-5, T1-2, T1-6 分区； C1-1 将消费 T1-3, T1-1, T1-9 分区； C2-0 将消费 T1-0, T1-4 分区； C2-1 将消费 T1-8, T1-7 分区； 2） Range（默认策略） Range是对每个Topic而言的（即一个Topic一个Topic分），首先对同一个Topic里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。然后用Partitions分区的个数除以消费者线程的总数来决定每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区。 假设n=分区数/消费者数量，m=分区数%消费者数量，那么前m个消费者每个分配n+1个分区，后面的（消费者数量-m）个消费者每个分配n个分区。 假如有10个分区，3个消费者线程，把分区按照序号排列0，1，2，3，4，5，6，7，8，9；消费者线程为C1-0，C2-0，C2-1，那么用partition数除以消费者线程的总数来决定每个消费者线程消费几个partition，如果除不尽，前面几个消费者将会多消费一个分区。在我们的例子里面，我们有10个分区，3个消费者线程，10/3 = 3，而且除除不尽，那么消费者线程C1-0将会多消费一个分区，所以最后分区分配的结果看起来是这样的： C1-0：0，1，2，3 C2-0：4，5，6 C2-1：7，8，9 3.3.3 offset 的维护 由于 consumer 在消费过程中可能会出现断电宕机等故障， consumer 恢复后，需要从故 障前的位置的继续消费，所以 consumer 需要实时记录自己消费到了哪个 offset，以便故障恢 复后继续消费。 Kafka 0.9 版本之前， consumer 默认将 offset 保存在 Zookeeper 中，从 0.9 版本开始， consumer 默认将 offset 保存在 Kafka 一个内置的 topic 中，该 topic 为__consumer_offsets。 1）修改配置文件 consumer.properties exclude.internal.topics=false 2）读取 offset 0.11.0.0 之前版本: bin/kafka-console-consumer.sh --topic __consumer_offsets --zookeeper hadoop102:2181 --formatter &quot;kafka.coordinator.GroupMetadataManager\\$OffsetsMessageFormatter&quot; --consumer.config config/consumer.properties --from-beginning 0.11.0.0 之后版本(含): bin/kafka-console-consumer.sh --topic __consumer_offsets -- zookeeper hadoop102:2181 --formatter &quot;kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageForm atter&quot; --consumer.config config/consumer.properties --frombeginning 3.3.4 消费者组案例 1） 需求：测试同一个消费者组中的消费者， 同一时刻只能有一个消费者消费。 2） 案例实操 （1）在 hadoop102、 hadoop103 上修改/opt/module/kafka/config/consumer.properties 配置 文件中的 group.id 属性为任意组名。 [atguigu@hadoop103 config]$ vi consumer.properties group.id=atguigu （2）在 hadoop102、 hadoop103 上分别启动消费者 [atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh \\ --zookeeper hadoop102:2181 --topic first --consumer.config config/consumer.properties [atguigu@hadoop103 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first --consumer.config config/consumer.properties （3）在 hadoop104 上启动生产者 [atguigu@hadoop104 kafka]$ bin/kafka-console-producer.sh \\ --broker-list hadoop102:9092 --topic first &gt;hello world （4）查看 hadoop102 和 hadoop103 的接收者。 同一时刻消费组内只有一个消费者接收到消息。 3.4 Kafka 高效读写数据 1）顺序写磁盘 Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直追加到文件末端， 为顺序写。 官网有数据表明，同样的磁盘，顺序写能到 600M/s，而随机写只有 100K/s。这 与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。 2）零复制技术 3）分布式 3.5 Zookeeper 在 Kafka 中的作用 Kafka 集群中有一个 broker 会被选举为 Controller，负责管理集群 broker 的上下线，所有 topic 的分区副本分配和 leader 选举等工作。Controller 的管理工作都是依赖于 Zookeeper 的。 以下为 partition 的 leader 选举过程： 3.6 Kafka 事务 Kafka 从 0.11 版本开始引入了事务支持。事务可以保证 Kafka 在 Exactly Once 语义的基 础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。 3.6.1 Producer 事务 为了实现跨分区跨会话的事务，需要引入一个全局唯一的 Transaction ID，并将 Producer 获得的PID 和Transaction ID 绑定。这样当Producer 重启后就可以通过正在进行的 Transaction ID 获得原来的 PID。 为了管理 Transaction， Kafka 引入了一个新的组件 Transaction Coordinator。 Producer 就是通过和 Transaction Coordinator 交互获得 Transaction ID 对应的任务状态。 Transaction Coordinator 还负责将事务所有写入 Kafka 的一个内部 Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。 3.6.2 Consumer 事务 上述事务机制主要是从 Producer 方面考虑，对于 Consumer 而言，事务的保证就会相对 较弱，尤其时无法保证 Commit 的信息被精确消费。这是由于 Consumer 可以通过 offset 访 问任意信息，而且不同的 Segment File 生命周期不同，同一事务的消息可能会出现重启后被 删除的情况。 ","link":"https://tinaxiawuhao.github.io/post/KM-0yw6ab/"},{"title":"kafka常用命令","content":"管理 ## 创建topic（4个分区，2个副本） bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 2 --partitions 4 --topic test ### kafka版本 &gt;= 2.2 bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test ## 分区扩容 ### kafka版本 &lt; 2.2 bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic1 --partitions 2 ### kafka版本 &gt;= 2.2 bin/kafka-topics.sh --bootstrap-server broker_host:port --alter --topic topic1 --partitions 2 ## 删除topic bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic test 查询 ## 查询集群描述 bin/kafka-topics.sh --describe --zookeeper 127.0.0.1:2181 ## 查询集群描述（新） bin/kafka-topics.sh --bootstrap-server localhost:9092 --topic foo --describe ## topic列表查询 bin/kafka-topics.sh --zookeeper 127.0.0.1:2181 --list ## topic列表查询（支持0.9版本+） bin/kafka-topics.sh --list --bootstrap-server localhost:9092 ## 消费者列表查询（存储在zk中的） bin/kafka-consumer-groups.sh --zookeeper localhost:2181 --list ## 消费者列表查询（支持0.9版本+） bin/kafka-consumer-groups.sh --new-consumer --bootstrap-server localhost:9092 --list ## 消费者列表查询（支持0.10版本+） bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list ## 显示某个消费组的消费详情（仅支持offset存储在zookeeper上的） bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper localhost:2181 --group test ## 显示某个消费组的消费详情（0.9版本 - 0.10.1.0 之前） bin/kafka-consumer-groups.sh --new-consumer --bootstrap-server localhost:9092 --describe --group test-consumer-group ## 显示某个消费组的消费详情（0.10.1.0版本+） bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group 发送和消费 ## 生产者 bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test ## 消费者（已失效） bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test ## 生产者（支持0.9版本+） bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test --producer.config config/producer.properties ## 消费者（支持0.9版本+，已失效） bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --new-consumer --from-beginning --consumer.config config/consumer.properties ## 消费者（最新） bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning --consumer.config config/consumer.properties ## kafka-verifiable-consumer.sh（消费者事件，例如：offset提交等） bin/kafka-verifiable-consumer.sh --broker-list localhost:9092 --topic test --group-id groupName ## 高级点的用法 bin/kafka-simple-consumer-shell.sh --brist localhost:9092 --topic test --partition 0 --offset 1234 --max-messages 10 切换leader ## kafka版本 &lt;= 2.4 bin/kafka-preferred-replica-election.sh --zookeeper zk_host:port/chroot ## kafka新版本 bin/kafka-preferred-replica-election.sh --bootstrap-server broker_host:port kafka自带压测命令 bin/kafka-producer-perf-test.sh --topic test --num-records 100 --record-size 1 --throughput 100 --producer-props bootstrap.servers=localhost:9092 kafka持续发送消息 持续发送消息到指定的topic中，且每条发送的消息都会有响应信息： kafka-verifiable-producer.sh --broker-list $(hostname -i):9092 --topic test --max-messages 100000 zookeeper-shell.sh 如果kafka集群的zk配置了chroot路径，那么需要加上/path。 bin/zookeeper-shell.sh localhost:2181[/path] ls /brokers/ids get /brokers/ids/0 迁移分区 创建规则json cat &gt; increase-replication-factor.json &lt;&lt;EOF {&quot;version&quot;:1, &quot;partitions&quot;:[ {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:4,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:5,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:6,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:7,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:8,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:9,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:10,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:11,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:12,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:13,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:14,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:15,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:16,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:17,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:18,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:19,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:20,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:21,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:22,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:23,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:24,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:25,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:26,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:27,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:28,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:29,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:30,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:31,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:32,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:33,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:34,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:35,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:36,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:37,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:38,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:39,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:40,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:41,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:42,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:43,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:44,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:45,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:46,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:47,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:48,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:49,&quot;replicas&quot;:[0,1]}] } EOF 执行 bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file increase-replication-factor.json --execute 验证 bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file increase-replication-factor.json --verify 删除消费者组 查询消费者组列表： kafka-consumer-groups.sh --bootstrap-server 172.31.1.245:9092 --list 查询消费者组明细： kafka-consumer-groups.sh --bootstrap-server {Kafka instance connection address} --describe --group {consumer group name} 删除消费者组： kafka-consumer-groups.sh --bootstrap-server {Kafka instance connection address} --delete --group {consumer group name} ","link":"https://tinaxiawuhao.github.io/post/cvzf4T4Wn/"},{"title":"zookeeper选举过程","content":"初始化选举 运行期间选举 ","link":"https://tinaxiawuhao.github.io/post/Rz0qPOGXr/"},{"title":"ReentrantLock源码详解","content":"ReentrantLock重入锁，是实现Lock接口的一个类，也是在实际编程中使用频率很高的一个锁，支持重入性，表示能够对共享资源能够重复加锁，即当前线程获取该锁再次获取不会被阻塞。 ReentrantLock还支持公平锁和非公平锁两种方式。那么，要想完完全全的弄懂ReentrantLock的话，主要也就是ReentrantLock同步语义的学习： 重入性的实现原理； 公平锁和非公平锁 ReentrantLock源码解析 加锁 //使用案例 class Bank{ /** * volatile实现 */ private int count=0; /** * 使用可重入锁 */ private Lock lock=new ReentrantLock(); public void getCount(){ System.out.println(&quot;账户余额为：&quot;+count); } /** * 同步方法实现存钱 * @param money */ public void save(int money){ lock.lock(); try { count+=money; System.out.println(System.currentTimeMillis()+&quot;存进：&quot;+money); } catch (Exception e) { // TODO Auto-generated catch block e.printStackTrace(); }finally { lock.unlock();//释放锁 } } /** * 同步代码块实现取钱 * @param money */ public void remove(int money){ if (count-money&lt;0) { System.err.println(&quot;余额不足。&quot;); return; } lock.lock(); try { count-=money; System.err.println(System.currentTimeMillis()+&quot;取出：&quot;+money); } catch (Exception e) { // TODO Auto-generated catch block e.printStackTrace(); }finally { lock.unlock(); } } } /** * Creates an instance of {@code ReentrantLock}. * This is equivalent to using {@code ReentrantLock(false)}. */ public ReentrantLock() { sync = new NonfairSync(); } /** * Creates an instance of {@code ReentrantLock} with the * given fairness policy. * * @param fair {@code true} if this lock should use a fair ordering policy */ public ReentrantLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); } 初始化默认使用非公平锁 公平锁和非公平锁继承AbstractQueuedSynchronizer接口（抽象的队列式的同步器，AQS定义了一套多线程访问共享资源的同步器框架） AQS框架 NonfairSync的类继承关系 FairSync的类继承关系 下面以非公平锁为例 /** * Performs lock. Try immediate barge, backing up to normal * acquire on failure. */ //加锁流程真正意义上的入口 final void lock() { //以cas方式尝试将AQS中的state从0更新为1 if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread());//获取锁成功则将当前线程标记为持有锁的线程,然后直接返回 else acquire(1);//获取锁失败则执行该方法 } 加锁时调用lock方法，首先判断AQS中的sate参数是否被标记,尝试以cas方式尝试将AQS中的state从0更新为1，成功将当前线程赋予AQS 失败则调用AQS类的acquire(1)方法 //非公平模式下尝试获取锁的方法 protected final boolean tryAcquire(int acquires) { return nonfairTryAcquire(acquires); } /** * Performs non-fair tryLock. tryAcquire is implemented in * subclasses, but both need nonfair try for trylock method. */ final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread();//获取当前线程实例 int c = getState();//获取state变量的值,即当前锁被重入的次数 if (c == 0) { //state为0,说明当前锁未被任何线程持有 if (compareAndSetState(0, acquires)) { //以cas方式获取锁 setExclusiveOwnerThread(current); //将当前线程标记为持有锁的线程 return true;//获取锁成功,非重入 } } else if (current == getExclusiveOwnerThread()) { //当前线程就是持有锁的线程,说明该锁被重入了 int nextc = c + acquires;//计算state变量要更新的值 if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc);//非同步方式更新state值 return true; //获取锁成功,重入 } return false; //走到这里说明尝试获取锁失败 } 这是非公平模式下获取锁的通用方法。它囊括了当前线程在尝试获取锁时的所有可能情况： 1.当前锁未被任何线程持有(state=0),则以cas方式获取锁,若获取成功则设置exclusiveOwnerThread为当前线程,然后返回成功的结果；若cas失败,说明在得到state=0和cas获取锁之间有其他线程已经获取了锁,返回失败结果。 2.若锁已经被当前线程获取(state&gt;0,exclusiveOwnerThread为当前线程),则将锁的重入次数加1(state+1),然后返回成功结果。因为该线程之前已经获得了锁,所以这个累加操作不用同步。 3.若当前锁已经被其他线程持有(state&gt;0,exclusiveOwnerThread不为当前线程),则直接返回失败结果 因为我们用state来统计锁被线程重入的次数,所以当前线程尝试获取锁的操作是否成功可以简化为:state值是否成功累加1,是则尝试获取锁成功,否则尝试获取锁失败。 其实这里还可以思考一个问题:nonfairTryAcquire已经实现了一个囊括所有可能情况的尝试获取锁的方式,为何在刚进入lock方法时还要通过compareAndSetState(0, 1)去获取锁,毕竟后者只有在锁未被任何线程持有时才能执行成功,我们完全可以把compareAndSetState(0, 1)去掉,对最后的结果不会有任何影响。这种在进行通用逻辑处理之前针对某些特殊情况提前进行处理的方式在后面还会看到,一个直观的想法就是它能提升性能，而代价是牺牲一定的代码简洁性。 退回到上层的acquire方法, public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; //当前线程尝试获取锁,若获取成功返回true,否则false acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) //只有当前线程获取锁失败才会执行者这部分代码 selfInterrupt(); } tryAcquire(arg)返回成功,则说明当前线程成功获取了锁(第一次获取或者重入),由取反和&amp;&amp;可知,整个流程到这结束，只有当前线程获取锁失败才会执行后面的判断。先来看addWaiter(Node.EXCLUSIVE)部分,这部分代码描述了当线程获取锁失败时如何安全的加入同步等待队列。 这部分逻辑在addWaiter()方法中 /** * Creates and enqueues node for current thread and given mode. * * @param mode Node.EXCLUSIVE for exclusive, Node.SHARED for shared * @return the new node */ private Node addWaiter(Node mode) { Node node = new Node(Thread.currentThread(), mode);//首先创建一个新节点,并将当前线程实例封装在内部,mode这里为null // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) { node.prev = pred; if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } enq(node);//入队的逻辑这里都有 return node; } 首先创建了一个新节点,并将当前线程实例封装在其内部,之后我们直接看enq(node)方法就可以了,中间这部分逻辑在enq(node)中都有,之所以加上这部分“重复代码”和尝试获取锁时的“重复代码”一样,对某些特殊情况 进行提前处理,牺牲一定的代码可读性换取性能提升。 /** * Inserts node into queue, initializing if necessary. See picture above. * @param node the node to insert * @return node's predecessor */ private Node enq(final Node node) { for (;;) { Node t = tail;//t指向当前队列的最后一个节点,队列为空则为null if (t == null) { // Must initialize //队列为空 if (compareAndSetHead(new Node())) //构造新结点,CAS方式设置为队列首元素,当head==null时更新成功 tail = head;//尾指针指向首结点 } else { //队列不为空 node.prev = t; if (compareAndSetTail(t, node)) { //CAS将尾指针指向当前结点,当t(原来的尾指针)==tail(当前真实的尾指针)时执行成功 t.next = node; //原尾结点的next指针指向当前结点 return t; } } } } 这里有两个CAS操作: compareAndSetHead(new Node()),CAS方式更新head指针,仅当原值为null时更新成功 /** * CAS head field. Used only by enq. */ private final boolean compareAndSetHead(Node update) { return unsafe.compareAndSwapObject(this, headOffset, null, update); } compareAndSetTail(t, node),CAS方式更新tial指针,仅当原值为t时更新成功 /** * CAS tail field. Used only by enq. */ private final boolean compareAndSetTail(Node expect, Node update) { return unsafe.compareAndSwapObject(this, tailOffset, expect, update); } 外层的for循环保证了所有获取锁失败的线程经过失败重试后最后都能加入同步队列。因为AQS的同步队列是不带哨兵结点的,故当队列为空时要进行特殊处理,这部分在if分句中。注意当前线程所在的结点不能直接插入 空队列,因为阻塞的线程是由前驱结点进行唤醒的。故先要插入一个结点作为队列首元素,当锁释放时由它来唤醒后面被阻塞的线程,从逻辑上这个队列首元素也可以表示当前正获取锁的线程,虽然并不一定真实持有其线程实例。 首先通过new Node()创建一个空结点，然后以CAS方式让头指针指向该结点(该结点并非当前线程所在的结点),若该操作成功,则将尾指针也指向该结点。这部分的操作流程可以用下图表示 当队列不为空,则执行通用的入队逻辑,这部分在else分句中 else { //队列不为空 node.prev = t; if (compareAndSetTail(t, node)) { //CAS将尾指针指向当前结点,当t(原来的尾指针)==tail(当前真实的尾指针)时执行成功 t.next = node; //原尾结点的next指针指向当前结点 return t; } 首先当前线程所在的结点的前向指针pre指向当前线程认为的尾结点,源码中用t表示。然后以CAS的方式将尾指针指向当前结点,该操作仅当tail=t,即尾指针在进行CAS前未改变时成功。若CAS执行成功,则将原尾结点的后向指针next指向新的尾结点。整个过程如下图所示 整个入队的过程并不复杂,是典型的CAS加失败重试的乐观锁策略。其中只有更新头指针和更新尾指针这两步进行了CAS同步,可以预见高并发场景下性能是非常好的。但是本着质疑精神我们不禁会思考下这么做真的线程安全吗？ 1.队列为空的情况: 因为队列为空,故head=tail=null,假设线程执行2成功,则在其执行3之前,因为tail=null,其他进入该方法的线程因为head不为null将在2处不停的失败,所以3即使没有同步也不会有线程安全问题。 2.队列不为空的情况: 假设线程执行5成功,则此时4的操作必然也是正确的(当前结点的prev指针确实指向了队列尾结点,换句话说tail指针没有改变,如若不然5必然执行失败),又因为4执行成功,当前节点在队列中的次序已经确定了,所以6何时执行对线程安全不会有任何影响,比如下面这种情况 为了确保真的理解了它,可以思考这个问题:把enq方法图中的4放到5之后,整个入队的过程还线程安全吗？ 到这为止,获取锁失败的线程加入同步队列的逻辑就结束了。但是线程加入同步队列后会做什么我们并不清楚,这部分在acquireQueued方法中 acquireQueued方法的源码 /** * Acquires in exclusive uninterruptible mode for thread already in * queue. Used by condition wait methods as well as acquire. * * @param node the node * @param arg the acquire argument * @return {@code true} if interrupted while waiting */ final boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { boolean interrupted = false; //死循环,正常情况下线程只有获得锁才能跳出循环 for (;;) { final Node p = node.predecessor();//获得当前线程所在结点的前驱结点 //第一个if分句 if (p == head &amp;&amp; tryAcquire(arg)) { setHead(node); //将当前结点设置为队列头结点 p.next = null; // help GC failed = false; return interrupted;//正常情况下死循环唯一的出口 } //第二个if分句 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; //判断是否要阻塞当前线程 parkAndCheckInterrupt()) //阻塞当前线程 interrupted = true; } } finally { if (failed) cancelAcquire(node); } } 这段代码主要的内容都在for循环中,这是一个死循环,主要有两个if分句构成。第一个if分句中,当前线程首先会判断前驱结点是否是头结点,如果是则尝试获取锁,获取锁成功则会设置当前结点为头结点(更新头指针)。为什么必须前驱结点为头结点才尝试去获取锁？因为头结点表示当前正占有锁的线程,正常情况下该线程释放锁后会通知后面结点中阻塞的线程,阻塞线程被唤醒后去获取锁,这是我们希望看到的。然而还有一种情况,就是前驱结点取消了等待,此时当前线程也会被唤醒,这时候就不应该去获取锁,而是往前回溯一直找到一个没有取消等待的结点,然后将自身连接在它后面。一旦我们成功获取了锁并成功将自身设置为头结点,就会跳出for循环。否则就会执行第二个if分句:确保前驱结点的状态为SIGNAL,然后阻塞当前线程。 先来看shouldParkAfterFailedAcquire(p, node)，从方法名上我们可以大概猜出这是判断是否要阻塞当前线程的,方法内容如下 /** * Checks and updates status for a node that failed to acquire. * Returns true if thread should block. This is the main signal * control in all acquire loops. Requires that pred == node.prev. * * @param pred node's predecessor holding status * @param node the node * @return {@code true} if thread should block */ private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) { int ws = pred.waitStatus; if (ws == Node.SIGNAL) //状态为SIGNAL /* * This node has already set status asking a release * to signal it, so it can safely park. */ return true; if (ws &gt; 0) { //状态为CANCELLED, /* * Predecessor was cancelled. Skip over predecessors and * indicate retry. */ do { node.prev = pred = pred.prev; } while (pred.waitStatus &gt; 0); pred.next = node; } else { //状态为初始化状态(ReentrentLock语境下) /* * waitStatus must be 0 or PROPAGATE. Indicate that we * need a signal, but don't park yet. Caller will need to * retry to make sure it cannot acquire before parking. */ compareAndSetWaitStatus(pred, ws, Node.SIGNAL); } return false; } 可以看到针对前驱结点pred的状态会进行不同的处理 1.pred状态为SIGNAL,则返回true,表示要阻塞当前线程。 2.pred状态为CANCELLED,则一直往队列头部回溯直到找到一个状态不为CANCELLED的结点,将当前节点node挂在这个结点的后面。 3.pred的状态为初始化状态,此时通过compareAndSetWaitStatus(pred, ws, Node.SIGNAL)方法将pred的状态改为SIGNAL。 其实这个方法的含义很简单,就是确保当前结点的前驱结点的状态为SIGNAL,SIGNAL意味着线程释放锁后会唤醒后面阻塞的线程。毕竟,只有确保能够被唤醒，当前线程才能放心的阻塞。 但是要注意只有在前驱结点已经是SIGNAL状态后才会执行后面的方法立即阻塞,对应上面的第一种情况。其他两种情况则因为返回false而重新执行一遍 for循环。这种延迟阻塞其实也是一种高并发场景下的优化,试想我如果在重新执行循环的时候成功获取了锁,是不是线程阻塞唤醒的开销就省了呢？ 最后我们来看看阻塞线程的方法parkAndCheckInterrupt shouldParkAfterFailedAcquire返回true表示应该阻塞当前线程,则会执行parkAndCheckInterrupt方法,这个方法比较简单,底层调用了LockSupport来阻塞当前线程,源码如下: /** * Convenience method to park and then check if interrupted * * @return {@code true} if interrupted */ private final boolean parkAndCheckInterrupt() { LockSupport.park(this); return Thread.interrupted(); } 该方法内部通过调用LockSupport的park方法来阻塞当前线程 LockSupport就是通过控制变量_counter来对线程阻塞唤醒进行控制的。原理有点类似于信号量机制。 当调用park()方法时，会将_counter置为0，同时判断前值，小于1说明前面被unpark过,则直接退出，否则将使该线程阻塞。 当调用unpark()方法时，会将_counter置为1，同时判断前值，小于1会进行线程唤醒，否则直接退出。 形象的理解，线程阻塞需要消耗凭证(permit)，这个凭证最多只有1个。当调用park方法时，如果有凭证，则会直接消耗掉这个凭证然后正常退出；但是如果没有凭证，就必须阻塞等待凭证可用；而unpark则相反，它会增加一个凭证，但凭证最多只能有1个。 为什么可以先唤醒线程后阻塞线程？ 因为unpark获得了一个凭证,之后调用park因为有凭证消费，故不会阻塞。 为什么唤醒两次后阻塞两次会阻塞线程。 因为凭证的数量最多为1，连续调用两次unpark和调用一次unpark效果一样，只会增加一个凭证；而调用两次park却需要消费两个凭证。 下面通过一张流程图来说明线程从加入同步队列到成功获取锁的过程 概括的说,线程在同步队列中会尝试获取锁,失败则被阻塞,被唤醒后会不停的重复这个过程,直到线程真正持有了锁,并将自身结点置于队列头部。 ReentrantLock非公平模式下的加锁流程如下 解锁 解锁源码如下： public void unlock() { sync.release(1); } public final boolean release(int arg) { if (tryRelease(arg)) { //释放锁(state-1),若释放后锁可被其他线程获取(state=0),返回true Node h = head; //当前队列不为空且头结点状态不为初始化状态(0) if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); //唤醒同步队列中被阻塞的线程 return true; } return false; } 正确找到sync的实现类,找到真正的入口方法,主要内容都在一个if语句中,先看下判断条件tryRelease方法 protected final boolean tryRelease(int releases) { int c = getState() - releases; //计算待更新的state值 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) { //待更新的state值为0,说明持有锁的线程未重入,一旦释放锁其他线程将能获取 free = true; setExclusiveOwnerThread(null);//清除锁的持有线程标记 } setState(c);//更新state值 return free; } tryRelease其实只是将线程持有锁的次数减1,即将state值减1,若减少后线程将完全释放锁(state值为0),则该方法将返回true,否则返回false。由于执行该方法的线程必然持有锁,故该方法不需要任何同步操作。 若当前线程已经完全释放锁,即锁可被其他线程使用,则还应该唤醒后续等待线程。不过在此之前需要进行两个条件的判断： h!=null是为了防止队列为空,即没有任何线程处于等待队列中,那么也就不需要进行唤醒的操作 h.waitStatus != 0是为了防止队列中虽有线程,但该线程还未阻塞,由前面的分析知,线程在阻塞自己前必须设置前驱结点的状态为SIGNAL,否则它不会阻塞自己。 接下来就是唤醒线程的操作,unparkSuccessor(h)源码如下 private void unparkSuccessor(Node node) { /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ Node s = node.next; if (s == null || s.waitStatus &gt; 0) { s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; } if (s != null) LockSupport.unpark(s.thread); } 一般情况下只要唤醒后继结点的线程就行了,但是后继结点可能已经取消等待,所以从队列尾部往前回溯,找到离头结点最近的正常结点,并唤醒其线程。 解锁流程源码总结 公平锁相比非公平锁的不同 公平锁模式下,对锁的获取有严格的条件限制。在同步队列有线程等待的情况下,所有线程在获取锁前必须先加入同步队列。队列中的线程按加入队列的先后次序获得锁。 从公平锁加锁的入口开始, 对比非公平锁,少了非重入式获取锁的方法,这是第一个不同点 接着看获取锁的通用方法tryAcquire(),该方法在线程未进入队列,加入队列阻塞前和阻塞后被唤醒时都会执行。 在真正CAS获取锁之前加了判断,内容如下 public final boolean hasQueuedPredecessors() { // The correctness of this depends on head being initialized // before tail and on head.next being accurate if the current // thread is first in queue. Node t = tail; // Read fields in reverse initialization order Node h = head; Node s; return h != t &amp;&amp; ((s = h.next) == null || s.thread != Thread.currentThread()); } 从方法名我们就可知道这是判断队列中是否有优先级更高的等待线程,队列中哪个线程优先级最高？由于头结点是当前获取锁的线程,队列中的第二个结点代表的线程优先级最高。 那么我们只要判断队列中第二个结点是否存在以及这个结点是否代表当前线程就行了。这里分了两种情况进行探讨: 第二个结点已经完全插入,但是这个结点是否就是当前线程所在结点还未知,所以通过s.thread != Thread.currentThread()进行判断,如果为true,说明第二个结点代表其他线程。 第二个结点并未完全插入,我们知道结点入队一共分三步： 1.待插入结点的pre指针指向原尾结点 2.CAS更新尾指针 3.原尾结点的next指针指向新插入结点 所以(s = h.next) == null 就是用来判断2刚执行成功但还未执行3这种情况的。这种情况第二个结点必然属于其他线程。 以上两种情况都会使该方法返回true,即当前有优先级更高的线程在队列中等待,那么当前线程将不会执行CAS操作去获取锁,保证了线程获取锁的顺序与加入同步队列的顺序一致，很好的保证了公平性,但也增加了获取锁的成本。 一些疑问的解答 为什么基于FIFO的同步队列可以实现非公平锁？ 由FIFO队列的特性知,先加入同步队列等待的线程会比后加入的线程更靠近队列的头部,那么它将比后者更早的被唤醒,它也就能更早的得到锁。从这个意义上,对于在同步队列中等待的线程而言,它们获得锁的顺序和加入同步队列的顺序一致，这显然是一种公平模式。然而,线程并非只有在加入队列后才有机会获得锁,哪怕同步队列中已有线程在等待,非公平锁的不公平之处就在于此。回看下非公平锁的加锁流程,线程在进入同步队列等待之前有两次抢占锁的机会: 第一次是非重入式的获取锁,只有在当前锁未被任何线程占有(包括自身)时才能成功; 第二次是在进入同步队列前,包含所有情况的获取锁的方式。 只有这两次获取锁都失败后,线程才会构造结点并加入同步队列等待。而线程释放锁时是先释放锁(修改state值),然后才唤醒后继结点的线程的。试想下这种情况,线程A已经释放锁,但还没来得及唤醒后继线程C,而这时另一个线程B刚好尝试获取锁,此时锁恰好不被任何线程持有,它将成功获取锁而不用加入队列等待。线程C被唤醒尝试获取锁,而此时锁已经被线程B抢占,故而其获取失败并继续在队列中等待。整个过程如下图所示 如果以线程第一次尝试获取锁到最后成功获取锁的次序来看,非公平锁确实很不公平。因为在队列中等待很久的线程相比还未进入队列等待的线程并没有优先权,甚至竞争也处于劣势:在队列中的线程要等待其他线程唤醒,在获取锁之前还要检查前驱结点是否为头结点。在锁竞争激烈的情况下,在队列中等待的线程可能迟迟竞争不到锁。这也就非公平在高并发情况下会出现的饥饿问题。那我们再开发中为什么大多使用会导致饥饿的非公平锁？很简单,因为它性能好啊。 为什么非公平锁性能好 非公平锁对锁的竞争是抢占式的(队列中线程除外),线程在进入等待队列前可以进行两次尝试,这大大增加了获取锁的机会。这种好处体现在两个方面: 1.线程不必加入等待队列就可以获得锁,不仅免去了构造结点并加入队列的繁琐操作,同时也节省了线程阻塞唤醒的开销,线程阻塞和唤醒涉及到线程上下文的切换和操作系统的系统调用,是非常耗时的。在高并发情况下,如果线程持有锁的时间非常短,短到线程入队阻塞的过程超过线程持有并释放锁的时间开销,那么这种抢占式特性对并发性能的提升会更加明显。 2.减少CAS竞争。如果线程必须要加入阻塞队列才能获取锁,那入队时CAS竞争将变得异常激烈,CAS操作虽然不会导致失败线程挂起,但不断失败重试导致的对CPU的浪费也不能忽视。除此之外,加锁流程中至少有两处通过将某些特殊情况提前来减少CAS操作的竞争,增加并发情况下的性能。一处就是获取锁时将非重入的情况提前,如下图所示 另一处就是入队的操作,将同步队列非空的情况提前处理 这两部分的代码在之后的通用逻辑处理中都有,很显然属于重复代码,但因为避免了执行无意义的流程代码,比如for循环,获取同步状态等,高并发场景下也能减少CAS竞争失败的可能。 读写锁ReentrantReadWriteLock 首先明确一下，不是说 ReentrantLock 不好，只是 ReentrantLock 某些时候有局限。如果使用 ReentrantLock，可能本身是为了防止线程 A 在写数据、线程 B 在读数据造成的数据不一致，但这样，如果线程 C 在读数据、线程 D 也在读数据，读数据是不会改变数据的，没有必要加锁，但是还是加锁了，降低了程序的性能。因为这个，才诞生了读写锁 ReadWriteLock。 ReadWriteLock 是一个读写锁接口，读写锁是用来提升并发程序性能的锁分离技术，ReentrantReadWriteLock 是 ReadWriteLock 接口的一个具体实现，实现了读写的分离，读锁是共享的，写锁是独占的，读和读之间不会互斥，读和写、写和读、写和写之间才会互斥，提升了读写的性能。 而读写锁有以下三个重要的特性： （1）公平选择性：支持非公平（默认）和公平的锁获取方式，吞吐量还是非公平优于公平。 （2）重进入：读锁和写锁都支持线程重进入。 ","link":"https://tinaxiawuhao.github.io/post/CxCcIUkNX/"},{"title":"netty零拷贝","content":"零拷贝的应用程序要求内核（kernel）直接将数据从磁盘文件拷贝到套接字（Socket），而无须通过应用程序。零拷贝不仅提高了应用程序的性能，而且减少了内核和用户模式见上下文切换。 数据传输：传统方法 从文件中读取数据，并将数据传输到网络上的另一个程序的场景：从下图可以看出，拷贝的操作需要4次用户模式和内核模式之间的上下文切换，而且在操作完成前数据被复制了4次。(DMA：直接内存拷贝) 从磁盘中copy放到一个内存buf中，然后将buf通过socket传输给用户,下面是伪代码实现： read(file, tmp_buf, len); write(socket, tmp_buf, len); 从图中可以看出文件经历了4次copy过程： 1.首先，调用read方法，文件从user模式拷贝到了kernel模式；（用户模式-&gt;内核模式的上下文切换，在内部发送sys_read() 从文件中读取数据，存储到一个内核地址空间缓存区中） 2.之后CPU控制将kernel模式数据拷贝到user模式下；（内核模式-&gt; 用户模式的上下文切换，read()调用返回，数据被存储到用户地址空间的缓存区中） 3.调用write时候，先将user模式下的内容copy到kernel模式下的socket的buffer中（用户模式-&gt;内核模式，数据再次被放置在内核缓存区中，send（）套接字调用） 4.最后将kernel模式下的socket buffer的数据copy到网卡设备中；（send套接字调用返回） 从图中看2，3两次copy是多余的，数据从kernel模式到user模式走了一圈，浪费了2次copy。 数据传输：mmap 优化 mmap 通过内存映射，将文件映射到内核缓冲区，同时，用户空间可以共享内核空间的数据。这样，在进行网络传输时，就可以减少内核空间到用户空间的拷贝次数。如下图： 如上图，user buffer 和 kernel buffer 共享 index.html。如果你想把硬盘的 index.html 传输到网络中，再也不用拷贝到用户空间，再从用户空间拷贝到 Socket 缓冲区。 现在，你只需要从内核缓冲区拷贝到 Socket 缓冲区即可，这将减少一次内存拷贝（从 4 次变成了 3 次），但不减少上下文切换次数。 数据传输：零拷贝方法 从传统的场景看，会注意到上图，第2次和第3次拷贝根本就是多余的。应用程序只是起到缓存数据被将传回到套接字的作用而已，别无他用。 应用程序使用zero-copy来请求kernel直接把disk的数据传输到socket中，而不是通过应用程序传输。zero-copy大大提高了应用程序的性能，并且减少了kernel和user模式的上下文切换。 数据可以直接从read buffer 读缓存区传输到套接字缓冲区，也就是省去了将操作系统的read buffer 拷贝到程序的buffer，以及从程序buffer拷贝到socket buffer的步骤，直接将read buffer拷贝到socket buffer。JDK NIO中的的transferTo() 方法就能够让您实现这个操作，这个实现依赖于操作系统底层的sendFile（）实现的： public void transferTo(long position, long count, WritableByteChannel target); 底层调用sendFile方法： #include &lt;sys/socket.h&gt; ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count); 使用了zero-copy技术后，整个过程如下： 1.transferTo()方法使得文件的内容直接copy到了一个read buffer（kernel buffer）中 2.然后数据（kernel buffer）copy到socket buffer中 3.最后将socket buffer中的数据copy到网卡设备（protocol engine）中传输； 这个显然是一个伟大的进步：这里上下文切换从4次减少到2次，同时把数据copy的次数从4次降低到3次； 但是这是zero-copy么，答案是否定的； linux 2.1 内核开始引入了sendfile函数，用于将文件通过socket传输。 sendfile(socket, file, len); 该函数通过一次调用完成了文件的传输。 该函数通过一次系统调用完成了文件的传输，减少了原来read/write方式的模式切换。此外更是减少了数据的copy，sendfile的详细过程如图： 通过sendfile传送文件只需要一次系统调用，当调用sendfile时： 1.首先通过DMA将数据从磁盘读取到kernel buffer中 2.然后将kernel buffer数据拷贝到socket buffer中 3.最后将socket buffer中的数据copy到网卡设备中（protocol buffer）发送； sendfile与read/write模式相比，少了一次copy。但是从上述过程中发现从kernel buffer中将数据copy到socket buffer是没有必要的； Linux2.4 内核对sendfile做了改进，如图： 改进后的处理过程如下： 将文件拷贝到kernel buffer中；(DMA引擎将文件内容copy到内核缓存区) 向socket buffer中追加当前要发生的数据在kernel buffer中的位置和偏移量； 根据socket buffer中的位置和偏移量直接将kernel buffer的数据copy到网卡设备（protocol engine）中； 从图中看到，linux 2.1内核中的 “数据被copy到socket buffer”的动作，在Linux2.4 内核做了优化，取而代之的是只包含关于数据的位置和长度的信息的描述符被追加到了socket buffer 缓冲区中。DMA引擎直接把数据从内核缓冲区传输到协议引擎（protocol engine），从而消除了最后一次CPU copy。经过上述过程，数据只经过了2次copy就从磁盘传送出去了。这个才是真正的Zero-Copy(这里的零拷贝是针对kernel来讲的，数据在kernel模式下是Zero-Copy)。 正是Linux2.4的内核做了改进，Java中的TransferTo()实现了Zero-Copy,如下图： Zero-Copy技术的使用场景有很多，比如Kafka, 又或者是Netty等，可以大大提升程序的性能。 首先我们说零拷贝，是从操作系统的角度来说的。因为内核缓冲区之间，没有数据是重复的（只有 kernel buffer 有一份数据，sendFile 2.1 版本实际上有 2 份数据，算不上零拷贝）。例如我们刚开始的例子，内核缓存区和 Socket 缓冲区的数据就是重复的。 而零拷贝不仅仅带来更少的数据复制，还能带来其他的性能优势，例如更少的上下文切换，更少的 CPU 缓存伪共享以及无 CPU 校验和计算。 mmap 和 sendFile 的区别。 mmap 适合小数据量读写，sendFile 适合大文件传输。 mmap 需要 4 次上下文切换，3 次数据拷贝；sendFile 需要 3 次上下文切换，最少 2 次数据拷贝。 sendFile 可以利用 DMA 方式，减少 CPU 拷贝，mmap 则不能（必须从内核拷贝到 Socket 缓冲区）。 在这个选择上：rocketMQ 在消费消息时，使用了 mmap。kafka 使用了 sendFile。 ","link":"https://tinaxiawuhao.github.io/post/Ts_mpMa6r/"},{"title":"netty异步任务调度 ( TaskQueue | ScheduleTaskQueue | SocketChannel 管理 )","content":" 一、 任务队列 任务队列的任务 Task 应用场景 : 自定义任务 : 自己开发的任务 , 然后将该任务提交到任务队列中 ; 自定义定时任务 : 自己开发的任务 , 然后将该任务提交到任务队列中 , 同时可以指定任务的执行时间 ; 其它线程调度任务 : 上面的任务都是在当前的 NioEventLoop ( 反应器 Reactor 线程 ) 中的任务队列中排队执行 , 在其它线程中也可以调度本线程的 Channel 通道与该线程对应的客户端进行数据读写 ; 二、 处理器 Handler 同步异步操作 在之前的 Netty 服务器与客户端项目中 , 用户自定义的 Handler 处理器 , 该处理器继承了 ChannelInboundHandlerAdapter 类 , 在重写的 public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception 方法中 , 执行的业务逻辑要注意以下两点 : 同步操作 : 如果在该业务逻辑中只执行一个短时间的操作 , 那么可以直接执行 ; 异步操作 : 如果在该业务逻辑中执行访问数据库 , 访问网络 , 读写本地文件 , 执行一系列复杂计算等耗时操作 , 肯定不能在该方法中处理 , 这样会阻塞整个线程 ; 正确的做法是将耗时的操作放入任务队列 TaskQueue , 异步执行 ; 在 ChannelInboundHandlerAdapter 的 channelRead 方法执行时 , 客户端与服务器端的反应器 Reactor 线程 NioEventLoop 是处于阻塞状态的 , 此时服务器端与客户端同时都处于阻塞状态 , 这样肯定不行 , 因为 NioEventLoop 需要为多个客户端服务 , 不能因为与单一客户端交互而产生阻塞 ; 三、 异步任务 ( 用户自定义任务 ) 用户自定义任务流程 : ① 获取通道 : 首先获取 通道 Channel ; ② 获取线程 : 获取通道对应的 EventLoop 线程 , 就是 NioEventLoop , 该 NioEventLoop 中封装了任务队列 TaskQueue ; ③ 任务入队 : 向任务队列 TaskQueue 中放入异步任务 Runnable , 调用 NioEventLoop 线程的 execute 方法 , 即可将上述 Runnable 异步任务放入任务队列 TaskQueue ; 多任务执行 : 如果用户连续向任务队列中放入了多个任务 , NioEventLoop 会按照顺序先后执行这些任务 , 注意任务队列中的任务 是先后执行 , 不是同时执行 ; 顺序执行任务 ( 不是并发 ) : 任务队列任务执行机制是顺序执行的 ; 先执行第一个 , 执行完毕后 , 从任务队列中获取第二个任务 , 执行完毕之后 , 依次从任务队列中取出任务执行 , 前一个任务执行完毕后 , 才从任务队列中取出下一个任务执行 ; 代码示例 : 监听到客户端上传数据后 , channelRead 回调 , 执行 获取通道 -&gt; 获取线程 -&gt; 异步任务调度 流程 ; /** * Handler 处理者, 是 NioEventLoop 线程中处理业务逻辑的类 * * 继承 : 该业务逻辑处理者 ( Handler ) 必须继承 Netty 中的 ChannelInboundHandlerAdapter 类 * 才可以设置给 NioEventLoop 线程 * * 规范 : 该 Handler 类中需要按照业务逻辑处理规范进行开发 */ public class ServerHandr extends ChannelInboundHandlerAdapter { /** * 读取数据 : 在服务器端读取客户端发送的数据 * @param ctx * 通道处理者上下文对象 : 封装了 管道 ( Pipeline ) , 通道 ( Channel ), 客户端地址信息 * 管道 ( Pipeline ) : 注重业务逻辑处理 , 可以关联很多 Handler * 通道 ( Channel ) : 注重数据读写 * @param msg * 客户端上传的数据 * @throws Exception */ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { // 1 . 从 ChannelHandlerContext ctx 中获取通道 Channel channel = ctx.channel(); // 2 . 获取通道对应的事件循环 EventLoop eventLoop = channel.eventLoop(); // 3 . 在 Runnable 中用户自定义耗时操作, 异步执行该操作, 该操作不能阻塞在此处执行 eventLoop.execute(new Runnable() { @Override public void run() { //执行耗时操作 } }); } } 四、 异步任务 ( 用户自定义定时任务 ) 用户自定义定时任务 与 用户自定义任务流程基本类似 , 有以下两个不同之处 : ① 调度方法 : 定时异步任务使用 schedule 方法进行调度 ; 普通异步任务使用 execute 方法进行调度 ; ② 任务队列 : 定时异步任务提交到 ScheduleTaskQueue 任务队列中 ; 普通异步任务提交到 TaskQueue 任务队列中 ; 用户自定义定时任务流程 : ① 获取通道 : 首先获取 通道 Channel ; ② 获取线程 : 获取通道对应的 EventLoop 线程 , 就是 NioEventLoop , 该 NioEventLoop 中封装了任务队列 TaskQueue ; ③ 任务入队 : 向任务队列 ScheduleTaskQueue 中放入异步任务 Runnable , 调用 NioEventLoop 线程的 schedule 方法 , 即可将上述 Runnable 异步任务放入任务队列 ScheduleTaskQueue ; 代码示例 : 监听到客户端上传数据后 , channelRead 回调 , 执行 获取通道 -&gt; 获取线程 -&gt; 异步任务调度 流程 ; /** * Handler 处理者, 是 NioEventLoop 线程中处理业务逻辑的类 * * 继承 : 该业务逻辑处理者 ( Handler ) 必须继承 Netty 中的 ChannelInboundHandlerAdapter 类 * 才可以设置给 NioEventLoop 线程 * * 规范 : 该 Handler 类中需要按照业务逻辑处理规范进行开发 */ public class ServerHandr extends ChannelInboundHandlerAdapter { /** * 读取数据 : 在服务器端读取客户端发送的数据 * @param ctx * 通道处理者上下文对象 : 封装了 管道 ( Pipeline ) , 通道 ( Channel ), 客户端地址信息 * 管道 ( Pipeline ) : 注重业务逻辑处理 , 可以关联很多 Handler * 通道 ( Channel ) : 注重数据读写 * @param msg * 客户端上传的数据 * @throws Exception */ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { // 1 . 从 ChannelHandlerContext ctx 中获取通道 Channel channel = ctx.channel(); // 2 . 获取通道对应的事件循环 EventLoop eventLoop = channel.eventLoop(); // 3 . 在 Runnable 中用户自定义耗时操作, 异步执行该操作, 该操作不能阻塞在此处执行 // schedule(Runnable command, long delay, TimeUnit unit) // Runnable command 参数 : 异步任务 // long delay 参数 : 延迟执行时间 // TimeUnit unit参数 : 延迟时间单位, 秒, 毫秒, 分钟 eventLoop.schedule(new Runnable() { @Override public void run() { //执行耗时操作 } }, 100, TimeUnit.MILLISECONDS); } } 五、 异步任务 ( 其它线程向本线程调度任务 ) 通过EventExecutorGroup线程池获取不同线程执行异步耗时任务 代码示例（一） // 服务器启动对象, 需要为该对象配置各种参数 ServerBootstrap bootstrap = new ServerBootstrap(); // 添加线程组异步执行耗时任务 final EventExecutorGroup businessGroup = new DefaultEventExecutorGroup(16); bootstrap.group(bossGroup, workerGroup) // 设置 主从 线程组 , 分别对应 主 Reactor 和 从 Reactor .channel(NioServerSocketChannel.class) // 设置 NIO 网络套接字通道类型 .option(ChannelOption.SO_BACKLOG, 128) // 设置线程队列维护的连接个数 .childOption(ChannelOption.SO_KEEPALIVE, true) // 设置连接状态行为, 保持连接状态 .childHandler( // 为 WorkerGroup 线程池对应的 NioEventLoop 设置对应的事件 处理器 Handler new ChannelInitializer&lt;SocketChannel&gt;() {// 创建通道初始化对象 @Override protected void initChannel(SocketChannel ch) throws Exception { // 该方法在服务器与客户端连接建立成功后会回调 // 为 管道 Pipeline 设置处理器 Hanedler ch.pipeline().addLast(businessGroup,new NettyServerHandler()); } } ); 代码示例（二） public class NettyServerHandler extends SimpleChannelInboundHandler&lt;Object&gt; { final EventExecutorGroup businessGroup = new DefaultEventExecutorGroup(16); @Override protected void channelRead0(ChannelHandlerContext ctx, Object obj) throws Exception { businessGroup.submit(new Callable&lt;Object&gt;() { @Override public Object call() throws Exception { //业务处理 return null; } }); } } ","link":"https://tinaxiawuhao.github.io/post/b3vVP1a3A/"},{"title":"netty_nio_Reactor模式","content":"Reactor有三种模式： 单reactor单线程工作原理图 dispatch与handler在同一个线程中处理. redis就是采用这种模式 单reactor多线程工作原理图 （1） reactor对象通过select监控客户端请求事件，收到事件后，通过dispatch进行分发 （2）如果建立连接请求，则由Acceptor通过accept处理连接请求，然后创建一个Handler对象处理完成连接后的各种事件 （3）如果不是连接请求，则由reactor分发调用连接对应的Handler来处理 （4）Handler只负责响应事件，不做具体的业务处理， 通过read读取数据后分发给后面的work线程池中的某个线程。 （5）work线程池会分配一个独立的线程完成真正的业务 ，并将处理完的业务结果返回给Handler （6）Handler收到响应结果后，通过send将结果返回给client 优点： （1） 可以充分利用多核cpu的处理能力 （2） 多线程数据共享和访问比较复杂 ，reactor处理了所有的事件监听和响应，而且是在单线程中运行，在高并发场景容易出现性能瓶颈 主从reactor多线程工作原理图 （1）Reactor主线程MainReactor对象通过select监听连接事件，收到连接事件后，通过Acceptor处理连接事件 （2）当Acceptor处理连接事件后，MainReactor将连接分配给SubReactor, （3）SubReactor将连接加入到连接监听队列进行监听，并创建Handler进行各种事件处理 （4）当有新的事件发生时， subreactor就会调用对应的handler处理， （5）handler通过read读取数据后，分发给后面的worker线程处理 （6）worker线程池会分配独立的worker线程进行业务处理，并返回结果 （7）handler收到响应结果后，再通过send将结果返回给client 实现Reactor模式我们需要实现以下几个类： InputSource: 外部输入类，用来表示需要reactor去处理的原始对象 Event: reactor模式的事件类，可以理解为将输入原始对象根据不同状态包装成一个事件类，reactor模式里处理的斗士event事件对象 EventType: 枚举类型表示事件的不同类型 EventHandler: 处理事件的抽象类，里面包含了不同事件处理器的公共逻辑和公共对象 AcceptEventHandler\\ReadEventhandler等: 继承自EventHandler的具体事件处理器的实现类，一般根据事件不同的状态来定义不同的处理器 Dispatcher: 事件分发器，整个reactor模式解决的主要问题就是在接收到任务后根据分发器快速进行分发给相应的事件处理器，不需要从开始状态就阻塞 Selector: 事件轮循选择器，selector主要实现了轮循队列中的事件状态，取出当前能够处理的状态 Acceptor:reactor的事件接收类，负责初始化selector和接收缓冲队列 Server:负责启动reactor服务并启动相关服务接收请求 InputSource.java import lombok.AllArgsConstructor; import lombok.Data; @Data @AllArgsConstructor public class InputSource { private final Object data; private final long id; } Event.java import lombok.Getter; import lombok.Setter; @Getter @Setter public class Event { private InputSource source; private EventType type; } EventType public enum EventType { ACCEPT, READ, WRITE; } EventHandler.java @Getter @Setter public abstract class EventHandler { private InputSource source; public abstract void handle(Event event); } AcceptEventHandler.java public class AcceptEventHandler extends EventHandler { private Selector selector; public AcceptEventHandler(Selector selector) { this.selector = selector; } @Override public void handle(Event event) { //处理Accept的event事件 if (event.getType() == EventType.ACCEPT) { //TODO 处理ACCEPT状态的事件 //将事件状态改为下一个READ状态，并放入selector的缓冲队列中 Event readEvent = new Event(); readEvent.setSource(event.getSource()); readEvent.setType(EventType.READ); selector.addEvent(readEvent); } } } Dispatcher.java import java.util.List; import java.util.Map; import java.util.concurrent.ConcurrentHashMap; public class Dispatcher { //通过ConcurrentHashMap来维护不同事件处理器 Map&lt;EventType, EventHandler&gt; eventHandlerMap = new ConcurrentHashMap&lt;EventType, EventHandler&gt;(); //本例只维护一个selector负责事件选择，netty为了保证性能实现了多个selector来保证循环处理性能，不同事件加入不同的selector的事件缓冲队列 Selector selector; Dispatcher(Selector selector) { this.selector = selector; } //在Dispatcher中注册eventHandler public void registEventHandler(EventType eventType, EventHandler eventHandler) { eventHandlerMap.put(eventType, eventHandler); } public void removeEventHandler(EventType eventType) { eventHandlerMap.remove(eventType); } public void handleEvents() { dispatch(); } //此例只是实现了简单的事件分发给相应的处理器处理，例子中的处理器都是同步，在reactor模式的典型实现NIO中都是在handle异步处理，来保证非阻塞 private void dispatch() { while (true) { List&lt;Event&gt; events = selector.select(); for (Event event : events) { EventHandler eventHandler = eventHandlerMap.get(event.getType()); eventHandler.handle(event); } } } } Selector.java import java.util.ArrayList; import java.util.List; import java.util.concurrent.BlockingQueue; import java.util.concurrent.LinkedBlockingQueue; public class Selector { //定义一个链表阻塞queue实现缓冲队列，用于保证线程安全 private final BlockingQueue&lt;Event&gt; eventQueue = new LinkedBlockingQueue&lt;Event&gt;(); //定义一个object用于synchronize方法块上锁 private final Object lock = new Object(); List&lt;Event&gt; select() { return select(0); } // List&lt;Event&gt; select(long timeout) { if (timeout &gt; 0) { if (eventQueue.isEmpty()) { synchronized (lock) { if (eventQueue.isEmpty()) { try { lock.wait(timeout); } catch (InterruptedException ignored) { } } } } } //TODO 例子中只是简单的将event列表全部返回，可以在此处增加业务逻辑，选出符合条件的event进行返回 List&lt;Event&gt; events = new ArrayList&lt;Event&gt;(); eventQueue.drainTo(events); return events; } public void addEvent(Event e) { //将event事件加入队列 boolean success = eventQueue.offer(e); if (success) { synchronized (lock) { //如果有新增事件则对lock对象解锁 lock.notify(); } } } } Acceptor.java import java.util.concurrent.BlockingQueue; import java.util.concurrent.LinkedBlockingQueue; public class Acceptor implements Runnable{ private final int port; // server socket port private final Selector selector; // 代表 serversocket，通过LinkedBlockingQueue来模拟外部输入请求队列 private final BlockingQueue&lt;InputSource&gt; sourceQueue = new LinkedBlockingQueue&lt;InputSource&gt;(); Acceptor(Selector selector, int port) { this.selector = selector; this.port = port; } //外部有输入请求后，需要加入到请求队列中 public void addNewConnection(InputSource source) { sourceQueue.offer(source); } public int getPort() { return this.port; } public void run() { while (true) { InputSource source = null; try { // 相当于 serversocket.accept()，接收输入请求，该例从请求队列中获取输入请求 source = sourceQueue.take(); } catch (InterruptedException e) { // ignore it; } //接收到InputSource后将接收到event设置type为ACCEPT，并将source赋值给event if (source != null) { Event acceptEvent = new Event(); acceptEvent.setSource(source); acceptEvent.setType(EventType.ACCEPT); selector.addEvent(acceptEvent); } } } } Server.java public class Server { Selector selector = new Selector(); Dispatcher eventLooper = new Dispatcher(selector); Acceptor acceptor; Server(int port) { acceptor = new Acceptor(selector, port); } public void start() { eventLooper.registEventHandler(EventType.ACCEPT, new AcceptEventHandler(selector)); new Thread(acceptor, &quot;Acceptor-&quot; + acceptor.getPort()).start(); eventLooper.handleEvents(); } } ","link":"https://tinaxiawuhao.github.io/post/wvy4g9Zbn/"},{"title":"saiku自动对接kylin","content":"saiku通过添加schema和datasource的形式管理对接入系统的数据源，然后提供界面作为直观的分析数据方式，界面产生mdx，由mondrian连接数据源，解析mdx和执行查询 kylin提供大规模数据的olap能力，通过saiku与kylin的对接，利用saiku的友好界面来很方面的查询 如上的整合，需要手动配置数据源，编写schema的操作，感觉比较繁琐，可以通过修改saiku的代码，到kylin中获取project和cube的各种信息，根据一定规则转换生成schema并作为数据源管理起来，这样就很直接将saiku与kylin无缝对接起来。 代码案例 saiku-wabapp #saiku-beans.properties sylin.user=ADMIN sylin.password=ADMIN sylin.cube.url=http://localhost:7070/kylin/api/cubes sylin.cubedesc.url=http://localhost:7070/kylin/api/cube_desc sylin.model.url=http://localhost:7070/kylin/api/model sylin.url=localhost:7070 //saiku-beans.xml &lt;bean id=&quot;repositoryDsManager&quot; class=&quot;org.saiku.service.datasource.RepositoryDatasourceManager&quot; init-method=&quot;load&quot; destroy-method=&quot;unload&quot;&gt; &lt;!--aop:scoped-proxy/--&gt; ...... &lt;property name=&quot;sylinUser&quot; value=&quot;${sylin.user}&quot;/&gt; &lt;property name=&quot;sylinPassWord&quot; value=&quot;${sylin.password}&quot;/&gt; &lt;property name=&quot;sylinCubeUrl&quot; value=&quot;${sylin.cube.url}&quot;/&gt; &lt;property name=&quot;sylinCubeDescUrl&quot; value=&quot;${sylin.cubedesc.url}&quot;/&gt; &lt;property name=&quot;sylinModelUrl&quot; value=&quot;${sylin.model.url}&quot;/&gt; &lt;property name=&quot;sylinUrl&quot; value=&quot;${sylin.url}&quot;/&gt; &lt;/bean&gt; saiku-service public class RepositoryDatasourceManager implements IDatasourceManager, ApplicationListener&lt;HttpSessionCreatedEvent&gt; { private String sylinUser; private String sylinPassWord; private String sylinCubeUrl; private String sylinCubeDescUrl; private String sylinModelUrl; private String sylinUrl; //get.set省略 ...... private void loadDatasources(Properties ext) { datasources.clear(); List&lt;DataSource&gt; exporteddatasources = null; try { String result = execute(sylinCubeUrl); JSONArray ja = JSON.parseArray(result); for (int i = 0; i &lt; ja.size(); i++) { JSONObject js = JSONObject.parseObject(ja.getString(i)); String newCubeName = js.getString(&quot;project&quot;) + &quot;#&quot; + js.getString(&quot;name&quot;); datasources.put(js.getString(&quot;name&quot;), getSaikuDatasource(newCubeName)); } } catch (Exception e) { log.error(&quot;Failed add sylin cube to datasource&quot;, e); e.printStackTrace(); } ...... } private SaikuDatasource getSaikuDatasource(String datasourceName) throws Exception { if (datasourceName.contains(&quot;#&quot;)) { String cubeName = datasourceName.split(&quot;#&quot;)[1].trim(); String cubeDescString = execute(sylinCubeDescUrl + &quot;/&quot; + cubeName); CubeDesc cubeDesc = OBJECT_MAPPER.readValue(JSON.parseArray(cubeDescString).getString(0), CubeDesc.class); //CubeDesc cubeDesc = JSONObject.parseObject(JSON.parseArray(cubeDescString).getString(0), CubeDesc.class); String modelName = cubeDesc.getModelName(); String cubeModelString = execute(sylinModelUrl + &quot;/&quot; + modelName); DataModelDesc modelDesc = OBJECT_MAPPER.readValue(cubeModelString, DataModelDesc.class); //DataModelDesc modelDesc = JSONObject.parseObject(cubeModelString, DataModelDesc.class); if (cubeDesc != null &amp;&amp; modelDesc != null) { addSchema(SchemaUtil.createSchema(datasourceName, cubeDesc, modelDesc), &quot;/datasources/&quot; + datasourceName.replace(&quot;#&quot;, &quot;.&quot;) + &quot;.xml&quot;, datasourceName); } String project = new String(); if (datasourceName.contains(&quot;#&quot;)) { project = datasourceName.split(&quot;#&quot;)[0].trim(); } else { project = datasourceName; } Properties properties = new Properties(); properties.put(&quot;location&quot;, &quot;jdbc:mondrian:Jdbc=jdbc:kylin://&quot; + sylinUrl + &quot;/&quot; + project + &quot;;JdbcDrivers=org.apache.kylin.jdbc.Driver&quot; + &quot;;Catalog=mondrian:///datasources/&quot; + datasourceName.replace(&quot;#&quot;, &quot;.&quot;) + &quot;.xml&quot;); properties.put(&quot;driver&quot;, &quot;mondrian.olap4j.MondrianOlap4jDriver&quot;); properties.put(&quot;username&quot;, sylinUser); properties.put(&quot;password&quot;, sylinPassWord); properties.put(&quot;security.enabled&quot;, false); properties.put(&quot;advanced&quot;, false); return new SaikuDatasource(cubeName, SaikuDatasource.Type.OLAP, properties); } return null; } private String execute(String url) throws URISyntaxException, IOException { int httpConnectionTimeoutMs = 30000; int httpSocketTimeoutMs = 120000; final HttpParams httpParams = new BasicHttpParams(); final PoolingClientConnectionManager cm = new PoolingClientConnectionManager(); HttpConnectionParams.setSoTimeout(httpParams, httpSocketTimeoutMs); HttpConnectionParams.setConnectionTimeout(httpParams, httpConnectionTimeoutMs); cm.setDefaultMaxPerRoute(20); cm.setMaxTotal(200); HttpGet get = newGet(url); get.setURI(new URI(url)); DefaultHttpClient client = new DefaultHttpClient(cm, httpParams); if (sylinUser != null &amp;&amp; sylinPassWord != null) { CredentialsProvider provider = new BasicCredentialsProvider(); UsernamePasswordCredentials credentials = new UsernamePasswordCredentials(sylinUser, sylinPassWord); provider.setCredentials(AuthScope.ANY, credentials); client.setCredentialsProvider(provider); } HttpResponse response = client.execute(get); if (response.getStatusLine().getStatusCode() != 200) { throw new IOException(&quot;Invalid response &quot; + response.getStatusLine().getStatusCode()); } String result = getContent(response); return result; } private HttpGet newGet(String url) { HttpGet get = new HttpGet(); addHttpHeaders(get); return get; } private void addHttpHeaders(HttpRequestBase method) { method.addHeader(&quot;Accept&quot;, &quot;application/json, text/plain, */*&quot;); method.addHeader(&quot;Content-Type&quot;, &quot;application/json&quot;); String basicAuth = DatatypeConverter .printBase64Binary((sylinUser + &quot;:&quot; + sylinPassWord).getBytes(StandardCharsets.UTF_8)); method.addHeader(&quot;Authorization&quot;, &quot;Basic &quot; + basicAuth); } private String getContent(HttpResponse response) throws IOException { InputStreamReader reader = null; BufferedReader rd = null; StringBuffer result = new StringBuffer(); try { reader = new InputStreamReader(response.getEntity().getContent(), StandardCharsets.UTF_8); rd = new BufferedReader(reader); String line = null; while ((line = rd.readLine()) != null) { result.append(line); } } finally { IOUtils.closeQuietly(reader); IOUtils.closeQuietly(rd); } return result.toString(); } } mondrian3.0语法工具类 package org.saiku.service.util; import org.apache.kylin.cube.model.CubeDesc; import org.apache.kylin.cube.model.DimensionDesc; import org.apache.kylin.metadata.model.*; import java.util.*; public class SchemaUtil1 { private static String newLine = &quot;\\r\\n&quot;; private static Set&lt;String&gt; aggSet = new HashSet&lt;String&gt;(){ { add(&quot;sum&quot;); add(&quot;min&quot;); add(&quot;max&quot;); add(&quot;count&quot;); add(&quot;count_distinct&quot;); } }; public static String createSchema(String dataSourceName, CubeDesc cubeDesc, DataModelDesc modelDesc) { StringBuffer sb = new StringBuffer(); sb = appendSchema(sb, dataSourceName, cubeDesc, modelDesc); return sb.toString(); } public static StringBuffer appendSchema(StringBuffer sb, String dataSourceName, CubeDesc cubeDesc, DataModelDesc modelDesc) { sb.append(&quot;&lt;?xml version='1.0'?&gt;&quot;).append(newLine) .append(&quot;&lt;Schema name='&quot; + dataSourceName.split(&quot;#&quot;)[0].trim()+&quot;'&gt;&quot;) .append(newLine); sb = appendCube(sb, dataSourceName, cubeDesc, modelDesc); sb.append(&quot;&lt;/Schema&gt;&quot;).append(newLine); return sb; } public static StringBuffer appendCube(StringBuffer sb, String cubeName, CubeDesc cubeDesc, DataModelDesc modelDesc) { sb.append(&quot;&lt;Cube name='&quot; + cubeName.split(&quot;#&quot;)[1] + &quot;'&gt;&quot;).append(newLine); sb.append(&quot;&lt;Table name='&quot;+dealTableName(modelDesc.getRootFactTableName())+&quot;'/&gt;&quot;).append(newLine); HashMap&lt;String,String&gt; aliasTableMap = new HashMap&lt;String,String&gt;(); HashMap&lt;String,String&gt; aliasTableJoinMap = new HashMap&lt;String,String&gt;(); aliasTableMap.put(dealTableName(modelDesc.getRootFactTableName()),dealTableName(modelDesc.getRootFactTableName())); for(JoinTableDesc joinTableDesc:modelDesc.getJoinTables()) { aliasTableMap.put(joinTableDesc.getAlias(),dealTableName(joinTableDesc.getTable())); if(joinTableDesc.getJoin().getPrimaryKey().length == 1){ aliasTableJoinMap.put(dealTableName(joinTableDesc.getAlias()),joinTableDesc.getJoin().getPrimaryKey()[0].concat(&quot;#&quot;).concat(joinTableDesc.getJoin().getForeignKey()[0])); } } for(DimensionDesc dimensionDesc: cubeDesc.getDimensions()){ // tableSet.add(dealTableName(dimensionDesc.getTable())); if(aliasTableMap.get(dealTableName(dimensionDesc.getTable())).equals(dealTableName(modelDesc.getRootFactTableName()))){ sb.append(&quot;&lt;Dimension name='&quot;+dimensionDesc.getName()+&quot;'&gt;&quot;).append(newLine); sb.append(&quot;&lt;Hierarchy name='&quot;+dimensionDesc.getName()+&quot;' hasAll='true' allMemberName='All &quot;+dimensionDesc.getName()+&quot;'&gt;&quot;).append(newLine); }else if(aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable()))==null){ continue; } else if(aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable()))!=null &amp;&amp; aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable())).split(&quot;#&quot;)[1].split(&quot;\\\\.&quot;)[0] .equals(dealTableName(modelDesc.getRootFactTableName()))){ sb.append(&quot;&lt;Dimension name='&quot;+dimensionDesc.getName()+&quot;' foreignKey='&quot;+aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable())) .split(&quot;#&quot;)[1].split(&quot;\\\\.&quot;)[1]+&quot;'&gt;&quot;).append(newLine); sb.append(&quot;&lt;Hierarchy name='&quot;+dimensionDesc.getName()+&quot;' hasAll='true' allMemberName='All &quot;+dimensionDesc.getName()+&quot;' primaryKey='&quot;+ aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable())).split(&quot;#&quot;)[0].split(&quot;\\\\.&quot;)[1]+&quot;'&gt;&quot;).append(newLine); sb.append(&quot;&lt;Table name='&quot;+aliasTableMap.get(dealTableName(dimensionDesc.getTable()))+&quot;'/&gt;&quot;).append(newLine); } else if(aliasTableJoinMap.get(aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable())).split(&quot;#&quot;)[1].split(&quot;\\\\.&quot;)[0]) .split(&quot;#&quot;)[1].split(&quot;\\\\.&quot;)[0].equals(dealTableName(modelDesc.getRootFactTableName()))){ sb.append(&quot;&lt;Dimension name='&quot;+dimensionDesc.getName()+&quot;' foreignKey='&quot;+aliasTableJoinMap.get(aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable())) .split(&quot;#&quot;)[1].split(&quot;\\\\.&quot;)[0]).split(&quot;#&quot;)[1].split(&quot;\\\\.&quot;)[1]+&quot;'&gt;&quot;).append(newLine); sb.append(&quot;&lt;Hierarchy name='&quot;+dimensionDesc.getName()+&quot;' hasAll='true' allMemberName='All &quot;+dimensionDesc.getName()+&quot;' primaryKey='&quot;+ aliasTableJoinMap.get(aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable())).split(&quot;#&quot;)[1].split(&quot;\\\\.&quot;)[0]) .split(&quot;#&quot;)[0].split(&quot;\\\\.&quot;)[1]+&quot;' primaryKeyTable='&quot;+aliasTableMap.get(aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable())) .split(&quot;#&quot;)[1].split(&quot;\\\\.&quot;)[0])+&quot;'&gt;&quot;).append(newLine); sb.append(&quot;&lt;Join leftKey='&quot;+aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable())).split(&quot;#&quot;)[1].split(&quot;\\\\.&quot;)[1]+&quot;' rightAlias='&quot;+aliasTableMap.get(dimensionDesc.getTable())+ &quot;' rightKey='&quot;+aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable())).split(&quot;#&quot;)[0].split(&quot;\\\\.&quot;)[1]+&quot;'&gt;&quot;).append(newLine); sb.append(&quot;&lt;Table name='&quot;+aliasTableMap.get(aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable())).split(&quot;#&quot;)[1].split(&quot;\\\\.&quot;)[0])+&quot;'/&gt;&quot;).append(newLine); sb.append(&quot;&lt;Table name='&quot;+aliasTableMap.get(dimensionDesc.getTable())+&quot;'/&gt;&quot;).append(newLine); sb.append(&quot;&lt;/Join&gt;&quot;).append(newLine); } else { continue; } Set&lt;String&gt; columns = getColumns(dimensionDesc); for(String column:columns){ sb.append(&quot;&lt;Level name='&quot;+column+&quot;' column='&quot;+column+&quot;' table='&quot;+aliasTableMap.get(dimensionDesc.getTable())+&quot;'/&gt;&quot;).append(newLine); } sb.append(&quot;&lt;/Hierarchy&gt;&quot;).append(newLine); sb.append(&quot;&lt;/Dimension&gt;&quot;).append(newLine); } for (MeasureDesc measureDesc : cubeDesc.getMeasures()) { int i=0; String table = measureDesc.getFunction().getParameter().getValue().split(&quot;\\\\.&quot;)[0]; final boolean flag = Arrays.stream(modelDesc.getJoinTables()).anyMatch(item -&gt; table.equals(dealTableName(item.getTable()))); if(table.equals(dealTableName(modelDesc.getRootFactTableName()))||flag||table.equals(&quot;1&quot;)){ addMeasure(sb,measureDesc,getColumn(cubeDesc,modelDesc,dealTableName(modelDesc.getRootFactTableName()))); } } sb.append(&quot;&lt;/Cube&gt;&quot;).append(newLine); return sb; } public static String dealTableName(String tableName){ if(tableName.contains(&quot;.&quot;)) return tableName.split(&quot;\\\\.&quot;)[1]; else return tableName; } public static StringBuffer addMeasure(StringBuffer sb, MeasureDesc measureDesc, String defaultColumn) { FunctionDesc funtionDesc = measureDesc.getFunction(); String aggregator = funtionDesc.getExpression().trim().toLowerCase(); if(aggSet.contains(aggregator.toLowerCase())){ //mondrian only have distinct-count if(aggregator.equals(&quot;count_distinct&quot;)){ aggregator = &quot;distinct-count&quot;; } if(funtionDesc.getParameter().getValue().equals(&quot;1&quot;)) { sb.append(&quot;&lt;Measure aggregator='&quot; + aggregator + &quot;' column='&quot; + defaultColumn + &quot;' name='&quot; + measureDesc.getName() + &quot;' visible='true'/&gt;&quot;) .append(newLine); } else sb.append(&quot;&lt;Measure aggregator='&quot; + aggregator + &quot;' column='&quot; + funtionDesc.getParameter().getValue().split(&quot;\\\\.&quot;)[1] + &quot;' name='&quot; + measureDesc.getName() + &quot;' visible='true'/&gt;&quot;) .append(newLine); return sb; } return sb; } public static String getColumn(CubeDesc cubeDesc,DataModelDesc dataModelDesc,String tableName){ List&lt;MeasureDesc&gt; measureDescList = cubeDesc.getMeasures(); for(MeasureDesc measureDesc:measureDescList){ if(measureDesc.getFunction().getParameter().getValue().split(&quot;\\\\.&quot;)[0].equals(tableName)){ return measureDesc.getFunction().getParameter().getValue().split(&quot;\\\\.&quot;)[1]; } } if(dataModelDesc.getMetrics().length&gt;0){ return dataModelDesc.getMetrics()[0]; } for(ModelDimensionDesc modelDimensionDesc:dataModelDesc.getDimensions()){ if(modelDimensionDesc.getTable().equals(tableName)){ return modelDimensionDesc.getColumns()[0]; } } return null; } public static Set&lt;String&gt; getColumns(DimensionDesc dimensionDesc){ Set&lt;String&gt; columns = new HashSet&lt;String&gt;(); if (dimensionDesc.getColumn() != null || dimensionDesc.getDerived() != null) { if(dimensionDesc.getColumn() != null) { columns.add(dimensionDesc.getColumn()); } if (dimensionDesc.getDerived() != null) { for (String derived : dimensionDesc.getDerived()) { columns.add(derived); } } } else { columns.add(dimensionDesc.getName()); } return columns; } } &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;Schema name=&quot;Mondrian&quot;&gt; &lt;!--模型定义--&gt; &lt;Cube name=&quot;Person&quot;&gt; &lt;!--立方体 ，一个立方体有多个维度--&gt; &lt;Table name=&quot;PERSON&quot; /&gt; &lt;!--立方体对应的表 --&gt; &lt;Dimension name=&quot;部门&quot; foreignKey=&quot;USERID&quot; &gt; &lt;!--定义维度 --&gt; &lt;Hierarchy hasAll=&quot;true&quot; primaryKey=&quot;USERID&quot; allMemberName=&quot;所有部门&quot; &gt; &lt;!--定义维度下面的层次，层次包含很多层 --&gt; &lt;Table name=&quot;PERSON&quot; alias=&quot;a&quot;/&gt; &lt;!--定义维度获取数据的来源-表 --&gt; &lt;Level name=&quot;部门&quot; column=&quot;DEPARTMENT&quot; uniqueMembers=&quot;true&quot; /&gt; &lt;!--定义层次的层，每个层对应数据库中对应的字段 --&gt; &lt;Level name=&quot;姓名&quot; column=&quot;USERNAME&quot; uniqueMembers=&quot;true&quot; /&gt; &lt;/Hierarchy&gt; &lt;/Dimension&gt; &lt;Dimension name=&quot;性别&quot; foreignKey=&quot;USERID&quot; &gt; &lt;Hierarchy hasAll=&quot;true&quot; primaryKey=&quot;USERID&quot; allMemberName=&quot;所有性别&quot;&gt; &lt;Table name=&quot;PERSON&quot; alias=&quot;b&quot; /&gt; &lt;Level name=&quot;性别&quot; column=&quot;SEX&quot; uniqueMembers=&quot;true&quot; /&gt; &lt;/Hierarchy&gt; &lt;/Dimension&gt; &lt;Dimension name=&quot;专业技术资格类别&quot; foreignKey=&quot;USERID&quot; &gt; &lt;Hierarchy hasAll=&quot;true&quot; primaryKey=&quot;USERID&quot; allMemberName=&quot;所有专业技术资格类别&quot;&gt; &lt;Table name=&quot;PERSON&quot; alias=&quot;c&quot; /&gt; &lt;Level name=&quot;资格类别&quot; column=&quot;ZYJSLB&quot; uniqueMembers=&quot;true&quot; /&gt; &lt;/Hierarchy&gt; &lt;/Dimension&gt; &lt;Dimension name=&quot;专业技术资格等级&quot; foreignKey=&quot;USERID&quot; &gt; &lt;Hierarchy hasAll=&quot;true&quot; primaryKey=&quot;USERID&quot; allMemberName=&quot;所有专业技术资格等级&quot;&gt; &lt;Table name=&quot;PERSON&quot; alias=&quot;d&quot; /&gt; &lt;Level name=&quot;资格等级&quot; column=&quot;ZYJSDJ&quot; uniqueMembers=&quot;true&quot; /&gt; &lt;/Hierarchy&gt; &lt;/Dimension&gt; &lt;Dimension name=&quot;职系&quot; foreignKey=&quot;USERID&quot; &gt; &lt;Hierarchy hasAll=&quot;true&quot; primaryKey=&quot;USERID&quot; allMemberName=&quot;所有职系&quot;&gt; &lt;Table name=&quot;PERSON&quot; alias=&quot;e&quot; /&gt; &lt;Level name=&quot;职系&quot; column=&quot;ZHIXI&quot; uniqueMembers=&quot;true&quot; /&gt; &lt;/Hierarchy&gt; &lt;/Dimension&gt; &lt;Dimension name=&quot;民族&quot; foreignKey=&quot;USERID&quot; &gt; &lt;Hierarchy hasAll=&quot;true&quot; primaryKey=&quot;USERID&quot; allMemberName=&quot;所有民族&quot;&gt; &lt;Table name=&quot;PERSON&quot; alias=&quot;f&quot; /&gt; &lt;Level name=&quot;民族&quot; column=&quot;NATIONALITY&quot; uniqueMembers=&quot;true&quot; /&gt; &lt;/Hierarchy&gt; &lt;/Dimension&gt; &lt;Dimension name=&quot;学历&quot; foreignKey=&quot;USERID&quot; &gt; &lt;Hierarchy hasAll=&quot;true&quot; primaryKey=&quot;USERID&quot; allMemberName=&quot;所有学历&quot;&gt; &lt;Table name=&quot;PERSON&quot; alias=&quot;g&quot; /&gt; &lt;Level name=&quot;学历&quot; column=&quot;XUELI&quot; uniqueMembers=&quot;true&quot; /&gt; &lt;/Hierarchy&gt; &lt;/Dimension&gt; &lt;Measure name=&quot;人数&quot; column=&quot;USERID&quot; aggregator=&quot;distinct count&quot; /&gt; &lt;!--指标/度量，采用distinct count聚合 --&gt; &lt;/Cube&gt; &lt;/Schema&gt; mondrian4.0语法工具类 package org.saiku.service.util; import org.apache.kylin.cube.model.CubeDesc; import org.apache.kylin.cube.model.DimensionDesc; import org.apache.kylin.cube.model.RowKeyDesc; import org.apache.kylin.metadata.model.*; import java.util.*; public class SchemaUtil { private static final String newLine = &quot;\\r\\n&quot;; private static Map&lt;String, String&gt; map; public static String createSchema(String dataSourceName, CubeDesc cubeDesc, DataModelDesc modelDesc) { StringBuffer sb = new StringBuffer(); sb = appendSchema(sb, dataSourceName, cubeDesc, modelDesc); // System.out.println(&quot;********************************&quot; + sb.toString()); return sb.toString(); } public static StringBuffer appendSchema(StringBuffer sb, String dataSourceName, CubeDesc cubeDesc, DataModelDesc modelDesc) { sb.append(&quot;&lt;?xml version='1.0'?&gt;&quot;).append(newLine) .append(&quot;&lt;Schema name='&quot;).append(dataSourceName.split(&quot;#&quot;)[0].trim()).append(&quot;' metamodelVersion='4.0'&gt;&quot;) // .append(&quot;&lt;Schema name='&quot; + dataSourceName + &quot;' metamodelVersion='4.0'&gt;&quot;) .append(newLine); appendTable(sb, modelDesc); appendDimension(sb, modelDesc); appendCube(sb, dataSourceName, cubeDesc, modelDesc); sb.append(&quot;&lt;/Schema&gt;&quot;).append(newLine); return sb; } public static StringBuffer appendTable(StringBuffer sb, DataModelDesc modelDesc) { StringBuilder linkSb = new StringBuilder(); //获取所有关系表 final List&lt;ModelDimensionDesc&gt; tables = modelDesc.getDimensions(); sb.append(&quot;&lt;PhysicalSchema&gt;&quot;).append(newLine); //添加表关联关系 final JoinTableDesc[] joinTables = modelDesc.getJoinTables(); for (JoinTableDesc joinTableDesc : joinTables) { String factTablename = dealModelTableName(joinTableDesc.getJoin().getForeignKey()[0]); String Column = dealTableName(joinTableDesc.getJoin().getForeignKey()[0]); String joinTablename = dealModelTableName(joinTableDesc.getJoin().getPrimaryKey()[0]); String joinColumn = dealTableName(joinTableDesc.getJoin().getPrimaryKey()[0]); linkSb.append(&quot;&lt;Link source='&quot;).append(factTablename).append(&quot;' target='&quot;).append(joinTablename).append(&quot;'&gt;&quot;).append(newLine) .append(&quot;&lt;ForeignKey&gt;&quot;).append(newLine) .append(&quot;&lt;Column name='&quot;).append(Column).append(&quot;'/&gt;&quot;).append(newLine) .append(&quot;&lt;/ForeignKey&gt;&quot;).append(newLine) .append(&quot;&lt;/Link&gt;&quot;).append(newLine); //清空map map = new HashMap&lt;&gt;(); tables.forEach(item -&gt; { if (item.getTable().equals(factTablename)) { map.put(factTablename, Column); } if (item.getTable().equals(joinTablename)) { map.put(joinTablename, joinColumn); } }); } //添加表 for (String tableName : map.keySet()) { sb.append(&quot;&lt;Table name='&quot;).append(tableName).append(&quot;'&gt;&quot;).append(newLine) .append(&quot;&lt;Key&gt;&quot;).append(newLine).append(&quot;&lt;Column name='&quot;).append(map.get(tableName)).append(&quot;'/&gt;&quot;).append(newLine) .append(&quot;&lt;/Key&gt;&quot;).append(newLine) .append(&quot;&lt;/Table&gt;&quot;).append(newLine); } sb.append(linkSb); linkSb.delete(0, linkSb.length()); sb.append(&quot;&lt;/PhysicalSchema&gt;&quot;).append(newLine); return sb; } public static Map&lt;String, JoinDesc&gt; getJoinDesc(DataModelDesc modelDesc) { Map&lt;String, JoinDesc&gt; joinDescMap = new HashMap&lt;String, JoinDesc&gt;(); for (JoinTableDesc lookupDesc : modelDesc.getJoinTables()) { if (!joinDescMap.containsKey(dealTableName(lookupDesc.getTable()))) joinDescMap.put(dealTableName(lookupDesc.getTable()), lookupDesc.getJoin()); } return joinDescMap; } public static void appendDimension(StringBuffer sb, DataModelDesc modelDesc) { StringBuilder hierSb = new StringBuilder(); for (ModelDimensionDesc dimensionDesc : modelDesc.getDimensions()) { sb.append(&quot;&lt;Dimension name='&quot;).append(dimensionDesc.getTable()).append(&quot;' key='&quot;).append(map.get(dimensionDesc.getTable())).append(&quot;' table='&quot;).append(dimensionDesc.getTable()).append(&quot;'&gt;&quot;).append(newLine); sb.append(&quot;&lt;Attributes&gt;&quot;).append(newLine); hierSb.append(&quot;&lt;Hierarchies&gt;&quot;).append(newLine); hierSb.append(&quot;&lt;Hierarchy name='&quot;).append(dimensionDesc.getTable()).append(&quot;' hasAll='true'&gt;&quot;).append(newLine); for (String column : dimensionDesc.getColumns()) { // add Attributes to stringbuffer addAttribute(sb, column); addHierarchy(hierSb, column); } sb.append(&quot;&lt;/Attributes&gt;&quot;).append(newLine); hierSb.append(&quot;&lt;/Hierarchy&gt;&quot;).append(newLine); hierSb.append(&quot;&lt;/Hierarchies&gt;&quot;).append(newLine); sb.append(hierSb); hierSb.delete(0, hierSb.length()); sb.append(&quot;&lt;/Dimension&gt;&quot;).append(newLine); } } public static Set&lt;String&gt; getColumns(DimensionDesc dimensionDesc) { Set&lt;String&gt; columns = new HashSet&lt;String&gt;(); if (dimensionDesc.getColumn() != null || dimensionDesc.getDerived() != null) { if (dimensionDesc.getColumn() != null) { // for (String column : dimensionDesc.getColumn()) { columns.add(dimensionDesc.getColumn()); // } } if (dimensionDesc.getDerived() != null) { columns.addAll(Arrays.asList(dimensionDesc.getDerived())); } } else { columns.add(dimensionDesc.getName()); } return columns; } public static StringBuffer addAttribute(StringBuffer sb, String attr) { sb.append(&quot;&lt;Attribute name='&quot;).append(attr).append(&quot;' keyColumn='&quot;).append(attr).append(&quot;' hasHierarchy='false'/&gt;&quot;).append(newLine); return sb; } public static void addHierarchy(StringBuilder sb, String attr) { sb.append(&quot;&lt;Level attribute='&quot;).append(attr).append(&quot;'/&gt;&quot;).append(newLine); } public static String dealTableName(String tableName) { if (tableName.contains(&quot;.&quot;)) return tableName.split(&quot;\\\\.&quot;)[1]; else return tableName; } public static String dealModelTableName(String tableName) { if (tableName.contains(&quot;.&quot;)) return tableName.split(&quot;\\\\.&quot;)[0]; else return tableName; } public static StringBuffer appendCube(StringBuffer sb, String cubeName, CubeDesc cubeDesc, DataModelDesc modelDesc) { sb.append(&quot;&lt;Cube name='&quot;).append(cubeName.split(&quot;#&quot;)[1].trim()).append(&quot;'&gt;&quot;).append(newLine); addCubeDimension(sb, modelDesc.getDimensions()); sb.append(&quot;&lt;MeasureGroups&gt;&quot;).append(newLine); Map&lt;String, Map&lt;String, MeasureDesc&gt;&gt; allMap = new HashMap(); MeasureDesc one = new MeasureDesc(); for (MeasureDesc measureDesc : cubeDesc.getMeasures()) { final String tableName = dealModelTableName(measureDesc.getFunction().getParameter().getValue()); final String columnName = dealTableName(measureDesc.getFunction().getParameter().getValue()); if (&quot;1&quot;.equals(tableName)) { one = measureDesc; } else { if (Objects.isNull(allMap.get(tableName))) { Map&lt;String, MeasureDesc&gt; map = new HashMap(); if (Objects.nonNull(one)) { map.put(columnName, one); one = null; } map.put(columnName, measureDesc); allMap.put(tableName, map); } else { allMap.get(tableName).put(columnName, measureDesc); } } } allMap.forEach((tableName, value) -&gt; { sb.append(&quot;&lt;MeasureGroup table='&quot;).append(dealTableName(tableName)).append(&quot;'&gt;&quot;).append(newLine); addDimensionLink(sb, modelDesc); sb.append(&quot;&lt;Measures&gt;&quot;).append(newLine); value.forEach((columnName, measureDesc) -&gt; { addMeasure(sb, measureDesc, getColumn(cubeDesc)); }); sb.append(&quot;&lt;/Measures&gt;&quot;).append(newLine); sb.append(&quot;&lt;/MeasureGroup&gt;&quot;).append(newLine); }); sb.append(&quot;&lt;/MeasureGroups&gt;&quot;).append(newLine); sb.append(&quot;&lt;/Cube&gt;&quot;).append(newLine); return sb; } public static void addCubeDimension(StringBuffer sb, List&lt;ModelDimensionDesc&gt; dimensionDescs) { sb.append(&quot;&lt;Dimensions&gt;&quot;).append(newLine); for (ModelDimensionDesc dimensionDesc : dimensionDescs) { sb.append(&quot;&lt;Dimension source='&quot;).append(dimensionDesc.getTable()).append(&quot;' visible='true'/&gt;&quot;).append(newLine); } sb.append(&quot;&lt;/Dimensions&gt;&quot;).append(newLine); } public static void addDimensionLink(StringBuffer sb, DataModelDesc modelDesc) { sb.append(&quot;&lt;DimensionLinks&gt;&quot;).append(newLine); for(ModelDimensionDesc dimensionDesc : modelDesc.getDimensions()) { if(dimensionDesc.getTable().contains(dealTableName(modelDesc.getRootFactTableName()))) { sb.append(&quot;&lt;FactLink dimension='&quot;).append(dimensionDesc.getTable()).append(&quot;'/&gt;&quot;).append(newLine); }else{ sb.append(&quot;&lt;ForeignKeyLink dimension='&quot;).append(dimensionDesc.getTable()).append(&quot;' foreignKeyColumn='&quot;).append(map.get(dimensionDesc.getTable())).append(&quot;'/&gt;&quot;).append(newLine); } } sb.append(&quot; &lt;/DimensionLinks&gt;&quot;).append(newLine); } public static StringBuffer addMeasure(StringBuffer sb, MeasureDesc measureDesc, String defaultColumn) { FunctionDesc funtionDesc = measureDesc.getFunction(); String aggregator = funtionDesc.getExpression().trim().toLowerCase(); //mondrian only have distinct-count if (aggregator.equals(&quot;count_distinct&quot;)) { aggregator = &quot;distinct-count&quot;; } if (funtionDesc.getParameter().getValue().equals(&quot;1&quot;)) { sb.append(&quot;&lt;Measure aggregator='&quot;).append(aggregator).append(&quot;' column='&quot;).append(dealTableName(defaultColumn)).append(&quot;' name='&quot;).append(measureDesc.getName()).append(&quot;' visible='true'/&gt;&quot;) .append(newLine); } else sb.append(&quot;&lt;Measure aggregator='&quot;).append(aggregator).append(&quot;' column='&quot;).append(dealTableName(funtionDesc.getParameter().getValue())).append(&quot;' name='&quot;).append(measureDesc.getName()).append(&quot;' visible='true'/&gt;&quot;) .append(newLine); return sb; } public static Set&lt;String&gt; getTables(List&lt;DimensionDesc&gt; dimensionDescList) { Set&lt;String&gt; tables = new HashSet&lt;String&gt;(); for (DimensionDesc dimensionDesc : dimensionDescList) { String table = dealTableName(dimensionDesc.getTable()); tables.add(table); } return tables; } public static String getColumn(CubeDesc cubeDesc) { RowKeyDesc rowKey = cubeDesc.getRowkey(); return rowKey.getRowKeyColumns()[0].getColumn(); } } &lt;?xml version='1.0'?&gt; &lt;Schema name='xiaowei' metamodelVersion='4.0'&gt; &lt;PhysicalSchema&gt; &lt;Table name='TBL_FARM_INCOME_STATICS'&gt; &lt;Key&gt; &lt;Column name='COMPANY_ID'/&gt; &lt;/Key&gt; &lt;/Table&gt; &lt;Table name='TBL_CUSTOMER'&gt; &lt;Key&gt; &lt;Column name='COMPANY_ID'/&gt; &lt;/Key&gt; &lt;/Table&gt; &lt;Link source='TBL_FARM_INCOME_STATICS' target='TBL_CUSTOMER'&gt; &lt;ForeignKey&gt; &lt;Column name='COMPANY_ID'/&gt; &lt;/ForeignKey&gt; &lt;/Link&gt; &lt;/PhysicalSchema&gt; &lt;Dimension name='TBL_FARM_INCOME_STATICS' table='TBL_FARM_INCOME_STATICS' key='COMPANY_ID'&gt; &lt;Attributes&gt; &lt;Attribute name='COMPANY_ID' keyColumn='COMPANY_ID' hasHierarchy='false'/&gt; &lt;Attribute name='INCOME_DATE' keyColumn='INCOME_DATE' hasHierarchy='false'/&gt; &lt;/Attributes&gt; &lt;Hierarchies&gt; &lt;Hierarchy name='TBL_FARM_INCOME_STATICS' allMemberName='All Warehouses'&gt; &lt;Level attribute='COMPANY_ID'/&gt; &lt;Level attribute='INCOME_DATE'/&gt; &lt;/Hierarchy&gt; &lt;/Hierarchies&gt; &lt;/Dimension&gt; &lt;Dimension name='TBL_CUSTOMER' table='TBL_CUSTOMER' key='COMPANY_ID'&gt; &lt;Attributes&gt; &lt;Attribute name='COMPANY_ID' keyColumn='COMPANY_ID' hasHierarchy='false'/&gt; &lt;Attribute name='CUSTOMER_TYPE' keyColumn='CUSTOMER_TYPE' hasHierarchy='false'/&gt; &lt;Attribute name='PHONE' keyColumn='PHONE' hasHierarchy='false'/&gt; &lt;/Attributes&gt; &lt;Hierarchies&gt; &lt;Hierarchy name='TBL_CUSTOMER' allMemberName='All Warehouses'&gt; &lt;Level attribute='COMPANY_ID'/&gt; &lt;Level attribute='CUSTOMER_TYPE'/&gt; &lt;Level attribute='PHONE'/&gt; &lt;/Hierarchy&gt; &lt;/Hierarchies&gt; &lt;/Dimension&gt; &lt;Cube name='tbl_farm_income_statics'&gt; &lt;Dimensions&gt; &lt;Dimension source='TBL_FARM_INCOME_STATICS' visible='true'/&gt; &lt;Dimension source='TBL_CUSTOMER' visible='true'/&gt; &lt;/Dimensions&gt; &lt;MeasureGroups&gt; &lt;MeasureGroup table='TBL_CUSTOMER'&gt; &lt;Measures&gt; &lt;Measure aggregator='sum' column='CONSUME_AMMOUT' name='CONSUME_AMMOUT_SUM' visible='true'/&gt; &lt;/Measures&gt; &lt;DimensionLinks&gt; &lt;FactLink dimension='TBL_FARM_INCOME_STATICS'/&gt; &lt;ForeignKeyLink dimension='TBL_CUSTOMER' foreignKeyColumn='COMPANY_ID'/&gt; &lt;/DimensionLinks&gt; &lt;/MeasureGroup&gt; &lt;MeasureGroup table='TBL_FARM_INCOME_STATICS'&gt; &lt;Measures&gt; &lt;Measure aggregator='count' column='COMPANY_ID' name='_COUNT_' visible='true'/&gt; &lt;Measure aggregator='sum' column='REFUND_AMOUNT' name='REFUND_AMOUNT_SUM' visible='true'/&gt; &lt;Measure aggregator='sum' column='COMMON_REFUND_AMOUNT' name='COMMON_REFUND_AMOUNT_SUM' visible='true'/&gt; &lt;Measure aggregator='sum' column='COMMON_INCOME' name='COMMON_INCOME_SUM' visible='true'/&gt; &lt;Measure aggregator='sum' column='SALE_AMOUNT' name='SALE_AMOUNT_SUM' visible='true'/&gt; &lt;Measure aggregator='sum' column='RENEW_AMOUNT' name='RENEW_AMOUNT_SUM' visible='true'/&gt; &lt;Measure aggregator='sum' column='TOTAL_AMOUNT' name='TOTAL_AMOUNT_SUM' visible='true'/&gt; &lt;/Measures&gt; &lt;DimensionLinks&gt; &lt;FactLink dimension='TBL_FARM_INCOME_STATICS'/&gt; &lt;ForeignKeyLink dimension='TBL_CUSTOMER' foreignKeyColumn='COMPANY_ID'/&gt; &lt;/DimensionLinks&gt; &lt;/MeasureGroup&gt; &lt;/MeasureGroups&gt; &lt;/Cube&gt; &lt;/Schema&gt; ","link":"https://tinaxiawuhao.github.io/post/0wQHSTDWt/"},{"title":"同步 异步 阻塞 非阻塞","content":"IO操作 IO分两阶段（一旦拿到数据后就变成了数据操作，不再是IO）： 1.数据准备阶段 2.内核空间复制数据到用户进程缓冲区（用户空间）阶段 在操作系统中，程序运行的空间分为内核空间和用户空间。 应用程序都是运行在用户空间的，所以它们能操作的数据也都在用户空间。 阻塞IO和非阻塞IO的区别在于第一步发起IO请求是否会被阻塞： 如果阻塞直到完成那么就是传统的阻塞IO，如果不阻塞，那么就是非阻塞IO。 一般来讲： 阻塞IO模型、非阻塞IO模型、IO复用模型(select/poll/epoll)、信号驱动IO模型都属于同步IO，因为阶段2是阻塞的(尽管时间很短)。 同步IO和异步IO的区别就在于第二个步骤是否阻塞： 如果不阻塞，而是操作系统帮你做完IO操作再将结果返回给你，那么就是异步IO 同步和异步IO 阻塞和非阻塞IO 同步和异步IO的概念： 同步是用户线程发起I/O请求后需要等待或者轮询内核I/O操作完成后才能继续执行 异步是用户线程发起I/O请求后仍需要继续执行，当内核I/O操作完成后会通知用户线程，或者调用用户线程注册的回调函数 阻塞和非阻塞IO的概念： 阻塞是指I/O操作需要彻底完成后才能返回用户空间 非阻塞是指I/O操作被调用后立即返回一个状态值，无需等I/O操作彻底完成 同步与异步（线程间调用） 同步与异步是对应于调用者与被调用者，它们是线程之间的关系，两个线程之间要么是同步的，要么是异步的 同步操作时，调用者需要等待被调用者返回结果，才会进行下一步操作 而异步则相反，调用者不需要等待被调用者返回调用，即可进行下一步操作，被调用者通常依靠事件、回调等机制来通知调用者结果 阻塞与非阻塞（线程内调用） 阻塞与非阻塞是对同一个线程来说的，在某个时刻，线程要么处于阻塞，要么处于非阻塞 阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态： 阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。 非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。 同步与异步调用/线程/通信 同步就是两种东西通过一种机制实现步调一致，异步是两种东西不必步调一致 一、同步调用与异步调用： 在用在调用场景中，无非是对调用结果的不同处理。 同步调用就是调用一但返回，就能知道结果，而异步是返回时不一定知道结果，还得通过其他机制来获知结果，如： a. 状态 b. 通知 c. 回调函数 二、同步线程与异步线程： 同步线程：即两个线程步调要一致，其中一个线程可能要阻塞等待另外一个线程的运行，要相互协商。快的阻塞一下等到慢的步调一致。 异步线程：步调不用一致，各自按各自的步调运行，不受另一个线程的影响。 三、同步通信与异步通信： 同步和异步是指：发送方和接收方是否协调步调一致 同步通信是指：发送方和接收方通过一定机制，实现收发步调协调。 如：发送方发出数据后，等接收方发回响应以后才发下一个数据包的通讯方式 异步通信是指：发送方的发送不管接收方的接收状态。 如：发送方发出数据后，不等接收方发回响应，接着发送下个数据包的通讯方式。 阻塞可以是实现同步的一种手段！例如两个东西需要同步，一旦出现不同步情况，我就阻塞快的一方，使双方达到同步。 同步是两个对象之间的关系，而阻塞是一个对象的状态。 四种组合方式 同步阻塞方式： 发送方发送请求之后一直等待响应。 接收方处理请求时进行的IO操作如果不能马上等到返回结果，就一直等到返回结果后，才响应发送方，期间不能进行其他工作。 同步非阻塞方式： 发送方发送请求之后，一直等待响应。 接受方处理请求时进行的IO操作如果不能马上的得到结果，就立即返回，取做其他事情。 但是由于没有得到请求处理结果，不响应发送方，发送方一直等待。 当IO操作完成以后，将完成状态和结果通知接收方，接收方再响应发送方，发送方才进入下一次请求过程。（实际不应用） 异步阻塞方式： 发送方向接收方请求后，不等待响应，可以继续其他工作。 接收方处理请求时进行IO操作如果不能马上得到结果，就一直等到返回结果后，才响应发送方，期间不能进行其他操作。 （实际不应用） 异步非阻塞方式： 发送方向接收方请求后，不等待响应，可以继续其他工作。 接收方处理请求时进行IO操作如果不能马上得到结果，也不等待，而是马上返回去做其他事情。 当IO操作完成以后，将完成状态和结果通知接收方，接收方再响应发送方。（效率最高） ","link":"https://tinaxiawuhao.github.io/post/1SOcADwUR/"},{"title":"NIO-FileChannel","content":"NIO介绍 在讲解Channel之前，首先了解一下NIO， Java NIO全称java non-blocking IO，是从Java 1.4版本开始引入的一个新的IO API（New IO），可以替代标准的Java IO API，NIO与原来的IO有同样的作用和目的，但是使用的方式完全不同。IO与 NIO区别： IO NIO 面向流（Stream Orientend） 面向缓冲区（Buffer Orientend） 阻塞IO（Blocking IO ) 非阻塞IO（Non Blocking IO） 选择器（Selector） NIO支持面向缓冲区的、基于通道的IO操作并以更加高效的方式进行文件的读写操作，其核心API为Channel(通道)，Buffer(缓冲区), Selector(选择器)。Channel负责传输，Buffer负责存储 。 缓冲区 public class BioTest { @Test public void test1() { //1.初始化缓冲区数组 ByteBuffer bf = ByteBuffer.allocate(1024); System.out.println(&quot;==========allocate============&quot;); System.out.println(bf.position()); System.out.println(bf.limit()); System.out.println(bf.capacity()); //2.put插入数据 bf.put(&quot;asasas&quot;.getBytes()); System.out.println(&quot;==========put============&quot;); System.out.println(bf.position()); System.out.println(bf.limit()); System.out.println(bf.capacity()); //3.改为读状态 bf.flip(); System.out.println(&quot;==========flip============&quot;); System.out.println(bf.position()); System.out.println(bf.limit()); System.out.println(bf.capacity()); //4.获取缓冲区数据 final byte[] bytes = new byte[bf.limit()]; bf.get(bytes); System.out.println(&quot;==========get============&quot;); System.out.println(new String(bytes,0,bytes.length)); System.out.println(bf.position()); System.out.println(bf.limit()); System.out.println(bf.capacity()); //5.重置读状态 bf.rewind(); System.out.println(&quot;==========rewind============&quot;); System.out.println(bf.position()); System.out.println(bf.limit()); System.out.println(bf.capacity()); //6.清除数据标识 bf.clear(); System.out.println(&quot;==========clear============&quot;); System.out.println(bf.position()); System.out.println(bf.limit()); System.out.println(bf.capacity()); } } 直接缓冲区与非直接缓冲区： 非直接缓冲区：通过 allocate() 方法分配缓冲区，将缓冲区建立在 JVM 的内存中 直接缓冲区：通过 allocateDirect() 方法分配直接缓冲区，将缓冲区建立在物理内存中。可以提高效率 字节缓冲区要么是直接的，要么是非直接的。如果为直接字节缓冲区，则 Java 虚拟机会尽最大努力直接在机 此缓冲区上执行本机 I/O 操作。也就是说，在每次调用基础操作系统的一个本机 I/O 操作之前（或之后）， 虚拟机都会尽量避免将缓冲区的内容复制到中间缓冲区中（或从中间缓冲区中复制内容）。 直接字节缓冲区可以通过调用此类的 allocateDirect() 工厂方法 来创建。此方法返回的 缓冲区进行分配和取消分配所需成本通常高于非直接缓冲区 。直接缓冲区的内容可以驻留在常规的垃圾回收堆之外，因此，它们对应用程序的内存需求量造成的影响可能并不明显。所以，建议将直接缓冲区主要分配给那些易受基础系统的本机 I/O 操作影响的大型、持久的缓冲区。一般情况下，最好仅在直接缓冲区能在程序性能方面带来明显好处时分配它们。 直接字节缓冲区还可以过 通过FileChannel 的 map() 方法 将文件区域直接映射到内存中来创建 。该方法返回MappedByteBuffer 。Java 平台的实现有助于通过 JNI 从本机代码创建直接字节缓冲区。如果以上这些缓冲区中的某个缓冲区实例指的是不可访问的内存区域，则试图访问该区域不会更改该缓冲区的内容，并且将会在访问期间或稍后的某个时间导致抛出不确定的异常。 字节缓冲区是直接缓冲区还是非直接缓冲区可通过调用其 isDirect() 方法来确定。提供此方法是为了能够在性能关键型代码中执行显式缓冲区管理。 非直接缓冲区： 直接缓冲区： 通道（Channel ） 通道表示打开到 IO 设备(例如：文件、套接字)的连接。若需要使用 NIO 系统，需要获取用于连接 IO 设备的通道以及用于容纳数据的缓冲区。然后操作缓冲区，对数据进行处理。 Channel相比IO中的Stream更加高效，可以异步双向传输，但是必须和buffer一起使用。 主要实现类 FileChannel，读写文件中的数据。 SocketChannel，通过TCP读写网络中的数据。 ServerSockectChannel，监听新进来的TCP连接，像Web服务器那样。对每一个新进来的连接都会创建一个SocketChannel。 DatagramChannel，通过UDP读写网络中的数据。 Channel聚集(gather)写入 聚集写入（ Gathering Writes）是指将多个 Buffer 中的数据“聚集”到 Channel。 特别注意：按照缓冲区的顺序，写入 position 和 limit 之间的数据到 Channel 。 Channel分散(scatter)读取 分散读取（ Scattering Reads）是指从 Channel 中读取的数据“分散” 到多个 Buffer 中。 特别注意：按照缓冲区的顺序，从 Channel 中读取的数据依次将 Buffer 填满。 “零拷贝”（FileChannel的transferTo和transferFrom） Java NIO中提供的FileChannel拥有transferTo和transferFrom两个方法，可直接把FileChannel中的数据拷贝到另外一个Channel，或者直接把另外一个Channel中的数据拷贝到FileChannel。该接口常被用于高效的网络/文件的数据传输和大文件拷贝。在操作系统支持的情况下，通过该方法传输数据并不需要将源数据从内核态拷贝到用户态，再从用户态拷贝到目标通道的内核态，同时也避免了两次用户态和内核态间的上下文切换，也即使用了“零拷贝”，所以其性能一般高于Java IO中提供的方法。 代码案例 /** * @author wuhao * @desc 本地io: * FileInputStreanm/FileOutputStream * RandomAccessFile * 网络io: * Socket * ServerSocket * DatagramSocket * @date 2021-09-17 15:22:26 */ public class ChannelTest { //利用通道完成文件的复制,非直接缓冲区 @Test @SneakyThrows public void test(){ FileInputStream fis = new FileInputStream(&quot;D:\\\\1.jpg&quot;); FileOutputStream fos = new FileOutputStream(&quot;D:\\\\2.jpg&quot;); //获取通道 FileChannel inChannel = fis.getChannel(); FileChannel outChannel = fos.getChannel(); //分配指定大小缓存区 ByteBuffer buff = ByteBuffer.allocate(1024);// position 0 ,limit 1024 //将通道的数据存入缓存区 while(inChannel.read(buff)!=-1){// position 1024 ,limit 1024 ,相当于put //切换读模式 buff.flip();//position 0 ,limit 1024 //将缓存去的数据写入通道 outChannel.write(buff);//position 1024 ,limit 1024,相当于get //清空缓冲区 buff.clear();//position 0 ,limit 1024 } outChannel.close(); inChannel.close(); fis.close(); fos.close(); } //利用通道完成文件的复制,直接缓冲区,利用物理内存映射文件 //会出现文件复制完了，但程序还没结束，原因是JVM资源还在用，当垃圾回收机制回收之后程序就会结束,不稳定 @Test @SneakyThrows public void test1(){ FileChannel inChannel = FileChannel.open(Paths.get(&quot;D:\\\\1.jpg&quot;), StandardOpenOption.READ); FileChannel outChannel = FileChannel.open(Paths.get(&quot;D:\\\\4.jpg&quot;), StandardOpenOption.READ,StandardOpenOption.WRITE,StandardOpenOption.CREATE); //内存映射文件 MappedByteBuffer inMapBuff = inChannel.map(FileChannel.MapMode.READ_ONLY, 0, inChannel.size());//==allocateDirect MappedByteBuffer outMapBuff = outChannel.map(FileChannel.MapMode.READ_WRITE, 0, inChannel.size()); byte[] by = new byte[inMapBuff.limit()]; inMapBuff.get(by); outMapBuff.put(by); outChannel.close(); inChannel.close(); } /** * 从源信道读取字节到这个通道的文件中。如果源通道的剩余空间小于 count 个字节，则所传输的字节数要小于请求的字节数。这种方法可能比从源通道读取并写入此通道的简单循环更有效率。 * @param SRC 源通道 * @param position 调动开始的文件内的位置，必须是非负的 * @param count 要传输的最大字节数，必须是非负 * @return 传输文件的大小（单位字节），可能为零， * public abstract long transferFrom(ReadableByteChannel src, long position, long count) throws IOException; */ //复制图片，利用直接缓存区 @Test @SneakyThrows public void test2(){ FileChannel inChannel = FileChannel.open(Paths.get(&quot;D:\\\\1.jpg&quot;), StandardOpenOption.READ); FileChannel outChannel = FileChannel.open(Paths.get(&quot;D:\\\\2.jpg&quot;), StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE); outChannel.transferFrom(inChannel, 0, inChannel.size()); inChannel.close(); outChannel.close(); } /** * 将字节从这个通道的文件传输到给定的可写字节通道。 * @param position 调动开始的文件内的位置，必须是非负的 * @param count 要传输的最大字节数，必须是非负 * @param target 目标通道 * @return 传输文件的大小（单位字节），可能为零， * public abstract long transferTo(long position, long count, WritableByteChannel target) throws IOException; */ //复制图片，利用直接缓存区 @Test @SneakyThrows public void test3(){ FileChannel inChannel = FileChannel.open(Paths.get(&quot;D:\\\\1.jpg&quot;), StandardOpenOption.READ); FileChannel outChannel = FileChannel.open(Paths.get(&quot;D:\\\\3.jpg&quot;), StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE); inChannel.transferTo(0, inChannel.size(), outChannel); inChannel.close(); outChannel.close(); } // 分散读取聚集写入实现文件复制 @Test @SneakyThrows public void test4(){ RandomAccessFile randomAccessFile = null; RandomAccessFile randomAccessFile1 = null; FileChannel inChannel = null; FileChannel outChannel = null; try { randomAccessFile = new RandomAccessFile(new File(&quot;d:\\\\old.txt&quot;), &quot;rw&quot;); randomAccessFile1 = new RandomAccessFile(new File(&quot;d:\\\\new.txt&quot;), &quot;rw&quot;); inChannel = randomAccessFile.getChannel(); outChannel = randomAccessFile1.getChannel(); // 分散为三个bytebuffer读取,capcity要设置的足够大，不然如果文件太大，会导致复制的内容不完整 ByteBuffer byteBuffer1 = ByteBuffer.allocate(1024); ByteBuffer byteBuffer2 = ByteBuffer.allocate(1024); ByteBuffer byteBuffer3 = ByteBuffer.allocate(10240); ByteBuffer[] bbs = new ByteBuffer[]{byteBuffer1,byteBuffer2,byteBuffer3}; inChannel.read(bbs);// 分散读取 // 切换为写入模式 for (int i = 0; i &lt; bbs.length; i++) { bbs[i].flip(); } outChannel.write(bbs); } catch (Exception ex) { ex.printStackTrace(); } } } ","link":"https://tinaxiawuhao.github.io/post/7Wmx9Q2PP/"},{"title":"服务器变慢诊断命令","content":"top命令详解 第一行，任务队列信息，同 uptime 命令的执行结果 系统时间：07:27:05 运行时间：up 1:57 min, 当前登录用户： 3 user 负载均衡(uptime) load average: 0.00, 0.00, 0.00 average后面的三个数分别是1分钟、5分钟、15分钟的负载情况。 load average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了 第二行，Tasks — 任务（进程） 总进程:150 total, 运行:1 running, 休眠:149 sleeping, 停止: 0 stopped, 僵尸进程: 0 zombie 第三行，cpu状态信息 0.0%us【user space】— 用户空间占用CPU的百分比。 0.3%sy【sysctl】— 内核空间占用CPU的百分比。 0.0%ni【】— 改变过优先级的进程占用CPU的百分比 99.7%id【idolt】— 空闲CPU百分比 0.0%wa【wait】— IO等待占用CPU的百分比 0.0%hi【Hardware IRQ】— 硬中断占用CPU的百分比 0.0%si【Software Interrupts】— 软中断占用CPU的百分比 第四行,内存状态 1003020k total, 234464k used, 777824k free, 24084k buffers【缓存的内存量】 第五行，swap交换分区信息 2031612k total, 536k used, 2031076k free, 505864k cached【缓冲的交换区总量】 备注： 可用内存=free + buffer + cached 对于内存监控，在top里我们要时刻监控第五行swap交换分区的used，如果这个数值在不断的变化，说明内核在不断进行内存和swap的数据交换，这是真正的内存不够用了。 第四行中使用中的内存总量（used）指的是现在系统内核控制的内存数， 第四行中空闲内存总量（free）是内核还未纳入其管控范围的数量。 纳入内核管理的内存不见得都在使用中，还包括过去使用过的现在可以被重复利用的内存，内核并不把这些可被重新使用的内存交还到free中去，因此在linux上free内存会越来越少，但不用为此担心。 第六行，空行 第七行以下：各进程（任务）的状态监控 PID — 进程id USER — 进程所有者 PR — 进程优先级 NI — nice值。负值表示高优先级，正值表示低优先级 VIRT — 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES RES — 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA SHR — 共享内存大小，单位kb S —进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程 %CPU — 上次更新到现在的CPU时间占用百分比 %MEM — 进程使用的物理内存百分比 TIME+ — 进程使用的CPU时间总计，单位1/100秒 COMMAND — 进程名称（命令名/命令行） 详解 **VIRT：virtual memory usage 虚拟内存 **1、进程“需要的”虚拟内存大小，包括进程使用的库、代码、数据等 2、假如进程申请100m的内存，但实际只使用了10m，那么它会增长100m，而不是实际的使用量 RES：resident memory usage 常驻内存 1、进程当前使用的内存大小，但不包括swap out 2、包含其他进程的共享 3、如果申请100m的内存，实际使用10m，它只增长10m，与VIRT相反 4、关于库占用内存的情况，它只统计加载的库文件所占内存大小 SHR：shared memory 共享内存 1、除了自身进程的共享内存，也包括其他进程的共享内存 2、虽然进程只使用了几个共享库的函数，但它包含了整个共享库的大小 3、计算某个进程所占的物理内存大小公式：RES – SHR 4、swap out后，它将会降下来 DATA 1、数据占用的内存。如果top没有显示，按f键可以显示出来。 2、真正的该程序要求的数据空间，是真正在运行中要使用的。 top 运行中可以通过 top 的内部命令对进程的显示方式进行控制。内部命令如下： s – 改变画面更新频率 l – 关闭或开启第一部分第一行 top 信息的表示 t – 关闭或开启第一部分第二行 Tasks 和第三行 Cpus 信息的表示 m – 关闭或开启第一部分第四行 Mem 和 第五行 Swap 信息的表示 N – 以 PID 的大小的顺序排列表示进程列表 P – 以 CPU 占用率大小的顺序排列进程列表 M – 以内存占用率大小的顺序排列进程列表 h – 显示帮助 n – 设置在进程列表所显示进程的数量 q – 退出 top s – 改变画面更新周期 vmstat命令详解 vmstat命令是最常见的Linux/Unix监控工具，可以展现给定时间间隔的服务器的状态值,包括服务器的CPU使用率，内存使用，虚拟内存交换情况,IO读写情况。这个命令是我查看Linux/Unix最喜爱的命令，一个是Linux/Unix都支持，二是相比top，我可以看到整个机器的CPU,内存,IO的使用情况，而不是单单看到各个进程的CPU使用率和内存使用率(使用场景不一样)。 一般vmstat工具的使用是通过两个数字参数来完成的，第一个参数是采样的时间间隔数，单位是秒，第二个参数是采样的次数，如: root@ubuntu:~# vmstat 2 1 procs -----------memory---------- ---swap-- -----io---- -system-- ----cpu---- r b swpd free buff cache si so bi bo in cs us sy id wa 1 0 0 3498472 315836 3819540 0 0 0 1 2 0 0 0 100 0 2表示每个两秒采集一次服务器状态，1表示只采集一次。 实际上，在应用过程中，我们会在一段时间内一直监控，不想监控直接结束vmstat就行了,例如: root@ubuntu:~# vmstat 2 procs -----------memory---------- ---swap-- -----io---- -system-- ----cpu---- r b swpd free buff cache si so bi bo in cs us sy id wa 1 0 0 3499840 315836 3819660 0 0 0 1 2 0 0 0 100 0 0 0 0 3499584 315836 3819660 0 0 0 0 88 158 0 0 100 0 0 0 0 3499708 315836 3819660 0 0 0 2 86 162 0 0 100 0 0 0 0 3499708 315836 3819660 0 0 0 10 81 151 0 0 100 0 1 0 0 3499732 315836 3819660 0 0 0 2 83 154 0 0 100 0 这表示vmstat每2秒采集数据，一直采集，直到我结束程序，这里采集了5次数据我就结束了程序。 参数详解 r 表示运行队列(就是说多少个进程真的分配到CPU)，我测试的服务器目前CPU比较空闲，没什么程序在跑，当这个值超过了CPU数目，就会出现CPU瓶颈了。这个也和top的负载有关系，一般负载超过了3就比较高，超过了5就高，超过了10就不正常了，服务器的状态很危险。top的负载类似每秒的运行队列。如果运行队列过大，表示你的CPU很繁忙，一般会造成CPU使用率很高。 b 表示阻塞的进程,这个不多说，进程阻塞，大家懂的。 swpd 虚拟内存已使用的大小，如果大于0，表示你的机器物理内存不足了，如果不是程序内存泄露的原因，那么你该升级内存了或者把耗内存的任务迁移到其他机器。 free 空闲的物理内存的大小，我的机器内存总共8G，剩余3415M。 buff Linux/Unix系统是用来存储，目录里面有什么内容，权限等的缓存，我本机大概占用300多M cache cache直接用来记忆我们打开的文件,给文件做缓冲，我本机大概占用300多M(这里是Linux/Unix的聪明之处，把空闲的物理内存的一部分拿来做文件和目录的缓存，是为了提高 程序执行的性能，当程序使用内存时，buffer/cached会很快地被使用。) si 每秒从磁盘读入虚拟内存的大小，如果这个值大于0，表示物理内存不够用或者内存泄露了，要查找耗内存进程解决掉。我的机器内存充裕，一切正常。 so 每秒虚拟内存写入磁盘的大小，如果这个值大于0，同上。 bi 块设备每秒接收的块数量，这里的块设备是指系统上所有的磁盘和其他块设备，默认块大小是1024byte，我本机上没什么IO操作，所以一直是0，但是我曾在处理拷贝大量数据(2-3T)的机器上看过可以达到140000/s，磁盘写入速度差不多140M每秒 bo 块设备每秒发送的块数量，例如我们读取文件，bo就要大于0。bi和bo一般都要接近0，不然就是IO过于频繁，需要调整。 in 每秒CPU的中断次数，包括时间中断 cs 每秒上下文切换次数，例如我们调用系统函数，就要进行上下文切换，线程的切换，也要进程上下文切换，这个值要越小越好，太大了，要考虑调低线程或者进程的数目,例如在apache和nginx这种web服务器中，我们一般做性能测试时会进行几千并发甚至几万并发的测试，选择web服务器的进程可以由进程或者线程的峰值一直下调，压测，直到cs到一个比较小的值，这个进程和线程数就是比较合适的值了。系统调用也是，每次调用系统函数，我们的代码就会进入内核空间，导致上下文切换，这个是很耗资源，也要尽量避免频繁调用系统函数。上下文切换次数过多表示你的CPU大部分浪费在上下文切换，导致CPU干正经事的时间少了，CPU没有充分利用，是不可取的。 us 用户CPU时间，我曾经在一个做加密解密很频繁的服务器上，可以看到us接近100,r运行队列达到80(机器在做压力测试，性能表现不佳)。 sy 系统CPU时间，如果太高，表示系统调用时间长，例如是IO操作频繁。 id 空闲 CPU时间，一般来说，id + us + sy = 100,一般我认为id是空闲CPU使用率，us是用户CPU使用率，sy是系统CPU使用率。 wt 等待IO CPU时间。 pid命令详解 pidstat是sysstat工具的一个命令，用于监控全部或指定进程的cpu、内存、线程、设备IO等系统资源的占用情况。pidstat首次运行时显示自系统启动开始的各项统计信息，之后运行pidstat将显示自上次运行该命令以后的统计信息。用户可以通过指定统计的次数和时间来获得所需的统计信息。 pidstat 的用法： pidstat [ 选项 ] [ &lt;时间间隔&gt; ] [ &lt;次数&gt; ] 如下图： 常用的参数： -u：默认的参数，显示各个进程的cpu使用统计 -r：显示各个进程的内存使用统计 -d：显示各个进程的IO使用情况 -p：指定进程号 -w：显示每个进程的上下文切换情况 -t：显示选择任务的线程的统计信息外的额外信息 -T { TASK | CHILD | ALL } 这个选项指定了pidstat监控的。TASK表示报告独立的task，CHILD关键字表示报告进程下所有线程统计信息。ALL表示报告独立的task和task下面的所有线程。 注意：task和子线程的全局的统计信息和pidstat选项无关。这些统计信息不会对应到当前的统计间隔，这些统计信息只有在子线程kill或者完成的时候才会被收集。 -V：版本号 -h：在一行上显示了所有活动，这样其他程序可以容易解析。 -I：在SMP环境，表示任务的CPU使用率/内核数量 -l：显示命令名和所有参数 pidstat pidstat -u -p ALL pidstat 和 pidstat -u -p ALL 是等效的。 pidstat 默认显示了所有进程的cpu使用率。 参数详解 PID：进程ID %usr：进程在用户空间占用cpu的百分比 %system：进程在内核空间占用cpu的百分比 %guest：进程在虚拟机占用cpu的百分比 %CPU：进程占用cpu的百分比 CPU：处理进程的cpu编号 Command：当前进程对应的命令 free命令详解 free 命令显示系统内存的使用情况，包括物理内存、交换内存(swap)和内核缓冲区内存。 如果加上 -h 选项，输出的结果会友好很多： 有时我们需要持续的观察内存的状况，此时可以使用 -s 选项并指定间隔的秒数： $ free -h -s 3 上面的命令每隔 3 秒输出一次内存的使用情况，直到你按下 ctrl + c。 由于 free 命令本身比较简单，所以本文的重点会放在如何通过 free 命令了解系统当前的内存使用状况。 输出说明 Mem 行(第二行)是内存的使用情况。 Swap 行(第三行)是交换空间的使用情况。 total 列显示系统总的可用物理内存和交换空间大小。 used 列显示已经被使用的物理内存和交换空间。 free 列显示还有多少物理内存和交换空间可用使用。 shared 列显示被共享使用的物理内存大小。 buff/cache 列显示被 buffer 和 cache 使用的物理内存大小。 available 列显示还可以被应用程序使用的物理内存大小。 df命令详解 Linux df（英文全拼：disk free） 命令用于显示目前在 Linux 系统上的文件系统磁盘使用情况统计。 语法 df [选项]... [FILE]... 文件-a, --all 包含所有的具有 0 Blocks 的文件系统 文件--block-size={SIZE} 使用 {SIZE} 大小的 Blocks 文件-h, --human-readable 使用人类可读的格式(预设值是不加这个选项的...) 文件-H, --si 很像 -h, 但是用 1000 为单位而不是用 1024 文件-i, --inodes 列出 inode 资讯，不列出已使用 block 文件-k, --kilobytes 就像是 --block-size=1024 文件-l, --local 限制列出的文件结构 文件-m, --megabytes 就像 --block-size=1048576 文件--no-sync 取得资讯前不 sync (预设值) 文件-P, --portability 使用 POSIX 输出格式 文件--sync 在取得资讯前 sync 文件-t, --type=TYPE 限制列出文件系统的 TYPE 文件-T, --print-type 显示文件系统的形式 文件-x, --exclude-type=TYPE 限制列出文件系统不要显示 TYPE 文件-v (忽略) 文件--help 显示这个帮手并且离开 文件--version 输出版本资讯并且离开 实例 显示文件系统的磁盘使用情况统计： # df Filesystem 1K-blocks Used Available Use% Mounted on /dev/sda6 29640780 4320704 23814388 16% / udev 1536756 4 1536752 1% /dev tmpfs 617620 888 616732 1% /run none 5120 0 5120 0% /run/lock none 1544044 156 1543888 1% /run/shm 第一列指定文件系统的名称，第二列指定一个特定的文件系统1K-块1K是1024字节为单位的总内存。用和可用列正在使用中，分别指定的内存量。 使用列指定使用的内存的百分比，而最后一栏&quot;安装在&quot;指定的文件系统的挂载点。 df也可以显示磁盘使用的文件系统信息： # df test Filesystem 1K-blocks Used Available Use% Mounted on /dev/sda6 29640780 4320600 23814492 16% / 用一个-i选项的df命令的输出显示inode信息而非块使用量。 df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda6 1884160 261964 1622196 14% / udev 212748 560 212188 1% /dev tmpfs 216392 477 215915 1% /run none 216392 3 216389 1% /run/lock none 216392 8 216384 1% /run/shm 显示所有的信息: # df --total Filesystem 1K-blocks Used Available Use% Mounted on /dev/sda6 29640780 4320720 23814372 16% / udev 1536756 4 1536752 1% /dev tmpfs 617620 892 616728 1% /run none 5120 0 5120 0% /run/lock none 1544044 156 1543888 1% /run/shm total 33344320 4321772 27516860 14% 我们看到输出的末尾，包含一个额外的行，显示总的每一列。 -h选项，通过它可以产生可读的格式df命令的输出： # df -h Filesystem Size Used Avail Use% Mounted on /dev/sda6 29G 4.2G 23G 16% / udev 1.5G 4.0K 1.5G 1% /dev tmpfs 604M 892K 603M 1% /run none 5.0M 0 5.0M 0% /run/lock none 1.5G 156K 1.5G 1% /run/shm 我们可以看到输出显示的数字形式的'G'（千兆字节），&quot;M&quot;（兆字节）和&quot;K&quot;（千字节）。 这使输出容易阅读和理解，从而使显示可读的。请注意，第二列的名称也发生了变化，为了使显示可读的&quot;大小&quot;。 iostat命令详解 用法：iostat [ 选项 ] [ &lt;时间间隔&gt; [ &lt;次数&gt; ]] 常用选项说明： -c：只显示系统CPU统计信息，即单独输出avg-cpu结果，不包括device结果 -d：单独输出Device结果，不包括cpu结果 -k/-m：输出结果以kB/mB为单位，而不是以扇区数为单位 -x:输出更详细的io设备统计信息 interval/count：每次输出间隔时间，count表示输出次数，不带count表示循环输出 说明：更多选项使用使用man iostat查看 实例 1、iostat，结果为从系统开机到当前执行时刻的统计信息 输出含义： avg-cpu: 总体cpu使用情况统计信息，对于多核cpu，这里为所有cpu的平均值。重点关注iowait值，表示CPU用于等待io请求的完成时间。 Device: 各磁盘设备的IO统计信息。各列含义如下： Device: 以sdX形式显示的设备名称 tps: 每秒进程下发的IO读、写请求数量 KB_read/s: 每秒从驱动器读入的数据量，单位为K。 KB_wrtn/s: 每秒从驱动器写入的数据量，单位为K。 KB_read: 读入数据总量，单位为K。 KB_wrtn: 写入数据总量，单位为K。 2、iostat -x -k -d 1 2。每隔1S输出磁盘IO的详细详细，总共采样2次。 以上各列的含义如下： rrqm/s: 每秒对该设备的读请求被合并次数，文件系统会对读取同块(block)的请求进行合并 wrqm/s: 每秒对该设备的写请求被合并次数 r/s: 每秒完成的读次数 w/s: 每秒完成的写次数 rkB/s: 每秒读数据量(kB为单位) wkB/s: 每秒写数据量(kB为单位) avgrq-sz:平均每次IO操作的数据量(扇区数为单位) avgqu-sz: 平均等待处理的IO请求队列长度 await: 平均每次IO请求等待时间(包括等待时间和处理时间，毫秒为单位) svctm: 平均每次IO请求的处理时间(毫秒为单位) %util: 采用周期内用于IO操作的时间比率，即IO队列非空的时间比率 重点关注参数 1、iowait% 表示CPU等待IO时间占整个CPU周期的百分比，如果iowait值超过50%，或者明显大于%system、%user以及%idle，表示IO可能存在问题。 2、avgqu-sz 表示磁盘IO队列长度，即IO等待个数。 3、await 表示每次IO请求等待时间，包括等待时间和处理时间 4、svctm 表示每次IO请求处理的时间 5、%util 表示磁盘忙碌情况，一般该值超过80%表示该磁盘可能处于繁忙状态。 ifstat命令详解 统计网络接口流量状态 下载 wget http://gael.roualland.free.fr/ifstat/ifstat-1.1.tar.gz 编译安装 tar -zxvf ifstat-1.1.tar.gz cd ifstat-1.1 ./configure make make install # 默认会安装到/usr/local/bin/目录中 注释：执行which ifstat输出/usr/local/bin/ifstat 选项 -l 监测环路网络接口（lo）。缺省情况下，ifstat监测活动的所有非环路网络接口。经使用发现，加上-l参数能监测所有的网络接口的信息，而不是只监测 lo的接口信息，也就是说，加上-l参数比不加-l参数会多一个lo接口的状态信息。 -a 监测能检测到的所有网络接口的状态信息。使用发现，比加上-l参数还多一个plip0的接口信息，搜索一下发现这是并口（网络设备中有一 个叫PLIP (Parallel Line Internet Protocol). 它提供了并口...） -z 隐藏流量是无的接口，例如那些接口虽然启动了但是未用的 -i 指定要监测的接口,后面跟网络接口名 -s 等于加-d snmp:[comm@][#]host[/nn]] 参数，通过SNMP查询一个远程主机 -h 显示简短的帮助信息 -n 关闭显示周期性出现的头部信息（也就是说，不加-n参数运行ifstat时最顶部会出现网络接口的名称，当一屏显示不下时，会再一次出现接口的名称，提示我们显示的流量信息具体是哪个网络接口的。加上-n参数把周期性的显示接口名称关闭，只显示一次） -t 在每一行的开头加一个时间 戳（能告诉我们具体的时间） -T 报告所有监测接口的全部带宽（最后一列有个total，显示所有的接口的in流量和所有接口的out流量，简单的把所有接口的in流量相加,out流量相 加） -w 用指定的列宽，而不是为了适应接口名称的长度而去自动放大列宽 -W 如果内容比终端窗口的宽度还要宽就自动换行 -S 在同一行保持状态更新（不滚动不换行）注：如果不喜欢屏幕滚动则此项非常方便，与bmon的显示方式类似 -b 用kbits/s显示带宽而不是kbytes/s -q 安静模式，警告信息不出现 -v 显示版本信息 -d 指定一个驱动来收集状态信息 实例 默认使用 #ifstat eth0 eth1 KB/s in KB/s out KB/s in KB/s out 0.07 0.20 0.00 0.00 0.07 0.15 0.58 0.00 默认ifstat不监控回环接口，显示的流量单位是KB。 ifstat -tT time eth0 eth1 eth2 eth3 Total HH:MM:ss KB/s in KB/s out KB/s in KB/s out KB/s in KB/s out KB/s in KB/s out KB/s in KB/s out 16:53:04 0.84 0.62 1256.27 1173.05 0.12 0.18 0.00 0.00 1257.22 1173.86 16:53:05 0.57 0.40 0.57 0.76 0.00 0.00 0.00 0.00 1.14 1.17 16:53:06 1.58 0.71 0.42 0.78 0.00 0.00 0.00 0.00 2.01 1.48 16:53:07 0.57 0.40 1.91 2.61 0.00 0.00 0.00 0.00 2.48 3.01 16:53:08 0.73 0.40 924.02 1248.91 0.00 0.00 0.00 0.00 924.76 1249.31 监控所有网络接口 ifstat -a lo eth0 eth1 KB/s in KB/s out KB/s in KB/s out KB/s in KB/s out 0.00 0.00 0.28 0.58 0.06 0.06 0.00 0.00 1.41 1.13 0.00 0.00 0.61 0.61 0.26 0.23 0.00 0.00 ","link":"https://tinaxiawuhao.github.io/post/Um55sd5Z3/"},{"title":"CAS/ABA/AtomicReference","content":"锁是用来做并发最简单的方式，当然代价也是最高的。 独占锁是一种悲观锁，synchronized就是一种独占锁；它假设最坏的情况，并且只有在确保其它线程不会造成干扰的情况下执行，会导致其它所有需要锁的线程挂起直到持有锁的线程释放锁。 所谓乐观锁就是每次不加锁,假设没有冲突而去完成某项操作;如果发生冲突了那就去重试，直到成功为止。 CAS(Compare And Swap)是一种有名的无锁算法。CAS算法是乐观锁的一种实现。CAS有3个操作数，内存值V，旧的预期值A，要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B并返回true，否则返回false。 注:synchronized和ReentrantLock都是悲观锁。 注:什么时候使用悲观锁效率更高、什么使用使用乐观锁效率更高，要根据实际情况来判断选择。 提示:atomic中包下的类，采用的即为CAS乐观算法。 以AtomicInteger的public final int getAndSet(int newValue)方法，进行简单说明， 该方法是这样的: 其调用了Unsafe类的public final int getAndSetInt(Object var1, long var2, int var4)方法: 而该方法又do{…}while(…)循环调用了本地方法public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5); 注:至于Windows/Linux下public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5)本地方法是如何实现的，推荐阅读https://blog.csdn.net/v123411739/article/details/79561458。 CAS(Compare And Swap)原理简述： 某一线程执行一个CAS逻辑(如上图线程A),如果中途有其他线程修改了共享变量的值(如:上图中线程A执行到笑脸那一刻时),导致这个线程的CAS逻辑运算后得到的值与期望结果不一致，那么这个线程会再次执行CAS逻辑(这里是一个do while循环),直到成功为止。 ABA问题： 就是说一个线程把数据A变为了B，然后又重新变成了A。此时另外一个线程读取的时候，发现A没有变化，就误以为是原来的那个A。这就是有名的ABA问题。 注:根据实际情况，判断是否处理ABA问题。如果ABA问题并不会影响我们的业务结果，可以选择性处理或不处理;如果 ABA会影响我们的业务结果的，这时就必须处理ABA问题了。 追注:对于AtomicInteger等,没有什么可修改的属性;且我们只在意其结果值，所以对于这些类来说，本身就算发生了ABA现象，也不会对原线程的结果造成什么影响。 AtomicReference原子应用类，可以保证你在修改对象引用时的线程安全性，比较时可以按照偏移量进行 怎样使用AtomicReference： AtomicReference&lt;String&gt; ar = new AtomicReference&lt;String&gt;(); ar.set(&quot;hello&quot;); //CAS操作更新 ar.compareAndSet(&quot;hello&quot;, &quot;hello1&quot;); AtomicReference的成员变量： private static final long serialVersionUID = -1848883965231344442L; //unsafe类,提供cas操作的功能 private static final Unsafe unsafe = Unsafe.getUnsafe(); //value变量的偏移地址,说的就是下面那个value,这个偏移地址在static块里初始化 private static final long valueOffset; //实际传入需要原子操作的那个类实例 private volatile V value; 类装载的时候初始化偏移地址： static { try { valueOffset = unsafe.objectFieldOffset (AtomicReference.class.getDeclaredField(&quot;value&quot;)); } catch (Exception ex) { throw new Error(ex); } } compareAndSet方法： //就是调用Unsafe的cas操作,传入对象,expect值,偏移地址,需要更新的值,即可,如果更新成功,返回true,如果失败,返回false public final boolean compareAndSet(V expect, V update) { return unsafe.compareAndSwapObject(this, valueOffset, expect, update); } 对于String变量来说,必须是对象相同才视为相同,而不是字符串的内容相同就可以相同: AtomicReference&lt;String&gt; ar = new AtomicReference&lt;String&gt;(); ar.set(&quot;hello&quot;); System.out.println(ar.compareAndSet(new String(&quot;hello&quot;), &quot;hello1&quot;));//false AtomicReference可解决volatile不具有原子性i++操作 AtomicReference可以保证结果是正确的. private static volatile Integer num1 = 0; private static AtomicReference&lt;Integer&gt; ar=new AtomicReference&lt;Integer&gt;(num1); public void dfasd111() throws InterruptedException{ for (int i = 0; i &lt; 1000; i++) { new Thread(new Runnable(){ @Override public void run() { for (int i = 0; i &lt; 10000; i++) while(true){ Integer temp=ar.get(); if(ar.compareAndSet(temp, temp+1))break; } } }).start(); } Thread.sleep(10000); System.out.println(ar.get()); //10000000 } 类似i++这样的&quot;读-改-写&quot;复合操作(在一个操作序列中, 后一个操作依赖前一次操作的结果), 在多线程并发处理的时候会出现问题, 因为可能一个线程修改了变量, 而另一个线程没有察觉到这样变化, 当使用原子变量之后, 则将一系列的复合操作合并为一个原子操作,从而避免这种问题, i++=&gt;i.incrementAndGet() 这里的compareAndSet方法即cas操作本身是原子的，但是在某些场景下会出现异常场景 此处说一下ABA问题： 比如，我们只是简单得要做一个数值加法，即使在我取得期望值后，这个数字被不断的修改，只要它最终改回了我的期望值，我的加法计算就不会出错。也就是说，当你修改的对象没有过程的状态信息，所有的信息都只保存于对象的数值本身。 但是，在现实中，还可能存在另外一种场景。就是我们是否能修改对象的值，不仅取决于当前值，还和对象的过程变化有关，这时，AtomicReference就无能为力了。 AtomicStampedReference与AtomicReference的区别： AtomicStampedReference它内部不仅维护了对象值，还维护了一个时间戳（我这里把它称为时间戳，实际上它可以使任何一个整数，它使用整数来表示状态值）。当AtomicStampedReference对应的数值被修改时，除了更新数据本身外，还必须要更新时间戳。当AtomicStampedReference设置对象值时，对象值以及时间戳都必须满足期望值，写入才会成功。因此，即使对象值被反复读写，写回原值，只要时间戳发生变化，就能防止不恰当的写入。 解决ABA问题 public static void main(String[] args) { String str1 = &quot;aaa&quot;; String str2 = &quot;bbb&quot;; AtomicStampedReference&lt;String&gt; reference = new AtomicStampedReference&lt;String&gt;(str1,1); reference.compareAndSet(str1,str2,reference.getStamp(),reference.getStamp()+1); System.out.println(&quot;reference.getReference() = &quot; + reference.getReference()); boolean b = reference.attemptStamp(str2, reference.getStamp() + 1); System.out.println(&quot;b: &quot;+b); System.out.println(&quot;reference.getStamp() = &quot;+reference.getStamp()); boolean c = reference.weakCompareAndSet(str2,&quot;ccc&quot;,4, reference.getStamp()+1); System.out.println(&quot;reference.getReference() = &quot;+reference.getReference()); System.out.println(&quot;c = &quot; + c); } 输出: reference.getReference() = bbb b: true reference.getStamp() = 3 reference.getReference() = bbb c = false ","link":"https://tinaxiawuhao.github.io/post/2nzKhQbUe/"},{"title":"代理模式","content":"静态代理、JDK动态代理以及CGLIB动态代理 代理模式是java中最常用的设计模式之一，尤其是在spring框架中广泛应用。对于java的代理模式，一般可分为：静态代理、动态代理、以及CGLIB实现动态代理。 对于上述三种代理模式，分别进行说明。 静态代理 静态代理其实就是在程序运行之前，提前写好被代理方法的代理类，编译后运行。在程序运行之前，class已经存在。 下面我们实现一个静态代理demo: 定义一个接口Target package com.test.proxy; public interface Target { public String execute(); } TargetImpl 实现接口Target package com.test.proxy; public class TargetImpl implements Target { @Override public String execute() { System.out.println(&quot;TargetImpl execute！&quot;); return &quot;execute&quot;; } } 代理类 package com.test.proxy; public class Proxy implements Target{ private Target target; public Proxy(Target target) { this.target = target; } @Override public String execute() { System.out.println(&quot;perProcess&quot;); String result = this.target.execute(); System.out.println(&quot;postProcess&quot;); return result; } } 测试类: package com.test.proxy; public class ProxyTest { public static void main(String[] args) { Target target = new TargetImpl(); Proxy p = new Proxy(target); String result = p.execute(); System.out.println(result); } } 运行结果: perProcess TargetImpl execute！ postProcess execute 静态代理需要针对被代理的方法提前写好代理类，如果被代理的方法非常多则需要编写很多代码，因此，对于上述缺点，通过动态代理的方式进行了弥补。 动态代理 jdk代理 动态代理主要是通过反射机制，在运行时动态生成所需代理的class. 接口 package com.test.dynamic; public interface Target { public String execute(); } 实现类 package com.test.dynamic; public class TargetImpl implements Target { @Override public String execute() { System.out.println(&quot;TargetImpl execute！&quot;); return &quot;execute&quot;; } } 代理类 package com.test.dynamic; import java.lang.reflect.InvocationHandler; import java.lang.reflect.Method; public class DynamicProxyHandler implements InvocationHandler{ private Target target; public DynamicProxyHandler(Target target) { this.target = target; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(&quot;========before==========&quot;); Object result = method.invoke(target,args); System.out.println(&quot;========after===========&quot;); return result; } } 测试类 package com.test.dynamic; import java.lang.reflect.Proxy; public class DynamicProxyTest { public static void main(String[] args) { Target target = new TargetImpl(); DynamicProxyHandler handler = new DynamicProxyHandler(target); Target proxySubject = (Target) Proxy.newProxyInstance(TargetImpl.class.getClassLoader(),TargetImpl.class.getInterfaces(),handler); String result = proxySubject.execute(); System.out.println(result); } } 运行结果： before== TargetImpl execute！ after=== execute 无论是动态代理还是静态带领，都需要定义接口，然后才能实现代理功能。这同样存在局限性，因此，为了解决这个问题，出现了第三种代理方式：cglib代理。 cglib代理 CGLib采用了非常底层的字节码技术，其原理是通过字节码技术为一个类创建子类，并在子类中采用方法拦截的技术拦截所有父类方法的调用，顺势织入横切逻辑。JDK动态代理与CGLib动态代理均是实现Spring AOP的基础。 目标类 package com.test.cglib; public class Target { public String execute() { String message = &quot;-----------test------------&quot;; System.out.println(message); return message; } } 通用代理类 package com.test.cglib; import net.sf.cglib.proxy.MethodInterceptor; import net.sf.cglib.proxy.MethodProxy; import java.lang.reflect.Method; public class MyMethodInterceptor implements MethodInterceptor{ @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable { System.out.println(&quot;&gt;&gt;&gt;&gt;MethodInterceptor start...&quot;); Object result = proxy.invokeSuper(obj,args); System.out.println(&quot;&gt;&gt;&gt;&gt;MethodInterceptor ending...&quot;); return &quot;result&quot;; } } 测试类 package com.test.cglib; import net.sf.cglib.proxy.Enhancer; public class CglibTest { public static void main(String[] args) { System.out.println(&quot;***************&quot;); Target target = new Target(); CglibTest test = new CglibTest(); Target proxyTarget = (Target) test.createProxy(Target.class); String res = proxyTarget.execute(); System.out.println(res); } public Object createProxy(Class targetClass) { Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(targetClass); enhancer.setCallback(new MyMethodInterceptor()); return enhancer.create(); } } 执行结果: MethodInterceptor start... -----------test------------ MethodInterceptor ending... result 代理对象的生成过程由Enhancer类实现，大概步骤如下： 生成代理类Class的二进制字节码； 通过Class.forName加载二进制字节码，生成Class对象； 通过反射机制获取实例构造，并初始化代理类对象。 ","link":"https://tinaxiawuhao.github.io/post/CO5c8R39E/"},{"title":"OOM","content":"Stack overflow public class StackOverFlowErrorDemo { public static void main(String[] args) { StackOverFlowError(); } private static void StackOverFlowError() { StackOverFlowError(); } } Exception in thread &quot;main&quot; java.lang.StackOverflowError at com.example.interview.oom.StackOverFlowErrorDemo.StackOverFlowError(StackOverFlowErrorDemo.java:14) 递归调用自身方法，不断向栈内压入栈帧，直到撑破栈空间，重复次数不确定 OutOfMemoryError：Java heap space /** * @author wuhao * @desc -Xmx10m -Xms10m -XX:+PrintGCDetails * 最大堆内存10M,初始堆内存10M,打印GC详细信息 * @date 2021-09-10 15:14:50 */ public class JavaHeapSpaceDemo { public static void main(String[] args) { // String str= &quot;java&quot;; // while (true){ // str+=str+new Random().nextInt(111111)+new Random().nextInt(121312); // str.intern(); // } //创建大对象 byte[] bytes=new byte[20*1024*1024]; } } Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space at com.example.interview.oom.JavaHeapSpaceDemo.main(JavaHeapSpaceDemo.java:18) 常量池，对象空间地址位于堆空间中，循环生成String或者生成超大对象会导致堆空间被占满溢出 OutOfMemoryError：GC overhead limit exceeded /** * @author wuhao * @desc -Xmx20m -Xms10m -XX:+PrintGCDetails * 最大堆内存20M,初始堆内存10M,堆外内存(直接内存)5M,打印GC信息 * GC overhead limit exceeded:超出GC开销限制 * @date 2021-09-10 15:24:48 */ public class GCOverHeadDemo { public static void main(String[] args) { int i=0; List&lt;String&gt; list=new ArrayList&lt;&gt;(); while (true){ list.add(String.valueOf(++i).intern()); } } } [Full GC (Ergonomics) [PSYoungGen: 2560K-&gt;0K(4608K)] [ParOldGen: 13581K-&gt;818K(10752K)] 16141K-&gt;818K(15360K), [Metaspace: 3626K-&gt;3626K(1056768K)], 0.0078165 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] Heap PSYoungGen total 4608K, used 219K [0x00000000ff980000, 0x0000000100000000, 0x0000000100000000) eden space 2560K, 8% used [0x00000000ff980000,0x00000000ff9b6f28,0x00000000ffc00000) from space 2048K, 0% used [0x00000000ffe00000,0x00000000ffe00000,0x0000000100000000) to space 2048K, 0% used [0x00000000ffc00000,0x00000000ffc00000,0x00000000ffe00000) ParOldGen total 10752K, used 818K [0x00000000fec00000, 0x00000000ff680000, 0x00000000ff980000) object space 10752K, 7% used [0x00000000fec00000,0x00000000fecccac0,0x00000000ff680000) Metaspace used 3719K, capacity 4536K, committed 4864K, reserved 1056768K class space used 406K, capacity 428K, committed 512K, reserved 1048576K Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: GC overhead limit exceeded at java.lang.Integer.toString(Integer.java:401) at java.lang.String.valueOf(String.java:3099) at com.example.interview.oom.GCOverHeadDemo.main(GCOverHeadDemo.java:17) 最大堆内存要大于初始堆内存给GC创造条件，如果两值相等就没有重新分配堆空间操作会直接爆出Java heap space异常 OutOfMemoryError：Direct buffer memory import java.nio.ByteBuffer; /** * @author wuhao * @desc -Xmx20m -Xms10m -XX:+PrintGCDetails -XX:MaxDirectMemorySize=5m * 最大堆内存20M,初始堆内存10M,堆外内存(直接内存)5M,打印GC信息 * @date 2021-09-10 15:38:57 */ public class DirectBufferMemoryDemo { public static void main(String[] args) { System.out.println(&quot;配置的DirectMemory&quot;+(sun.misc.VM.maxDirectMemory()/(double)1024/1024)+&quot;mb&quot;); ByteBuffer bf=ByteBuffer.allocateDirect(6*1024*1024); } } [GC (Allocation Failure) [PSYoungGen: 2048K-&gt;504K(2560K)] 2048K-&gt;758K(9728K), 0.0008256 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 配置的DirectMemory5.0mb [GC (System.gc()) [PSYoungGen: 908K-&gt;504K(2560K)] 1162K-&gt;862K(9728K), 0.0009251 secs] [Times: user=0.06 sys=0.00, real=0.00 secs] [Full GC (System.gc()) [PSYoungGen: 504K-&gt;0K(2560K)] [ParOldGen: 358K-&gt;801K(7168K)] 862K-&gt;801K(9728K), [Metaspace: 3500K-&gt;3500K(1056768K)], 0.0046698 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Direct buffer memory at java.nio.Bits.reserveMemory(Bits.java:694) at java.nio.DirectByteBuffer.&lt;init&gt;(DirectByteBuffer.java:123) at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311) at com.example.interview.oom.DirectBufferMemoryDemo.main(DirectBufferMemoryDemo.java:13) Heap PSYoungGen total 2560K, used 1068K [0x00000000ff980000, 0x00000000ffc80000, 0x0000000100000000) eden space 2048K, 52% used [0x00000000ff980000,0x00000000ffa8b200,0x00000000ffb80000) from space 512K, 0% used [0x00000000ffc00000,0x00000000ffc00000,0x00000000ffc80000) to space 512K, 0% used [0x00000000ffb80000,0x00000000ffb80000,0x00000000ffc00000) ParOldGen total 7168K, used 801K [0x00000000fec00000, 0x00000000ff300000, 0x00000000ff980000) object space 7168K, 11% used [0x00000000fec00000,0x00000000fecc86d0,0x00000000ff300000) Metaspace used 4057K, capacity 4568K, committed 4864K, reserved 1056768K class space used 446K, capacity 460K, committed 512K, reserved 1048576K allocateDirect分配的字节缓冲区用中文叫做直接缓冲区（DirectByteBuffer），用allocate分配的ByteBuffer叫做堆字节缓冲区(HeapByteBuffer).. 其实根据类名就可以看出，HeapByteBuffer所创建的字节缓冲区就是在jvm堆中的，即内部所维护的java字节数组。而DirectByteBuffer是直接操作操作系统本地代码创建的内存缓冲数组（c、c++的数组）。 HeapByteBuffer底层其实是java的字节数组，而java字节数组是一个java对象，对象的内存是由jvm的堆进行管理的，那么不可避免的是GC时年轻代的eden、suvivor到老年代的各种复制以及回收。。。当字节数组比较小的时候还好说，如果是大对象，那么对于jvm的GC来说是一个很大的负担。。而使用DirectByteBuffer，则是把字节数组交给操作系统管理（堆外内存） OutOfMemoryError：Unable to create to native thread public class UnableToCreateToNativeThreadDemo { public static void main(String[] args) { for (int i = 0; ; i++) { System.out.println(&quot;================ i=&quot; + i); new Thread(() -&gt; { try { Thread.sleep(Integer.MAX_VALUE); } catch (InterruptedException e) { e.printStackTrace(); } }, &quot;&quot; + i).start(); } } } linux一般用户可以最大创建1024个线程，不要在电脑上随意尝试 meta space /** * @author wuhao * @desc -XX:MetaspaceSize=8m -XX:MaxMetaspaceSize=8m -XX:+PrintGCDetails * @date 2021-09-13 10:36:15 */ public class MetaSpaceOOMDemo { static class OOMTest{ } public static void main(String[] args) { int i=0; try { while (true){ i++; Enhancer enhancer=new Enhancer(); enhancer.setSuperclass(OOMTest.class); enhancer.setUseCache(false); enhancer.setCallback(new MethodInterceptor() { @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable { return methodProxy.invokeSuper(o,args); } }); enhancer.create(); } }catch (Throwable e){ System.out.println(&quot;=========&quot;+i+&quot;次后发生了异常&quot;); } } } [Full GC (Last ditch collection) [PSYoungGen: 0K-&gt;0K(116224K)] [ParOldGen: 2016K-&gt;2016K(225792K)] 2016K-&gt;2016K(342016K), [Metaspace: 9911K-&gt;9911K(1058816K)], 0.0093402 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] Heap PSYoungGen total 116224K, used 3425K [0x00000000d6400000, 0x00000000de400000, 0x0000000100000000) eden space 115712K, 2% used [0x00000000d6400000,0x00000000d67584a0,0x00000000dd500000) from space 512K, 0% used [0x00000000de380000,0x00000000de380000,0x00000000de400000) to space 5120K, 0% used [0x00000000dda00000,0x00000000dda00000,0x00000000ddf00000) ParOldGen total 225792K, used 2016K [0x0000000082c00000, 0x0000000090880000, 0x00000000d6400000) object space 225792K, 0% used [0x0000000082c00000,0x0000000082df8128,0x0000000090880000) Metaspace used 9942K, capacity 10090K, committed 10240K, reserved 1058816K class space used 884K, capacity 913K, committed 1024K, reserved 1048576K Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Metaspace ","link":"https://tinaxiawuhao.github.io/post/QvbaHSogu/"},{"title":"类加载","content":"类加载过程 在java中编译并不进行链接工作，类型的加载、链接和初始化工作都是在jvm执行过程中进行的。在Java程序启动时，jvm通过加载指定的类，然后调用该类的main方法而启动。在JVM启动过程中， 外部class字节码文件会经过一系列过程转化为JVM中执行的数据，这一系列过程我们称为类加载过程。 类加载整体流程 从类被JVM加载到内存开始到卸载出内存为止，整个生命周期包括：加载、链接、初始化、使用和卸载五个过程。其中链接又包括验证、准备和解析三个过程。如下图所示： 类加载时机 java虚拟机规范通过对初始化阶段进行严格规定，来保证初始化的完成，而作为其之前的必须启动的过程，加载、验证、准备也需要在此之前开始。 Java虚拟机规定，以下五种情况必须对类进行初始化： 虚拟机在用户指定包含main方法的主类后启动时，必须先对主类进行初始化。 当使用new关键字对类进行实例化时、读取或者写入类的静态字段时、调用类的静态方法时，必须先触发对该类的实例化。 使用反射对类进行反射调用时，如果该类没有初始化先对其进行初始化。 初始化一个类，而该类的父类还未初始化，需要先对其父类进行初始化。 在JDK1.7之后的版本中使用动态语言支持，java.lang.invoke.MethodHandle实例解析的结果是REF_getStatic、REF_putStatic、REF_invokeStatic的方法句柄，而该句柄对应的类 还未初始化，必须先触发其实例化。 加载 在加载阶段，虚拟机需要完成三件事： 通过一个类的全限定名来获取此类的class字节码二进制流。 将这个字节码二进制流中的静态存储结构转化为方法区中的运行时数据结构。 在内存中生成一个代表该类的java.lang.Class对象，作为方法区中这个类的各种数据的访问入口。 对于Class对象，Java虚拟机规范并没有规定要存储在堆中，HotSpot虚拟机将其存放在方法区中。 验证 验证作为链接的第一步，大致会完成四个阶段的检验： 文件格式验证：该阶段主要在字节流转化为方法区中的运行时数据时，负责检查字节流是否符合Class文件规范，保证其可以正确的被解析并存储在方法区中。后面的检查都是基于方法区的 存储结构进行检验，不会再直接操作字节流。 元数据验证：该阶段负责分析存储于方法区的结构是否符合Java语言规范。此阶段进行数据类型的校验，保证符合不存在非法的元数据信息。 字节码验证：元数据验证保证了字节码中的数据符合语言的规范，该阶段则负责分析数据流和控制流，确定方法体的合法性，保证被校验的方法在运行时不会危害虚拟机的运行。 符号引用验证：在解析阶段会将虚拟机中的符号引用转化为直接引用，该阶段则负责对各种符号引用进行匹配性校验，保证外部依赖真实存在，并且符合外部依赖类、字段、方法的访问性。 准备 准备阶段正式为类的字段变量（被static修饰的类变量）分配内存并设置初始值。这些变量存储在方法区中。当类字段为常量类型（即被static final修饰），由于字段的值已经确定，并不会在后面修改，此时会直接赋值为指定的值。 解析 解析阶段将常量池中的符号引用替换为直接引用。在字节码文件中，类、接口、字段、方法等类型都是由一组符号来表示。其形式由java虚拟机规范中的Class文件格式定义。在虚拟机执行 指定指令之前，需要将符号引用转化为目标的指针、相对偏移量或者句柄，这样可以通过此类直接引用在内存中定位调用的具体位置。 初始化 在类的class文件中。包含两个特殊的方法：clinit和init，这两方法由编译器自动生成，分别代表类构造器和构造函数，其中构造函数编程实现，初始化阶段就是负责调用类构造器，来初始化 变量和资源。 clinit方法由编译器自动收集类的赋值动作和静态语句块（static）中的语句合并生成的，有以下特点： 编译器收集顺序又代码顺序决定，静态语句块只能访问它之前定义的变量，在它之后定义的变量只能进行赋值不能访问。 虚拟机保证在子类的clinit方法执行前，父类的clinit已经执行完毕。 clinit不是必须的，如果一个类或接口没有变量赋值和静态代码块，则编译器可以不生成clinit。 虚拟机会保证clinit方法在多线程中被正确的加锁和同步。如果多个线程同时初始化一个类，那么只有一个线程执行clinit，其他线程会被阻塞。 双亲委派模型 类加载器 定义：实现类加载阶段的“通过一个里的全限定名来获取描述此类的二进制字节流”的动作的代码模块成为“类加载器”。对于任意一个类，都需要由加载它的类加载器和这个类本身一同确立其在java虚拟机中的唯一性，每一个类加载器，都拥有一个独立的类名称空间。比较两个类是否“相等”，只有在这两个类是同一个类加载器加载的前提下才有意义。 类加载器种类 从Java虚拟机的角度只有两种类加载器： （1）启动类加载器（BootStrap ClassLoader），这个类加载器使用C++语言实现，是虚拟机自身的一部分。 （2）另一种就是所有其他类的加载器，这些类加载器都是由Java语言实现，独立于虚拟机外部，并且都继承自抽象类java.lang.ClassLoader。 从Java开发人员的角度，类加载器还可分为3种系统提供的类加载器和用户自定义的类加载器。 （1）启动类加载器（BootStrap ClassLoader）：负责加载存放java_home\\lib目录中的，或者被-Xbootclasspath参数所指定的路径中的类。 （2）扩展类加载器（Extension ClassLoader）：这个加载器sun.misc.Launcher ExtClassLoader实现，它负责加载java_home\\lib\\ext目录中的，或者被java.ext.dirs系统变量所指定的路径中的所有类库，开发者可以直接使用扩展类加载器。 如果应用程序中没有自定义的类加载器，一般情况下 这个就是程序中默认的类加载器。 （3）自定义类加载器（User ClassLoader）：用户自定义的类加载器。用户在编写自己定义的类加载器时，如果需要把请求委派给引导类加载器，那直接使用numm代替即可。要创建用户自己 的类加载器，只需要继承java.lang.ClassLoader，然后覆盖它的findClass(String name)方法即可。如果要符合双亲委派模型，则重写findClass()方法。如果要破坏的话，则重写 loadClass()方法。 双亲委派模型 上图展示的类加载器之间的这种层次关系称为类加载器的双亲委派模型。 双亲委派模型要求除了顶层的启动类加载器之外，其余的类加载器都应当有自己的父类加载器。 类加载器的双亲委派模型在jdk1.2被引入，但它不是一个强制性的约束模型，而是Java设计者推荐给开发者的一种类加载器的实现方式。 双亲委派模型的工作过程如下： 当一个类加载器收到某个类的加载请求时，该类加载器不会去加载该类，而是先查看该类加载器的缓存空间是否已经加载了该类，如果已经加载过了就直接返回，如果不存在，则委托给父亲加载器去加载（也是先查看父亲加载器的缓存空间是否已经加载过该类，如果已加载则直接返回），每一层都是如此，如果缓存空间中都没有加载过该类的记录，最终类加载的请求就会传送到顶端的启动类加载器，如果启动类加载器不能加载该类，则返回给拓展类加载器（儿子）去尝试加载，如果还是不能加载，则再到应用类加载器，再到自定义加载器加载 对象的创建、存储和访问 对象的创建 类加载检查：虚拟机遇到一条new指令，首先检查这个指令的参数是否能在常量池中（Class文件的静态常量池）定位到这个类的符号引用，并且检查这个符号引用代表的类是否 已经被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 分配内存：对象所需内存大小在类加载完成后便可完全确定，为对象分配空间的任务等同于把一块确定大小的内存从Java堆中划分出来。但是不同垃圾回收器的算法会导致堆内存存在两种情况：绝对规整和相互交错。（比如标记清楚算法和标记整理算法） （1）指针碰撞：假设Java堆内存是绝对规整的，所有用过的内存都存放在一起，空闲的内存存放在另一边，中间放着一个指示器作为分界点的指示器，所分配的内存就仅仅是把那个指针向空闲空间那边挪动一段与对象大小相等的距离，这种分配方式成为”指针碰撞“。 （2）空闲列表：如果是相互交错的，那么虚拟机会维护一个列表，记录哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划给对象实例，并更新列表上的记录。这种分配方式成为”空闲列表“。 分配内存的并发问题：即使是仅仅修改一个指针所指向的位置，在并发情况下也不是线程安全的，可能出现正在给对象A分配内存，指针还没来得及修改，对象B又同时使用了原来的 指针来分配内存的情况。针对这个问题有两种解决方案： （1）失败重试：对分配内存空间的动作进行同步处理，虚拟机采用CAS和失败重试机制保证更新操作的原子性。 （2）本地线程分配缓存：哪个线程要分配内存，就在哪个线程的TLAB（Thread Local Allocation Buffer）上分配，只有TLAB用完并分配新的TLAB时，才需要同步锁定。 内存空间初始化零值：内存分配完成后，虚拟机需要将分配到的内存空间都初始化零值，这一步操作保证了对象的实例字段（成员变量）在Java代码中可以不赋值就直接使用，程序能够访问到这些字段的数据类型所对应的零值。 对象设置：接下来虚拟机会对对象进行必要的设置，例如这个对象是哪个类的实例，如何才能找到类的元数据信息、对象的哈希吗、对象的GC分代年龄等信息。这些信息存放在对象头中。至此一个新的对象产生了。 实例构造器的init方法：虽然对象产生了，但是init方法并没有执行，所欲字段还需要赋值（包括成员变量赋值，普通语句块执行，构造函数执行等。） Clinit和init Clinit 类构造器的方法，与类的初始化有关。例如静态变量（类变量）和静态对象赋值，静态语句块的执行。如果一个类中没有静态语句块，也没有静态变量或静态对象的赋值， 那么编译器可以不为这个类生成方法。 init 实例构造器（即成员变量，成员对象等），例如成员变量和成员对象的赋值，普通语句块的执行，构造函数的执行。 对象的内存布局 在HotSpot虚拟机中，对象在内存中存储的布局可以分为三个区域：对象头、实例数据和对齐填充。 对象头 对象头包括两部分信息：运行时数据和类型指针。 运行时数据 第一部分用于存储对象自身的运行时数据，如哈希吗（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等。 下面是HotSpot虚拟机对象头Mark Word： 类型指针 对象头的另一部分是类型指针，即对象指向他元数据的指针，虚拟机可以通过这个指针确定这个对象是哪个类的实例。但是如果对象是一个Java数组，那么在对象头中还必须有一块用于记录数据长度的数据。 对象的实例数据 接着数据头的是对象的实例数据，这部分是真正存储的有效信息。无论是从父类中继承下来的还是在子类中定义的，都需要记录下来。 对齐填充 最后一部分对齐填充并不是必然存在的，也没有特别的含义，仅仅起着占位符的作用。由于HotSpot虚拟机的自动内存管理系统要求对象的起始地址必须是8字节的整数倍，也就是 对象的大小必须是8字节的整数倍。而对象头部分是8字节的倍数，当实例数据没有对齐的时候，需要对齐填充凑够8字节的整数倍。 对象的访问定位 建立对象是为了使用对象，我们的Java程序需要通过栈上的引用数据来操作堆上的具体对象。对象的访问方式取决于虚拟机的实现，目前主流的访问方式有使用句柄和直接指针两种。 句柄引用和直接引用不同在于：使用句柄引用的话，那么Java对堆中将会划分出一块内存来作为句柄池，引用中存储的就是对象的句柄地址，但是直接引用引用中存储的直接就是对象地址。Java使用的是直接指针访问对象的方式，因为它最大的好处就是速度更快，它节省了一次指针定位的时间开销，由于对象的访问在Java中非常频繁，因此这类开销积少成多后也是一项 非常可观的执行成本。 下面是通过直接指针访问对象 ","link":"https://tinaxiawuhao.github.io/post/DikSLfU_z/"},{"title":"线程池ThreadPoolExecutor","content":"Executors目前提供了5种不同的线程池创建配置： newCachedThreadPool（），它是用来处理大量短时间工作任务的线程池，具有几个鲜明特点：它会试图缓存线程并重用，当无缓存线程可用时，就会创建新的工作线程；如果线程闲置时间超过60秒，则被终止并移除缓存；长时间闲置时，这种线程池，不会消耗什么资源。其内部使用SynchronousQueue作为工作队列。 newFixedThreadPool（int nThreads），重用指定数目（nThreads）的线程，其背后使用的是无界的工作队列，任何时候最多有nThreads个工作线程是活动的。这意味着，如果任务数量超过了活动线程数目，将在工作队列中等待空闲线程出现；如果工作线程退出，将会有新的工作线程被创建，以补足指定数目nThreads。 newSingleThreadExecutor()，它的特点在于工作线程数目限制为1，操作一个无界的工作队列，所以它保证了所有的任务都是被顺序执行，最多会有一个任务处于活动状态，并且不予许使用者改动线程池实例，因此可以避免改变线程数目。 newSingleThreadScheduledExecutor()和newScheduledThreadPool(int corePoolSize)，创建的是个ScheduledExecutorService，可以进行定时或周期性的工作调度，区别在于单一工作线程还是多个工作线程。 newWorkStealingPool(int parallelism)，这是一个经常被人忽略的线程池，Java 8 才加入这个创建方法，其内部会构建ForkJoinPool，利用Work-Stealing算法，并行地处理任务，不保证处理顺序。 public class MyThreadPool { public static void main(String[] args) { // ExecutorService executorService = Executors.newFixedThreadPool(5); // ExecutorService executorService= Executors.newSingleThreadExecutor(); // ExecutorService executorService= Executors.newCachedThreadPool(); // ExecutorService executorService= Executors.newScheduledThreadPool(5); ExecutorService executorService = new ThreadPoolExecutor(2, 5, 2, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(3), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy() ); ThreadPoolInit(executorService); } private static void ThreadPoolInit(ExecutorService executorService) { try { for (int i = 1; i &lt;= 10; i++) { executorService.execute(() -&gt; { System.out.println(Thread.currentThread().getName() + &quot;办理业务&quot;); }); } } catch (Exception e) { e.printStackTrace(); } finally { executorService.shutdown(); } } } Executor框架的基本组成 而我们创建时，一般使用它的子类：ThreadPoolExecutor. public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) 这是其中最重要的一个构造方法，这个方法决定了创建出来的线程池的各种属性，下面依靠一张图来更好的理解线程池和这几个参数： （1）corePoolSize：线程池中常驻核心线程数 （2）maximumPoolSize：线程池能够容纳同时执行的最大线程数，此值必须大于等于1 （3）keepAliveTime：多余的空闲线程存活时间。当前线程池数量超过corePoolSize时，当空闲时间到达keepAliveTime值时，多余空闲线程会被销毁直到只剩下corePoolSize个线程为止。 （4）unit：keepAliveTime的时间单位 （5）workQueue：任务队列，被提交但尚未执行的任务 （6）threadFactory：表示生成线程池中的工作线程的线程工厂，用于创建线程，一般为默认线程工厂即可 （7）handler：拒绝策略，表示当队列满了并且工作线程大于等于线程池的最大线程数（maximumPoolSize）时如何来拒绝来请求的Runnable的策略 handler的拒绝策略： 有四种： 第一种AbortPolicy:不执行新任务，直接抛出异常，提示线程池已满 第二种DisCardPolicy:不执行新任务，也不抛出异常 第三种DisCardOldSetPolicy:将消息队列中的第一个任务替换为当前新进来的任务执行 第四种CallerRunsPolicy:直接调用execute来执行当前任务 在spring项目中的使用 先创建一个线程池的配置，让Spring Boot加载，用来定义如何创建一个ThreadPoolTaskExecutor，要使用@Configuration和@EnableAsync这两个注解，表示这是个配置类，并且是线程池的配置类 @Configuration @EnableAsync public class ExecutorConfig { private static final Logger logger = LoggerFactory.getLogger(ExecutorConfig.class); @Value(&quot;${async.executor.thread.core_pool_size}&quot;) private int corePoolSize; @Value(&quot;${async.executor.thread.max_pool_size}&quot;) private int maxPoolSize; @Value(&quot;${async.executor.thread.queue_capacity}&quot;) private int queueCapacity; @Value(&quot;${async.executor.thread.name.prefix}&quot;) private String namePrefix; @Bean(name = &quot;asyncServiceExecutor&quot;) public Executor asyncServiceExecutor() { logger.info(&quot;start asyncServiceExecutor&quot;); ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); //配置核心线程数 executor.setCorePoolSize(corePoolSize); //配置最大线程数 executor.setMaxPoolSize(maxPoolSize); //配置队列大小 executor.setQueueCapacity(queueCapacity); //配置线程池中的线程的名称前缀 executor.setThreadNamePrefix(namePrefix); // rejection-policy：当pool已经达到max size的时候，如何处理新任务 // CALLER_RUNS：不在新线程中执行任务，而是有调用者所在的线程来执行 executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); //执行初始化 executor.initialize(); return executor; } } @Value是我配置在application.properties，可以参考配置，自由定义 # 异步线程配置 # 配置核心线程数 async.executor.thread.core_pool_size = 5 # 配置最大线程数 async.executor.thread.max_pool_size = 5 # 配置队列大小 async.executor.thread.queue_capacity = 99999 # 配置线程池中的线程的名称前缀 async.executor.thread.name.prefix = async-service- 创建一个Service接口，是异步线程的接口 public interface AsyncService { /** * 执行异步任务 * 可以根据需求，自己加参数拟定，我这里就做个测试演示 */ void executeAsync(); } 实现类 @Service public class AsyncServiceImpl implements AsyncService { private static final Logger logger = LoggerFactory.getLogger(AsyncServiceImpl.class); @Override @Async(&quot;asyncServiceExecutor&quot;) public void executeAsync() { logger.info(&quot;start executeAsync&quot;); System.out.println(&quot;异步线程要做的事情&quot;); System.out.println(&quot;可以在这里执行批量插入等耗时的事情&quot;); logger.info(&quot;end executeAsync&quot;); } } 将Service层的服务异步化，在executeAsync()方法上增加注解@Async(&quot;asyncServiceExecutor&quot;)，asyncServiceExecutor方法是前面ExecutorConfig.java 中的方法名，表明executeAsync方法进入的线程池是asyncServiceExecutor方法创建的 接下来就是在Controller里或者是哪里通过注解@Autowired注入这个Service @Autowired private AsyncService asyncService; @GetMapping(&quot;/async&quot;) public void async(){ asyncService.executeAsync(); } 用postmain或者其他工具来多次测试请求一下 2018-07-16 22:15:47.655 INFO 10516 --- [async-service-5] c.u.d.e.executor.impl.AsyncServiceImpl : start executeAsync 异步线程要做的事情 可以在这里执行批量插入等耗时的事情 2018-07-16 22:15:47.655 INFO 10516 --- [async-service-5] c.u.d.e.executor.impl.AsyncServiceImpl : end executeAsync 2018-07-16 22:15:47.770 INFO 10516 --- [async-service-1] c.u.d.e.executor.impl.AsyncServiceImpl : start executeAsync 异步线程要做的事情 可以在这里执行批量插入等耗时的事情 2018-07-16 22:15:47.770 INFO 10516 --- [async-service-1] c.u.d.e.executor.impl.AsyncServiceImpl : end executeAsync 2018-07-16 22:15:47.816 INFO 10516 --- [async-service-2] c.u.d.e.executor.impl.AsyncServiceImpl : start executeAsync 异步线程要做的事情 可以在这里执行批量插入等耗时的事情 2018-07-16 22:15:47.816 INFO 10516 --- [async-service-2] c.u.d.e.executor.impl.AsyncServiceImpl : end executeAsync 2018-07-16 22:15:48.833 INFO 10516 --- [async-service-3] c.u.d.e.executor.impl.AsyncServiceImpl : start executeAsync 异步线程要做的事情 可以在这里执行批量插入等耗时的事情 2018-07-16 22:15:48.834 INFO 10516 --- [async-service-3] c.u.d.e.executor.impl.AsyncServiceImpl : end executeAsync 2018-07-16 22:15:48.986 INFO 10516 --- [async-service-4] c.u.d.e.executor.impl.AsyncServiceImpl : start executeAsync 异步线程要做的事情 可以在这里执行批量插入等耗时的事情 2018-07-16 22:15:48.987 INFO 10516 --- [async-service-4] c.u.d.e.executor.impl.AsyncServiceImpl : end executeAsync 通过以上日志可以发现，[async-service-]是有多个线程的，显然已经在我们配置的线程池中执行了，并且每次请求中，controller的起始和结束日志都是连续打印的，表明每次请求都快速响应了，而耗时的操作都留给线程池中的线程去异步执行； 虽然我们已经用上了线程池，但是还不清楚线程池当时的情况，有多少线程在执行，多少在队列中等待呢？这里我创建了一个ThreadPoolTaskExecutor的子类，在每次提交线程的时候都会将当前线程池的运行状况打印出来 import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor; import org.springframework.util.concurrent.ListenableFuture; import java.util.concurrent.Callable; import java.util.concurrent.Future; import java.util.concurrent.ThreadPoolExecutor; /** * @Author: ChenBin * @Date: 2018/7/16/0016 22:19 */ public class VisiableThreadPoolTaskExecutor extends ThreadPoolTaskExecutor { private static final Logger logger = LoggerFactory.getLogger(VisiableThreadPoolTaskExecutor.class); private void showThreadPoolInfo(String prefix) { ThreadPoolExecutor threadPoolExecutor = getThreadPoolExecutor(); if (null == threadPoolExecutor) { return; } logger.info(&quot;{}, {},taskCount [{}], completedTaskCount [{}], activeCount [{}], queueSize [{}]&quot;, this.getThreadNamePrefix(), prefix, threadPoolExecutor.getTaskCount(), threadPoolExecutor.getCompletedTaskCount(), threadPoolExecutor.getActiveCount(), threadPoolExecutor.getQueue().size()); } @Override public void execute(Runnable task) { showThreadPoolInfo(&quot;1. do execute&quot;); super.execute(task); } @Override public void execute(Runnable task, long startTimeout) { showThreadPoolInfo(&quot;2. do execute&quot;); super.execute(task, startTimeout); } @Override public Future&lt;?&gt; submit(Runnable task) { showThreadPoolInfo(&quot;1. do submit&quot;); return super.submit(task); } @Override public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) { showThreadPoolInfo(&quot;2. do submit&quot;); return super.submit(task); } @Override public ListenableFuture&lt;?&gt; submitListenable(Runnable task) { showThreadPoolInfo(&quot;1. do submitListenable&quot;); return super.submitListenable(task); } @Override public &lt;T&gt; ListenableFuture&lt;T&gt; submitListenable(Callable&lt;T&gt; task) { showThreadPoolInfo(&quot;2. do submitListenable&quot;); return super.submitListenable(task); } } 如上所示，showThreadPoolInfo方法中将任务总数、已完成数、活跃线程数，队列大小都打印出来了，然后Override了父类的execute、submit等方法，在里面调用showThreadPoolInfo方法，这样每次有任务被提交到线程池的时候，都会将当前线程池的基本情况打印到日志中； 修改ExecutorConfig.java的asyncServiceExecutor方法，将ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor()改为ThreadPoolTaskExecutor executor = new VisiableThreadPoolTaskExecutor() @Bean(name = &quot;asyncServiceExecutor&quot;) public Executor asyncServiceExecutor() { logger.info(&quot;start asyncServiceExecutor&quot;); //在这里修改 ThreadPoolTaskExecutor executor = new VisiableThreadPoolTaskExecutor(); //配置核心线程数 executor.setCorePoolSize(corePoolSize); //配置最大线程数 executor.setMaxPoolSize(maxPoolSize); //配置队列大小 executor.setQueueCapacity(queueCapacity); //配置线程池中的线程的名称前缀 executor.setThreadNamePrefix(namePrefix); // rejection-policy：当pool已经达到max size的时候，如何处理新任务 // CALLER_RUNS：不在新线程中执行任务，而是有调用者所在的线程来执行 executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); //执行初始化 executor.initialize(); return executor; } 再次启动该工程测试 2018-07-16 22:23:30.951 INFO 14088 --- [nio-8087-exec-2] u.d.e.e.i.VisiableThreadPoolTaskExecutor : async-service-, 2. do submit,taskCount [0], completedTaskCount [0], activeCount [0], queueSize [0] 2018-07-16 22:23:30.952 INFO 14088 --- [async-service-1] c.u.d.e.executor.impl.AsyncServiceImpl : start executeAsync 异步线程要做的事情 可以在这里执行批量插入等耗时的事情 2018-07-16 22:23:30.953 INFO 14088 --- [async-service-1] c.u.d.e.executor.impl.AsyncServiceImpl : end executeAsync 2018-07-16 22:23:31.351 INFO 14088 --- [nio-8087-exec-3] u.d.e.e.i.VisiableThreadPoolTaskExecutor : async-service-, 2. do submit,taskCount [1], completedTaskCount [1], activeCount [0], queueSize [0] 2018-07-16 22:23:31.353 INFO 14088 --- [async-service-2] c.u.d.e.executor.impl.AsyncServiceImpl : start executeAsync 异步线程要做的事情 可以在这里执行批量插入等耗时的事情 2018-07-16 22:23:31.353 INFO 14088 --- [async-service-2] c.u.d.e.executor.impl.AsyncServiceImpl : end executeAsync 2018-07-16 22:23:31.927 INFO 14088 --- [nio-8087-exec-5] u.d.e.e.i.VisiableThreadPoolTaskExecutor : async-service-, 2. do submit,taskCount [2], completedTaskCount [2], activeCount [0], queueSize [0] 2018-07-16 22:23:31.929 INFO 14088 --- [async-service-3] c.u.d.e.executor.impl.AsyncServiceImpl : start executeAsync 异步线程要做的事情 可以在这里执行批量插入等耗时的事情 2018-07-16 22:23:31.930 INFO 14088 --- [async-service-3] c.u.d.e.executor.impl.AsyncServiceImpl : end executeAsync 2018-07-16 22:23:32.496 INFO 14088 --- [nio-8087-exec-7] u.d.e.e.i.VisiableThreadPoolTaskExecutor : async-service-, 2. do submit,taskCount [3], completedTaskCount [3], activeCount [0], queueSize [0] 2018-07-16 22:23:32.498 INFO 14088 --- [async-service-4] c.u.d.e.executor.impl.AsyncServiceImpl : start executeAsync 异步线程要做的事情 可以在这里执行批量插入等耗时的事情 2018-07-16 22:23:32.499 INFO 14088 --- [async-service-4] c.u.d.e.executor.impl.AsyncServiceImpl : end executeAsync 注意这一行日志： 2018-07-16 22:23:32.496 INFO 14088 --- [nio-8087-exec-7] u.d.e.e.i.VisiableThreadPoolTaskExecutor : async-service-, 2. do submit,taskCount [3], completedTaskCount [3], activeCount [0], queueSize [0] 这说明提交任务到线程池的时候，调用的是submit(Callable task)这个方法，当前已经提交了3个任务，完成了3个，当前有0个线程在处理任务，还剩0个任务在队列中等待，线程池的基本情况一路了然 ","link":"https://tinaxiawuhao.github.io/post/y8VkHlUlp/"},{"title":"手写一个生产者消费者模式","content":"import lombok.SneakyThrows; import org.springframework.util.StringUtils; import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.BlockingQueue; import java.util.concurrent.TimeUnit; import java.util.concurrent.atomic.AtomicInteger; class MyResource { private volatile Boolean FLAG = true; private final AtomicInteger atomicInteger = new AtomicInteger(); BlockingQueue&lt;String&gt; blockingQueue = null; public MyResource(BlockingQueue&lt;String&gt; blockingQueue) { this.blockingQueue = blockingQueue; System.out.println(blockingQueue.getClass().getName()); } @SneakyThrows public void myProd() { String data = null; while (FLAG) { data = String.valueOf(atomicInteger.incrementAndGet()); if (blockingQueue.offer(data, 2L, TimeUnit.SECONDS)) { System.out.println(Thread.currentThread().getName() + &quot;插入数据&quot; + data + &quot;成功&quot;); } else { System.out.println(Thread.currentThread().getName() + &quot;插入数据&quot; + data + &quot;失败&quot;); } TimeUnit.SECONDS.sleep(1L); } System.out.println(Thread.currentThread().getName() + &quot;生产停止&quot;); } @SneakyThrows public void myConsumer() { String result = null; while (FLAG) { result = blockingQueue.poll(2L, TimeUnit.SECONDS); if (StringUtils.isEmpty(result)) { FLAG = false; System.out.println(Thread.currentThread().getName() + &quot;超过2秒没有取到值，结束&quot;); return; } System.out.println(Thread.currentThread().getName() + &quot;消费数据&quot; + result + &quot;成功&quot;); } } public void stop(){ FLAG=false; } } public class ProdConsumer_BlockQueue { @SneakyThrows public static void main(String[] args) { MyResource myResource = new MyResource(new ArrayBlockingQueue&lt;&gt;(10)); new Thread(() -&gt; { System.out.println(Thread.currentThread().getName() + &quot;生产线程启动&quot;); myResource.myProd(); }, &quot;Prod&quot;).start(); new Thread(() -&gt; { System.out.println(Thread.currentThread().getName() + &quot;消费线程启动&quot;); myResource.myConsumer(); }, &quot;Consumer&quot;).start(); TimeUnit.SECONDS.sleep(5L); System.out.println(&quot;5秒时间到，结束&quot;); myResource.stop(); } } ","link":"https://tinaxiawuhao.github.io/post/5QxJ3UtmK/"},{"title":"阻塞队列BlockingQueue","content":"为什么要使用阻塞队列 之前，介绍了一下 ThreadPoolExecutor 的各参数的含义（线程池ThreadPoolExecutor），其中有一个 BlockingQueue，它是一个阻塞队列。那么，小伙伴们有没有想过，为什么此处的线程池要用阻塞队列呢？ 我们知道队列是先进先出的。当放入一个元素的时候，会放在队列的末尾，取出元素的时候，会从队头取。那么，当队列为空或者队列满的时候怎么办呢。 这时，阻塞队列，会自动帮我们处理这种情况。 当阻塞队列为空的时候，从队列中取元素的操作就会被阻塞。当阻塞队列满的时候，往队列中放入元素的操作就会被阻塞。 而后，一旦空队列有数据了，或者满队列有空余位置时，被阻塞的线程就会被自动唤醒。 这就是阻塞队列的好处，你不需要关心线程何时被阻塞，也不需要关心线程何时被唤醒，一切都由阻塞队列自动帮我们完成。我们只需要关注具体的业务逻辑就可以了。 而这种阻塞队列经常用在生产者消费者模式中。（可参看：手写一个生产者消费者模式） 常用的阻塞队列 那么，一般我们用到的阻塞队列有哪些呢。下面，通过idea的类图，列出来常用的阻塞队列，然后一个一个讲解（不懂怎么用的，可以参考这篇文章：怎么用IDEA快速查看类图关系）。 阻塞队列中，所有常用的方法都在 BlockingQueue 接口中定义。如 插入元素的方法： put，offer，add。移除元素的方法： remove，poll，take。 它们有四种不同的处理方式，第一种是在失败时抛出异常，第二种是在失败时返回特殊值，第三种是一直阻塞当前线程，最后一种是在指定时间内阻塞，否则返回特殊值。（以上特殊值，是指在插入元素时，失败返回false，在取出元素时，失败返回null） 抛异常 特殊值 阻塞 超时 插入 add(e) offer(e) put(e) offer(e,time,unit) 移除 remove() poll() take() poll(time,unit) 1） ArrayBlockingQueue 这是一个由数组结构组成的有界阻塞队列。首先看下它的构造方法，有三个。 第一个可以指定队列的大小，第二个还可以指定队列是否公平，不指定的话，默认是非公平。它是使用 ReentrantLock 的公平锁和非公平锁实现的（后续讲解AQS时，会详细说明）。 简单理解就是，ReentrantLock 内部会维护一个有先后顺序的等待队列，假如有五个任务一起过来，都被阻塞了。如果是公平的，则等待队列中等待最久的任务就会先进入阻塞队列。如果是非公平的，那么这五个线程就需要抢锁，谁先抢到，谁就先进入阻塞队列。 第三个构造方法，是把一个集合的元素初始化到阻塞队列中。 另外，ArrayBlockingQueue 没有实现读写分离，也就是说，读和写是不能同时进行的。因为，它读写时用的是同一把锁，如下图所示： 2) LinkedBlockingQueue 这是一个由链表结构组成的有界阻塞队列。它的构造方法有三个。 可以看到和 ArrayBlockingQueue 的构造方法大同小异，不过是，LinkedBlockingQueue 可以不指定队列的大小，默认值是 Integer.MAX_VALUE 。 但是，最好不要这样做，建议指定一个固定大小。因为，如果生产者的速度比消费者的速度大的多的情况下，这会导致阻塞队列一直膨胀，直到系统内存被耗尽（此时，还没达到队列容量的最大值）。 此外，LinkedBlockingQueue 实现了读写分离，可以实现数据的读和写互不影响，这在高并发的场景下，对于效率的提高无疑是非常巨大的。 3） SynchronousQueue 这是一个没有缓冲的无界队列。什么意思，看一下它的 size 方法： 总是返回 0 ，因为它是一个没有容量的队列。 当执行插入元素的操作时，必须等待一个取出操作。也就是说，put元素的时候，必须等待 take 操作。 那么，有的同学就好奇了，这没有容量，还叫什么队列啊，这有什么意义呢。 我的理解是，这适用于并发任务不大，而且生产者和消费者的速度相差不多的场景下，直接把生产者和消费者对接，不用经过队列的入队出队这一系列操作。所以，效率上会高一些。 可以去查看一下 Excutors.newCachedThreadPool 方法用的就是这种队列。 这个队列有两个构造方法，用于传入是公平还是非公平，默认是非公平。 4）PriorityBlockingQueue 这是一个支持优先级排序的无界队列。有四个构造方法： 可以指定初始容量大小（注意初始容量并不代表最大容量），或者不指定，默认大小为 11。也可以传入一个比较器，把元素按一定的规则排序，不指定比较器的话，默认是自然顺序。 PriorityBlockingQueue 是基于二叉树最小堆实现的，每当取元素的时候，就会把优先级最高的元素取出来。我们测试一下： public class Person { private int id; private String name; public int getId() { return id; } public void setId(int id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } @Override public String toString() { return &quot;Person{&quot; + &quot;id=&quot; + id + &quot;, name='&quot; + name + '\\'' + '}'; } public Person(int id, String name) { this.id = id; this.name = name; } public Person() { } } public class QueueTest { public static void main(String[] args) throws InterruptedException { PriorityBlockingQueue&lt;Person&gt; priorityBlockingQueue = new PriorityBlockingQueue&lt;&gt;(1, new Comparator&lt;Person&gt;() { @Override public int compare(Person o1, Person o2) { return o1.getId() - o2.getId(); } }); Person p2 = new Person(7, &quot;李四&quot;); Person p1 = new Person(9, &quot;张三&quot;); Person p3 = new Person(6, &quot;王五&quot;); Person p4 = new Person(2, &quot;赵六&quot;); priorityBlockingQueue.add(p1); priorityBlockingQueue.add(p2); priorityBlockingQueue.add(p3); priorityBlockingQueue.add(p4); //由于二叉树最小堆实现，用这种方式直接打印元素，不能保证有序 System.out.println(priorityBlockingQueue); System.out.println(priorityBlockingQueue.take()); System.out.println(priorityBlockingQueue); System.out.println(priorityBlockingQueue.take()); System.out.println(priorityBlockingQueue); } } 打印结果： [Person{id=2, name='赵六'}, Person{id=6, name='王五'}, Person{id=7, name='李四'}, Person{id=9, name='张三'}] Person{id=2, name='赵六'} [Person{id=6, name='王五'}, Person{id=9, name='张三'}, Person{id=7, name='李四'}] Person{id=6, name='王五'} [Person{id=7, name='李四'}, Person{id=9, name='张三'}] 可以看到，第一次取出的是 id 最小值 2， 第二次取出的是 6 。 5）DelayQueue 这是一个带有延迟时间的无界阻塞队列。队列中的元素，只有等延时时间到了，才能取出来。此队列一般用于过期数据的删除，或任务调度。以下，模拟一下定长时间的数据删除。 首先定义数据元素，需要实现 Delayed 接口，实现 getDelay 方法用于计算剩余时间，和 CompareTo方法用于优先级排序。 public class DelayData implements Delayed { private int id; private String name; //数据到期时间 private long endTime; private TimeUnit timeUnit = TimeUnit.MILLISECONDS; public int getId() { return id; } public void setId(int id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public long getEndTime() { return endTime; } public void setEndTime(long endTime) { this.endTime = endTime; } public DelayData(int id, String name, long endTime) { this.id = id; this.name = name; //需要把传入的时间endTime 加上当前系统时间，作为数据的到期时间 this.endTime = endTime + System.currentTimeMillis(); } public DelayData() { } @Override public long getDelay(TimeUnit unit) { return this.endTime - System.currentTimeMillis(); } @Override public int compareTo(Delayed o) { return o.getDelay(this.timeUnit) - this.getDelay(this.timeUnit) &lt; 0 ? 1: -1; } } 模拟三条数据，分别设置不同的过期时间： public class ProcessData { public static void main(String[] args) throws InterruptedException { DelayQueue&lt;DelayData&gt; delayQueue = new DelayQueue&lt;&gt;(); DelayData a = new DelayData(5, &quot;A&quot;, 5000); DelayData b = new DelayData(8, &quot;B&quot;, 8000); DelayData c = new DelayData(2, &quot;C&quot;, 2000); delayQueue.add(a); delayQueue.add(b); delayQueue.add(c); System.out.println(&quot;开始计时时间:&quot; + System.currentTimeMillis()); for (int i = 0; i &lt; 3; i++) { DelayData data = delayQueue.take(); System.out.println(&quot;id:&quot;+data.getId()+&quot;，数据:&quot;+data.getName()+&quot;被移除，当前时间:&quot;+System.currentTimeMillis()); } } } 最后结果： 开始计时时间:1583333583216 id:2，数据:C被移除，当前时间:1583333585216 id:5，数据:A被移除，当前时间:1583333588216 id:8，数据:B被移除，当前时间:1583333591216 可以看到，数据是按过期时间长短，按顺序移除的。C的时间最短 2 秒，然后过了 3 秒 A 也过期，再过 3 秒，B 过期。 ","link":"https://tinaxiawuhao.github.io/post/volSqv4Kf/"},{"title":"Redis的缓存淘汰策略LRU","content":"redis缓存淘汰策略与Redis键的过期删除策略并不完全相同，前者是在Redis内存使用超过一定值的时候（一般这个值可以配置）使用的淘汰策略；而后者是通过定期删除+惰性删除两者结合的方式进行内存淘汰的。 Redis内存不足的缓存淘汰策略 noeviction：当内存使用超过配置的时候会返回错误，不会驱逐任何键 allkeys-lru：加入键的时候，如果过限，首先通过LRU算法驱逐最久没有使用的键 volatile-lru：加入键的时候如果过限，首先从设置了过期时间的键集合中驱逐最久没有使用的键 allkeys-random：加入键的时候如果过限，从所有key随机删除 volatile-random：加入键的时候如果过限，从过期键的集合中随机驱逐 volatile-ttl：从配置了过期时间的键中驱逐马上就要过期的键 volatile-lfu：从所有配置了过期时间的键中驱逐使用频率最少的键 allkeys-lfu：从所有键中驱逐使用频率最少的键 lru算法实现 取巧算法 import java.util.LinkedHashMap; import java.util.Map; class LRUCache&lt;K,V&gt; extends LinkedHashMap&lt;K,V&gt; { private int capacity; /** * the ordering mode * - &lt;tt&gt;true&lt;/tt&gt; for access-order, * - &lt;tt&gt;false&lt;/tt&gt; for insertion-order * @param capacity */ public LRUCache(int capacity) { super(capacity, 0.75F, true); this.capacity = capacity; } //数据超过容量大小删除 @Override protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) { return super.size()&gt;capacity; } } 数据结构实现 import java.util.HashMap; import java.util.Map; class LRUCacheDemo { //构建承载体node class Node&lt;K, V&gt; { K key; V value; Node&lt;K, V&gt; prev; Node&lt;K, V&gt; next; public Node() { this.prev = this.next = null; } public Node(K key, V value) { this.key = key; this.value = value; this.prev = this.next = null; } } //构建双向队列 class DoubleLinkedList&lt;K, V&gt; { Node&lt;K, V&gt; head; Node&lt;K, V&gt; tail; public DoubleLinkedList() { head = new Node&lt;&gt;(); tail = new Node&lt;&gt;(); head.next = tail; tail.prev = head; } //添加头节点 public void addHead(Node&lt;K, V&gt; node) { node.next = head.next; node.prev = head; head.next.prev = node; head.next = node; } //删除节点 public void removeNode(Node&lt;K, V&gt; node) { node.next.prev = node.prev; node.prev.next = node.next; node.prev = null; node.next = null; } //获取最后一个节点 public Node getLast() { return tail.prev; } } private int cacheSize; Map&lt;Object, Node&lt;Object, Object&gt;&gt; map; DoubleLinkedList&lt;Object, Object&gt; doubleLinkedList; public LRUCacheDemo(int cacheSize) { this.cacheSize = cacheSize; this.map = new HashMap&lt;&gt;(); this.doubleLinkedList = new DoubleLinkedList&lt;&gt;(); } public Object get(Object key) { if (!map.containsKey(key)) { return -1; } final Node&lt;Object, Object&gt; node = map.get(key); this.doubleLinkedList.removeNode(node); this.doubleLinkedList.addHead(node); return node.value; } public void put(Object key, Object value) { if (map.containsKey(key)) { final Node&lt;Object, Object&gt; node = map.get(key); node.value = value; map.put(key, node); this.doubleLinkedList.removeNode(node); this.doubleLinkedList.addHead(node); }else{ if(map.size()==this.cacheSize){ final Node last = this.doubleLinkedList.getLast(); map.remove(last.key); doubleLinkedList.removeNode(last); } //新增 Node&lt;Object,Object&gt; newNode=new Node&lt;&gt;(key,value); map.put(key, newNode); this.doubleLinkedList.addHead(newNode); } } } ","link":"https://tinaxiawuhao.github.io/post/bGEB86JC1/"},{"title":"list、set、map等集合类线程不安全的问题及解决方法","content":"List ArrayList不是线程安全类，在多线程同时写的情况下，会抛出java.util.ConcurrentModificationException异常。 private static void listNotSafe() { List&lt;String&gt; list=new ArrayList&lt;&gt;(); for (int i = 1; i &lt;= 30; i++) { new Thread(() -&gt; { list.add(UUID.randomUUID().toString().substring(0, 8)); System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + list); }, String.valueOf(i)).start(); } } 解决方法： 使用Vector（ArrayList所有方法加synchronized，太重）。 使用Collections.synchronizedList()转换成线程安全类。 使用java.concurrent.CopyOnWriteArrayList（推荐）。 CopyOnWriteArrayList这是JUC的类，通过写时复制来实现读写分离。比如其add()方法，就是先复制一个新数组，长度为原数组长度+1，然后将新数组最后一个元素设为添加的元素。 public boolean add(E e) { final ReentrantLock lock = this.lock; lock.lock(); try { //得到旧数组 Object[] elements = getArray(); int len = elements.length; //复制新数组 Object[] newElements = Arrays.copyOf(elements, len + 1); //设置新元素 newElements[len] = e; //设置新数组 setArray(newElements); return true; } finally { lock.unlock(); } } Set 跟List类似，HashSet和TreeSet都不是线程安全的，与之对应的有CopyOnWriteSet这个线程安全类。这个类底层维护了一个CopyOnWriteArrayList数组。 private final CopyOnWriteArrayList&lt;E&gt; al; public CopyOnWriteArraySet() { al = new CopyOnWriteArrayList&lt;E&gt;(); } 使用Collections.synchronizedList()转换成线程安全类。 HashSet和HashMap HashSet底层是用HashMap实现的。既然是用HashMap实现的，那HashMap.put()需要传两个参数，而HashSet.add()只传一个参数，这是为什么？实际上HashSet.add()就是调用的HashMap.put()，只不过Value被写死了，是一个private static final Object对象。 Map HashMap不是线程安全的，Hashtable是线程安全的，但是跟Vector类似，太重量级。所以也有类似CopyOnWriteMap，只不过叫ConcurrentHashMap。 关于集合安全类 import java.util.*; import java.util.concurrent.ConcurrentHashMap; import java.util.concurrent.CopyOnWriteArrayList; import java.util.concurrent.CopyOnWriteArraySet; public class ContainerNotSafeDemo { public static void main(String[] args) { listNotSafe(); setNoSafe(); mapNotSafe(); } private static void mapNotSafe() { //Map&lt;String,String&gt; map=new HashMap&lt;&gt;(); Map&lt;String, String&gt; map = new ConcurrentHashMap&lt;&gt;(); for (int i = 1; i &lt;= 30; i++) { new Thread(() -&gt; { map.put(Thread.currentThread().getName(), UUID.randomUUID().toString().substring(0, 8)); System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + map); }, String.valueOf(i)).start(); } } private static void setNoSafe() { //Set&lt;String&gt; set=new HashSet&lt;&gt;(); Set&lt;String&gt; set = new CopyOnWriteArraySet&lt;&gt;(); for (int i = 1; i &lt;= 30; i++) { new Thread(() -&gt; { set.add(UUID.randomUUID().toString().substring(0, 8)); System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + set); }, String.valueOf(i)).start(); } } private static void listNotSafe() { //List&lt;String&gt; list=new ArrayList&lt;&gt;(); List&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;(); for (int i = 1; i &lt;= 30; i++) { new Thread(() -&gt; { list.add(UUID.randomUUID().toString().substring(0, 8)); System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + list); }, String.valueOf(i)).start(); } } } ","link":"https://tinaxiawuhao.github.io/post/GALzQf_b-/"},{"title":"gradle依赖,插件","content":"依赖 configurations 设置configurations 配置依赖信息 build.gradle configurations { // 针对需要组件API的消费者的配置 exposedApi { // canBeResolved 为true 则为可解析配置，为消费者 canBeResolved = false // canBeConsumed 为true 则为消费析配置，为生产者 canBeConsumed = true } // 为需要实现该组件的消费者提供的配置。 exposedRuntime { canBeResolved = false canBeConsumed = true } } 依赖方式 模块依赖 build.gradle dependencies { runtimeOnly group: 'org.springframework', name: 'spring-core', version: '2.5' runtimeOnly 'org.springframework:spring-core:2.5', 'org.springframework:spring-aop:2.5' runtimeOnly( [group: 'org.springframework', name: 'spring-core', version: '2.5'], [group: 'org.springframework', name: 'spring-aop', version: '2.5'] ) runtimeOnly('org.hibernate:hibernate:3.0.5') { transitive = true } runtimeOnly group: 'org.hibernate', name: 'hibernate', version: '3.0.5', transitive: true runtimeOnly(group: 'org.hibernate', name: 'hibernate', version: '3.0.5') { transitive = true } } 文件依赖 build.gradle dependencies { runtimeOnly files('libs/a.jar', 'libs/b.jar') runtimeOnly fileTree('libs') { include '*.jar' } } 项目依赖 build.gradle dependencies { implementation project(':shared') } 依赖方式 compileOnly —用于编译生产代码所必需的依赖关系，但不应该属于运行时类路径的一部分 implementation（取代compile）-用于编译和运行时 runtimeOnly（取代runtime）-仅在运行时使用，不用于编译 testCompileOnly—与compileOnly测试相同 testImplementation —测试相当于 implementation testRuntimeOnly —测试相当于 runtimeOnly repositories 流行的公共存储库包括Maven Central， Bintray JCenter和Google Android存储库。 repositories { mavenCentral() // Maven Central存储库 jcenter() // JCenter Maven存储库 google() // Google Maven存储库 mavenLocal() // 将本地Maven缓存添加为存储库（不推荐） //flat存储库解析器 flatDir { dirs 'lib' } flatDir { dirs 'lib1', 'lib2' } //添加定制的Maven仓库 maven { url &quot;http://repo.mycompany.com/maven2&quot; // 为JAR文件添加附加的Maven存储库 artifactUrls &quot;http://repo.mycompany.com/jars&quot; } //Ivy ivy { url &quot;http://repo.mycompany.com/repo&quot; layout &quot;maven&quot; // 有效的命名布局值是'gradle'（默认值）'maven'和'ivy'。 } } 声明存储库过滤器 声明存储库内容 build.gradle repositories { maven { url &quot;https://repo.mycompany.com/maven2&quot; content { // this repository *only* contains artifacts with group &quot;my.company&quot; includeGroup &quot;my.company&quot; } } jcenter { content { // this repository contains everything BUT artifacts with group starting with &quot;my.company&quot; excludeGroupByRegex &quot;my\\\\.company.*&quot; } } } 默认情况下，存储库包含所有内容，不包含任何内容： 如果声明include，那么它排除了一切 include 以外的内容。 如果声明exclude，则它将包括除exclude之外的所有内容。 如果声明include和exclude，则它仅包括显式包括但不排除的内容。 分割快照和发行版 build.gradle repositories { maven { url &quot;https://repo.mycompany.com/releases&quot; mavenContent { releasesOnly() } } maven { url &quot;https://repo.mycompany.com/snapshots&quot; mavenContent { snapshotsOnly() } } } 支持的元数据源 受支持的元数据源 元数据源 描述 排序 Maven Ivy/flat dir gradleMetadata() 寻找Gradle.module文件 1 是 是 mavenPom() 查找Maven.pom文件 2 是 是 ivyDescriptor() 查找ivy.xml文件 2 没有 是 artifact() 直接寻找artifact 3 是 是 从Gradle 5.3开始，解析元数据文件（无论是Ivy还是Maven）时，Gradle将寻找一个标记，指示存在匹配的Gradle Module元数据文件。如果找到它，它将代替Ivy或Maven文件使用。 从Gradle5.6开始，您可以通过添加ignoreGradleMetadataRedirection()到metadataSources声明来禁用此行为。 例.不使用gradle元数据重定向的Maven存储库 build.gradle repositories { maven { url &quot;http://repo.mycompany.com/repo&quot; metadataSources { mavenPom() artifact() ignoreGradleMetadataRedirection() } } } GAV坐标 GAV坐标一般指group，artifact，version 变体 构建变体是针对不同环境的配置，例如android开发中一般有debug和release两种变体 声明功能变体 可以通过应用java或java-library插件来声明功能变体。以下代码说明了如何声明名为mongodbSupport的功能： 示例1.声明一个功能变量 Groovy``Kotlin build.gradle group = 'org.gradle.demo' version = '1.0' java { registerFeature('mongodbSupport') { usingSourceSet(sourceSets.main) } } 元数据 从存储库中提取的每个模块都有与之关联的元数据，例如其组，名称，版本以及它提供的带有工件和依赖项的不同变体 可配置组件元数据规则的示例 build.gradle class TargetJvmVersionRule implements ComponentMetadataRule { final Integer jvmVersion @Inject TargetJvmVersionRule(Integer jvmVersion) { this.jvmVersion = jvmVersion } @Inject ObjectFactory getObjects() { } void execute(ComponentMetadataContext context) { context.details.withVariant(&quot;compile&quot;) { attributes { attribute(TargetJvmVersion.TARGET_JVM_VERSION_ATTRIBUTE, jvmVersion) attribute(Usage.USAGE_ATTRIBUTE, objects.named(Usage, Usage.JAVA_API)) } } } } dependencies { components { withModule(&quot;commons-io:commons-io&quot;, TargetJvmVersionRule) { params(7) } withModule(&quot;commons-collections:commons-collections&quot;, TargetJvmVersionRule) { params(8) } } implementation(&quot;commons-io:commons-io:2.6&quot;) implementation(&quot;commons-collections:commons-collections:3.2.2&quot;) } 可以通过以下方法进行修改变体： allVariants：修改组件的所有变体 withVariant(name)：修改由名称标识的单个变体 addVariant(name)或addVariant(name, base)：从头开始 或通过 复制 现有变体的详细信息（基础）向组件添加新变体 可以调整每个变体的以下详细信息： 标识变体的属性-attributes {}块 该变体提供的功能-withCapabilities { }块 变体的依赖项，包括丰富的版本-withDependencies {}块 变体的依赖关系约束，包括丰富版本-withDependencyConstraints {}块 构成变体实际内容的已发布文件的位置-withFiles { }块 平台 使用平台 获取平台中声明的版本 build.gradle dependencies { // get recommended versions from the platform project api platform(project(':platform')) // no version required api 'commons-httpclient:commons-httpclient' } platform表示法是一种简写表示法，实际上在后台执行了一些操作： 它将org.gradle.category属性设置为platform，这意味着Gradle将选择依赖项的 平台 组件。 它默认设置endorseStrictVersions行为， 这意味着如果平台声明了严格的依赖关系，则将强制执行它们。 这意味着默认情况下，对平台的依赖项会触发该平台中定义的所有严格版本的继承， 这对于平台作者确保所有使用者在依赖项的版本方面都遵循自己的决定很有用。 可以通过显式调用doNotEndorseStrictVersions方法来将其关闭。 例.依靠一个BOM导入其依赖约束 build.gradle dependencies { // import a BOM implementation platform('org.springframework.boot:spring-boot-dependencies:1.5.8.RELEASE') // define dependencies without versions implementation 'com.google.code.gson:gson' implementation 'dom4j:dom4j' } 导入BOM，确保其定义的版本覆盖找到的任何其他版本 build.gradle dependencies { // import a BOM. The versions used in this file will override any other version found in the graph implementation enforcedPlatform('org.springframework.boot:spring-boot-dependencies:1.5.8.RELEASE') // define dependencies without versions implementation 'com.google.code.gson:gson' implementation 'dom4j:dom4j' // this version will be overridden by the one found in the BOM implementation 'org.codehaus.groovy:groovy:1.8.6' } Capability 声明组件的capability build.gradle configurations { apiElements { outgoing { capability(&quot;com.acme:my-library:1.0&quot;) capability(&quot;com.other:module:1.1&quot;) } } runtimeElements { outgoing { capability(&quot;com.acme:my-library:1.0&quot;) capability(&quot;com.other:module:1.1&quot;) } } } 解决冲突 按Capability（能力）解决冲突，（若存在相同能力的依赖性会失败） build.gradle @CompileStatic class AsmCapability implements ComponentMetadataRule { void execute(ComponentMetadataContext context) { context.details.with { if (id.group == &quot;asm&quot; &amp;&amp; id.name == &quot;asm&quot;) { allVariants { it.withCapabilities { // Declare that ASM provides the org.ow2.asm:asm capability, but with an older version it.addCapability(&quot;org.ow2.asm&quot;, &quot;asm&quot;, id.version) } } } } } } 一个带有日志框架隐式冲突的构建文件 build.gradle dependencies { // Activate the &quot;LoggingCapability&quot; rule components.all(LoggingCapability) } @CompileStatic class LoggingCapability implements ComponentMetadataRule { final static Set&lt;String&gt; LOGGING_MODULES = [&quot;log4j&quot;, &quot;log4j-over-slf4j&quot;] as Set&lt;String&gt; void execute(ComponentMetadataContext context) { context.details.with { if (LOGGING_MODULES.contains(id.name)) { allVariants { it.withCapabilities { // Declare that both log4j and log4j-over-slf4j provide the same capability it.addCapability(&quot;log4j&quot;, &quot;log4j&quot;, id.version) } } } } } } 版本 版本规则 Gradle支持不同的版本字符串声明方式： 一个确切的版本：比如1.3，1.3.0-beta3，1.0-20150201.131010-1 一个Maven风格的版本范围：例如 [1.0,) [1.1, 2.0) (1.2, 1.5] [和]的符号表示包含性约束; (和)表示排他性约束。 当上界或下界缺失时，该范围没有上界或下界。 符号]可以被用来代替(用于排他性下界，[代替)用于排他性上界。例如]1.0, 2.0[ 前缀版本范围：例如 1.+ 1.3.+ 仅包含与+之前部分完全匹配的版本。 +本身的范围将包括任何版本。 一个latest-status版本：例如latest.integration，latest.release Maven的SNAPSHOT版本标识符：例如1.0-SNAPSHOT，1.4.9-beta1-SNAPSHOT 版本排序 每个版本均分为其组成的“部分”： 字符[. - _ +]用于分隔版本的不同“部分”。 同时包含数字和字母的任何部分都将分为以下各个部分： 1a1 == 1.a.1 仅比较版本的各个部分。实际的分隔符并不重要：1.a.1 == 1-a+1 == 1.a-1 == 1a1 使用以下规则比较2个版本的等效部分： 如果两个部分都是数字，则最高数字值 较高 ：1.1&lt;1.2 如果一个部分是数值，则认为它 高于 非数字部分：1.a&lt;1.1 如果两个部分都不是数字，则按字母顺序比较，区分大小写：1.A&lt; 1.B&lt; 1.a&lt;1.b 有额外数字部分的版本被认为比没有数字部分的版本高：1.1&lt;1.1.0 带有额外的非数字部分的版本被认为比没有数字部分的版本低：1.1.a&lt;1.1 某些字符串值出于排序目的具有特殊含义： 字符串dev被认为比任何其他字符串部分低：1.0-dev&lt; 1.0-alpha&lt; 1.0-rc。 字符串rc、release和final被认为比任何其他字符串部分都高（按顺序排列：1.0-zeta&lt; 1.0-rc&lt; 1.0-release&lt; 1.0-final&lt; 1.0。 字符串SNAPSHOT没有特殊意义，和其他字符串部分一样按字母顺序排序：1.0-alpha&lt; 1.0-SNAPSHOT&lt; 1.0-zeta&lt; 1.0-rc&lt; 1.0。 数值快照版本没有特殊意义，和其他数值部分一样进行排序：1.0&lt; 1.0-20150201.121010-123&lt; 1.1。 简单来说:数字&gt;final&gt;release&gt;rc&gt;字母&gt;dev 声明没有版本的依赖 对于较大的项目，建议的做法是声明没有版本的依赖项， 并将依赖项约束 用于版本声明。 优势在于，依赖关系约束使您可以在一处管理所有依赖关系的版本，包括可传递的依赖关系。 build.gradle dependencies { implementation 'org.springframework:spring-web' } dependencies { constraints { implementation 'org.springframework:spring-web:5.0.2.RELEASE' } } 依赖方式 strictly 与该版本符号不匹配的任何版本将被排除。这是最强的版本声明。 在声明的依赖项上，strictly可以降级版本。 在传递依赖项上，如果无法选择此子句可接受的版本，将导致依赖项解析失败。 有关详细信息，请参见覆盖依赖项版本。 该术语支持动态版本。 定义后，将覆盖先前的require声明并清除之前的 reject。 require 表示所选版本不能低于require可接受的版本，但可以通过冲突解决方案提高，即使更高版本具有排他性更高的界限。 这就是依赖项上的直接版本所转换的内容。该术语支持动态版本。 定义后，将覆盖先前的strictly声明并清除之前的 reject。 prefer 这是一个非常软的版本声明。仅当对该模块的版本没有更强的非动态观点时，才适用。 该术语不支持动态版本。 定义可以补充strictly或require。 在级别层次结构之外还有一个附加术语： reject 声明模块不接受特定版本。如果唯一的可选版本也被拒绝，这将导致依赖项解析失败。该术语支持动态版本。 动态版本 build.gradle plugins { id 'java-library' } repositories { mavenCentral() } dependencies { implementation 'org.springframework:spring-web:5.+' } 版本快照 声明一个版本变化的依赖 build.gradle plugins { id 'java-library' } repositories { mavenCentral() maven { url 'https://repo.spring.io/snapshot/' } } dependencies { implementation 'org.springframework:spring-web:5.0.3.BUILD-SNAPSHOT' } 以编程方式控制依赖项缓存 您可以使用ResolutionStrategy 对配置进行编程来微调缓存的某些方面。 如果您想永久更改设置，则编程方式非常有用。 默认情况下，Gradle将动态版本缓存24小时。 要更改Gradle将解析后的版本缓存为动态版本的时间，请使用： 例.动态版本缓存控制 build.gradle configurations.all { resolutionStrategy.cacheDynamicVersionsFor 10, 'minutes' } 默认情况下，Gradle会将更改的模块缓存24小时。 要更改Gradle将为更改的模块缓存元数据和工件的时间，请使用： 例.改变模块缓存控制 build.gradle configurations.all { resolutionStrategy.cacheChangingModulesFor 4, 'hours' } 锁定配置 锁定特定配置 build.gradle configurations { compileClasspath { resolutionStrategy.activateDependencyLocking() } } 锁定所有配置 build.gradle dependencyLocking { lockAllConfigurations() } 解锁特定配置 build.gradle configurations { compileClasspath { resolutionStrategy.deactivateDependencyLocking() } } 锁定buildscript类路径配置 如果将插件应用于构建，则可能还需要利用依赖锁定。为了锁定用于脚本插件的classpath配置，请执行以下操作： build.gradle buildscript { configurations.classpath { resolutionStrategy.activateDependencyLocking() } } 使用锁定模式微调依赖项锁定行为 虽然默认锁定模式的行为如上所述，但是还有其他两种模式可用： Strict模式 ：在该模式下，除了上述验证外，如果被标记为锁定的配置没有与之相关联的锁定状态，则依赖性锁定将失败。 Lenient模式：在这种模式下，依存关系锁定仍将固定动态版本，但除此之外，依赖解析的变化不再是错误。 锁定模式可以从dependencyLocking块中进行控制，如下所示： build.gradle dependencyLocking { lockMode = LockMode.STRICT } 版本冲突 用force强制执行一个依赖版本 build.gradle dependencies { implementation 'org.apache.httpcomponents:httpclient:4.5.4' implementation('commons-codec:commons-codec:1.9') { force = true } } 排除特定依赖声明的传递依赖 build.gradle dependencies { implementation('commons-beanutils:commons-beanutils:1.9.4') { exclude group: 'commons-collections', module: 'commons-collections' } } 版本冲突时失败 build.gradle configurations.all { resolutionStrategy { failOnVersionConflict() } } 使用动态版本时失败 build.gradle configurations.all { resolutionStrategy { failOnDynamicVersions() } } 改变版本时失败 build.gradle configurations.all { resolutionStrategy { failOnChangingVersions() } } 解析无法再现时失败 build.gradle configurations.all { resolutionStrategy { failOnNonReproducibleResolution() } } 插件 插件作用：将插件应用于项目可以使插件扩展项目的功能。它可以执行以下操作： 扩展Gradle模型（例如，添加可以配置的新DSL元素） 根据约定配置项目（例如，添加新任务或配置合理的默认值） 应用特定的配置（例如，添加组织存储库或强制执行标准） 简单来说，插件可以拓展项目功能，如任务，依赖，拓展属性，约束 插件类型 二进制插件 ：通过实现插件接口以编程方式编写二进制插件，或使用Gradle的一种DSL语言以声明方式编写二进制插件 脚本插件 ：脚本插件是其他构建脚本，可以进一步配置构建，并通常采用声明式方法来操纵构建 插件通常起初是脚本插件（因为它们易于编写），然后，随着代码变得更有价值，它被迁移到可以轻松测试并在多个项目或组织之间共享的二进制插件。 应用插件 二进制插件 实现了org.gradle.api.Plugin接口 apply plugin: 'com.android.application' apply plugin apply plugin: 'java' //id == apply plugin: org.gradle.api.plugins.JavaPlugin //类型 == apply plugin: JavaPlugin //org.gradle.api.plugins默认导入 plugins DSL plugins { id 'java' //应用核心插件 id 'com.jfrog.bintray' version '0.4.1' //应用社区插件 id 'com.example.hello' version '1.0.0' apply false //使用`apply false`语法告诉Gradle不要将插件应用于当前项目 } 脚本插件 脚本插件会自动解析，可以从本地文件系统或远程位置的脚本中应用。可以将多个脚本插件（任意一种形式）应用于给定目标。 apply from:'version.gradle' apply可传入内容 void apply(Map&lt;String,? options); void apply(Closure closure); void apply(Action&lt;? super ObjectConfigurationAction&gt; action); 定义插件 定义一个带有ID的buildSrc插件 buildSrc / build.gradle plugins { id 'java-gradle-plugin' } gradlePlugin { plugins { myPlugins { id = 'my-plugin' implementationClass = 'my.MyPlugin' } } } 第三方插件 通过将插件添加到构建脚本classpath中，然后应用该插件，可以将已发布为外部jar文件的二进制插件添加到项目中。可以使用buildscript {}块将外部jar添加到构建脚本classpath中。 buildscript { repositories { google() jcenter() } dependencies { classpath &quot;com.android.tools.build:gradle:4.0.1&quot; } } 插件管理 pluginManagement {}块只能出现在settings.gradle文件中，必须是文件中的第一个块，也可以以settings形式出现在初始化脚本中。 settings.gradle pluginManagement { plugins { } resolutionStrategy { } repositories { } } rootProject.name = 'plugin-management' init.gradle settingsEvaluated { settings -&gt; settings.pluginManagement { plugins { } resolutionStrategy { } repositories { } } } 例：通过pluginManagement管理插件版本。 settings.gradle pluginManagement { plugins { id 'com.example.hello' version &quot;${helloPluginVersion}&quot; } } gradle.properties helloPluginVersion=1.0.0 自定义插件存储库 要指定自定义插件存储库，请使用repositories {}块其中的pluginManagement {}： settings.gradle pluginManagement { repositories { maven { url '../maven-repo' } gradlePluginPortal() ivy { url '../ivy-repo' } } } java库 导入java apply plugin:'java' 自定义路径 sourceSets { main { java { srcDirs = ['src'] } } test { java { srcDirs = ['test'] } } } sourceSets { main { java { srcDir 'thirdParty/src/main/java' } } } 导入依赖 repositories { jcenter() } dependencies { implementation group:'com.android.support',name:'appcompat-v7',version:'28.0.0' implementation 'com.android.support:appcompat-v7:28.0.0' implementation protect(':p') implementation file('libs/ss.jar','libs/ss2.jar') implementation fileTree(dir: &quot;libs&quot;, include: [&quot;*.jar&quot;]) } 多项目 设置 settings.gradle include ':app' rootProject.name = &quot;GradleTest&quot; 安卓实用 设置签名 android { signingConfig = { release { storeFile file(&quot;MYKEY.keystore&quot;) storePassword &quot;storePassword&quot; keyAlias &quot;keyAlias&quot; keyPassword &quot;keyPassword&quot; } } } 自定义输出apk文件名称 applicationVariants.all { variant -&gt; variant.outputs.all { output -&gt; def fileName = &quot;自定义名称_${variant.versionName}_release.apk&quot; def outFile = output.outputFile if (outFile != null &amp;&amp; outFile.name.endsWith('.apk')) { outputFileName = fileName } } } 动态AndroidManifest &lt;meta-data android:name=&quot;paramName&quot; android:value=&quot;${PARAM_NAME}&quot;&gt; android { productFlavors{ google{ manifestPlaceholders.put(&quot;PARAM_NAME&quot;,'google') } } } 多渠道 android { productFlavors{ google{ }, baidu{ } } productFlavors.all{ flavor-&gt; manifestPlaceholders.put(&quot;PARAM_NAME&quot;,name) } } adb设置 adb工具 android { adbOptions{ timeOutInMs = 5000 //5s超时 installOptions '-r','-s' //安装指令 } } dexOptions dex工具 android { dexOptions{ incremental true //增量 javaMaxHeapSize '4G'//dx最大队内存 jumboMode true //强制开启jumbo跳过65535限制 preDexLibraries true //提高增量构建速度 threadCount 1 //dx线程数量 } } Ant 例.将嵌套元素传递给Ant任 build.gradle task zip { doLast { ant.zip(destfile: 'archive.zip') { fileset(dir: 'src') { include(name: '**.xml') exclude(name: '**.java') } } } } 例.使用Ant类型 build.gradle task list { doLast { def path = ant.path { fileset(dir: 'libs', includes: '*.jar') } path.list().each { println it } } } 例.使用定制的Ant任务 build.gradle task check { doLast { ant.taskdef(resource: 'checkstyletask.properties') { classpath { fileset(dir: 'libs', includes: '*.jar') } } ant.checkstyle(config: 'checkstyle.xml') { fileset(dir: 'src') } } } Lint Lint：android tool目录下的工具，一个代码扫描工具，能够帮助我们识别资源、代码结构存在的问题 lintOptions android { lintOptions{ abortOnError true //发生错误时推出Gradle absolutePaths true //配置错误输出是否显示绝对路径 check 'NewApi','InlinedApi' // 检查lint check的issue id enable 'NewApi','InlinedApi' //启动 lint check的issue id disable 'NewApi','InlinedApi' //关闭 lint check的issue id checkAllWarnings true //检查所有警告issue ignoreWarning true //忽略警告检查，默认false checkReleaseBuilds true //检查致命错误，默认true explainIssues true //错误报告是否包含解释说明，默认true htmlOutput new File(&quot;/xx.html&quot;) //html报告输出路径 htmlReport true // 是否生成html报告，默认true lintConfig new File(&quot;/xx.xml&quot;) //lint配置 noLines true // 输出不带行号 默认true quite true // 安静模式 showAll true //是否显示所有输出，不截断 } } ","link":"https://tinaxiawuhao.github.io/post/Fe3q50EYd/"},{"title":"gradle-Logging","content":"Logging 日志级别 日志级别 说明 ERROR 错误讯息 QUIET 重要信息消息 WARNING 警告讯息 LIFECYCLE 进度信息消息 INFO 信息讯息 DEBUG 调试信息 选择日志级别 可以通过命令行选项或者gradle.properties文件配置 选项 输出日志级别 没有记录选项 LIFECYCLE及更高 -q or--quiet QUIET及更高 -w or --warn WARNING及更高 -i or --info INFO及更高 -d or --debug DEBUG及更高版本（即所有日志消息） Stacktrace命令行选项 -s or --stacktrace 打印简洁的堆栈跟踪信息，推荐使用 -S or --full-stacktrace 打印完整的堆栈跟踪信息。 编写日志 使用stdout编写日志消息 println 'A message which is logged at QUIET level' 编写自己的日志消息 logger.quiet('An info log message which is always logged.') logger.error('An error log message.') logger.warn('A warning log message.') logger.lifecycle('A lifecycle info log message.') logger.info('An info log message.') logger.debug('A debug log message.') logger.trace('A trace log message.') // Gradle never logs TRACE level logs // 用占位符写一条日志消息 logger.info('A {} log message', 'info') logger构建脚本提供了一个属性，该脚本是Logger的实例。该接口扩展了SLF4JLogger接口，并向其中添加了一些Gradle特定的方法 使用SLF4J写入日志消息 import org.slf4j.LoggerFactory def slf4jLogger = LoggerFactory.getLogger('some-logger') slf4jLogger.info('An info log message logged using SLF4j') 构建生命周期 Gradle的核心是一种基于依赖性的编程语言，这意味着你可以定义任务和任务之间的依赖关系。 Gradle保证这些任务按照其依赖关系的顺序执行，并且每个任务只执行一次。 这些任务形成一个定向无环图。 构建阶段 Gradle构建具有三个不同的阶段。 初始化 Gradle支持单项目和多项目构建。在初始化阶段，Gradle决定要参与构建的项目，并为每个项目创建一个Project实例。 配置 在此阶段，将配置项目对象。执行作为构建一部分的 所有 项目的构建脚本。 执行 Gradle确定要在配置阶段创建和配置的任务子集。子集由传递给gradle命令的任务名称参数和当前目录确定。然后Gradle执行每个选定的任务。 设置文件 默认名称是settings.gradle 项目构建在多项目层次结构的根项目中必须具有一个settings.gradle文件。 对于单项目构建，设置文件是可选的 初始化 查找settings.gradle文件判断是否多项目 没有settings.gradle或settings.gradle没有多项目配置则为单项目 例：将test任务添加到每个具有特定属性集的项目 build.gradle allprojects { afterEvaluate { project -&gt; if (project.hasTests) { println &quot;Adding test task to $project&quot; project.task('test') { doLast { println &quot;Running tests for $project&quot; } } } } } 输出 gradle -q test &gt; gradle -q test Adding test task to project ':project-a' Running tests for project ':project-a' 初始化脚本 初始化脚本与Gradle中的其他脚本相似。但是，这些脚本在构建开始之前运行。初始化脚本不能访问buildSrc项目中的类。 使用初始化脚本 有几种使用初始化脚本的方法： 在命令行中指定一个文件。命令行选项是-I或-init-script，后面是脚本的路径。 命令行选项可以出现一次以上，每次都会添加另一个 init 脚本。 如果命令行上指定的文件不存在，编译将失败。 在 USER_HOME /.gradle/目录中放置一个名为init.gradle（或init.gradle.ktsKotlin）的文件。 在 USER_HOME /.gradle/init.d/目录中放置一个以.gradle（或.init.gradle.ktsKotlin）结尾的文件。 在Gradle发行版的 GRADLE_HOME /init.d/目录中放置一个以.gradle（或.init.gradle.ktsKotlin）结尾的文件。这使您可以打包包含一些自定义构建逻辑和插件的自定义Gradle发行版。您可以将其与Gradle Wrapper结合使用，以使自定义逻辑可用于企业中的所有内部版本。 如果发现一个以上的初始化脚本，它们将按照上面指定的顺序依次执行。 示例 build.gradle repositories { mavenCentral() } task showRepos { doLast { println &quot;All repos:&quot; println repositories.collect { it.name } } } init.gradle allprojects { repositories { mavenLocal() } } 运行任务： gradle --init-script init.gradle -q showRepos 初始化脚本里面依赖添加依赖 init.gradle initscript { repositories { mavenCentral() } dependencies { classpath 'org.apache.commons:commons-math:2.0' } } 多项目 可在settings.gradle文件中设置多个项目关系，如下项目结构 . ├── app │ ... │ └── build.gradle └── settings.gradle settings.gradle rootProject.name = 'basic-multiproject' //根项目名 include 'app' //子项目 子项目间依赖 dependencies { implementation(project(&quot;:shared&quot;)) } ","link":"https://tinaxiawuhao.github.io/post/wYNztHepH/"},{"title":"Groovy基础","content":"Groovy基础 基本规则 没有分号 方法括号可以省略 方法可以不写return，返回最后一句代码 代码块可以作为参数传递 定义 def param = 'hello world' def param1 = &quot;hello world&quot; println &quot;${param1} ,li&quot; ${}里面可以放变量，也可以是表达式，只有双引号里面可以使用 声明变量 可以在构建脚本中声明两种变量：局部变量和额外属性。 局部变量 局部变量用def关键字声明。它们仅在声明它们的范围内可见。局部变量是基础Groovy语言的功能。 def dest = &quot;dest&quot; task copy(type: Copy) { from &quot;source&quot; into dest } ext属性 Gradle的域模型中的所有增强对象都可以容纳额外的用户定义属性。 可以通过拥有对象的ext属性添加，读取和设置其他属性。可以使用一个ext块一次添加多个属性。 plugins { id 'java' } ext { springVersion = &quot;3.1.0.RELEASE&quot; emailNotification = &quot;build@master.org&quot; } sourceSets.all { ext.purpose = null } sourceSets { main { purpose = &quot;production&quot; } test { purpose = &quot;test&quot; } plugin { purpose = &quot;production&quot; } } task printProperties { doLast { println springVersion println emailNotification sourceSets.matching { it.purpose == &quot;production&quot; }.each { println it.name } } } 输出 gradle -q printProperties &gt; gradle -q printProperties 3.1.0.RELEASE build@master.org main plugin 变量范围：本地和脚本范围 用类型修饰符声明的变量在闭包中可见，但在方法中不可见。 String localScope1 = 'localScope1' def localScope2 = 'localScope2' scriptScope = 'scriptScope' println localScope1 println localScope2 println scriptScope closure = { println localScope1 println localScope2 println scriptScope } def method() { try { localScope1 } catch (MissingPropertyException e) { println 'localScope1NotAvailable' } try { localScope2 } catch(MissingPropertyException e) { println 'localScope2NotAvailable' } println scriptScope } closure.call() method() 输出 groovy scope.groovy &gt; groovy作用域 localScope1 localScope2 scriptScope localScope1 localScope2 scriptScope localScope1NotAvailable localScope2NotAvailable scriptScope 对象 使用对象 您可以按照以下易读的方式配置任意对象。 build.gradle import java.text.FieldPosition task configure { doLast { def pos = configure(new FieldPosition(10)) { beginIndex = 1 endIndex = 5 } println pos.beginIndex println pos.endIndex } } &gt; gradle -q configure 1 5 使用外部脚本配置任意对象 您也可以使用外部脚本配置任意对象。 build.gradle task configure { doLast { def pos = new java.text.FieldPosition(10) // Apply the script apply from: 'other.gradle', to: pos println pos.beginIndex println pos.endIndex } } other.gradle // Set properties. beginIndex = 1 endIndex = 5 输出 gradle -q configure &gt; gradle -q configure 1 5 属性访问器 Groovy自动将属性引用转换为对适当的getter或setter方法的调用。 build.gradle // Using a getter method println project.buildDir println getProject().getBuildDir() // Using a setter method project.buildDir = 'target' getProject().setBuildDir('target') 闭包 闭包（闭合代码块，可以引用传入的变量） task testClosure { doLast { func { println it } funa { a, b -&gt; println a + b } } } def funa(closure) { closure(10, 3) } def func(closure) { closure(10) } 闭包委托 每个闭包都有一个delegate对象，Groovy使用该对象来查找不是闭包的局部变量或参数的变量和方法引用。 例.闭包委托 class Info { int id; String code; def log() { println(&quot;code:${code};id:${id}&quot;) } } def info(Closure&lt;Info&gt; closure) { Info p = new Info() closure.delegate = p // 委托模式优先 closure.setResolveStrategy(Closure.DELEGATE_FIRST) closure(p) } task configClosure { doLast { info { code = &quot;cix&quot; id = 1 log() } } } 输出 &gt; Task :configClosure code:cix;id:1 BUILD SUCCESSFUL in 276ms 例：使用必包委托设置依赖 dependencies { assert delegate == project.dependencies testImplementation('junit:junit:4.13') delegate.testImplementation('junit:junit:4.13') } 方法 方法调用上的可选括号 括号对于方法调用是可选的。 build.gradle test.systemProperty 'some.prop', 'value' test.systemProperty('some.prop', 'value') 闭包作为方法中的最后一个参数 当方法的最后一个参数是闭包时，可以将闭包放在方法调用之后： build.gradle repositories { println &quot;in a closure&quot; } repositories() { println &quot;in a closure&quot; } repositories({ println &quot;in a closure&quot; }) 集合 List def list = [1,2,3,4] println list[0] // 1 println list[-1] // 4 最后一个 println list[-2] // 3 倒数第二个 println list[0..2] // 第1-3个 list.each { //迭代 println it } Map def map= ['name':'li', 'age':18] println map[name] // li println map.age // 18 list.each { //迭代 println &quot;${it.key}:${it.value}&quot; } JavaBean class A{ private int a; //可通过A().a 获取修改 public int getB(){//可通过A().b获取，但不能修改 1 } } build.gradle // List literal test.includes = ['org/gradle/api/**', 'org/gradle/internal/**'] List&lt;String&gt; list = new ArrayList&lt;String&gt;() list.add('org/gradle/api/**') list.add('org/gradle/internal/**') test.includes = list // Map literal. Map&lt;String, String&gt; map = [key1:'value1', key2: 'value2'] // Groovy will coerce named arguments // into a single map argument apply plugin: 'java' 默认导入 为了使构建脚本更简洁，Gradle自动向Gradle脚本添加了一些类。 import org.gradle.* import org.gradle.api.* import org.gradle.api.artifacts.* import org.gradle.api.artifacts.component.* import org.gradle.api.artifacts.dsl.* import org.gradle.api.artifacts.ivy.* import org.gradle.api.artifacts.maven.* import org.gradle.api.artifacts.query.* import org.gradle.api.artifacts.repositories.* import org.gradle.api.artifacts.result.* import org.gradle.api.artifacts.transform.* import org.gradle.api.artifacts.type.* import org.gradle.api.artifacts.verification.* import org.gradle.api.attributes.* import org.gradle.api.attributes.java.* import org.gradle.api.capabilities.* import org.gradle.api.component.* import org.gradle.api.credentials.* import org.gradle.api.distribution.* import org.gradle.api.distribution.plugins.* import org.gradle.api.execution.* import org.gradle.api.file.* import org.gradle.api.initialization.* import org.gradle.api.initialization.definition.* import org.gradle.api.initialization.dsl.* import org.gradle.api.invocation.* import org.gradle.api.java.archives.* import org.gradle.api.jvm.* import org.gradle.api.logging.* import org.gradle.api.logging.configuration.* import org.gradle.api.model.* import org.gradle.api.plugins.* import org.gradle.api.plugins.antlr.* import org.gradle.api.plugins.quality.* import org.gradle.api.plugins.scala.* import org.gradle.api.provider.* import org.gradle.api.publish.* import org.gradle.api.publish.ivy.* import org.gradle.api.publish.ivy.plugins.* import org.gradle.api.publish.ivy.tasks.* import org.gradle.api.publish.maven.* import org.gradle.api.publish.maven.plugins.* import org.gradle.api.publish.maven.tasks.* import org.gradle.api.publish.plugins.* import org.gradle.api.publish.tasks.* import org.gradle.api.reflect.* import org.gradle.api.reporting.* import org.gradle.api.reporting.components.* import org.gradle.api.reporting.dependencies.* import org.gradle.api.reporting.dependents.* import org.gradle.api.reporting.model.* import org.gradle.api.reporting.plugins.* import org.gradle.api.resources.* import org.gradle.api.services.* import org.gradle.api.specs.* import org.gradle.api.tasks.* import org.gradle.api.tasks.ant.* import org.gradle.api.tasks.application.* import org.gradle.api.tasks.bundling.* import org.gradle.api.tasks.compile.* import org.gradle.api.tasks.diagnostics.* import org.gradle.api.tasks.incremental.* import org.gradle.api.tasks.javadoc.* import org.gradle.api.tasks.options.* import org.gradle.api.tasks.scala.* import org.gradle.api.tasks.testing.* import org.gradle.api.tasks.testing.junit.* import org.gradle.api.tasks.testing.junitplatform.* import org.gradle.api.tasks.testing.testng.* import org.gradle.api.tasks.util.* import org.gradle.api.tasks.wrapper.* import org.gradle.authentication.* import org.gradle.authentication.aws.* import org.gradle.authentication.http.* import org.gradle.build.event.* import org.gradle.buildinit.plugins.* import org.gradle.buildinit.tasks.* import org.gradle.caching.* import org.gradle.caching.configuration.* import org.gradle.caching.http.* import org.gradle.caching.local.* import org.gradle.concurrent.* import org.gradle.external.javadoc.* import org.gradle.ide.visualstudio.* import org.gradle.ide.visualstudio.plugins.* import org.gradle.ide.visualstudio.tasks.* import org.gradle.ide.xcode.* import org.gradle.ide.xcode.plugins.* import org.gradle.ide.xcode.tasks.* import org.gradle.ivy.* import org.gradle.jvm.* import org.gradle.jvm.application.scripts.* import org.gradle.jvm.application.tasks.* import org.gradle.jvm.platform.* import org.gradle.jvm.plugins.* import org.gradle.jvm.tasks.* import org.gradle.jvm.tasks.api.* import org.gradle.jvm.test.* import org.gradle.jvm.toolchain.* import org.gradle.language.* import org.gradle.language.assembler.* import org.gradle.language.assembler.plugins.* import org.gradle.language.assembler.tasks.* import org.gradle.language.base.* import org.gradle.language.base.artifact.* import org.gradle.language.base.compile.* import org.gradle.language.base.plugins.* import org.gradle.language.base.sources.* import org.gradle.language.c.* import org.gradle.language.c.plugins.* import org.gradle.language.c.tasks.* import org.gradle.language.coffeescript.* import org.gradle.language.cpp.* import org.gradle.language.cpp.plugins.* import org.gradle.language.cpp.tasks.* import org.gradle.language.java.* import org.gradle.language.java.artifact.* import org.gradle.language.java.plugins.* import org.gradle.language.java.tasks.* import org.gradle.language.javascript.* import org.gradle.language.jvm.* import org.gradle.language.jvm.plugins.* import org.gradle.language.jvm.tasks.* import org.gradle.language.nativeplatform.* import org.gradle.language.nativeplatform.tasks.* import org.gradle.language.objectivec.* import org.gradle.language.objectivec.plugins.* import org.gradle.language.objectivec.tasks.* import org.gradle.language.objectivecpp.* import org.gradle.language.objectivecpp.plugins.* import org.gradle.language.objectivecpp.tasks.* import org.gradle.language.plugins.* import org.gradle.language.rc.* import org.gradle.language.rc.plugins.* import org.gradle.language.rc.tasks.* import org.gradle.language.routes.* import org.gradle.language.scala.* import org.gradle.language.scala.plugins.* import org.gradle.language.scala.tasks.* import org.gradle.language.scala.toolchain.* import org.gradle.language.swift.* import org.gradle.language.swift.plugins.* import org.gradle.language.swift.tasks.* import org.gradle.language.twirl.* import org.gradle.maven.* import org.gradle.model.* import org.gradle.nativeplatform.* import org.gradle.nativeplatform.platform.* import org.gradle.nativeplatform.plugins.* import org.gradle.nativeplatform.tasks.* import org.gradle.nativeplatform.test.* import org.gradle.nativeplatform.test.cpp.* import org.gradle.nativeplatform.test.cpp.plugins.* import org.gradle.nativeplatform.test.cunit.* import org.gradle.nativeplatform.test.cunit.plugins.* import org.gradle.nativeplatform.test.cunit.tasks.* import org.gradle.nativeplatform.test.googletest.* import org.gradle.nativeplatform.test.googletest.plugins.* import org.gradle.nativeplatform.test.plugins.* import org.gradle.nativeplatform.test.tasks.* import org.gradle.nativeplatform.test.xctest.* import org.gradle.nativeplatform.test.xctest.plugins.* import org.gradle.nativeplatform.test.xctest.tasks.* import org.gradle.nativeplatform.toolchain.* import org.gradle.nativeplatform.toolchain.plugins.* import org.gradle.normalization.* import org.gradle.platform.base.* import org.gradle.platform.base.binary.* import org.gradle.platform.base.component.* import org.gradle.platform.base.plugins.* import org.gradle.play.* import org.gradle.play.distribution.* import org.gradle.play.platform.* import org.gradle.play.plugins.* import org.gradle.play.plugins.ide.* import org.gradle.play.tasks.* import org.gradle.play.toolchain.* import org.gradle.plugin.devel.* import org.gradle.plugin.devel.plugins.* import org.gradle.plugin.devel.tasks.* import org.gradle.plugin.management.* import org.gradle.plugin.use.* import org.gradle.plugins.ear.* import org.gradle.plugins.ear.descriptor.* import org.gradle.plugins.ide.* import org.gradle.plugins.ide.api.* import org.gradle.plugins.ide.eclipse.* import org.gradle.plugins.ide.idea.* import org.gradle.plugins.javascript.base.* import org.gradle.plugins.javascript.coffeescript.* import org.gradle.plugins.javascript.envjs.* import org.gradle.plugins.javascript.envjs.browser.* import org.gradle.plugins.javascript.envjs.http.* import org.gradle.plugins.javascript.envjs.http.simple.* import org.gradle.plugins.javascript.jshint.* import org.gradle.plugins.javascript.rhino.* import org.gradle.plugins.signing.* import org.gradle.plugins.signing.signatory.* import org.gradle.plugins.signing.signatory.pgp.* import org.gradle.plugins.signing.type.* import org.gradle.plugins.signing.type.pgp.* import org.gradle.process.* import org.gradle.swiftpm.* import org.gradle.swiftpm.plugins.* import org.gradle.swiftpm.tasks.* import org.gradle.testing.base.* import org.gradle.testing.base.plugins.* import org.gradle.testing.jacoco.plugins.* import org.gradle.testing.jacoco.tasks.* import org.gradle.testing.jacoco.tasks.rules.* import org.gradle.testkit.runner.* import org.gradle.vcs.* import org.gradle.vcs.git.* import org.gradle.work.* import org.gradle.workers.* 导入依赖 buildscript { repositories { mavenCentral() } dependencies { classpath group: 'commons-codec', name: 'commons-codec', version: '1.2' } } ","link":"https://tinaxiawuhao.github.io/post/4WWjxpN2N/"},{"title":"gradle项目与任务","content":"项目与任务 Gradle中的所有内容都位于两个基本概念之上： projects ：每个Gradle构建都由一个或多个 projects组成 ，一个projects代表什么取决于您使用Gradle做的事情。例如，一个projects可能代表一个JAR库或一个Web应用程序。 tasks ：每个projects由一个或多个 tasks组成 。tasks代表构建执行的一些原子工作。这可能是编译某些类，创建JAR，生成Javadoc或将一些存档发布到存储库。 项目 表.项目属性 名称 类型 默认值 project Project 该Project实例 name String 项目目录的名称。 path String 项目的绝对路径。 description String 项目说明。 projectDir File 包含构建脚本的目录。 buildDir File projectDir /build group Object unspecified version Object unspecified ant ant build 一个AntBuilder实例 任务 定义任务 使用字符串作为任务名称定义任务 使用tasks容器定义任务 使用DSL特定语法定义任务 例： build.gradle // 使用字符串作为任务名称定义任务 task('hello') { doLast { println &quot;hello&quot; } } // 使用tasks容器定义任务 tasks.create('hello') { doLast { println &quot;hello&quot; } } // 使用DSL特定语法定义任务 task(hello) { doLast { println &quot;hello&quot; } } task('copy', type: Copy) { from(file('srcDir')) into(buildDir) } 定位任务 使用DSL特定语法访问任务 通过任务集合访问任务 通过路径访问 按任务类型访问任务 task hello task copy(type: Copy) // 使用DSL特定语法访问任务 println hello.name println project.hello.name println copy.destinationDir println project.copy.destinationDir // 通过任务集合访问任务 println tasks.hello.name println tasks.named('hello').get().name println tasks.copy.destinationDir println tasks.named('copy').get().destinationDir //按任务类型访问任务 tasks.withType(Tar).configureEach { enabled = false } task test { dependsOn tasks.withType(Copy) } 通过路径访问 project-a / build.gradle task hello build.gradle task hello println tasks.getByPath('hello').path println tasks.getByPath(':hello').path println tasks.getByPath('project-a:hello').path println tasks.getByPath(':project-a:hello').path 配置任务 使用API配置任务 例： build.gradle Copy myCopy = tasks.getByName(&quot;myCopy&quot;) myCopy.from 'resources' myCopy.into 'target' myCopy.include('**/*.txt', '**/*.xml', '**/*.properties') 使用DSL特定语法配置任务 例： build.gradle // Configure task using Groovy dynamic task configuration block myCopy { from 'resources' into 'target' } myCopy.include('**/*.txt', '**/*.xml', '**/*.properties') 用配置块定义一个任务 例： build.gradle task copy(type: Copy) { from 'resources' into 'target' include('**/*.txt', '**/*.xml', '**/*.properties') } 将参数传递给任务构造函数 与Task在创建后配置可变属性相反，您可以将参数值传递给Task类的构造函数。为了将值传递给Task构造函数，您必须使用@javax.inject.Inject注释相关的构造函数。 首先创建带有@Inject构造函数的任务类 class CustomTask extends DefaultTask { final String message final int number @Inject CustomTask(String message, int number) { this.message = message this.number = number } } 然后创建一个任务，并在参数列表的末尾传递构造函数参数。 tasks.create('myTask', CustomTask, 'hello', 42) 你也可以使用Map创建带有构造函数参数的任务 task myTask(type: CustomTask, constructorArgs: ['hello', 42]) 向任务添加依赖项 从另一个项目添加对任务的依赖 project('project-a') { task taskX { dependsOn ':project-b:taskY' doLast { println 'taskX' } } } project('project-b') { task taskY { doLast { println 'taskY' } } } 使用任务对象添加依赖 task taskX { doLast { println 'taskX' } } task taskY { doLast { println 'taskY' } } taskX.dependsOn taskY 使用惰性块添加依赖项 task taskX { doLast { println 'taskX' } } // Using a Groovy Closure taskX.dependsOn { tasks.findAll { task -&gt; task.name.startsWith('lib') } } task lib1 { doLast { println 'lib1' } } task lib2 { doLast { println 'lib2' } } task notALib { doLast { println 'notALib' } } 任务排序 控制任务排序的两种方式： must run after ：必须在之后运行 should run after：应该在之后运行 例 task taskX { doLast { println 'taskX' } } task taskY { doLast { println 'taskY' } } taskY.mustRunAfter taskX should run after被忽略的情况 引入排序周期。 使用并行执行时，除了 &quot;should run after &quot;任务外，一个任务的所有依赖关系都已被满足， 引入排序周期例子 task taskX { doLast { println 'taskX' } } task taskY { doLast { println 'taskY' } } task taskZ { doLast { println 'taskZ' } } taskX.dependsOn taskY taskY.dependsOn taskZ taskZ.shouldRunAfter taskX 为任务添加描述 您可以在任务中添加描述。执行gradle tasks时将显示此描述。 build.gradle task copy(type: Copy) { description 'Copies the resource directory to the target directory.' from 'resources' into 'target' include('**/*.txt', '**/*.xml', '**/*.properties') } 跳过任务 onlyIf跳过 hello.onlyIf { !project.hasProperty('skipHello') } //StopExecutionException跳过 compile.doFirst { if (true) { throw new StopExecutionException() } } 禁用任务 task disableMe { doLast { println 'This should not be printed if the task is disabled.' } } disableMe.enabled = false 任务超时 task hangingTask() { doLast { Thread.sleep(100000) } timeout = Duration.ofMillis(500) } 任务规则 有时您想执行一个任务，该任务的行为取决于较大或无限数量的参数值范围。提供此类任务的一种非常好的表达方式是任务规则： 任务规则 tasks.addRule(&quot;Pattern: ping&lt;ID&gt;&quot;) { String taskName -&gt; if (taskName.startsWith(&quot;ping&quot;)) { task(taskName) { doLast { println &quot;Pinging: &quot; + (taskName - 'ping') } } } } task groupPing { dependsOn pingServer1, pingServer2 } &gt; gradle -q groupPing Ping：Server1 Ping：Server2 终结器任务 计划运行终结任务时，终结任务会自动添加到任务图中。即使完成任务失败，也将执行终结器任务。 task taskX { doLast { println 'taskX' } } task taskY { doLast { println 'taskY' } } taskX.finalizedBy taskY &gt; gradle -q taskX TaskX TaskY 动态任务 Groovy或Kotlin的功能可用于定义任务以外的其他功能。例如，您还可以使用它来动态创建任务。 build.gradle 4.times { counter -&gt; task &quot;task$counter&quot; { doLast { println &quot;I'm task number $counter&quot; } } } gradle -q task1 输出 &gt; gradle -q task1 I'm task number 1 Groovy_DSL快捷方式符号 访问任务有一种方便的表示法。每个任务都可以作为构建脚本的属性来使用： 例.作为构建脚本的属性访问任务 build.gradle task hello { doLast { println 'Hello world!' } } hello.doLast { println &quot;Greetings from the $hello.name task.&quot; } 输出 gradle -q hello &gt; gradle -q hello Hello world! Greetings from the hello task. 这将启用非常可读的代码，尤其是在使用插件提供的任务（例如compile任务）时。 额外任务属性 您可以将自己的属性添加到任务。要添加名为的属性myProperty，请设置ext.myProperty为初始值。从那时起，可以像预定义的任务属性一样读取和设置属性。 build.gradle task myTask { ext.myProperty = &quot;myValue&quot; } task printTaskProperties { doLast { println myTask.myProperty } } 输出 gradle -q printTaskProperties &gt; gradle -q printTaskProperties myValue 额外的属性不仅限于任务。您可以在Extra属性中阅读有关它们的更多信息。 默认任务 Gradle允许您定义一个或多个默认任务。 build.gradle defaultTasks 'clean', 'run' task clean { doLast { println 'Default Cleaning!' } } task run { doLast { println 'Default Running!' } } task other { doLast { println &quot;I'm not a default task!&quot; } } 输出 gradle -q &gt; gradle -q Default Cleaning! Default Running! 这等效于运行gradle clean run。在多项目构建中，每个子项目可以有其自己的特定默认任务。如果子项目未指定默认任务，则使用父项目的默认任务（如果已定义）。 ","link":"https://tinaxiawuhao.github.io/post/lhyr_DiZc/"},{"title":"Gradle基础","content":"Gradle概述 Gradle是专注于灵活性和性能的开源构建自动化工具，一般使用Groovy或KotlinDSL编写构建脚本。 本文只使用Groovy Gradle的特点： 高性能 Gradle通过仅运行需要运行的任务来避免不必要的工作。 可以使用构建缓存来重用以前运行的任务输出，甚至可以使用其他计算机（具有共享的构建缓存）重用任务输出。 JVM基础 Gradle在JVM上运行。熟悉Java的用户来可以在构建逻辑中使用标准Java API，例如自定义任务类型和插件。 这使得Gradle跨平台更加简单。（Gradle不仅限于构建JVM项目，它甚至附带对构建本机项目的支持。） 约束 和Maven一样，Gradle通过实现约束使常见类型的项目（例如Java项目）易于构建。 应用适当的插件，您可以轻松地为许多项目使用精简的构建脚本。 但是这些约定并没有限制您：Gradle允许您覆盖它们，添加自己的任务以及对基于约定的构建进行许多其他自定义操作。 可扩展性 您可以轻松扩展Gradle以提供您自己的任务类型甚至构建模型。 IDE支持 支持IDE：Android Studio，IntelliJ IDEA，Eclipse和NetBeans。 Gradle还支持生成将项目加载到Visual Studio所需的解决方案文件。 可洞察性 构建扫描提供了有关构建运行的广泛信息，可用于识别构建问题。他们特别擅长帮助您确定构建性能的问题。 您还可以与其他人共享构建扫描，如果您需要咨询以解决构建问题，这将特别有用。 您需要了解有关Gradle的五件事 本节在官方文档里面反复提及，具体可见Gradle文档 1. Gradle是通用的构建工具 Gradle允许您构建任何软件，因为它不关心你具体的工作。 2. 核心模型基于任务 Gradle将其构建模型建模为任务（工作单元）的有向无环图（DAG）。这意味着构建实质上配置了一组任务，并根据它们的依赖关系将它们连接在一起以创建该DAG。创建任务图后，Gradle将确定需要按顺序运行的任务，然后继续执行它们。 任务本身包括以下部分，它们通过依赖链接在一起： 动作-做某事的工作，例如复制文件或编译源代码 输入-操作使用或对其进行操作的值，文件和目录 输出-操作修改或生成的文件和目录 3. Gradle有几个固定的构建阶段 重要的是要了解Gradle分三个阶段评估和执行构建脚本： 初始化 设置构建环境，并确定哪些项目将参与其中。 配置 构造和配置构建的任务图，然后根据用户要运行的任务确定需要运行的任务和运行顺序。 执行 运行在配置阶段结束时选择的任务。 这些阶段构成了Gradle的构建生命周期。 4. Gradle的扩展方式不止一种 Gradle捆绑的构建逻辑不可能满足所有构建情况，大多数构建都有一些特殊要求，你需要添加自定义构建逻辑。Gradle提供了多种机制来扩展它，例如： 自定义任务类型。 自定义任务动作。 项目和任务的额外属性。 自定义约束。 自定义module。 5. 构建脚本针对API运行 可以将Gradle的构建脚本视为可执行代码，但设计良好的构建脚本描述了构建软件需要哪些步骤，而不关心这些步骤应该如何完成工作。 由于Gradle在JVM上运行，因此构建脚本也可以使用标准Java API。Groovy构建脚本可以另外使用Groovy API，而Kotlin构建脚本可以使用Kotlin。 功能的生命周期 功能可以处于以下四种状态之一： Internal：内部功能，不提供接口 Incubating： 孵化功能。在成为公共功能之前会继续更改 Public：公共功能，可放心使用 Deprecated：废弃功能，将在未来删除 Gradle安装 安装JDK 安装JDK过程已有太多资料，本文不做详细介绍。可使用命令检测自己电脑是否成功安装 安装Gradle 用软件包安装Gradle SDKMAN sdk install gradle Homebrew brew install gradle 手动安装（推荐方式） 下载 services.gradle.org/distributions （全部版本目录地址，可以查看最新版本） services.gradle.org/distributions/gradle-7.5-all.zip （截止至2022.07.18最新） 建议下载：services.gradle.org/distributions/gradle-7.0.2-all.zip (支持jdk8) 文件介绍 gradle-7.0.2-docs.zip //文档 gradle-7.0.2-src.zip //源码 gradle-7.0.2-bin.zip //软件包 gradle-7.0.2-all.zip //全部文件 bin ：运行文件 lib：依赖库 docs：文档 src：源文件 init.d :初始化脚本目录，可自己添加 配置环境变量 export GRADLE_HOME=/Users/temp/gradle-7.0.2 export PATH=$PATH:$GRADLE_HOME/bin 运行 输入gradle -v 检测是否配置成功 HelloWord 编写一个build.gradle文件，输入以下内容 task hello{ doLast { println 'Hello World' } } 命令行输入gradle -q hello即可运行 Gradle Wrapper 定义 Gradle Wrapper是一个脚本，可调用Gradle的声明版本，并在必要时预先下载。因此，开发人员可以快速启动并运行Gradle项目，而无需遵循手动安装过程 添加wrapper 在build.gradle同级目录下使用命令gradle wrapper可以生成gradle wrapper目录 gradle wrapper gradle-wrapper.jar WrapJAR文件，其中包含用于下载Gradle发行版的代码。 gradle-wrapper.properties 一个属性文件，负责配置Wrapper运行时行为，例如与该版本兼容的Gradle版本。请注意，更多常规设置（例如，将 Wrap配置为使用代理）需要进入其他文件。 gradlew， gradlew.bat 一个shell脚本和一个Windows批处理脚本，用于使用 Wrap程序执行构建。 可以通过命令控制生成选项 #用于下载和执行 Wrap程序的Gradle版本。 --gradle-version #Wrap使用的Gradle分布类型。可用的选项是bin和all。默认值为bin。 --distribution-type #指向Gradle分发ZIP文件的完整URL。使用此选项，--gradle-version并且--distribution- type过时的网址已经包含此信息。如果您想在公司网络内部托管Gradle发行版，则此选项非常有价值。 --gradle-distribution-url #SHA256哈希和用于验证下载的Gradle分布。 --gradle-distribution-sha256-sum 例： gradle wrapper --gradle-version 7.0.2 --distribution-type all Wrapper属性文件 一般生成Wrapper会得到如下属性文件 gradle-wrapper.properties distributionBase=GRADLE_USER_HOME distributionPath=wrapper/dists distributionUrl=https\\://services.gradle.org/distributions/gradle-7.4.1-bin.zip zipStoreBase=GRADLE_USER_HOME zipStorePath=wrapper/dists GRADLE_USER_HOME是你的环境变量，如果没配置，则默认是用户目录下的.gradle文件夹 distributionBase 下载的 Gradle压缩包解压后存储的主目录 distributionPath 相对于 distributionBase的解压后的 Gradle压缩包的路径 zipStoreBase 同 distributionBase，只不过是存放 zip压缩包的 zipStorePath 同 distributionPath，只不过是存放 zip压缩包的 distributionUrl Gradle发行版压缩包的下载地址 使用wrapper构建 在 gradlew目录下执行命令： windows： gradlew.bat build shell： ./gradlew build 升级 更改gradle-wrapper.properties文件中的distributionUrl属性 使用gradlew wrap --gradle-version 命令 ./gradlew wrap --gradle-version 7.4.2 自定义Gradle_Wrap 可以通过自定义wrapper少去一些重复操作或定制功能，如 build.gradle tasks.named('wrapper') { distributionType = Wrapper.DistributionType.ALL } task wrapper(type: Wrapper) { gradleVersion = '7.4.2' } Gradle 环境 环境变量 GRADLE_OPTS 指定启动Gradle客户端VM时要使用的JVM参数。客户端VM仅处理命令行输入/输出，因此很少需要更改其VM选项。实际的构建由Gradle守护程序运行，不受此环境变量的影响。 GRADLE_USER_HOME 指定Gradle用户的主目录（如果未设置，则默认为$USER_HOME/.gradle）。 JAVA_HOME 指定要用于客户端VM的JDK安装目录。除非Gradle属性文件使用org.gradle.java.home指定了另一个虚拟机，否则此虚拟机也用于守护程序。 注意：命令行选项和系统属性优先于环境变量。 Gradle属性 你可以通过以下方式自己配置你的项目属性，如果存在多个，则从上到下优先读取 ： 系统属性，例如在命令行上设置 -Dgradle.user.home GRADLE_USER_HOME目录中的gradle.properties 项目根目录中的gradle.properties Gradle安装目录中的gradle.properties gradle.properties # 当设置为true时，Gradle将在可能的情况下重用任何先前构建的任务输出，从而使构建速度更快 org.gradle.caching=true # 设置为true时，单个输入属性哈希值和每个任务的构建缓存键都记录在控制台上。 org.gradle.caching.debug=true # 启用按需孵化配置，Gradle将尝试仅配置必要的项目。 org.gradle.configureondemand=true # 自定义控制台输出的颜色或详细程度。默认值取决于Gradle的调用方式。可选(auto,plain,rich,verbose) org.gradle.console=auto # 当设置true的Gradle守护进程来运行构建。默认值为true。 org.gradle.daemon=true # 在指定的空闲毫秒数后，Gradle守护程序将自行终止。默认值为10800000（3小时）。 org.gradle.daemon.idletimeout=10800000 # 设置true为时，Gradle将在启用远程调试的情况下运行构建，侦听端口5005。 # 请注意，这等同于添加-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005到JVM命令行，并且将挂起虚拟机，直到连接了调试器。 # 默认值为false。 org.gradle.debug=true # 指定用于Gradle构建过程的Java主页。可以将值设置为jdk或jre位置，但是，根据您的构建方式，使用JDK更安全。 # 如果未指定设置，则从您的环境（JAVA_HOME或的路径java）派生合理的默认值。这不会影响用于启动Gradle客户端VM的Java版本（请参阅环境变量）。 org.gradle.java.home=/usr/bin/java # 指定用于Gradle守护程序的JVM参数。该设置对于配置JVM内存设置以提高构建性能特别有用。这不会影响Gradle客户端VM的JVM设置。 org.gradle.jvmargs=-Xmx2048m # 当设置为quiet,warn,lifecycle,info,debug时，Gradle将使用此日志级别。这些值不区分大小写。该lifecycle级别是默认级别。 # 可选(quiet,warn,lifecycle,info,debug) org.gradle.logging.level=debug # 配置后，Gradle将分叉到org.gradle.workers.maxJVM以并行执行项目 org.gradle.parallel=true # 指定Gradle守护程序及其启动的所有进程的调度优先级。默认值为normal。(low,normal) org.gradle.priority=normal # 在监视文件系统时配置详细日志记录。 默认为关闭 。 org.gradle.vfs.verbose=true # 切换观看文件系统。允许Gradle在下一个版本中重用有关文件系统的信息。 默认为关闭 。 org.gradle.vfs.watch=true # 当设置为all，summary或者none，Gradle会使用不同的预警类型的显示器。(all,fail,summary,none) org.gradle.warning.mode=all # 配置后，Gradle将最多使用给定数量的工人。默认值为CPU处理器数。 org.gradle.workers.max=5 系统属性 # 指定用户名以使用HTTP基本认证从服务器下载Gradle发行版 systemProp.gradle.wrapperUser = myuser # 指定使用Gradle Wrapper下载Gradle发行版的密码 systemProp.gradle.wrapperPassword = mypassword # 指定Gradle用户的主目录 systemProp.gradle.user.home=(path to directory) 项目属性 org.gradle.project.foo = bar 守护程序 Gradle在Java虚拟机（JVM）上运行，并使用一些支持库，这些库需要很短的初始化时间。但有时启动会比较慢。 解决此问题的方法是Gradle Daemon ：这是一个长期存在的后台进程，与以前相比，它可以更快地执行构建。 可通过命令获取运行守护程序状态 IDLE为空闲，BUSY为繁忙，STOPPED则已关闭 守护程序默认打开，可通过以下属性关闭 .gradle/gradle.properties org.gradle.daemon=false 也可用命令gradle --stop手动关闭守护程序 Gradle命令行 命令行格式 gradle [taskName ...] [--option-name ...] 如果指定了多个任务，则应以空格分隔。 选项和参数之间建议使用=来指定。 --console=plain 启用行为的选项具有长形式的选项，并带有由指定的反函数--no-。以下是相反的情况。 --build-cache --no-build-cache 许多命令具有缩写。例如以下命令是等效的： --help -h 使用Wrapper时候应该用./gradlew或gradlew.bat取代gradle #获取帮助 gradle -? gradle -h gradle -help # 显示所选项目的子项目列表，以层次结构显示。 gradle projects #查看可执行task gradle task #查看可执行task帮助 gradle help -task # 在Gradle构建中，通常的`build`任务是指定组装所有输出并运行所有检查。 gradle build # 执行所有验证任务（包括test和linting）。 gradle check # 清理项目 gradle clean #强制刷新依赖 gradle --refresh-dependencies assemble #缩写调用 gradle startCmd == gradle sc # 执行任务 gradle myTask # 执行多个任务 gradle myTask test # 执行 dist任务但排除test任务 gradle dist --exclude-task test # 强制执行任务 gradle test --rerun-tasks # 持续构建 # gradle test --continue # 生成扫描会提供完整的可视化报告，说明哪些依赖项存在于哪些配置，可传递依赖项和依赖项版本选择中。 $ gradle myTask --scan # 所选项目的依赖项列表 $ gradle dependencies ","link":"https://tinaxiawuhao.github.io/post/Ke4qDxE4l/"},{"title":"gradle详情","content":"一 依赖管理 implementation:会将指定的依赖添加到编译路径，并且会将该依赖打包到输出，但是这个依赖在编译时不能暴露给其他模块，例如依赖此模块的其他模块。这种方式指定的依赖在编译时只能在当前模块中访问。 api:使用api配置的依赖会将对应的依赖添加到编译路径，并将依赖打包输出，但是这个依赖是可以传递的，比如模块A依赖模块B，B依赖库C，模块B在编译时能够访问到库C，但是与implemetation不同的是，在模块A中库C也是可以访问的。 compileOnly:compileOnly修饰的依赖会添加到编译路径中，但是不会打包，因此只能在编译时访问，且compileOnly修饰的依赖不会传递。 runtimeOnly:这个与compileOnly相反，它修饰的依赖不会添加到编译路径中，但是被打包，运行时使用。没有使用过。 annotationProcessor:用于注解处理器的依赖配置 dependencies { // 本地项目 api project(':com.test.core:core-common') //外部依赖 implementation 'org.springframework.boot:spring-boot-starter' compileOnly 'org.projectlombok:lombok' annotationProcessor 'org.projectlombok:lombok' testImplementation 'org.springframework.boot:spring-boot-starter-test' } 二 仓库 远程仓库 使用Maven中央仓库，maven仓库的URL为：http://repo1.maven.org/maven2/ repositories { mavenCentral() } 使用Maven远程仓库 repositories { //阿里云远程仓库 maven { url 'https://maven.aliyun.com/repository/central'} maven { url 'https://maven.aliyun.com/repository/public' } mavenCentral() } 本地仓库 配置Gradle使用maven本地仓库：CRADLE_USER_HOME：D:.m2\\repository 修改配置build.gradle： /** * 指定所使用的仓库，mavenCentral()表示使用中央仓库， * 此刻项目中所需要的jar包都会默认从中央仓库下载到本地指定目录 * 配置mavenLocal()表示引入jar包的时候，先从本地仓库中找，没有再去中央仓库下载 */ repositories { mavenLocal() mavenCentral() } 修改或者添加额外的私有仓库地址 直接修改 settings.gradle 来添加其它仓库： // settings.gradle //pluginManagement {}块只能出现在settings.gradle文件中，必须是文件中的第一个块，也可以以settings形式出现在初始化脚本中。 pluginManagement { repositories { maven { url 'https://maven.aliyun.com/repository/central'} maven { url 'https://maven.aliyun.com/repository/public' } mavenCentral() gradlePluginPortal() maven { url 'https://repo.spring.io/release' } if (version.endsWith('-SNAPSHOT')) { maven { url &quot;https://repo.spring.io/snapshot&quot; } } } } 三 多项目构建 多项目构成：allProjects = root项目+各子项目 settings文件声明了所需的配置来实例化项目的层次结构。在默认情况下，这个文件被命名为settings.gradle，并且和根项目的build.gradle文件放在一起，该文件在初始化阶段被执行。根项目就像一个容器，子项目会迭代访问它的配置并注入到自己的配置中。 多项目构建 多项目构建总是需要指定一个树根，树中的每一个节点代表一个项目，每一个Project对象都指定有一个表示在树中位置的路径；在设置文件中我们还可以使用一套方法来自定义构建项目树。 settings.gradle作用就是用于多项目构建，一般像这样： //父模块名称 rootProject.name = 'nacos' //引入子模块 include 'sentinel' include 'openfeign' include 'loadbalancer' include 'gateway' include 'rocketmq' include 'sleuth' include 'seata' include 'provide' findProject(':iot-service:user-center')?.name = 'user-center' buildScript buildScript块的repositories主要是为了Gradle脚本自身的执行，获取脚本依赖插件。 buildscript中的声明是gradle脚本自身需要使用的资源。可以声明的资源包括依赖项、第三方插件、maven仓库地址等。 gradle在执行脚本时，会优先执行buildscript代码块中的内容，然后才会执行剩余的build脚本。 buildscript { ext { //spring-cloud-dependencies 2020.0.0 默认不在加载bootstrap 配置文件， //如果项目中要用bootstrap 配置文件 需要手动添加spring-cloud-starter-bootstrap 依赖，不然启动项目会报错的。 set('springCloudVersion', &quot;2021.0.3&quot;) //定义一个变量，统一规定springboot的版本 set('springBootVersion', &quot;2.6.8&quot;) set('alibabaVersion', &quot;2.2.7.RELEASE&quot;) set('lombok', &quot;1.18.16&quot;) set('knife4j', &quot;2.0.9&quot;) set('hutool', &quot;4.6.3&quot;) set('rocketmq', &quot;2.2.2&quot;) } repositories { mavenLocal() maven { url 'https://maven.aliyun.com/repository/central' } maven { url 'https://maven.aliyun.com/repository/public' } mavenCentral() } dependencies { //用来打包 classpath(&quot;org.springframework.boot:spring-boot-gradle-plugin:${springBootVersion}&quot;) } } ext：ext是自定义属性，现在很多人都喜欢把所有关于版本的信息都利用ext放在另一个自己新建的gradle文件中集中管理。 allprojects allprojects块的repositories用于多项目构建，为所有项目提供共同所需依赖包。而子项目可以配置自己的repositories以获取自己独需的依赖包。 buildscript和allprojects的作用和区别： buildscript中的声明是gradle脚本自身需要使用的资源，就是说它自己需要的资源，跟你其它模块其实并没有什么关系。而allprojects声明的却是所有module所需要使用的资源，就是说你的每个module都需要用同一个第三库的时候，可以在allprojects里面声明。 allprojects { apply plugin: 'java-library' apply plugin: 'idea' //下面两句必须在所有子项目中添加，否则导致import了看起来没问题，但是编译时找不到其他模块的类。 group = 'com.example' version = '0.0.1-SNAPSHOT' // 指定JDK版本 sourceCompatibility = 1.8 targetCompatibility = 1.8 repositories { mavenLocal() maven { url 'https://maven.aliyun.com/repository/central' } maven { url 'https://maven.aliyun.com/repository/public' } mavenCentral() } //指定编码格式 tasks.withType(JavaCompile) { options.encoding = &quot;UTF-8&quot; } } subprojects子项目通用配置 subprojects是对所有Child Project的配置: subprojects { apply plugin: 'java-library' apply plugin: 'idea' apply plugin: 'org.springframework.boot' //dependency-management 插件 apply plugin: 'io.spring.dependency-management' repositories { mavenLocal() maven { url 'https://maven.aliyun.com/repository/central' } maven { url 'https://maven.aliyun.com/repository/public' } mavenCentral() } dependencies { implementation(enforcedPlatform(&quot;org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}&quot;)) implementation(enforcedPlatform(&quot;com.alibaba.cloud:spring-cloud-alibaba-dependencies:${alibabaVersion}&quot;)) implementation(enforcedPlatform(&quot;org.springframework.boot:spring-boot-dependencies:${springBootVersion}&quot;)) implementation &quot;com.github.xiaoymin:knife4j-spring-boot-starter:${knife4j}&quot; implementation &quot;cn.hutool:hutool-all:${hutool}&quot; implementation 'org.springframework.boot:spring-boot-starter' implementation 'com.alibaba.cloud:spring-cloud-starter-alibaba-nacos-config' implementation('com.alibaba.cloud:spring-cloud-starter-alibaba-nacos-discovery'){ exclude group: 'org.springframework.cloud', module: 'spring-cloud-starter-netflix-ribbon' } implementation 'org.springframework.cloud:spring-cloud-starter-bootstrap' compileOnly(&quot;org.projectlombok:lombok:${lombok}&quot;) annotationProcessor 'org.springframework.boot:spring-boot-configuration-processor' annotationProcessor(&quot;org.projectlombok:lombok:${lombok}&quot;) testImplementation(&quot;org.springframework.boot:spring-boot-starter-test:${springBootVersion}&quot;) } jar { manifest.attributes provider: 'gradle' } } 四 gradle.properties gradle中的常用属性可以写在gradle.properties中。 一般我们都把全局属性都编写在一个工具类中，如果是有环境的切换的话，那么我们还会定义一个标志来进行相应的变换。对于项目而言，有时候需要配置某些敏感信息。比如密码，帐号等。而这些信息需要被很多类共同使用，所以必须有一个全局的配置。当需要把项目push到git上时，我们不希望别人看到我们项目的key，token等。我们可以将这些信息设置在gradle.properties中。 只有在Android中才可使用。 AppKey = 1234567890 在build.gradle(module app)中进行变量的重定义，即将配置内容转化成java能够使用的形式: android { buildTypes { release { minifyEnabled true proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro' //buildConfigField用于给BuildConfig文件添加一个字段 buildConfigField(&quot;String&quot;,&quot;KEY&quot;,&quot;\\&quot;${AppKey}\\&quot;&quot;) } debug{ buildConfigField(&quot;String&quot;,&quot;KEY&quot;,&quot;\\&quot;${AppKey}\\&quot;&quot;) } } } 五 Gradle 插件(Plugins) Gradle 也可以用下面的方式声明使用的插件： // plugins DSL plugins { id 'org.springframework.boot' version '2.2.1.RELEASE' id 'io.spring.dependency-management' version '1.0.8.RELEASE' id 'java' } // apply plugin apply plugin: 'java-library' apply plugin: 'idea' apply plugin: 'org.springframework.boot' //dependency-management 插件 apply plugin: 'io.spring.dependency-management' 其实是从 Gradle 官方的插件仓库https://plugins.gradle.org/m2/下载的。 六 常见的task命令 build:当运行gradle build命令时Gradle将会编译和测试你的代码，并且创建一个包含类和资源的JAR文件。 clean:当运行gradle clean命令时Gradle将会删除build生成的目录和所有生成的文件。 assemble:当运行gradle assemble命令时Gradle将会编译并打包代码，但是并不运行单元测试。 check:当运行gradle check命令时Gradle将会编译并测试你的代码，其他的插件会加入更多的检查步骤。 七 gradle-wrapper Wrapper是对Gradle的一层包装，便于在团队开发过程中统一Gradle构建的版本号，这样大家都可以使用统一的Gradle版本进行构建。 distributionBase=GRADLE_USER_HOME distributionPath=wrapper/dists distributionUrl=https\\://services.gradle.org/distributions/gradle-7.4.2-bin.zip #distributionUrl=file:///E:/gradle/gradle-7.4.2-all.zip zipStoreBase=GRADLE_USER_HOME zipStorePath=wrapper/dists distributionUrl是要下载的gradle的地址，使用哪个版本的gradle，就在这里修改。 gradle的3种版本： gradle-xx-all.zip是完整版，包含了各种二进制文件，源代码文件，和离线的文档。 gradle-xx-bin.zip是二进制版，只包含了二进制文件（可执行文件），没有文档和源代码。 gradle-xx-src.zip是源码版，只包含了Gradle源代码，不能用来编译你的工程。 zipStoreBase和zipStorePath组合在一起，是下载的gradle-3.1-bin.zip所存放的位置。 zipStorePath是zipStoreBase指定的目录下的子目录。 distributionBase和distributionPath组合在一起，是解压gradle-5.6.4-bin.zip之后的文件的存放位置。 distributionPath是distributionBase指定的目录下的子目录。 下载位置可以和解压位置不一样。 zipStoreBase和distributionBase有两种取值：GRADLE_USER_HOME和PROJECT。 其中，GRADLE_USER_HOME表示用户目录。 在windows下是%USERPROFILE%/.gradle，例如C:\\Users&lt;user_name&gt;.gradle\\。 在linux下是$HOME/.gradle，例如~/.gradle。 PROJECT表示工程的当前目录，即gradlew所在的目录。 八 依赖分组 Gradle 依赖是分组的 ,分组是在 org.gradle.api.Project 中的 configurations 中配置的 ,如 &quot; implementation &quot; , &quot; compile &quot; 等都是分组 , 这些分组都是在 org.gradle.api.Project#configurations 中进行配置 , 也就是 build.gradle#configurations 中配置 ; build.gradle#configurations 自定义依赖分组 在 build.gradle 中配置 configurations : configurations { hello { } } 则可以在 dependencies 中使上述在 configurations 配置的依赖分组 hello , dependencies { hello 'com.android.support:appcompat-v7:28.0.0' } 九 依赖版本冲突 Gradle对解决传递依赖提供了两种策略，使用最新版本或者直接导致构建失败。默认的策略是使用最新版本。虽然这样的策略能够解决一些问题，但是还是不够。常见的一种情况是，NoSuchMethond或者ClassNotFound。这时候，你可能需要一些特殊手段，比如排除不想要的传递依赖。 排除传递依赖的方式有两种： 1.使用transitive = false排除 2.在具体的某个dependency中使用exclude排除 3.使用force强制依赖某个版本 (1) 方案一：针对 A 或 D 配置 transitive。 这里针对A配置，不解析A模块的传递依赖，因此当前Module的依赖关系树中不再包含 B1 和 C，这里需要手动添加依赖 C dependencies { implementation A { transitive = false } implementation C implementation D { //transitive = false } } (2) 方案二：针对 A 或 D 配置 exclude规则，此处针对A配置，依赖关系树中不再包含B1 dependencies { implementation A { exclude B1 } implementation D { //exclude B2 } } (3) 方案三：使用force强制依赖某个版本，如强制依赖 B1 或者 B2 以下是在顶层build.gradle中配置，强制所有module依赖B1 configurations.all { resolutionStrategy { force B1 // force B2 } } 十 springcloud-gradle管理 parent:settings.gradle rootProject.name = 'nacos' include 'sentinel' include 'openfeign' include 'loadbalancer' include 'gateway' include 'rocketmq' include 'sleuth' include 'seata' include 'provide' parent:build.gradle buildscript { ext { //spring-cloud-dependencies 2020.0.0 默认不在加载bootstrap 配置文件， //如果项目中要用bootstrap 配置文件 需要手动添加spring-cloud-starter-bootstrap 依赖，不然启动项目会报错的。 set('springCloudVersion', &quot;2021.0.3&quot;) //定义一个变量，统一规定springboot的版本 set('springBootVersion', &quot;2.6.8&quot;) set('alibabaVersion', &quot;2.2.7.RELEASE&quot;) set('lombok', &quot;1.18.16&quot;) set('knife4j', &quot;2.0.9&quot;) set('hutool', &quot;4.6.3&quot;) set('rocketmq', &quot;2.2.2&quot;) } repositories { mavenLocal() maven { url 'https://maven.aliyun.com/repository/central' } maven { url 'https://maven.aliyun.com/repository/public' } mavenCentral() } dependencies { //用来打包 classpath(&quot;org.springframework.boot:spring-boot-gradle-plugin:${springBootVersion}&quot;) } } allprojects { apply plugin: 'java-library' apply plugin: 'idea' //下面两句必须在所有子项目中添加，否则导致import了看起来没问题，但是编译时找不到其他模块的类。 group = 'com.example' version = '0.0.1-SNAPSHOT' // 指定JDK版本 sourceCompatibility = 1.8 targetCompatibility = 1.8 repositories { mavenLocal() maven { url 'https://maven.aliyun.com/repository/central' } maven { url 'https://maven.aliyun.com/repository/public' } mavenCentral() } //指定编码格式 tasks.withType(JavaCompile) { options.encoding = &quot;UTF-8&quot; } } subprojects { apply plugin: 'java-library' apply plugin: 'idea' apply plugin: 'org.springframework.boot' //dependency-management 插件 apply plugin: 'io.spring.dependency-management' dependencies { implementation(enforcedPlatform(&quot;org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}&quot;)) implementation(enforcedPlatform(&quot;com.alibaba.cloud:spring-cloud-alibaba-dependencies:${alibabaVersion}&quot;)) implementation(enforcedPlatform(&quot;org.springframework.boot:spring-boot-dependencies:${springBootVersion}&quot;)) implementation &quot;com.github.xiaoymin:knife4j-spring-boot-starter:${knife4j}&quot; implementation &quot;cn.hutool:hutool-all:${hutool}&quot; implementation 'org.springframework.boot:spring-boot-starter' implementation 'com.alibaba.cloud:spring-cloud-starter-alibaba-nacos-config' implementation('com.alibaba.cloud:spring-cloud-starter-alibaba-nacos-discovery'){ exclude group: 'org.springframework.cloud', module: 'spring-cloud-starter-netflix-ribbon' } implementation 'org.springframework.cloud:spring-cloud-starter-bootstrap' compileOnly(&quot;org.projectlombok:lombok:${lombok}&quot;) annotationProcessor 'org.springframework.boot:spring-boot-configuration-processor' annotationProcessor(&quot;org.projectlombok:lombok:${lombok}&quot;) testImplementation(&quot;org.springframework.boot:spring-boot-starter-test:${springBootVersion}&quot;) } jar { manifest.attributes provider: 'gradle' } } gateway:build.gradle dependencies { implementation &quot;org.springframework.cloud:spring-cloud-starter-gateway&quot; implementation &quot;org.springframework.cloud:spring-cloud-starter-loadbalancer&quot; } openfeign:build.gradle dependencies { implementation 'org.springframework.boot:spring-boot-starter-web' implementation &quot;org.springframework.cloud:spring-cloud-starter-openfeign&quot; implementation &quot;org.springframework.cloud:spring-cloud-starter-loadbalancer&quot; } rocketmq:build.gradle dependencies { implementation 'org.springframework.boot:spring-boot-starter-web' implementation &quot;org.apache.rocketmq:rocketmq-spring-boot-starter:${rocketmq}&quot; } sentinel:build.gradle dependencies { implementation 'org.springframework.boot:spring-boot-starter-web' implementation 'com.alibaba.cloud:spring-cloud-starter-alibaba-sentinel' implementation 'org.springframework.boot:spring-boot-starter-actuator' } 项目构建--Gradle--Docker打包 在build.gradle中引入插件。 id &quot;com.google.cloud.tools.jib&quot; version &quot;3.2.1&quot; 配置 配置打包时的基础镜像、容器配置、私服地址等，和Maven 插件中的一样，只是采用闭包的书写方式。 Jib 官网文档 jib { // 基础镜像，来自dockerhub,如果是私服，需要加上鉴权信息，和to下的auth节点相同 // https://hub.docker.com/ from { image = 'xx' } // 构建后的镜像名称以及私服地址、鉴权信息 to { image = 'xx' auth { username = '登录账号' password = '登录密码' } } // 容器相关设置 container { // 创建时间 creationTime = new Date() // JVM 启动参数 jvmFlags = ['-Djava.security.egd=file:/dev/./urandom', '-Dspring.profiles.active=prod', '-Dfile.encoding=utf-8', '-Duser.timezone=GMT+08'] // 启动类 // mainClass = 'com.xxx.RunApplication' // 容器在运行时公开的端口 ports = ['8080'] // 放置应用程序内容的容器上的根目录 appRoot = '/deploy/service' } } 使用IdeaDocker插件布署 Idea安装插件 安装docker插件 配置docker： 配置Dockerfile文件 1）新建Dockerfile文件会自动弹出。 在工程根目录下新建Dockerfile文件，内容如下： FROM openjdk:8 COPY build/libs/iot-eruake-1.0.0.jar app.jar RUN bash -c &quot;touch /app.jar&quot; EXPOSE 8080 ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;] 创建docker镜像 发布完成 ","link":"https://tinaxiawuhao.github.io/post/29jyrX-eN/"},{"title":"IdWorker雪花算法","content":"package com.ihrm.common.utils; import java.lang.management.ManagementFactory; import java.net.InetAddress; import java.net.NetworkInterface; //雪花算法代码实现 public class IdWorker { // 时间起始标记点，作为基准，一般取系统的最近时间（一旦确定不能变动） private final static long twepoch = 1288834974657L; // 机器标识位数 private final static long workerIdBits = 5L; // 数据中心标识位数 private final static long datacenterIdBits = 5L; // 机器ID最大值 private final static long maxWorkerId = -1L ^ (-1L &lt;&lt; workerIdBits); // 数据中心ID最大值 private final static long maxDatacenterId = -1L ^ (-1L &lt;&lt; datacenterIdBits); // 毫秒内自增位 private final static long sequenceBits = 12L; // 机器ID偏左移12位 private final static long workerIdShift = sequenceBits; // 数据中心ID左移17位 private final static long datacenterIdShift = sequenceBits + workerIdBits; // 时间毫秒左移22位 private final static long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits; //毫秒内自增最大值 private final static long sequenceMask = -1L ^ (-1L &lt;&lt; sequenceBits); /* 上次生产id时间戳 */ private static long lastTimestamp = -1L; // 0，并发控制 private long sequence = 0L; private final long workerId; // 数据标识id部分 private final long datacenterId; public IdWorker() { this.datacenterId = getDatacenterId(maxDatacenterId); this.workerId = getMaxWorkerId(datacenterId, maxWorkerId); } /** * @param workerId 工作机器ID * @param datacenterId 序列号 */ public IdWorker(long workerId, long datacenterId) { if (workerId &gt; maxWorkerId || workerId &lt; 0) { throw new IllegalArgumentException(String.format(&quot;worker Id can't be greater than % d or less than 0&quot;, maxWorkerId)); } if (datacenterId &gt; maxDatacenterId || datacenterId &lt; 0) { throw new IllegalArgumentException(String.format(&quot;datacenter Id can't be greater than % d or less than 0&quot;, maxDatacenterId)); } this.workerId = workerId; this.datacenterId = datacenterId; } /** * 获取下一个ID * * @return */ public synchronized long nextId() { long timestamp = timeGen(); if (timestamp &lt; lastTimestamp) { throw new RuntimeException(String.format(&quot;Clock moved backwards. Refusing to generate id for %d milliseconds &quot;, lastTimestamp - timestamp)); } if (lastTimestamp == timestamp) { // 当前毫秒内，则+1 sequence = (sequence + 1) &amp; sequenceMask; if (sequence == 0) { // 当前毫秒内计数满了，则等待下一秒 timestamp = tilNextMillis(lastTimestamp); } } else { sequence = 0L; } lastTimestamp = timestamp; // ID偏移组合生成最终的ID，并返回ID long nextId = ((timestamp - twepoch) &lt;&lt; timestampLeftShift) | (datacenterId &lt;&lt; datacenterIdShift) | (workerId &lt;&lt; workerIdShift) | sequence; return nextId; } private long tilNextMillis(final long lastTimestamp) { long timestamp = this.timeGen(); while (timestamp &lt;= lastTimestamp) { timestamp = this.timeGen(); } return timestamp; } private long timeGen() { return System.currentTimeMillis(); } /** * &lt;p&gt; * 获取 maxWorkerId * &lt;/p&gt; */ protected static long getMaxWorkerId(long datacenterId, long maxWorkerId) { StringBuffer mpid = new StringBuffer(); mpid.append(datacenterId); String name = ManagementFactory.getRuntimeMXBean().getName(); if (!name.isEmpty()) { /* * GET jvmPid */ mpid.append(name.split(&quot;@&quot;)[0]); } /* * MAC + PID 的 hashcode 获取16个低位 */ return (mpid.toString().hashCode() &amp; 0xffff) % (maxWorkerId + 1); } /** * &lt;p&gt; * 数据标识id部分 * &lt;/p&gt; */ protected static long getDatacenterId(long maxDatacenterId) { long id = 0L; try { InetAddress ip = InetAddress.getLocalHost(); NetworkInterface network = NetworkInterface.getByInetAddress(ip); if (network == null) { id = 1L; } else { byte[] mac = network.getHardwareAddress(); id = ((0x000000FF &amp; (long) mac[mac.length - 1]) | (0x0000FF00 &amp; (((long) mac[mac.length - 2]) &lt;&lt; 8))) &gt;&gt; 6; id = id % (maxDatacenterId + 1); } } catch (Exception e) { System.out.println(&quot; getDatacenterId: &quot; + e.getMessage()); } return id; } } ","link":"https://tinaxiawuhao.github.io/post/4WluurBt3/"},{"title":"RestTemplate","content":"spring环境下 首先导入springboot 的 web 包 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; 在启动类同包下创建RestTemplate.java类 import org.apache.http.client.HttpClient; import org.apache.http.client.config.RequestConfig; import org.apache.http.impl.client.DefaultHttpRequestRetryHandler; import org.apache.http.impl.client.HttpClientBuilder; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.http.client.ClientHttpRequestFactory; import org.springframework.http.client.HttpComponentsClientHttpRequestFactory; import org.springframework.web.client.RestTemplate; @Configuration public class RestTempleConfig { @Bean public RestTemplate restTemplate() { //生成一个设置了连接超时时间、请求超时时间 RequestConfig config = RequestConfig.custom() .setConnectionRequestTimeout(10000) .setConnectTimeout(10000) .setSocketTimeout(30000).build(); // 设置异常重试 HttpClientBuilder builder = HttpClientBuilder.create() .setDefaultRequestConfig(config) .setRetryHandler(new DefaultHttpRequestRetryHandler(3, false)); HttpClient httpClient = builder.build(); ClientHttpRequestFactory requestFactory = new HttpComponentsClientHttpRequestFactory(httpClient); RestTemplate restTemplate = new RestTemplate(requestFactory); // 日志拦截 //restTemplate.setInterceptors(Collections.singletonList(new RestTemplateConsumerLogger())); return restTemplate; } } 非spring环境下 导入相关依赖包(注意版本相适应) &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;5.2.16.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-core&lt;/artifactId&gt; &lt;version&gt;2.12.1&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.12.1&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt; &lt;version&gt;2.12.1&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; RestTemplate RestTemplate定义了36个与REST资源交互的方法，大多数都对应于HTTP的方法。 其中只有11个独立的方法，其中有十个有三种重载形式，而第十一个则重载了六次，这样一共形成了36个方法。 getForEntity() 发送一个HTTP GET请求，返回的ResponseEntity包含了响应体所映射成的对象 getForObject() 发送一个HTTP GET请求，返回的请求体将映射为一个对象 postForEntity() POST 数据到一个URL，返回包含一个对象的ResponseEntity，这个对象是从响应体中映射得到的 postForObject() POST 数据到一个URL，返回根据响应体匹配形成的对象 exchange() 在URL上执行特定的HTTP方法，返回包含对象的ResponseEntity，这个对象是从响应体中映射得到的 execute() 在URL上执行特定的HTTP方法，返回一个从响应体映射得到的对象 delete() 在特定的URL上对资源执行HTTP DELETE操作 headForHeaders() 发送HTTP HEAD请求，返回包含特定资源URL的HTTP头 optionsForAllow() 发送HTTP OPTIONS请求，返回对特定URL的Allow头信息 postForLocation() POST 数据到一个URL，返回新创建资源的URL put() PUT 资源到特定的URL getForEntity get请求就和正常在浏览器url上发送请求一样 RestTemplate restTemplate = new RestTemplate(); ResponseEntity&lt;String&gt; responseEntity=restTemplate.getForEntity(url+&quot;?name={1}&quot;, String.class, &quot;username&quot;); String body = responseEntity.getBody(); RestTemplate restTemplate = new RestTemplate(); ResponseEntity&lt;TokenBeen&gt; responseEntity =restTemplate.getForEntity(url+&quot;?name={1}&quot;, TokenBeen.class, &quot;username&quot;); if(responseEntity!=null){ TokenBeen body = responseEntity.getBody(); } #注意map的key要和参数中占位符相同 RestTemplate restTemplate = new RestTemplate(); Map&lt;String, String&gt; params = new HashMap&lt;&gt;(); params.put(&quot;name&quot;, &quot;username&quot;); ResponseEntity&lt;String&gt; responseEntity = restTemplate.getForEntity(url+&quot;?name={name}&quot;, String.class, params); String body = responseEntity.getBody(); RestTemplate restTemplate = new RestTemplate(); UriComponents uriConponents = UriComponentsBuilder.fromUriString(url+&quot;?name={name}&quot;).build().expand(&quot;username&quot;).encode(); URI uri = uriConponents.toUri(); ResponseEntity&lt;String&gt; responseEntity = restTemplate.getForEntity(uri, String.class); String body = responseEntity.getBody(); @GetMapping(&quot;getForEntity/{id}&quot;) public User getById(@PathVariable(name = &quot;id&quot;) String id) { ResponseEntity&lt;User&gt; response = restTemplate.getForEntity(&quot;http://localhost/get/{id}&quot;, User.class, id); User user = response.getBody(); return user; } getForObject getForObject 和 getForEntity 用法几乎相同,getForObject函数可以看作是对getForEntity进一步封装,指示返回值返回的是响应体,省去了我们 再去 getBody() RestTemplate restTemplate = new RestTemplate(); //注意参数中是uri UriComponents uriConponents = UriComponentsBuilder.fromUriString(url+&quot;?name={name}&quot;).build().expand(&quot;username&quot;).encode(); URI uri = uriConponents.toUri(); String body = restTemplate.getForObject(uri, String.class); RestTemplate restTemplate = new RestTemplate(); //注意参数中是uri UriComponents uriConponents = UriComponentsBuilder.fromUriString(url+&quot;?name={name}&quot;).build().expand(&quot;username&quot;).encode(); URI uri = uriConponents.toUri(); TokenBeen body = restTemplate.getForObject(uri, TokenBeen.class); @GetMapping(&quot;getForObject/{id}&quot;) public User getById(@PathVariable(name = &quot;id&quot;) String id) { User user = restTemplate.getForObject(&quot;http://localhost/get/{id}&quot;, User.class, id); return user; } postForEntity public &lt;T&gt; T postForObject(String url, @Nullable Object request, Class&lt;T&gt; responseType, Object... uriVariables)throws RestClientException {} public &lt;T&gt; T postForObject(String url, @Nullable Object request, Class&lt;T&gt; responseType, Map&lt;String, ?&gt; uriVariables) throws RestClientException {} public &lt;T&gt; T postForObject(URI url, @Nullable Object request, Class&lt;T&gt; responseType) throws RestClientException {} RestTemplate restTemplate = new RestTemplate(); HttpHeaders headers = new HttpHeaders(); //header参数 MediaType type = MediaType.parseMediaType(&quot;application/json; charset=UTF-8&quot;); headers.setContentType(type); headers.add(&quot;Accept&quot;, MediaType.APPLICATION_JSON.toString()); //body参数 JSONObject param = new JSONObject(); param.put(&quot;username&quot;, &quot;123&quot;); HttpEntity&lt;JSONObject&gt; formEntity = new HttpEntity&lt;&gt;(param, headers); //发送请求 String result = restTemplate.postForObject(url, formEntity, String.class); @RequestMapping(&quot;saveUser&quot;) public String save(User user) { ResponseEntity&lt;String&gt; response = restTemplate.postForEntity(&quot;http://localhost/save&quot;, user, String.class); String body = response.getBody(); return body; } postForObject 用法与 getForObject 一样 @RequestMapping(&quot;saveUser&quot;) public String save(User user) { String body = restTemplate.postForObject(&quot;http://localhost/save&quot;, user, String.class); return body; } exchange @PostMapping(&quot;demo&quot;) public void demo(Integer id, String name){ HttpHeaders headers = new HttpHeaders();//header参数 headers.add(&quot;authorization&quot;,Auth); headers.setContentType(MediaType.APPLICATION_JSON); JSONObject content = new JSONObject();//放入body中的json参数 content.put(&quot;userId&quot;, id); content.put(&quot;name&quot;, name); //post发送 HttpEntity&lt;JSONObject&gt; request = new HttpEntity&lt;&gt;(content,headers); //组装 ResponseEntity&lt;String&gt; response = template.exchange(&quot;http://localhost:8080/demo&quot;,HttpMethod.POST,request,String.class); //返回指定对象 ParameterizedTypeReference&lt;User&gt; responseBodyType = new ParameterizedTypeReference&lt;RestBean&lt;String&gt;&gt;() {}; User user = template.exchange(&quot;http://localhost:8080/demo&quot;,HttpMethod.POST,request,responseBodyType); //get发送 HttpEntity&lt;String&gt; request = new HttpEntity&lt;&gt;(&quot;parameters&quot;,headers); //组装 ResponseEntity&lt;String&gt; response = template.exchange(&quot;http://localhost:8080/demo&quot;,HttpMethod.GET,request,String.class); //返回指定对象 ResponseEntity&lt;User&gt; response = template.exchange(&quot;http://localhost:8080/demo&quot;,HttpMethod.GET,request,User.class); } ","link":"https://tinaxiawuhao.github.io/post/fFTlwaLS_/"},{"title":"SpringBoot  JWT实现","content":"SpringBoot JWT实现 （只是实现了jwt,没有生成证书,安全性得不到保障,证书安全验证查看Spring security JWT） &lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt&lt;/artifactId&gt; &lt;version&gt;0.6.0&lt;/version&gt; &lt;/dependency&gt; import io.jsonwebtoken.JwtBuilder; import io.jsonwebtoken.Jwts; import io.jsonwebtoken.SignatureAlgorithm; import java.util.Date; public class CreateJwtTest3 { public static void main(String[] args) { //为了方便测试，我们将过期时间设置为1分钟 long now = System.currentTimeMillis();//当前时间 long exp = now + 1000 * 60;//过期时间为1分钟 JwtBuilder builder = Jwts.builder().setId(&quot;888&quot;) .setSubject(&quot;小白&quot;) .setIssuedAt(new Date()) .signWith(SignatureAlgorithm.HS256, &quot;itcast&quot;) .setExpiration(new Date(exp)) .claim(&quot;roles&quot;, &quot;admin&quot;) //自定义claims存储数据 .claim(&quot;logo&quot;, &quot;logo.png&quot;); System.out.println(builder.compact()); } } import io.jsonwebtoken.Claims; import io.jsonwebtoken.Jwts; import org.apache.commons.lang3.time.DateFormatUtils; import java.util.Date; public class ParseJwtTest { public static void main(String[] args) { String token = &quot;eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI4ODgiLCJzdWIiOiLlsI_nmb0iLCJpYXQiOjE1NjExMDE3MzIsImV4cCI6MTU2MTEwMTc5MSwicm9sZXMiOiJhZG1pbiIsImxvZ28iOiJsb2dvLnBuZyJ9.5iVVdTw747L3ScHeCqle-bwj3cezK8NnE7VilQWOr8Y&quot;; Claims claims = Jwts.parser().setSigningKey(&quot;itcast&quot;).parseClaimsJws(token).getBody(); System.out.println(&quot;id:&quot; + claims.getId()); System.out.println(&quot;subject:&quot; + claims.getSubject()); System.out.println(&quot;roles:&quot; + claims.get(&quot;roles&quot;)); System.out.println(&quot;logo:&quot; + claims.get(&quot;logo&quot;)); System.out.println(&quot;签发时间:&quot;+ DateFormatUtils.format(claims.getIssuedAt(),&quot;yyyy-MM-dd hh:mm:ss&quot;)); System.out.println(&quot;过期时间:&quot;+DateFormatUtils.format(claims.getExpiration(),&quot;yyyy-MM-dd hh:mm:ss&quot;)); System.out.println(&quot;当前时间:&quot;+DateFormatUtils.format(new Date(),&quot;yyyy-MM-dd hh:mm:ss&quot;)); } } ","link":"https://tinaxiawuhao.github.io/post/ZsckGVdlz/"},{"title":"JWT介绍","content":"JWT介绍 在介绍JWT之前先看一下传统校验令牌的方法，如下图： 资源服务器授权流程如上图，客户端先去授权服务器申请令牌，申请令牌后，携带令牌访问资源服务器，资源服务器访问授权服务校验令牌的合法性，授权服务会返回校验结果，如果校验成功会返回用户信息给资源服务器，资源服务器如果接收到的校验结果通过了，则返回资源给客户端 问题： 传统授权方法的问题是用户每次请求资源服务，资源服务都需要携带令牌访问认证服务去校验令牌的合法性，并根据令牌获取用户的相关信息，性能低下。 解决： 使用JWT的思路是，用户认证通过会得到一个JWT令牌，JWT令牌中已经包括了用户相关的信息，客户端只需要携带JWT访问资源服务，资源服务根据事先约定的算法自行完成令牌校验，无需每次都请求认证服务完成授权。 JWT令牌授权过程如下图： 什么是JWT？ JSON Web Token（JWT）是一个开放的行业标准（RFC 7519），它定义了一种简介的、自包含的协议格式，用于在通信双方传递json对象，传递的信息经过数字签名可以被验证和信任。JWT可以使用HMAC算法或使用RSA的公钥/私钥对来签名，防止被篡改。 官网：https://jwt.io/ 标准：https://tools.ietf.org/html/rfc7519 优点： jwt基于json，非常方便解析。 可以在令牌中自定义丰富的内容，易扩展。 通过非对称加密算法及数字签名技术，JWT防止篡改，安全性高。 资源服务使用JWT可不依赖认证服务即可完成授权。 缺点： JWT令牌较长，占存储空间比较大。 令牌结构 通过学习JWT令牌结构为自定义jwt令牌打好基础。 JWT令牌由三部分组成，每部分中间使用点（.）分隔，比如：xxxxx.yyyyy.zzzzz Header 头部包括令牌的类型（即JWT）及使用的哈希算法（如HMAC SHA256或RSA） 一个例子如下： 下边是Header部分的内容 { &quot;alg&quot;: &quot;HS256&quot;, &quot;typ&quot;: &quot;JWT&quot; } 将上边的内容使用Base64Url编码，得到一个字符串就是JWT令牌的第一部分。 Payload 第二部分是负载，内容也是一个json对象，它是存放有效信息的地方，它可以存放jwt提供的现成字段，比如：iss（签发者）,exp（过期时间戳）, sub（面向的用户）等，也可自定义字段。 此部分不建议存放敏感信息，因为此部分可以解码还原原始内容。 最后将第二部分负载使用Base64Url编码，得到一个字符串就是JWT令牌的第二部分。 { &quot;sub&quot;: &quot;1234567890&quot;, &quot;name&quot;: &quot;456&quot;, &quot;admin&quot;: true } Signature 第三部分是签名，此部分用于防止jwt内容被篡改。这个部分使用base64url将前两部分进行编码，编码后使用点（.）连接组成字符串，最后使用header中声明签名算法进行签名。 HMACSHA256(base64UrlEncode(header) + &quot;.&quot; +base64UrlEncode(payload),secret) base64UrlEncode(header)：jwt令牌的第一部分。 base64UrlEncode(payload)：jwt令牌的第二部分。 secret：签名所使用的密钥。 JWT入门 Spring Security 提供对JWT的支持，本节我们使用Spring Security 提供的JwtHelper来创建JWT令牌，校验JWT令牌等操作。 生成私钥和公钥 JWT令牌生成采用非对称加密算法 生成密钥证书 下边命令生成密钥证书，采用RSA 算法每个证书包含公钥和私钥 keytool -genkeypair -alias xckey -keyalg RSA -keypass xuecheng -keystore xc.keystore -storepass xuechengkeystore Keytool 是一个java提供的证书管理工具 -alias：密钥的别名 -keyalg：使用的hash算法 -keypass：密钥的访问密码 -keystore：密钥库文件名，xc.keystore保存了生成的证书 -storepass：密钥库的访问密码 查询证书信息 keytool -list -keystore xc.keystore 删除别名 keytool -delete -alias xckey -keystore xc.keystore 导出公钥 openssl是一个加解密工具包，这里使用openssl来导出公钥信息。安装 openssl：http://slproweb.com/products/Win32OpenSSL.html 配置openssl的path环境变量，本文配置在D:\\OpenSSL-Win64\\bin cmd进入xc.keystore文件所在目录执行如下命令： keytool ‐list ‐rfc ‐‐keystore xc.keystore | openssl x509 ‐inform pem ‐pubkey 输入密钥库密码： 下边这一段就是公钥内容： -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAijyxMdq4S6L1Af1rtB8SjCZHNgsQG8JTfGy55eYvzG0B/E4AudR2prSRBvF7NYPL47scRCNPgLnvbQczBHbBug6uOr78qnWsYxHlW6Aa5dI5NsmOD4DLtSw8eX0hFyK5Fj6ScYOSFBz9cd1nNTvx2+oIv0lJDcpQdQhsfgsEr1ntvWterZt/8r7xNN83gHYuZ6TM5MYvjQNBc5qC7Krs9wM7UoQuL+s0X6RlOib7/mcLn/lFLsLDdYQAZkSDx/6+t+1oHdMarChIPYT1sx9Dwj2j2mvFNDTKKKKAq0cv14Vrhz67Vjmz2yMJePDqUi0JYS2r0iIo7n8vN7s83v5uOQIDAQAB -----END PUBLIC KEY----- 将上边的公钥拷贝到文本文件中，合并为一行。 生成jwt令牌 在认证工程创建测试类，测试jwt令牌的生成与验证。 @Test public void testCreateJwt(){ //证书文件 String key_location = &quot;xc.keystore&quot;; //密钥库密码 String keystore_password = &quot;xuechengkeystore&quot;; //访问证书路径 ClassPathResource resource = new ClassPathResource(key_location); //密钥工厂 KeyStoreKeyFactory keyStoreKeyFactory = new KeyStoreKeyFactory(resource, keystore_password.toCharArray()); //密钥的密码，此密码和别名要匹配 String keypassword = &quot;xuecheng&quot;; //密钥别名 String alias = &quot;xckey&quot;; //密钥对（密钥和公钥） KeyPair keyPair = keyStoreKeyFactory.getKeyPair(alias,keypassword.toCharArray()); //私钥 RSAPrivateKey aPrivate = (RSAPrivateKey) keyPair.getPrivate(); //定义payload信息 Map&lt;String, Object&gt; tokenMap = new HashMap&lt;&gt;(); tokenMap.put(&quot;id&quot;, &quot;123&quot;); tokenMap.put(&quot;name&quot;, &quot;mrt&quot;); tokenMap.put(&quot;roles&quot;, &quot;r01,r02&quot;); tokenMap.put(&quot;ext&quot;, &quot;1&quot;); //生成jwt令牌 Jwt jwt = JwtHelper.encode(JSON.toJSONString(tokenMap), new RsaSigner(aPrivate)); //取出jwt令牌 String token = jwt.getEncoded(); System.out.println(&quot;token=&quot;+token); } //资源服务使用公钥验证jwt的合法性，并对jwt解码 验证jwt令牌 @Test public void testVerify(){ //jwt令牌 String token =&quot;eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHQiOiIxIiwicm9sZXMiOiJyMDEscjAyIiwibmFtZSI6Im1ydCIsImlkIjoiMTIzIn0.KK7_67N5d1Dthd1PgDHMsbi0UlmjGRcm_XJUUwseJ2eZyJJWoPP2IcEZgAU3tUaaKEHUf9wSRwaDgwhrwfyIcSHbs8oy3zOQEL8j5AOjzBBs7vnRmB7DbSaQD7eJiQVJOXO1QpdmEFgjhc_IBCVTJCVWgZw60IEW1_Lg5tqaLvCiIl26K48pJB5fle2zgYMzqR1L2LyTFkq39rG57VOqqSCi3dapsZQd4ctq95SJCXgGdrUDWtD52rp5o6_0uq‐mrbRdRxkrQfsa1j8C5IW2‐T4eUmiN3f9wF9JxUK1__XC1OQkOn‐ZTBCdqwWIygDFbU7sf6KzfHJTm5vfjp6NIA&quot;; //公钥 String publickey = &quot;‐‐‐‐‐BEGIN PUBLIC KEY‐‐‐‐‐ MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAijyxMdq4S6L1Af1rtB8SjCZHNgsQG8JTfGy55eYvzG0B/E4AudR2prSRBvF7NYPL47scRCNPgLnvbQczBHbBug6uOr78qnWsYxHlW6Aa5dI5NsmOD4DLtSw8eX0hFyK5Fj6ScYOSFBz9cd1nNTvx2+oIv0lJDcpQdQhsfgsEr1ntvWterZt/8r7xNN83gHYuZ6TM5MYvjQNBc5qC7Krs9wM7UoQuL+s0X6RlOib7/mcLn/lFLsLDdYQAZkSDx/6+t+1oHdMarChIPYT1sx9Dwj2j2mvFNDTKKKKAq0cv14Vrhz67Vjmz2yMJePDqUi0JYS2r0iIo7n8vN7s83v5u OQIDAQAB ‐‐‐‐‐END PUBLIC KEY‐‐‐‐‐&quot;; //校验jwt Jwt jwt = JwtHelper.decodeAndVerify(token, new RsaVerifier(publickey)); //获取jwt原始内容 String claims = jwt.getClaims(); //jwt令牌 String encoded = jwt.getEncoded(); System.out.println(encoded); } ","link":"https://tinaxiawuhao.github.io/post/wWtT_Oc9B/"},{"title":"zookeeper概述","content":"zookeeper是什么 Zookeeper 分布式服务框架是Apache Hadoop 的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。 简单的说，zookeeper=文件系统+通知机制。 zookeeper提供了什么 1、 文件系统 Zookeeper维护一个类似文件系统的数据结构： ​ ​ 每个子目录项如 NameService 都被称作为 znode，和文件系统一样，我们能够自由的增加、删除znode，在一个znode下增加、删除子znode，唯一的不同在于znode是可以存储数据的。 有四种类型的znode： PERSISTENT-持久化目录节点 客户端与zookeeper断开连接后，该节点依旧存在 PERSISTENT_SEQUENTIAL-持久化顺序编号目录节点 客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号 EPHEMERAL-临时目录节点 客户端与zookeeper断开连接后，该节点被删除 EPHEMERAL_SEQUENTIAL-临时顺序编号目录节点 客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号 2、 通知机制 ​ 客户端注册监听它关心的目录节点，当目录节点发生变化（数据改变、被删除、子目录节点增加删除）时，zookeeper会通知客户端。 我们能用zookeeper做什么 1、 命名服务 ​ 这个似乎最简单，在zookeeper的文件系统里创建一个目录，即有唯一的path。在我们使用tborg无法确定上游程序的部署机器时即可与下游程序约定好path，通过path即能互相探索发现，不见不散了。 2、 配置管理 ​ 程序总是需要配置的，如果程序分散部署在多台机器上，要逐个改变配置就变得困难。好吧，现在把这些配置全部放到zookeeper上去，保存在 Zookeeper 的某个目录节点中，然后所有相关应用程序对这个目录节点进行监听，一旦配置信息发生变化，每个应用程序就会收到 Zookeeper 的通知，然后从 Zookeeper 获取新的配置信息应用到系统中就好。 ​ 3、 集群管理 所谓集群管理无在乎两点：是否有机器退出和加入、选举master。 ​ 对于第一点，所有机器约定在父目录GroupMembers下创建临时目录节点，然后监听父目录节点的子节点变化消息。一旦有机器挂掉，该机器与 zookeeper的连接断开，其所创建的临时目录节点被删除，所有其他机器都收到通知：某个兄弟目录被删除，于是，所有人都知道：它下船了。新机器加入 也是类似，所有机器收到通知：新兄弟目录加入。 ​ 对于第二点，我们稍微改变一下，所有机器创建临时顺序编号目录节点，每次选取编号最小的机器作为master就好。 ​ 4、 分布式锁 ​ 有了zookeeper的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占，另一个是控制时序。 ​ 对于第一类，我们将zookeeper上的一个znode看作是一把锁，通过createznode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。厕所有言：来也冲冲，去也冲冲，用完删除掉自己创建的distribute_lock 节点就释放出锁。 ​ 对于第二类， /distribute_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选master一样，编号最小的获得锁，用完删除，依次方便。 ​ 5、队列管理 两种类型的队列： 同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。 队列按照 FIFO 方式进行入队和出队操作。 第一类，在约定目录下创建临时目录节点，监听节点数目是否是我们要求的数目。 第二类，和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。 ​ 终于了解完我们能用zookeeper做什么了，可是作为一个程序员，我们总是想狂热了解zookeeper是如何做到这一点的，单点维护一个文件系统没有什么难度，可是如果是一个集群维护一个文件系统保持数据的一致性就非常困难了。 6、分布式与数据复制 Zookeeper作为一个集群提供一致的数据服务，自然，它要在所有机器间做数据复制。数据复制的好处： 容错 一个节点出错，不致于让整个系统停止工作，别的节点可以接管它的工作； 提高系统的扩展能力 把负载分布到多个节点上，或者增加节点来提高系统的负载能力； 提高性能 让客户端本地访问就近的节点，提高用户访问速度。 从客户端读写访问的透明度来看，数据复制集群系统分下面两种： 写主(WriteMaster) 对数据的修改提交给指定的节点。读无此限制，可以读取任何一个节点。这种情况下客户端需要对读与写进行区别，俗称读写分离； 写任意(Write Any) 对数据的修改可提交给任意的节点，跟读一样。这种情况下，客户端对集群节点的角色与变化透明。 ​ 对zookeeper来说，它采用的方式是写任意。通过增加机器，它的读吞吐能力和响应能力扩展性非常好，而写，随着机器的增多吞吐能力肯定下降（这 也是它建立observer的原因），而响应能力则取决于具体实现方式，是延迟复制保持最终一致性，还是立即复制快速响应。 我们关注的重点还是在如何保证数据在集群所有机器的一致性，这就涉及到paxos算法。 7、数据一致性与paxos算法 ​ 据说Paxos算法的难理解与算法的知名度一样令人敬仰，所以我们先看如何保持数据的一致性，这里有个原则就是： 在一个分布式数据库系统中，如果各节点的初始状态一致，每个节点都执行相同的操作序列，那么他们最后能得到一个一致的状态。 ​ Paxos算法解决的什么问题呢，解决的就是保证每个节点执行相同的操作序列。好吧，这还不简单，master维护一个全局写队列，所有写操作都必须 放入这个队列编号，那么无论我们写多少个节点，只要写操作是按编号来的，就能保证一致性。没错，就是这样，可是如果master挂了呢。 ​ Paxos算法通过投票来对写操作进行全局编号，同一时刻，只有一个写操作被批准，同时并发的写操作要去争取选票，只有获得过半数选票的写操作才会被 批准（所以永远只会有一个写操作得到批准），其他的写操作竞争失败只好再发起一轮投票，就这样，在日复一日年复一年的投票中，所有写操作都被严格编号排 序。编号严格递增，当一个节点接受了一个编号为100的写操作，之后又接受到编号为99的写操作（因为网络延迟等很多不可预见原因），它马上能意识到自己 数据不一致了，自动停止对外服务并重启同步过程。任何一个节点挂掉都不会影响整个集群的数据一致性（总2n+1台，除非挂掉大于n台）。 总结 ​ Zookeeper 作为 Hadoop 项目中的一个子项目，是 Hadoop 集群管理的一个必不可少的模块，它主要用来控制集群中的数据，如它管理 Hadoop 集群中的 NameNode，还有 Hbase 中 Master Election、Server 之间状态同步等。 Zookeeper工作原理 ​ ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，它包含一个简单的原语集，分布式应用程序可以基于它实现同步服务，配置维护和 命名服务等。Zookeeper是hadoop的一个子项目，其发展历程无需赘述。在分布式应用中，由于工程师不能很好地使用锁机制，以及基于消息的协调 机制不适合在某些应用中使用，因此需要有一种可靠的、可扩展的、分布式的、可配置的协调机制来统一系统的状态。Zookeeper的目的就在于此。 Zookeeper的基本概念 1.1 角色 Zookeeper中的角色主要有以下三类，如下表所示： ​ 系统模型如图所示： ​ 1.2 设计目的 最终一致性：client不论连接到哪个Server，展示给它都是同一个视图，这是zookeeper最重要的性能。 可靠性：具有简单、健壮、良好的性能，如果消息m被一台服务器接受，那么它将被所有的服务器接受。 实时性：Zookeeper保证客户端将在一个时间间隔范围内获得服务器的更新信息，或者服务器失效的信息。但由于网络延时等原因，Zookeeper不能保证两个客户端能同时得到刚更新的数据，如果需要最新数据，应该在读数据之前调用sync()接口。 等待无关（wait-free）：慢的或者失效的client不得干预快速的client的请求，使得每个client都能有效的等待。 原子性：更新只能成功或者失败，没有中间状态。 顺序性：包括全局有序和偏序两种：全局有序是指如果在一台服务器上消息a在消息b前发布，则在所有Server上消息a都将在消息b前被发布；偏序是指如果一个消息b在消息a后被同一个发送者发布，a必将排在b前面。 ZooKeeper的流程设计 ​ Zookeeper的核心是原子广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。Zab协议有两种模式，它们分 别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和 leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。 ​ 为了保证事务的顺序一致性，zookeeper采用了递增的事务id号（zxid）来标识事务。所有的提议（proposal）都在被提出的时候加上 了zxid。实现中zxid是一个64位的数字，它高32位是epoch用来标识leader关系是否改变，每次一个leader被选出来，它都会有一个 新的epoch，标识当前属于那个leader的统治时期。低32位用于递增计数。 每个Server在工作过程中有三种状态： LOOKING：当前Server不知道leader是谁，正在搜寻 LEADING：当前Server即为选举出来的leader FOLLOWING：leader已经选举出来，当前Server与之同步 2.1 选主流程 当leader崩溃或者leader失去大多数的follower，这时候zk进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的 Server都恢复到一个正确的状态。Zk的选举算法有两种：一种是基于basic paxos实现的，另外一种是基于fast paxos算法实现的。系统默认的选举算法为fast paxos。先介绍basic paxos流程： ​ 1 .选举线程由当前Server发起选举的线程担任，其主要功能是对投票结果进行统计，并选出推荐的Server； ​ 2 .选举线程首先向所有Server发起一次询问(包括自己)； ​ 3 .选举线程收到回复后，验证是否是自己发起的询问(验证zxid是否一致)，然后获取对方的id(myid)，并存储到当前询问对象列表中，最后获取对方提议的leader相关信息( id,zxid)，并将这些信息存储到当次选举的投票记录表中； ​ 4. 收到所有Server回复以后，就计算出zxid最大的那个Server，并将这个Server相关信息设置成下一次要投票的Server； ​ 5. 线程将当前zxid最大的Server设置为当前Server要推荐的Leader，如果此时获胜的Server获得n/2 + 1的Server票数， 设置当前推荐的leader为获胜的Server，将根据获胜的Server相关信息设置自己的状态，否则，继续这个过程，直到leader被选举出来。 通过流程分析我们可以得出：要使Leader获得多数Server的支持，则Server总数必须是奇数2n+1，且存活的Server的数目不得少于n+1. 每个Server启动后都会重复以上流程。在恢复模式下，如果是刚从崩溃状态恢复的或者刚启动的server还会从磁盘快照中恢复数据和会话信息，zk会记录事务日志并定期进行快照，方便在恢复时进行状态恢复。选主的具体流程图如下所示： ​ fast paxos流程是在选举过程中，某Server首先向所有Server提议自己要成为leader，当其它Server收到提议以后，解决epoch和 zxid的冲突，并接受对方的提议，然后向对方发送接受提议完成的消息，重复这个流程，最后一定能选举出Leader。其流程图如下所示： 2.2 同步流程 选完leader以后，zk就进入状态同步过程。 ​ 1. leader等待server连接； ​ 2 .Follower连接leader，将最大的zxid发送给leader； ​ 3 .Leader根据follower的zxid确定同步点； ​ 4 .完成同步后通知follower 已经成为uptodate状态； ​ 5 .Follower收到uptodate消息后，又可以重新接受client的请求进行服务了。 流程图如下所示： ​ 2.3 工作流程 2.3.1 Leader工作流程 Leader主要有三个功能： ​ 1 .恢复数据； ​ 2 .维持与Learner的心跳，接收Learner请求并判断Learner的请求消息类型； ​ 3 .Learner的消息类型主要有PING消息、REQUEST消息、ACK消息、REVALIDATE消息，根据不同的消息类型，进行不同的处理。 ​ PING消息是指Learner的心跳信息；REQUEST消息是Follower发送的提议信息，包括写请求及同步请求；ACK消息是 Follower的对提议的回复，超过半数的Follower通过，则commit该提议；REVALIDATE消息是用来延长SESSION有效时间。 Leader的工作流程简图如下所示，在实际实现中，流程要比下图复杂得多，启动了三个线程来实现功能。 ​ 2.3.2 Follower工作流程 Follower主要有四个功能： ​ 1. 向Leader发送请求（PING消息、REQUEST消息、ACK消息、REVALIDATE消息）； ​ 2 .接收Leader消息并进行处理； ​ 3 .接收Client的请求，如果为写请求，发送给Leader进行投票； ​ 4 .返回Client结果。 Follower的消息循环处理如下几种来自Leader的消息： ​ 1 .PING消息： 心跳消息； ​ 2 .PROPOSAL消息：Leader发起的提案，要求Follower投票； ​ 3 .COMMIT消息：服务器端最新一次提案的信息； ​ 4 .UPTODATE消息：表明同步完成； ​ 5 .REVALIDATE消息：根据Leader的REVALIDATE结果，关闭待revalidate的session还是允许其接受消息； ​ 6 .SYNC消息：返回SYNC结果到客户端，这个消息最初由客户端发起，用来强制得到最新的更新。 Follower的工作流程简图如下所示，在实际实现中，Follower是通过5个线程来实现功能的。 ​ 对于observer的流程不再叙述，observer流程和Follower的唯一不同的地方就是observer不会参加leader发起的投票。 ","link":"https://tinaxiawuhao.github.io/post/DOckY7njv/"},{"title":"Gossip协议","content":"Gossip Gossip协议是一个通信协议，一种传播消息的方式，灵感来自于：瘟疫、社交网络等。使用Gossip协议的有：Redis Cluster、Consul、Apache Cassandra等。 六度分隔理论 说到社交网络，就不得不提著名的六度分隔理论。1967年，哈佛大学的心理学教授Stanley Milgram想要描绘一个连结人与社区的人际连系网。做过一次连锁信实验，结果发现了“六度分隔”现象。简单地说：“你和任何一个陌生人之间所间隔的人不会超过六个，也就是说，最多通过六个人你就能够认识任何一个陌生人。 数学解释该理论：若每个人平均认识260人，其六度就是260↑6 =1,188,137,600,000。消除一些节点重复，那也几乎覆盖了整个地球人口若干多多倍，这也是Gossip协议的雏形。 原理 Gossip协议基本思想就是：一个节点想要分享一些信息给网络中的其他的一些节点。于是，它周期性的随机选择一些节点，并把信息传递给这些节点。这些收到信息的节点接下来会做同样的事情，即把这些信息传递给其他一些随机选择的节点。一般而言，信息会周期性的传递给N个目标节点，而不只是一个。这个N被称为fanout（这个单词的本意是扇出）。 用途 Gossip协议的主要用途就是信息传播和扩散：即把一些发生的事件传播到全世界。它们也被用于数据库复制，信息扩散，集群成员身份确认，故障探测等。 基于Gossip协议的一些有名的系统：Apache Cassandra，Redis（Cluster模式），Consul等。 图解 接下来通过多张图片剖析Gossip协议是如何运行的。如下图所示，Gossip协议是周期循环执行的。图中的公式表示Gossip协议把信息传播到每一个节点需要多少次循环动作，需要说明的是，公式中的20表示整个集群有20个节点，4表示某个节点会向4个目标节点传播消息： Gossip Protocol 如下图所示，红色的节点表示其已经“受到感染”，即接下来要传播信息的源头，连线表示这个初始化感染的节点能正常连接的节点（其不能连接的节点只能靠接下来感染的节点向其传播消息）。并且N等于4，我们假设4根较粗的线路，就是它第一次传播消息的线路： first infected node 第一次消息完成传播后，新增了4个节点会被“感染”，即这4个节点也收到了消息。这时候，总计有5个节点变成红色： infected nodes 那么在下一次传播周期时，总计有5个节点，且这5个节点每个节点都会向4个节点传播消息。最后，经过3次循环，20个节点全部被感染（都变成红色节点），即说明需要传播的消息已经传播给了所有节点： infected all nodes 需要说明的是，20个节点且设置fanout=4，公式结果是2.16，这只是个近似值。真实传递时，可能需要3次甚至4次循环才能让所有节点收到消息。这是因为每个节点在传播消息的时候，是随机选择N个节点的，这样的话，就有可能某个节点会被选中2次甚至更多次。 发送消息 由前面对Gossip协议图解分析可知，节点传播消息是周期性的，并且每个节点有它自己的周期。另外，节点发送消息时的目标节点数由参数fanout决定。至于往哪些目标节点发送，则是随机的。 一旦消息被发送到目标节点，那么目标节点也会被感染。一旦某个节点被感染，那么它也会向其他节点传播消息，试图感染更多的节点。最终，每一个节点都会被感染，即消息被同步给了所有节点： 可扩展性 Gossip协议是可扩展的，因为它只需要O(logN) 个周期就能把消息传播给所有节点。某个节点在往固定数量节点传播消息过程中，并不需要等待确认（ack），并且，即使某条消息传播过程中丢失，它也不需要做任何补偿措施。打个比方，某个节点本来需要将消息传播给4个节点，但是由于网络或者其他原因，只有3个消息接收到消息，即使这样，这对最终所有节点接收到消息是没有任何影响的。 如下表格所示，假定fanout=4，那么在节点数分别是20、40、80、160时，消息传播到所有节点需要的循环次数对比，在节点成倍扩大的情况下，循环次数并没有增加很多。所以，Gossip协议具备可扩展性： 失败容错 Gossip也具备失败容错的能力，即使网络故障等一些问题，Gossip协议依然能很好的运行。因为一个节点会多次分享某个需要传播的信息，即使不能连通某个节点，其他被感染的节点也会尝试向这个节点传播信息。 健壮性 Gossip协议下，没有任何扮演特殊角色的节点（比如leader等）。任何一个节点无论什么时候下线或者加入，并不会破坏整个系统的服务质量。 然而，Gossip协议也有不完美的地方，例如，拜占庭问题（Byzantine）。即，如果有一个恶意传播消息的节点，Gossip协议的分布式系统就会出问题。 作者：阿飞的博客 原文地址：https://www.jianshu.com/p/54eab117e6ae ","link":"https://tinaxiawuhao.github.io/post/BmWrFaJAQ/"},{"title":"CAP/BASE","content":"CAP原则 CAP原则又称CAP定理，指的是在一个分布式系统中， Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可得兼。 CAP原则是NOSQL数据库的基石。 分布式系统的CAP理论：理论首先把分布式系统中的三个特性进行了如下归纳： 一致性（C）：在分布式系统中的所有数据备份，在同一时刻是否同样的值。（等同于所有节点访问同一份最新的数据副本） 可用性（A）：在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求。（对数据更新具备高可用性） 分区容忍性（P）：以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据正常返回，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。 一致性与可用性的决择编辑 CAP理论就是说在分布式存储系统中，最多只能实现上面的两点。而由于当前的网络硬件肯定会出现延迟丢包等问题，所以分区容忍性是我们必须需要实现的。所以我们只能在一致性和可用性之间进行权衡，没有NoSQL系统能同时保证这三点。 对于web2.0网站来说，关系数据库的很多主要特性却往往无用武之地 数据库事务一致性需求 很多web实时系统并不要求严格的数据库事务，对读一致性的要求很低，有些场合对写一致性要求并不高。允许实现最终一致性。 数据库的写实时性和读实时性需求 对关系数据库来说，插入一条数据之后立刻查询，是肯定可以读出来这条数据的，但是对于很多web应用来说，并不要求这么高的实时性，比方说发一条消息之 后，过几秒乃至十几秒之后，我的订阅者才看到这条动态是完全可以接受的。 对复杂的SQL查询，特别是多表关联查询的需求 任何大数据量的web系统，都非常忌讳多个大表的关联查询，以及复杂的数据分析类型的报表查询，特别是SNS类型的网站，从需求以及产品设计角 度，就避免了这种情况的产生。往往更多的只是单表的主键查询，以及单表的简单条件分页查询，SQL的功能被极大的弱化了。 CAP定理的证明 现在我们就来证明一下，为什么不能同时满足三个特性？ 假设有两台服务器，一台放着应用A和数据库V，一台放着应用B和数据库V，他们之间的网络可以互通，也就相当于分布式系统的两个部分。 在满足一致性的时候，两台服务器 N1和N2，一开始两台服务器的数据是一样的，DB0=DB0。在满足可用性的时候，用户不管是请求N1或者N2，都会得到立即响应。在满足分区容错性的情况下，N1和N2有任何一方宕机，或者网络不通的时候，都不会影响N1和N2彼此之间的正常运作。 当用户通过N1中的A应用请求数据更新到服务器DB0后，这时N1中的服务器DB0变为DB1，通过分布式系统的数据同步更新操作，N2服务器中的数据库V0也更新为了DB1，这时，用户通过B向数据库发起请求得到的数据就是即时更新后的数据DB1。 上面是正常运作的情况，但分布式系统中，最大的问题就是网络传输问题，现在假设一种极端情况，N1和N2之间的网络断开了，但我们仍要支持这种网络异常，也就是满足分区容错性，那么这样能不能同时满足一致性和可用性呢？ 假设N1和N2之间通信的时候网络突然出现故障，有用户向N1发送数据更新请求，那N1中的数据DB0将被更新为DB1，由于网络是断开的，N2中的数据库仍旧是DB0； 如果这个时候，有用户向N2发送数据读取请求，由于数据还没有进行同步，应用程序没办法立即给用户返回最新的数据DB1，怎么办呢？有二种选择，第一，牺牲数据一致性，响应旧的数据DB0给用户；第二，牺牲可用性，阻塞等待，直到网络连接恢复，数据更新操作完成之后，再给用户响应最新的数据DB1。 上面的过程比较简单，但也说明了要满足分区容错性的分布式系统，只能在一致性和可用性两者中，选择其中一个。也就是说分布式系统不可能同时满足三个特性。这就需要我们在搭建系统时进行取舍了，那么，怎么取舍才是更好的策略呢? 取舍策略 CAP三个特性只能满足其中两个，那么取舍的策略就共有三种： CA without P： 如果不要求P（不允许分区），则C（强一致性）和A（可用性）是可以保证的。但放弃P的同时也就意味着放弃了系统的扩展性，也就是分布式节点受限，没办法部署子节点，这是违背分布式系统设计的初衷的。传统的关系型数据库RDBMS：Oracle、MySQL就是CA。 CP without A： 如果不要求A（可用），相当于每个请求都需要在服务器之间保持强一致，而P（分区）会导致同步时间无限延长(也就是等待数据同步完才能正常访问服务)，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。设计成CP的系统其实不少，最典型的就是分布式数据库，如Redis、HBase等。对于这些分布式数据库来说，数据的一致性是最基本的要求，因为如果连这个标准都达不到，那么直接采用关系型数据库就好，没必要再浪费资源来部署分布式数据库。 AP wihtout C： 要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。典型的应用就如某米的抢购手机场景，可能前几秒你浏览商品的时候页面提示是有库存的，当你选择完商品准备下单的时候，系统提示你下单失败，商品已售完。这其实就是先在 A（可用性）方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，虽然多少会影响一些用户体验，但也不至于造成用户购物流程的严重阻塞。 BASE理论 BASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的简写，BASE是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的结论，是基于CAP定理逐步演化而来的，其核心思想是即使无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。接下来我们着重对BASE中的三要素进行详细讲解。 基本可用 基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性——但请注意，这绝不等价于系统不可用，以下两个就是“基本可用”的典型例子。 响应时间上的损失：正常情况下，一个在线搜索引擎需要0.5秒内返回给用户相应的查询结果，但由于出现异常（比如系统部分机房发生断电或断网故障），查询结果的响应时间增加到了1~2秒。 功能上的损失：正常情况下，在一个电子商务网站上进行购物，消费者几乎能够顺利地完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。 弱状态 弱状态也称为软状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。 最终一致性 最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性 亚马逊首席技术官Werner Vogels在于2008年发表的一篇文章中对最终一致性进行了非常详细的介绍。他认为最终一致性是一种特殊的弱一致性：系统能够保证在没有其他新的更新操作的情况下，数据最终一定能够达到一致的状态，因此所有客户端对系统的数据访问都能够获取到最新的值。同时，在没有发生故障的前提下，数据达到一致状态的时间延迟，取决于网络延迟，系统负载和数据复制方案设计等因素。 在实际工程实践中，最终一致性存在以下五类主要变种。 因果一致性： 因果一致性是指，如果进程A在更新完某个数据项后通知了进程B，那么进程B之后对该数据项的访问都应该能够获取到进程A更新后的最新值，并且如果进程B要对该数据项进行更新操作的话，务必基于进程A更新后的最新值，即不能发生丢失更新情况。与此同时，与进程A无因果关系的进程C的数据访问则没有这样的限制。 读己之所写： 读己之所写是指，进程A更新一个数据项之后，它自己总是能够访问到更新过的最新值，而不会看到旧值。也就是说，对于单个数据获取者而言，其读取到的数据一定不会比自己上次写入的值旧。因此，读己之所写也可以看作是一种特殊的因果一致性。 会话一致性： 会话一致性将对系统数据的访问过程框定在了一个会话当中：系统能保证在同一个有效的会话中实现“读己之所写”的一致性，也就是说，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。 单调读一致性： 单调读一致性是指如果一个进程从系统中读取出一个数据项的某个值后，那么系统对于该进程后续的任何数据访问都不应该返回更旧的值。 单调写一致性： 单调写一致性是指，一个系统需要能够保证来自同一个进程的写操作被顺序地执行。 以上就是最终一致性的五类常见的变种，在时间系统实践中，可以将其中的若干个变种互相结合起来，以构建一个具有最终一致性的分布式系统。事实上，可以将其中的若干个变种相互结合起来，以构建一个具有最终一致性特性的分布式系统。事实上，最终一致性并不是只有那些大型分布式系统才设计的特性，许多现代的关系型数据库都采用了最终一致性模型。在现代关系型数据库中，大多都会采用同步和异步方式来实现主备数据复制技术。在同步方式中，数据的复制国耻鞥通常是更新事务的一部分，因此在事务完成后，主备数据库的数据就会达到一致。而在异步方式中，备库的更新往往存在延时，这取决于事务日志在主备数据库之间传输的时间长短，如果传输时间过长或者甚至在日志传输过程中出现异常导致无法及时将事务应用到备库上，那么狠显然，从备库中读取的的数据将是旧的，因此就出现了不一致的情况。当然，无论是采用多次重试还是认为数据订正，关系型数据库还是能搞保证最终数据达到一致——这就是系统提供最终一致性保证的经典案例。 总的来说，BASE理论面向的是大型高可用可扩展的分布式系统，和传统事务的ACID特性使相反的，它完全不同于ACID的强一致性模型，而是提出通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。但同时，在实际的分布式场景中，不同业务单元和组件对数据一致性的要求是不同的，因此在具体的分布式系统架构设计过程中，ACID特性与BASE理论往往又会结合在一起使用。 ","link":"https://tinaxiawuhao.github.io/post/cxyiXFW2S/"},{"title":"MyBatis 缓存详解","content":"缓存是一般的ORM 框架都会提供的功能，目的就是提升查询的效率和减少数据库的压力。跟Hibernate 一样，MyBatis 也有一级缓存和二级缓存，并且预留了集成第三方缓存的接口。 缓存体系结构： MyBatis 跟缓存相关的类都在cache 包里面，其中有一个Cache 接口，只有一个默认的实现类 PerpetualCache，它是用HashMap 实现的。 所有的缓存实现类总体上可分为三类：基本缓存、淘汰算法缓存、装饰器缓存。 缓存实现类 描述 作用 装饰条件 基本缓存 缓存基本实现类 默认是PerpetualCache，也可以自定义比如RedisCache等，具备基本功能的缓存类 无 LruCache LRU策略的缓存 当缓存达到上限时，删除最近最少使用的缓存 eviction=“LRU” （默认） FifoCache FIFO策略的缓存 当缓存达到上限时，删除最先入队的缓存 eviction=“FIFO” SoftCache/WeakCache 带清理策略的缓存 通过JVM的软引用和弱引用来实现缓存，当JVM内存不足时，会自动清理掉这些缓存 eviction=“SOFT”/eviction=“WEAK” LoggingCache 带日志功能的缓存 比如输出缓存命中率 基本 SynchronizedCache 同步缓存 基于Synchronized关键字实现，解决并发问题 基本 BlockingCache 阻塞缓存 通过在get/put方式中加锁，保证只有一个线程操作缓存，基于Java重入锁实现 blocking=true SerializedCache 支持序列化的缓存 将对象序列化以后存到缓存中，取出是反序列化 readOnly=false(默认) ScheduledCache 定时调度的缓存 在进行 get/put/remove/getSize 等操作前，判断 缓存时间是否超过了设置的最长缓存时间（默认是 一小时），如果是则清空缓存–即每隔一段时间清 空一次缓存 flushInterval不为空 TransactionalCache 事务缓存 在二级缓存中使用，可以一次存入多个缓存，删除多个缓存 在TransactionalCacheManager中用Map维护对应关系 一级缓存（本地缓存） 一级缓存也叫本地缓存，MyBatis 的一级缓存是在会话（SqlSession）层面进行缓存的。MyBatis 的一级缓存是默认开启的，不需要任何的配置。首先我们必须去弄清楚一个问题，在MyBatis 执行的流程里面，涉及到这么多的对象，那么缓存PerpetualCache 应该放在哪个对象里面去维护？如果要在同一个会话里面共享一级缓存，这个对象肯定是在SqlSession 里面创建的，作为SqlSession 的一个属性。 DefaultSqlSession 里面只有两个属性，Configuration 是全局的，所以缓存只可能放在Executor 里面维护——SimpleExecutor/ReuseExecutor/BatchExecutor 的父类BaseExecutor的构造函数中持有了PerpetualCache。在同一个会话里面，多次执行相同的SQL 语句，会直接从内存取到缓存的结果，不会再发送SQL 到数据库。但是不同的会话里面，即使执行的SQL 一模一样（通过一个Mapper 的同一个方法的相同参数调用），也不能使用到一级缓存。 如下图所示，MyBatis会在一次会话的表示----一个SqlSession对象中创建一个本地缓存(local cache)，对于每一次查询，都会尝试根据查询的条件去本地缓存中查找是否在缓存中，如果在缓存中，就直接从缓存中取出，然后返回给用户；否则，从数据库读取数据，将查询结果存入缓存并返回给用户。 一级缓存的生命周期有多长？ MyBatis在开启一个数据库会话时，会 创建一个新的SqlSession对象，SqlSession对象中会有一个新的Executor对象，Executor对象中持有一个新的PerpetualCache对象；当会话结束时，SqlSession对象及其内部的Executor对象还有PerpetualCache对象也一并释放掉。 如果SqlSession调用了close()方法，会释放掉一级缓存PerpetualCache对象，一级缓存将不可用； 如果SqlSession调用了clearCache()，会清空PerpetualCache对象中的数据，但是该对象仍可使用； SqlSession中执行了任何一个update操作(update()、delete()、insert()) ，都会清空PerpetualCache对象的数据，但是该对象可以继续使用； SqlSession 一级缓存的工作流程： 对于某个查询，根据statementId,params,rowBounds来构建一个key值，根据这个key值去缓存Cache中取出对应的key值存储的缓存结果 判断从Cache中根据特定的key值取的数据数据是否为空，即是否命中； 如果命中，则直接将缓存结果返回； 如果没命中： 去数据库中查询数据，得到查询结果； 将key和查询到的结果分别作为key,value对存储到Cache中； 将查询结果返回； 接下来我们来验证一下，MyBatis 的一级缓存到底是不是只能在一个会话里面共享，以及跨会话（不同session）操作相同的数据会产生什么问题。判断是否命中缓存：如果再次发送SQL 到数据库执行，说明没有命中缓存；如果直接打印对象，说明是从内存缓存中取到了结果。 在同一个session 中共享（不同session 不能共享） 同一个会话中，update（包括delete）会导致一级缓存被清空 其他会话更新了数据，导致读取到脏数据（一级缓存不能跨会话共享） 一级缓存的不足： 使用一级缓存的时候，因为缓存不能跨会话共享，不同的会话之间对于相同的数据可能有不一样的缓存。在有多个会话或者分布式环境下，会存在脏数据的问题。如果要解决这个问题，就要用到二级缓存。MyBatis 一级缓存（MyBaits 称其为 Local Cache）无法关闭，但是有两种级别可选： session 级别的缓存，在同一个 sqlSession 内，对同样的查询将不再查询数据库，直接从缓存中。 statement 级别的缓存，避坑： 为了避免这个问题，可以将一级缓存的级别设为 statement 级别的，这样每次查询结束都会清掉一级缓存。 二级缓存 二级缓存是用来解决一级缓存不能跨会话共享的问题的，范围是namespace 级别的，可以被多个SqlSession 共享（只要是同一个接口里面的相同方法，都可以共享），生命周期和应用同步。如果你的MyBatis使用了二级缓存，并且你的Mapper和select语句也配置使用了二级缓存，那么在执行select查询的时候，MyBatis会先从二级缓存中取输入，其次才是一级缓存，即MyBatis查询数据的顺序是：二级缓存 —&gt; 一级缓存 —&gt; 数据库。 作为一个作用范围更广的缓存，它肯定是在SqlSession 的外层，否则不可能被多个SqlSession 共享。而一级缓存是在SqlSession 内部的，所以第一个问题，肯定是工作在一级缓存之前，也就是只有取不到二级缓存的情况下才到一个会话中去取一级缓存。第二个问题，二级缓存放在哪个对象中维护呢？ 要跨会话共享的话，SqlSession 本身和它里面的BaseExecutor 已经满足不了需求了，那我们应该在BaseExecutor 之外创建一个对象。 实际上MyBatis 用了一个装饰器的类来维护，就是CachingExecutor。如果启用了二级缓存，MyBatis 在创建Executor 对象的时候会对Executor 进行装饰。CachingExecutor 对于查询请求，会判断二级缓存是否有缓存结果，如果有就直接返回，如果没有委派交给真正的查询器Executor 实现类，比如SimpleExecutor 来执行查询，再走到一级缓存的流程。最后会把结果缓存起来，并且返回给用户。 开启二级缓存的方法 第一步：配置 mybatis.configuration.cache-enabled=true，只要没有显式地设置cacheEnabled=false，都会用CachingExecutor 装饰基本的执行器。 第二步：在Mapper.xml 中配置标签： &lt;cache type=&quot;org.apache.ibatis.cache.impl.PerpetualCache&quot; size=&quot;1024&quot; eviction=&quot;LRU&quot; flushInterval=&quot;120000&quot; readOnly=&quot;false&quot;/&gt; 基本上就是这样。这个简单语句的效果如下: 映射语句文件中的所有 select 语句的结果将会被缓存。 映射语句文件中的所有 insert、update 和 delete 语句会刷新缓存。 缓存会使用最近最少使用算法（LRU, Least Recently Used）算法来清除不需要的缓存。 缓存不会定时进行刷新（也就是说，没有刷新间隔）。 缓存会保存列表或对象（无论查询方法返回哪种）的 1024 个引用。 缓存会被视为读/写缓存，这意味着获取到的对象并不是共享的，可以安全地被调用者修改，而不干扰其他调用者或线程所做的潜在修改。 这个更高级的配置创建了一个 FIFO 缓存，每隔 60 秒刷新，最多可以存储结果对象或列表的 512 个引用，而且返回的对象被认为是只读的，因此对它们进行修改可能会在不同线程中的调用者产生冲突。可用的清除策略有： LRU – 最近最少使用：移除最长时间不被使用的对象。 FIFO – 先进先出：按对象进入缓存的顺序来移除它们。 SOFT – 软引用：基于垃圾回收器状态和软引用规则移除对象。 WEAK – 弱引用：更积极地基于垃圾收集器状态和弱引用规则移除对象。 默认的清除策略是 LRU。 flushInterval（刷新间隔）属性可以被设置为任意的正整数，设置的值应该是一个以毫秒为单位的合理时间量。 默认情况是不设置，也就是没有刷新间隔，缓存仅仅会在调用语句时刷新。 size（引用数目）属性可以被设置为任意正整数，要注意欲缓存对象的大小和运行环境中可用的内存资源。默认值是 1024。 readOnly（只读）属性可以被设置为 true 或 false。只读的缓存会给所有调用者返回缓存对象的相同实例。 因此这些对象不能被修改。这就提供了可观的性能提升。而可读写的缓存会（通过序列化）返回缓存对象的拷贝。 速度上会慢一些，但是更安全，因此默认值是 false。 注：二级缓存是事务性的。这意味着，当 SqlSession 完成并提交时，或是完成并回滚，但没有执行 flushCache=true的 insert/delete/update 语句时，缓存会获得更新。 Mapper.xml 配置了之后，select()会被缓存。update()、delete()、insert()会刷新缓存。：如果cacheEnabled=true，Mapper.xml 没有配置标签，还有二级缓存吗？（没有）还会出现CachingExecutor 包装对象吗？（会） 只要cacheEnabled=true 基本执行器就会被装饰。有没有配置，决定了在启动的时候会不会创建这个mapper 的Cache 对象，只是最终会影响到CachingExecutorquery 方法里面的判断。如果某些查询方法对数据的实时性要求很高，不需要二级缓存，怎么办？我们可以在单个Statement ID 上显式关闭二级缓存（默认是true）： &lt;select id=&quot;selectBlog&quot; resultMap=&quot;BaseResultMap&quot; useCache=&quot;false&quot;&gt; 第三方缓存做二级缓存 除了MyBatis 自带的二级缓存之外，我们也可以通过实现Cache 接口来自定义二级缓存。MyBatis 官方提供了一些第三方缓存集成方式，比如ehcache 和redis：https://github.com/mybatis/redis-cache ,这里就不过多介绍了。当然，我们也可以使用独立的缓存服务，不使用MyBatis 自带的二级缓存。 pom 文件引入依赖： &lt;dependency&gt; &lt;groupId&gt;org.mybatis.caches&lt;/groupId&gt; &lt;artifactId&gt;mybatis-redis&lt;/artifactId&gt; &lt;version&gt;1.0.0-beta2&lt;/version&gt; &lt;/dependency&gt; MybatisRedisCache import com.xxx.util.JsonUtils; import org.apache.ibatis.cache.Cache; import org.apache.logging.log4j.LogManager; import org.apache.logging.log4j.Logger; import org.springframework.data.redis.core.RedisTemplate; import java.util.concurrent.locks.ReadWriteLock; import java.util.concurrent.locks.ReentrantReadWriteLock; /** * Mybatis - redis二级缓存 * */ public final class MybatisRedisCache implements Cache { /** * 日志工具类 */ private static final Logger logger = LogManager.getLogger(MybatisRedisCache.class); /** * 读写锁 */ private final ReadWriteLock readWriteLock = new ReentrantReadWriteLock(); /** * ID */ private String id; /** * 集成redisTemplate */ private static RedisTemplate redisTemplate; public MybatisRedisCache() { } public MybatisRedisCache(String id) { if (id == null) { throw new IllegalArgumentException(&quot;Cache instances require an ID&quot;); } else { logger.debug(&quot;MybatisRedisCache.id={}&quot;, id); this.id = id; } } @Override public String getId() { return this.id; } @Override public int getSize() { try { Long size = redisTemplate.opsForHash().size(this.id.toString()); logger.debug(&quot;MybatisRedisCache.getSize: {}-&gt;{}&quot;, id, size); return size.intValue(); } catch (Exception e) { e.printStackTrace(); } return 0; } @Override public void putObject(final Object key, final Object value) { try { logger.debug(&quot;MybatisRedisCache.putObject: {}-&gt;{}-&gt;{}&quot;, id, key, JsonUtils.toJson(value)); redisTemplate.opsForHash().put(this.id.toString(), key.toString(), value); } catch (Exception e) { e.printStackTrace(); } } @Override public Object getObject(final Object key) { try { Object hashVal = redisTemplate.opsForHash().get(this.id.toString(), key.toString()); logger.debug(&quot;MybatisRedisCache.getObject: {}-&gt;{}-&gt;{}&quot;, id, key, JsonUtils.toJson(hashVal)); return hashVal; } catch (Exception e) { e.printStackTrace(); return null; } } @Override public Object removeObject(final Object key) { try { redisTemplate.opsForHash().delete(this.id.toString(), key.toString()); logger.debug(&quot;MybatisRedisCache.removeObject: {}-&gt;{}-&gt;{}&quot;, id, key); } catch (Exception e) { e.printStackTrace(); } return null; } @Override public void clear() { try { redisTemplate.delete(this.id.toString()); logger.debug(&quot;MybatisRedisCache.clear: {}&quot;, id); } catch (Exception e) { e.printStackTrace(); } } @Override public ReadWriteLock getReadWriteLock() { return this.readWriteLock; } @Override public String toString() { return &quot;MybatisRedisCache {&quot; + this.id + &quot;}&quot;; } /** * 设置redisTemplate * * @param redisTemplate */ public void setRedisTemplate(RedisTemplate redisTemplate) { MybatisRedisCache.redisTemplate = redisTemplate; } } RedisConfig import com.fasterxml.jackson.annotation.JsonAutoDetect; import com.fasterxml.jackson.annotation.PropertyAccessor; import com.fasterxml.jackson.databind.ObjectMapper; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.redis.connection.RedisConnectionFactory; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.data.redis.serializer.Jackson2JsonRedisSerializer; import org.springframework.data.redis.serializer.StringRedisSerializer; /** * Redis配置 * */ @Configuration public class RedisConfig { /** * 配置RedisTemplate * * @param factory * @return */ @Bean public RedisTemplate redisTemplate(RedisConnectionFactory factory, Jackson2JsonRedisSerializer redisJsonSerializer) { RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate&lt;&gt;(); //redis连接工厂 template.setConnectionFactory(factory); StringRedisSerializer stringRedisSerializer = new StringRedisSerializer(); //redis.key序列化器 template.setKeySerializer(stringRedisSerializer); //redis.value序列化器 template.setValueSerializer(redisJsonSerializer); //redis.hash.key序列化器 template.setHashKeySerializer(stringRedisSerializer); //redis.hash.value序列化器 template.setHashValueSerializer(redisJsonSerializer); //调用其他初始化逻辑 template.afterPropertiesSet(); //这里设置redis事务一致 template.setEnableTransactionSupport(true); return template; } /** * 配置redis Json序列化器 * * @return */ @Bean public Jackson2JsonRedisSerializer redisJsonSerializer() { //使用Jackson2JsonRedisSerializer来序列化和反序列化redis的value值（默认使用JDK的序列化方式） Jackson2JsonRedisSerializer serializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper mapper = new ObjectMapper(); mapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); mapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); serializer.setObjectMapper(mapper); return serializer; } } 开启Mybatis二级缓存设置 方式1：mybatis-config.xml &lt;configuration&gt; &lt;settings&gt; &lt;!-- 开启二级缓存 --&gt; &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt; &lt;/settings&gt; ... &lt;/configuration&gt; 方式2：Springboot - application.properties #使全局的映射器启用或禁用缓存。 mybatis.configuration.cache-enabled=true Mapper.xml 配置，type 使用RedisCache： &lt;cache type=&quot;org.mybatis.caches.redis.RedisCache&quot; eviction=&quot;FIFO&quot; flushInterval=&quot;60000&quot; size=&quot;512&quot; readOnly=&quot;true&quot;/&gt; redis.properties 配置： host=localhost port=6379 connectionTimeout=5000 soTimeout=5000 database=0 ","link":"https://tinaxiawuhao.github.io/post/P-yYZmx9j/"},{"title":"springBoot概述","content":"Spring Boot 优点 容易上手，提升开发效率，为 Spring 开发提供一个更快、更广泛的入门体验。 开箱即用，远离繁琐的配置。 提供了一系列大型项目通用的非业务性功能，例如：内嵌服务器、安全管理、运行数据监控、运行状况检查和外部化配置等。 没有代码生成，也不需要XML配置。 避免大量的 Maven 导入和各种版本冲突。 Spring Boot 的核心注解是哪个？它主要由哪几个注解组成的？ 启动类上面的注解是@SpringBootApplication，它也是 Spring Boot 的核心注解，主要组合包含了以下 3 个注解： @SpringBootConfiguration：组合了 @Configuration 注解，实现配置文件的功能。 @EnableAutoConfiguration：打开自动配置的功能，也可以关闭某个自动配置的选项，如关闭数据源自动配置功能： @SpringBootApplication(exclude = { DataSourceAutoConfiguration.class })。 @ComponentScan：Spring组件扫描。 所有其它 Spring 组件(如Controller层、Service层、Dao层、定时器等)都必须放在应用 @SpringBootApplication 注解所在类的同包或者其子包下面，因为应用从启动类开始启动，然后会扫描启动类同包及其子包下面的组件，如果放在其它地方则会因为扫描不到而加载不了 JavaConfig Spring JavaConfig 是 Spring 社区的产品，它提供了配置 Spring IoC 容器的纯Java 方法。因此它有助于避免使用 XML 配置。使用 JavaConfig 的优点在于： （1）面向对象的配置。由于配置被定义为 JavaConfig 中的类，因此用户可以充分利用 Java 中的面向对象功能。一个配置类可以继承另一个，重写它的@Bean 方法等。 （2）减少或消除 XML 配置。基于依赖注入原则的外化配置的好处已被证明。但是，许多开发人员不希望在 XML 和 Java 之间来回切换。JavaConfig 为开发人员提供了一种纯 Java 方法来配置与 XML 配置概念相似的 Spring 容器。从技术角度来讲，只使用 JavaConfig 配置类来配置容器是可行的，但实际上很多人认为将JavaConfig 与 XML 混合匹配是理想的。 （3）类型安全和重构友好。JavaConfig 提供了一种类型安全的方法来配置 Spring容器。由于 Java 5.0 对泛型的支持，现在可以按类型而不是按名称检索 bean，不需要任何强制转换或基于字符串的查找。 /** * @ConfigurationProperties 表示 告诉 SpringBoot 将本类中的所有属性和配置文件中相关的配置进行绑定； * prefix = &quot;user&quot; 表示 将配置文件中 key 为 user 的下面所有的属性与本类属性进行一一映射注入值，如果配置文件中 * 不存在 &quot;user&quot; 的 key，则不会为 POJO 注入值，属性值仍然为默认值 * @Component 将本来标识为一个 Spring 组件，因为只有是容器中的组件，容器才会为 @ConfigurationProperties 提供此注入功能 * @PropertySource (value = { &quot; classpath : user.properties &quot; }) 指明加载类路径下的哪个配置文件来注入值 */ @PropertySource(value = {&quot;classpath:user.properties&quot;}) @Component @ConfigurationProperties(prefix = &quot;user&quot;) public class User { private Integer id; private String lastName; private Integer age; private Date birthday; private List&lt;String&gt; colorList; private Map&lt;String, String&gt; cityMap; } /** * 文中的@Configuration 可以替换为@Component运行结果是一样的，但是两者是有不同的，@Configuration会为配置类生成CGLIB代理Class，@Component不会 */ @PropertySource(value = {&quot;classpath:user.properties&quot;}) @Configuration public class User { @Value(${user.id}) private Integer id; @Value(${user.lastName}) private String lastName; @Value(${user.age}) private Integer age; @Value(${user.birthday}) private Date birthday; @Value(${user.colorList}) private List&lt;String&gt; colorList; @Value(${user.maps}) private Map&lt;String, String&gt; cityMap; } user.properties user.id=111 user.lastName=张无忌 user.age=120 user.birthday=2018/07/11 user.colorList=red,yellow,green,blacnk user.cityMap.mapK1=长沙市 user.cityMap.mapK2=深圳市 user.maps=&quot;{mapK1: '长沙市', mapK2: '深圳市'}&quot; Spring Boot 自动配置 注解 @EnableAutoConfiguration, @Configuration, @ConditionalOnClass 就是自动配置的核心， @EnableAutoConfiguration 给容器导入META-INF/spring.factories 里定义的自动配置类。 筛选有效的自动配置类。 每一个自动配置类结合对应的 xxxProperties.java 读取配置文件进行自动配置功能 Spring Boot 配置加载顺序 Spring Boot 支持多种外部配置方式，如下所示，从上往下加载优先级由高到低，内容相同时覆盖，不相同时累加。 如果在不同的目录中存在多个配置文件，它的读取顺序是： 1、config/application.properties（项目根目录中config目录下） 2、config/application.yml 3、application.properties（项目根目录下） 4、application.yml 5、resources/config/application.properties（项目resources目录中config目录下） 6、resources/config/application.yml 7、resources/application.properties（项目的resources目录下） 8、resources/application.yml YAML 配置 YAML 现在可以算是非常流行的一种配置文件格式了，无论是前端还是后端，都可以见到 YAML 配置。那么 YAML 配置和传统的 properties 配置相比到底有哪些优势呢？ 配置有序，在一些特殊的场景下，配置有序很关键 支持数组，数组中的元素可以是基本数据类型也可以是对象 简洁 相比 properties 配置文件，YAML 还有一个缺点，就是不支持 @PropertySource 注解导入自定义的 YAML 配置。 Spring Boot 是否可以使用 XML 配置 Spring Boot 推荐使用 Java 配置而非 XML 配置，但是 Spring Boot 中也可以使用 XML 配置，通过 @ImportResource 注解可以引入一个 XML 配置。 spring boot 核心配置文件bootstrap.properties和 application.properties 有何区别 单纯做 Spring Boot 开发，可能不太容易遇到 bootstrap.properties 配置文件，但是在结合 Spring Cloud 时，这个配置就会经常遇到了，特别是在需要加载一些远程配置文件的时侯。 spring boot 核心的两个配置文件： bootstrap (. yml 或者 . properties)：boostrap 由父 ApplicationContext 加载的，比 applicaton 优先加载，配置在应用程序上下文的引导阶段生效。一般来说我们在 Spring Cloud Config 或者 Nacos 中会用到它。且 boostrap 里面的属性不能被覆盖； application (. yml 或者 . properties)： 由ApplicatonContext 加载，用于 spring boot 项目的自动化配置。 Spring Profiles spring: profiles: active: devel #指定激活哪个环境配置，激活后，第一个文档内容失效;不指定时，以第一个文档为准 server: port: 8083 --- #&quot;---&quot;用于分隔不同的profiles（）文档块 spring: profiles: devel #指定环境标识为&quot;devel&quot;,相当于&quot;application-{profile}.properties/yml&quot;中的profile server: port: 8081 --- spring: profiles: deploy #指定环境标识为&quot;deploy&quot;,相当于&quot;application-{profile}.properties/yml&quot;中的profile server: port: 8082 比较一下 Spring Security 和 Shiro 各自的优缺点 由于 Spring Boot 官方提供了大量的非常方便的开箱即用的 Starter ，包括 Spring Security 的 Starter ，使得在 Spring Boot 中使用 Spring Security 变得更加容易，甚至只需要添加一个依赖就可以保护所有的接口，所以，如果是 Spring Boot 项目，一般选择 Spring Security 。当然这只是一个建议的组合，单纯从技术上来说，无论怎么组合，都是没有问题的。Shiro 和 Spring Security 相比，主要有如下一些特点： Spring Security 是一个重量级的安全管理框架；Shiro 则是一个轻量级的安全管理框架 Spring Security 概念复杂，配置繁琐；Shiro 概念简单、配置简单 Spring Security 功能强大；Shiro 功能简单 Spring Boot 中如何解决跨域问题 跨域可以在前端通过 JSONP 来解决，但是 JSONP 只可以发送 GET 请求，无法发送其他类型的请求，在 RESTful 风格的应用中，就显得非常鸡肋，因此我们推荐在后端通过 CORS，(Cross-origin resource sharing） 来解决跨域问题。这种解决方案并非 Spring Boot 特有的，在传统的 SSM 框架中，就可以通过 CORS 来解决跨域问题，只不过之前我们是在 XML 文件中配置 CORS ，现在可以通过实现WebMvcConfigurer接口然后重写addCorsMappings方法解决跨域问题。 @Configuration public class CorsConfig implements WebMvcConfigurer { @Override public void addCorsMappings(CorsRegistry registry) { registry.addMapping(&quot;/**&quot;) .allowedOrigins(&quot;*&quot;) .allowCredentials(true) .allowedMethods(&quot;GET&quot;, &quot;POST&quot;, &quot;PUT&quot;, &quot;DELETE&quot;, &quot;OPTIONS&quot;) .maxAge(3600); } } 项目中前后端分离部署，所以需要解决跨域的问题。 我们使用cookie存放用户登录的信息，在spring拦截器进行权限控制，当权限不符合时，直接返回给用户固定的json结果。 当用户登录以后，正常使用；当用户退出登录状态时或者token过期时，由于拦截器和跨域的顺序有问题，出现了跨域的现象。 我们知道一个http请求，先走filter，到达servlet后才进行拦截器的处理，如果我们把cors放在filter里，就可以优先于权限拦截器执行。 @Configuration public class CorsConfig { @Bean public CorsFilter corsFilter() { CorsConfiguration corsConfiguration = new CorsConfiguration(); corsConfiguration.addAllowedOrigin(&quot;*&quot;); corsConfiguration.addAllowedHeader(&quot;*&quot;); corsConfiguration.addAllowedMethod(&quot;*&quot;); corsConfiguration.setAllowCredentials(true); UrlBasedCorsConfigurationSource urlBasedCorsConfigurationSource = new UrlBasedCorsConfigurationSource(); urlBasedCorsConfigurationSource.registerCorsConfiguration(&quot;/**&quot;, corsConfiguration); return new CorsFilter(urlBasedCorsConfigurationSource); } } Spring Boot 中的监视器 Spring boot actuator 是 spring 启动框架中的重要功能之一。Spring boot 监视器可帮助您访问生产环境中正在运行的应用程序的当前状态。有几个指标必须在生产环境中进行检查和监控。即使一些外部应用程序可能正在使用这些服务来向相关人员触发警报消息。监视器模块公开了一组可直接作为 HTTP URL 访问的REST 端点来检查状态。 注册 Servlet 三大组件 Servlet、Filter、Listener 继承，接口实现方式 ServletRegistrationBean 注册 Servlet 1、自定义类继承 javax.servlet.http.HttpServlet，然后重写其 doGet 与 doPost 方法，在方法中编写控制代码； 2、第二步将 ServletRegistrationBean 组件添加到 Spring 容器中 import javax.servlet.ServletException; import javax.servlet.http.HttpServlet; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import java.io.IOException; /** * Created by Administrator * 标准的 Servlet 实现 HttpServlet；重写其 doGet 、doPost 方法 */ public class BookServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { this.doPost(req, resp); } @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { System.out.println(&quot;:com.lct.servlet.BookServlet:&quot; + req.getRequestURL()); /**讲求转发到后台的 user/users 请求去，即会进入*/ req.getRequestDispatcher(&quot;user/users&quot;).forward(req, resp); } } 3、上面 Serlvet 转发到下面的 UserControllr 控制器中 4、@Configuration 配置类相当于以前的 beans.xml 中的配置，将 ServletRegistrationBean 也添加到 Spring 容器中来 import com.lct.component.MyLocaleResolve; import com.lct.servlet.BookServlet; import org.springframework.boot.web.server.WebServerFactoryCustomizer; import org.springframework.boot.web.servlet.ServletRegistrationBean; import org.springframework.boot.web.servlet.server.ConfigurableServletWebServerFactory; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.web.servlet.LocaleResolver; /** * Created by Administrator * 自定义配置类 */ @Configuration public class MyMvcConfig { /** * 注册 Servlet 三大组件 之 Servlet * 添加 ServletRegistrationBean ，就相当于以前在 web.xml 中配置的 &lt;servlet&gt;&lt;/servlet&gt;标签 */ @Bean public ServletRegistrationBean myServlet() { /**第二个参数是个不定参数组，可以配置映射多个请求 * 相当于以前在 web.xml中配置的 &lt;servlet-mapptin&gt;&lt;/servlet-mapptin&gt;*/ ServletRegistrationBean registrationBean = new ServletRegistrationBean(new BookServlet(), &quot;/bookServlet&quot;); return registrationBean; } } 5、运行测试： FilterRegistrationBean 注册 Filter 1、Filter(过滤器) 是 Servlet 技术中最实用的技术之一 2、Web 开发人员通过 Filter 技术，对 web 服务器管理的所有 web 资源(如动态的 Jsp、 Servlet，以及静态的 image、 html、CSS、JS 文件等) 进行过滤拦截，从而实现一些特殊的功能(如实现 URL 级别的权限访问控制、过滤敏感词汇、压缩响应信息等) 3、Filter 主要用于对用户请求进行预处理，也可以对 HttpServletResponse 进行后期处理(如编码设置，返回时禁用浏览器缓存等) 4、Filter 使用完整流程：Filter 对用户请求进行预处理，接着将请求交给 Servlet 进行处理并生成响应，最后 Filter 再对服务器响应进行后处理。 5、Servlet 的 Filter 经常会拿来与 Spring MVC 的 Interceptor(拦截器) 做对比 ​ 1）拦截器是基于 Java 的反射机制的，而过滤器是基于函数回调 ​ 2）拦截器不依赖与 servle t容器，过滤器依赖与 servlet 容器 ​ 3）拦截器可以访问 action 上下文、值栈里的对象，而过滤器不能访问 ​ 4）在 action 的生命周期中，拦截器可以多次被调用，而过滤器只能在容器初始化时被调用一次 ​ 5）拦截器可以获取 IOC 容器中的各个 bean，而过滤器就不行，这点很重要，在拦截器里注入一个 service，可以调用业务逻辑 ​ 6）SpringMVC 有自己的拦截器 import javax.servlet.*; import javax.servlet.http.HttpServletRequest; import java.io.IOException; /** * Created by Administrator * 标准 Servlet 过滤器，实现 javax.servlet.Filter 接口 * 并重写它的 三个方法 */ public class SystemFilter implements Filter { @Override public void init(FilterConfig filterConfig) throws ServletException { System.out.println(&quot;javax.servlet.Filter：：服务器启动....&quot;); } @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { /** * 转为 HttpServletRequest 输出请求路径 容易查看 请求地址 */ HttpServletRequest request = (HttpServletRequest) servletRequest; System.out.println(&quot;javax.servlet.Filter：：过滤器放行前....&quot; + request.getRequestURL()); filterChain.doFilter(servletRequest, servletResponse); System.out.println(&quot;javax.servlet.Filter：：过滤器返回后....&quot; + request.getRequestURL()); } @Override public void destroy() { System.out.println(&quot;javax.servlet.Filter：：服务器关闭....&quot;); } } 6、使用 FilterRegistrationBean 添加 FIlter ： import com.lct.component.MyLocaleResolve; import com.lct.filter.SystemFilter; import com.lct.servlet.BookServlet; import org.springframework.boot.web.server.WebServerFactoryCustomizer; import org.springframework.boot.web.servlet.FilterRegistrationBean; import org.springframework.boot.web.servlet.ServletRegistrationBean; import org.springframework.boot.web.servlet.server.ConfigurableServletWebServerFactory; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.web.servlet.LocaleResolver; import javax.servlet.DispatcherType; import java.util.Arrays; /** * Created by Administrator * 自定义配置类 */ @Configuration public class MyMvcConfig { /** * 注册 Servlet 三大组件 之 Filter (过滤器) * 添加 FilterRegistrationBean ，就相当于以前在 web.xml 中配置的 &lt;filter&gt;&lt;/filter&gt; 标签 */ @Bean public FilterRegistrationBean myFilter() { FilterRegistrationBean registrationBean = new FilterRegistrationBean(); /**同样添加自定义的 Filter*/ registrationBean.setFilter(new SystemFilter()); /**然后设置过滤的路径，参数是个集合 ,相当于 web.xml中配置的 &lt;filter-mapptin&gt;&lt;/filter-mapptin&gt; * &quot;/*&quot;: 表示过滤所有 get 与 post 请求*/ registrationBean.setUrlPatterns(Arrays.asList(&quot;/*&quot;)); /** * setDispatcherTypes 相当于 web.xml 配置中 &lt;filter-mapptin&gt; 下的 &lt;dispatcher&gt; 标签 * 用于过滤非常规的 get 、post 请求 * REQUEST：默认方式，写了之后会过滤所有静态资源的请求 * FORWARD：过滤所有的转发请求，无论是 jsp 中的 &lt;jsp:forward&lt;/&gt;、&lt;%@ page errorPage= %&gt;、还是后台的转发 * INCLUDE：过滤 jsp 中的动态包含&lt;jsp:include 请求 * ERROR：过滤在 web.xml 配置的全局错误页面 * 了解即可，实际中也很少这么做 */ registrationBean.setDispatcherTypes(DispatcherType.REQUEST); return registrationBean; } } ServletListenerRegistrationBean 注册 Listener 1、自定义监听器： import javax.servlet.ServletContextEvent; import javax.servlet.ServletContextListener; /** * Created by Administrator on 2018/8/11 0011. * 标准 Servlet 监听器，实现 javax.servlet.ServletContextListener 接口 * 然后实现方法 * ServletContextListener：属于 Servlet 应用启动关闭监听器，监听容器初始化与销毁 */ public class SystemListener implements ServletContextListener { @Override public void contextInitialized(ServletContextEvent servletContextEvent) { System.out.println(&quot;com.lct.listener.SystemListener::服务器启动.....&quot;); } @Override public void contextDestroyed(ServletContextEvent servletContextEvent) { System.out.println(&quot;com.lct.listener.SystemListener::服务器关闭.....&quot;); } } 2、注册 ServletListenerRegistrationBean： import com.lct.component.MyLocaleResolve; import com.lct.filter.SystemFilter; import com.lct.listener.SystemListener; import com.lct.servlet.BookServlet; import org.springframework.boot.web.server.WebServerFactoryCustomizer; import org.springframework.boot.web.servlet.FilterRegistrationBean; import org.springframework.boot.web.servlet.ServletListenerRegistrationBean; import org.springframework.boot.web.servlet.ServletRegistrationBean; import org.springframework.boot.web.servlet.server.ConfigurableServletWebServerFactory; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.web.servlet.LocaleResolver; import java.util.Arrays; /** * Created by Administrator * 自定义配置类 */ @Configuration public class MyMvcConfig { /** * 注册 Servlet 三大组件 之 Listner * 添加 ServletListenerRegistrationBean ，就相当于以前在 web.xml 中配置的 &lt;listener&gt;&lt;/listener&gt;标签 */ @Bean public ServletListenerRegistrationBean myListener() { /**ServletListenerRegistrationBean&lt;T extends EventListener&gt; 属于的是泛型，可以注册常见的任意监听器 * 将自己的监听器注册进来*/ ServletListenerRegistrationBean registrationBean = new ServletListenerRegistrationBean(new SystemListener()); return registrationBean; } } 注解方式 1、Servlet 三大组件 Servlet、Filter、Listener 在传统项目中需要在 web.xml 中进行相应的配置。Servlet 3.0 开始在 javax.servlet.annotation 包下提供 3 个对应 的 @WebServlet、@WebFilter、@WebListener 注解来简化操作。 2、Spring Boot 应用中这三个注解默认是不被扫描的，需要在项目启动类上添加 @ServletComponentScan 注解, 表示对 Servlet 组件扫描。 @WebServlet import javax.servlet.ServletException; import javax.servlet.annotation.WebServlet; import javax.servlet.http.HttpServlet; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import java.io.IOException; /** * 标准的 Servlet ，实现 javax.servlet.http.HttpServlet. 重写其 doGet 、doPost 方法 * name :表示 servlet 名称，可以不写，默认为空 * urlPatterns: 表示请求的路径，如 http://ip:port/context-path/userServlet */ @WebServlet(name = &quot;UserServlet&quot;, urlPatterns = {&quot;/userServlet&quot;}) public class UserServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { this.doPost(req, resp); } @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { StringBuffer requestURL = req.getRequestURL(); System.out.println(&quot;com.wmx.servlet.UserServlet -- &quot; + requestURL); resp.sendRedirect(&quot;/index.html&quot;);//浏览器重定向到服务器下的 index.html 页面 } } @WebFilter import javax.servlet.*; import javax.servlet.annotation.WebFilter; import javax.servlet.http.HttpServletRequest; import java.io.IOException; /** * 标准 Servlet 过滤器，实现 javax.servlet.Filter 接口，并重现它的 3 个方法 * filterName：表示过滤器名称，可以不写 * value：配置请求过滤的规则，如 &quot;/*&quot; 表示过滤所有请求，包括静态资源，如 &quot;/user/*&quot; 表示 /user 开头的所有请求 */ @WebFilter(filterName = &quot;SystemFilter&quot;, value = {&quot;/*&quot;}) public class SystemFilter implements Filter { @Override public void init(FilterConfig filterConfig) throws ServletException { System.out.println(&quot;com.wmx.servlet.SystemFilter -- 系统启动...&quot;); } @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { //转为 HttpServletRequest 输出请求路径 HttpServletRequest request = (HttpServletRequest) servletRequest; System.out.println(&quot;com.wmx.servlet.SystemFilter -- 过滤器放行前....&quot; + request.getRequestURL()); filterChain.doFilter(servletRequest, servletResponse); System.out.println(&quot;com.wmx.servlet.SystemFilter -- 过滤器返回后....&quot; + request.getRequestURL()); } @Override public void destroy() { System.out.println(&quot;com.wmx.servlet.SystemFilter -- 系统关闭...&quot;); } } @WebListener import javax.servlet.ServletContextEvent; import javax.servlet.ServletContextListener; import javax.servlet.annotation.WebListener; /** * 标准 Servlet 监听器，实现 javax.servlet.ServletContextListener 接口，并重写方法 * ServletContextListener 属于 Servlet 应用启动关闭监听器，监听容器初始化与销毁。常用的监听器还有： * ServletRequestListener：HttpServletRequest 对象的创建和销毁监听器 * HttpSessionListener：HttpSession 数据对象创建和销毁监听器 * HttpSessionAttributeListener 监听HttpSession中属性变化 * ServletRequestAttributeListener 监听ServletRequest中属性变化 */ @WebListener public class SystemListener implements ServletContextListener { @Override public void contextInitialized(ServletContextEvent sce) { System.out.println(&quot;com.wmx.servlet.SystemListener -- 服务器启动.&quot;); } @Override public void contextDestroyed(ServletContextEvent sce) { System.out.println(&quot;com.wmx.servlet.SystemListener -- 服务器关闭.&quot;); } } @ServletComponentScan Spring Boot 应用中这三个注解默认是不被扫描的，需要在项目启动类上添加 @ServletComponentScan 注解, 表示对 Servlet 组件扫描。 import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.boot.web.servlet.ServletComponentScan; @SpringBootApplication @ServletComponentScan //对 servlet 注解进行扫描 public class RedisStuWebApplication { public static void main(String[] args) { SpringApplication.run(RedisStuWebApplication.class, args); } } ","link":"https://tinaxiawuhao.github.io/post/O58auJO44/"},{"title":"SpringMVC工作原理","content":"SpringMVC的工作原理图： SpringMVC流程 1、 用户发送请求至前端控制器DispatcherServlet。 2、 DispatcherServlet收到请求调用HandlerMapping处理器映射器。 3、 处理器映射器找到具体的处理器(可以根据xml配置、注解进行查找)，生成处理器对象及处理器拦截器(如果有则生成)一并返回给DispatcherServlet。 4、 DispatcherServlet调用HandlerAdapter处理器适配器。 5、 HandlerAdapter经过适配调用具体的处理器(Controller，也叫后端控制器)。 6、 Controller执行完成返回ModelAndView。 7、 HandlerAdapter将controller执行结果ModelAndView返回给DispatcherServlet。 8、 DispatcherServlet将ModelAndView传给ViewReslover视图解析器。 9、 ViewReslover解析后返回具体View。 10、DispatcherServlet根据View进行渲染视图（即将模型数据填充至视图中）。 11、 DispatcherServlet响应用户。 组件说明： 以下组件通常使用框架提供实现： DispatcherServlet：作为前端控制器，整个流程控制的中心，控制其它组件执行，统一调度，降低组件之间的耦合性，提高每个组件的扩展性。 HandlerMapping：通过扩展处理器映射器实现不同的映射方式，例如：配置文件方式，实现接口方式，注解方式等。 HandlAdapter：通过扩展处理器适配器，支持更多类型的处理器。 ViewResolver：通过扩展视图解析器，支持更多类型的视图解析，例如：jsp、freemarker、pdf、excel等。 组件： 1、前端控制器DispatcherServlet（不需要工程师开发）,由框架提供 作用：接收请求，响应结果，相当于转发器，中央处理器。有了dispatcherServlet减少了其它组件之间的耦合度。 用户请求到达前端控制器，它就相当于mvc模式中的c，dispatcherServlet是整个流程控制的中心，由它调用其它组件处理用户的请求，dispatcherServlet的存在降低了组件之间的耦合性。 2、处理器映射器HandlerMapping(不需要工程师开发),由框架提供 作用：根据请求的url查找Handler HandlerMapping负责根据用户请求找到Handler即处理器，springmvc提供了不同的映射器实现不同的映射方式，例如：配置文件方式，实现接口方式，注解方式等。 3、处理器适配器HandlerAdapter 作用：按照特定规则（HandlerAdapter要求的规则）去执行Handler 通过HandlerAdapter对处理器进行执行，这是适配器模式的应用，通过扩展适配器可以对更多类型的处理器进行执行。 4、处理器Handler(需要工程师开发) 注意：编写Handler时按照HandlerAdapter的要求去做，这样适配器才可以去正确执行Handler Handler 是继DispatcherServlet前端控制器的后端控制器，在DispatcherServlet的控制下Handler对具体的用户请求进行处理。 由于Handler涉及到具体的用户业务请求，所以一般情况需要工程师根据业务需求开发Handler。 5、视图解析器View resolver(不需要工程师开发),由框架提供 作用：进行视图解析，根据逻辑视图名解析成真正的视图（view） View Resolver负责将处理结果生成View视图，View Resolver首先根据逻辑视图名解析成物理视图名即具体的页面地址，再生成View视图对象，最后对View进行渲染将处理结果通过页面展示给用户。 springmvc框架提供了很多的View视图类型，包括：jstlView、freemarkerView、pdfView等。 一般情况下需要通过页面标签或页面模版技术将模型数据通过页面展示给用户，需要由工程师根据业务需求开发具体的页面。 6、视图View(需要工程师开发jsp...) View是一个接口，实现类支持不同的View类型（jsp、freemarker、pdf...） 核心架构的具体流程步骤如下： 1、首先用户发送请求——&gt;DispatcherServlet，前端控制器收到请求后自己不进行处理，而是委托给其他的解析器进行处理，作为统一访问点，进行全局的流程控制； 2、DispatcherServlet——&gt;HandlerMapping， HandlerMapping 将会把请求映射为HandlerExecutionChain 对象（包含一个Handler 处理器（页面控制器）对象、多个HandlerInterceptor 拦截器）对象，通过这种策略模式，很容易添加新的映射策略； 3、DispatcherServlet——&gt;HandlerAdapter，HandlerAdapter 将会把处理器包装为适配器，从而支持多种类型的处理器，即适配器设计模式的应用，从而很容易支持很多类型的处理器； 4、HandlerAdapter——&gt;处理器功能处理方法的调用，HandlerAdapter 将会根据适配的结果调用真正的处理器的功能处理方法，完成功能处理；并返回一个ModelAndView 对象（包含模型数据、逻辑视图名）； 5、ModelAndView的逻辑视图名——&gt; ViewResolver， ViewResolver 将把逻辑视图名解析为具体的View，通过这种策略模式，很容易更换其他视图技术； 6、View——&gt;渲染，View会根据传进来的Model模型数据进行渲染，此处的Model实际是一个Map数据结构，因此很容易支持其他视图技术； 7、返回控制权给DispatcherServlet，由DispatcherServlet返回响应给用户，到此一个流程结束。 下边两个组件通常情况下需要开发： Handler：处理器，即后端控制器用controller表示。 View：视图，即展示给用户的界面，视图中通常需要标签语言展示模型数据。 在讲SpringMVC之前我们先来看一下什么是MVC模式 MVC：MVC是一种设计模式 MVC的原理图： 分析： M-Model 模型（完成业务逻辑：有javaBean构成，service+dao+entity） V-View 视图（做界面的展示 jsp，html……） C-Controller 控制器（接收请求—&gt;调用模型—&gt;根据结果派发页面） springMVC是什么： springMVC是一个MVC的开源框架，springMVC=struts2+spring，springMVC就相当于是Struts2加上sring的整合，但是这里有一个疑惑就是，springMVC和spring是什么样的关系呢？这个在百度百科上有一个很好的解释：意思是说，springMVC是spring的一个后续产品，其实就是spring在原有基础上，又提供了web应用的MVC模块，可以简单的把springMVC理解为是spring的一个模块（类似AOP，IOC这样的模块），网络上经常会说springMVC和spring无缝集成，其实springMVC就是spring的一个子模块，所以根本不需要同spring进行整合。 SpringMVC的原理图： 看到这个图大家可能会有很多的疑惑，现在我们来看一下这个图的步骤：（可以对比MVC的原理图进行理解） 第一步:用户发起请求到前端控制器（DispatcherServlet） 第二步：前端控制器请求处理器映射器（HandlerMappering）去查找处理器（Handle）：通过xml配置或者注解进行查找 第三步：找到以后处理器映射器（HandlerMappering）像前端控制器返回执行链（HandlerExecutionChain） 第四步：前端控制器（DispatcherServlet）调用处理器适配器（HandlerAdapter）去执行处理器（Handler） 第五步：处理器适配器去执行Handler 第六步：Handler执行完给处理器适配器返回ModelAndView 第七步：处理器适配器向前端控制器返回ModelAndView 第八步：前端控制器请求视图解析器（ViewResolver）去进行视图解析 第九步：视图解析器像前端控制器返回View 第十步：前端控制器对视图进行渲染 第十一步：前端控制器向用户响应结果 看到这些步骤我相信大家很感觉非常的乱，这是正常的，但是这里主要是要大家理解springMVC中的几个组件： 前端控制器（DispatcherServlet）：接收请求，响应结果，相当于电脑的CPU。 处理器映射器（HandlerMapping）：根据URL去查找处理器 处理器（Handler）：（需要程序员去写代码处理逻辑的） 处理器适配器（HandlerAdapter）：会把处理器包装成适配器，这样就可以支持多种类型的处理器，类比笔记本的适配器（适配器模式的应用） 视图解析器（ViewResovler）：进行视图解析，多返回的字符串，进行处理，可以解析成对应的页面 ","link":"https://tinaxiawuhao.github.io/post/7cW_XVJ41/"},{"title":"elasticSearch基础概念汇总","content":"1.什么是ElasticSearch？ Elasticsearch是一个基于Lucene的搜索引擎。它提供了具有HTTP Web界面和无架构JSON文档的分布式，多租户能力的全文搜索引擎。Elasticsearch是用Java开发的，根据Apache许可条款作为开源发布。它可以用于全文搜索，结构化搜索以及分析，当然你也可以将这三者进行组合。 2.为什么要使用Elasticsearch? 用数据库，也可以实现搜索的功能，为什么还需要搜索引擎呢？ 就像 Stackoverflow 的网友说的： A relational database can store data and also index it. A search engine can index data but also store it. 数据库（理论上来讲，ES 也是数据库，这里的数据库，指的是关系型数据库），首先是存储，搜索只是顺便提供的功能， 而搜索引擎，首先是搜索，但是不把数据存下来就搜不了，所以只好存一存。 术业有专攻，专攻搜索的搜索引擎，自然会提供更强大的搜索能力。。 Elasticsearch是分布式的。不需要其他组件，分发是实时的，被叫做”Push replication”。 Elasticsearch 完全支持 Apache Lucene 的接近实时的搜索。 处理多租户（multitenancy）不需要特殊配置，而Solr则需要更多的高级设置。 Elasticsearch 采用 Gateway 的概念，使得完备份更加简单。 各节点组成对等的网络结构，某些节点出现故障时会自动分配其他节点代替其进行工作。 3.Elasticsearch是如何实现Master选举的？ Elasticsearch的选主是ZenDiscovery模块负责的，主要包含Ping（节点之间通过这个RPC来发现彼此）和Unicast（单播模块包含一个主机列表以控制哪些节点需要ping通）这两部分； 对所有可以成为master的节点（node.master: true）根据nodeId字典排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个（第0位）节点，暂且认为它是master节点。 如果对某个节点的投票数达到一定的值（可以成为master节点数n/2+1）并且该节点自己也选举自己，那这个节点就是master。否则重新选举一直到满足上述条件。 补充：master节点的职责主要包括集群、节点和索引的管理，不负责文档级别的管理；data节点可以关闭http功能。 4.Elasticsearch中如何避免脑裂？ 为了避免产生脑裂，ES采用了常见的分布式系统思路，保证选举出的master被多数派(quorum)的master-eligible node认可，以此来保证只有一个master。这个quorum通过以下配置进行配置： conf/elasticsearch.yml: discovery.zen.minimum_master_nodes: 2 5.Elasticsearch中的倒排索引 为什么叫倒排索引 在没有搜索引擎时，我们是直接输入一个网址，然后获取网站内容，这时我们的行为是：document -&gt; to -&gt; words 通过文章，获取里面的单词，此谓「正向索引」，forward index.后来，我们希望能够输入一个单词，找到含有这个单词，或者和这个单词有关系的文章：word -&gt; to -&gt; documents于是我们把这种索引，成为inverted index，直译过来，应该叫「反向索引」，国内翻译成「倒排索引」。 倒排索引的内部结构 首先，在数据生成的时候，比如爬虫爬到一篇文章，这时我们需要对这篇文章进行分析，将文本拆解成一个个单词。 这个过程很复杂，比如“生存还是死亡”，你要如何让分词器自动将它分解为“生存”、“还是”、“死亡”三个词语，然后把“还是”这个无意义的词语干掉。这里不展开，感兴趣的同学可以查看关于「分析器」的内容。 接着，把这两个词语以及它对应的文档id存下来： word documentId 生存 1 死亡 1 接着爬虫继续爬，又爬到一个含有“生存”的文档，于是索引变成： word documentId 生存 1，2 死亡 1 下次搜索“生存”，就会返回文档ID是 1、2两份文档。然而上面这套索引的实现，给小孩子当玩具玩还行，要上生产环境，那还远着。想想看，这个世界上那么多单词，中文、英文、日文、韩文 … 你每次搜索一个单词，我都要全局遍历一遍，很明显不行。于是有了排序，我们需要对单词进行排序，像 B+ 树一样，可以在页里实现二分查找。光排序还不行，你单词都放在磁盘呢，磁盘 IO 慢的不得了，所以 Mysql 特意把索引缓存到了内存。你说好，我也学 Mysql 的，放内存，3，2，1，放，哐当，内存爆了。哪本字典，会把所有单词都贴在目录里的？ 所以，上图： Lucene 的倒排索，增加了最左边的一层「字典树」term index，它不存储所有的单词，只存储单词前缀，通过字典树找到单词所在的块，也就是单词的大概位置，再在块里二分查找，找到对应的单词，再找到单词对应的文档列表。 当然，内存寸土寸金，能省则省，所以 Lucene 还用了 FST（Finite State Transducers）对它进一步压缩。 最右边的 Posting List ，别看它只是存一个文档 ID 数组，但是它在设计时，遇到的问题可不少。 Frame Of Reference 原生的 Posting List 有两个痛点： 如何压缩以节省磁盘空间 如何快速求交并集（intersections and unions） 先来聊聊压缩。我们来简化下 Lucene 要面对的问题，假设有这样一个数组：[73, 300, 302, 332, 343, 372] Lucene 里，数据是按 Segment 存储的，每个 Segment 最多存 65536 个文档 ID， 所以文档 ID 的范围，从 0 到 2^16-1，所以如果不进行任何处理，那么每个元素都会占用 2 bytes ，对应上面的数组，就是 6 * 2 = 12 bytes. 怎么压缩呢？ 压缩，就是尽可能降低每个数据占用的空间，同时又能让信息不失真，能够还原回来。 Step 1：Delta-encode —— 增量编码 我们只记录元素与元素之间的增量，于是数组变成了：[73, 227, 2, 30, 11, 29] Step 2：Split into blocks —— 分割成块 Lucene里每个块是 256 个文档 ID，这样可以保证每个块，增量编码后，每个元素都不会超过 256（1 byte）.为了方便演示，我们假设每个块是 3 个文档 ID： [73, 227, 2], [30, 11, 29] Step 3：Bit packing —— 按需分配空间 对于第一个块，[73, 227, 2]，最大元素是227，需要 8 bits，好，那我给你这个块的每个元素，都分配 8 bits的空间。但是对于第二个块，[30, 11, 29]，最大的元素才30，只需要 5 bits，那我就给你每个元素，只分配 5 bits 的空间，足矣。这一步，可以说是把吝啬发挥到极致，精打细算，按需分配。 以上三个步骤，共同组成了一项编码技术，Frame Of Reference（FOR）： Roaring bitmaps 接着来聊聊 Posting List 的第二个痛点 —— 如何快速求交并集（intersections and unions）。 在 Lucene 中查询，通常不只有一个查询条件，比如我们想搜索： 含有“生存”相关词语的文档 文档发布时间在最近一个月 文档发布者是平台的特约作者 这样就需要根据三个字段，去三棵倒排索引里去查，当然，磁盘里的数据，上一节提到过，用了 FOR 进行压缩，所以我们要把数据进行反向处理，即解压，才能还原成原始的文档 ID，然后把这三个文档 ID 数组在内存中做一个交集。 即使没有多条件查询， Lucene 也需要频繁求并集，因为 Lucene 是分片存储的。 同样，我们把 Lucene 遇到的问题，简化成一道算法题。 假设有下面三个数组： [64, 300, 303, 343] [73, 300, 302, 303, 343, 372] [303, 311, 333, 343] 求它们的交集。 Option 1: Integer 数组 直接用原始的文档 ID ，可能你会说，那就逐个数组遍历一遍吧，遍历完就知道交集是什么了。 其实对于有序的数组，用跳表（skip table）可以更高效，这里就不展开了，因为不管是从性能，还是空间上考虑，Integer 数组都不靠谱，假设有100M 个文档 ID，每个文档 ID 占 2 bytes，那已经是 200 MB，而这些数据是要放到内存中进行处理的，把这么大量的数据，从磁盘解压后丢到内存，内存肯定撑不住。 Option 2: Bitmap 假设有这样一个数组：[3,6,7,10] 那么我们可以这样来表示：[0,0,1,0,0,1,1,0,0,1] 看出来了么，对，我们用 0 表示角标对应的数字不存在，用 1 表示存在。 这样带来了两个好处： 节省空间：既然我们只需要0和1，那每个文档 ID 就只需要 1 bit，还是假设有 100M 个文档，那只需要 100M bits = 100M * 1/8 bytes = 12.5 MB，比之前用 Integer 数组 的 200 MB，优秀太多 运算更快：0 和 1，天然就适合进行位运算，求交集，「与」一下，求并集，「或」一下，一切都回归到计算机的起点 Option 3: Roaring Bitmaps 细心的你可能发现了，bitmap 有个硬伤，就是不管你有多少个文档，你占用的空间都是一样的，之前说过，Lucene Posting List 的每个 Segement 最多放 65536 个文档ID，举一个极端的例子，有一个数组，里面只有两个文档 ID：[0, 65535]用 Bitmap，要怎么表示？[1,0,0,0,….(超级多个0),…,0,0,1] 你需要 65536 个 bit，也就是 65536/8 = 8192 bytes，而用 Integer 数组，你只需要 2 * 2 bytes = 4 bytes 呵呵，死板的 bitmap。可见在文档数量不多的时候，使用 Integer 数组更加节省内存。 我们来算一下临界值，很简单，无论文档数量多少，bitmap都需要 8192 bytes，而 Integer 数组则和文档数量成线性相关，每个文档 ID 占 2 bytes，所以：8192 / 2 = 4096当文档数量少于 4096 时，用 Integer 数组，否则，用 bitmap. 这里补充一下 Roaring bitmaps 和 之前讲的 Frame Of Reference 的关系。 Frame Of Reference 是压缩数据，减少磁盘占用空间，所以当我们从磁盘取数据时，也需要一个反向的过程，即解压，解压后才有我们上面看到的这样子的文档ID数组：[73, 300, 302, 303, 343, 372] ，接着我们需要对数据进行处理，求交集或者并集，这时候数据是需要放到内存进行处理的，我们有三个这样的数组，这些数组可能很大，而内存空间比磁盘还宝贵，于是需要更强有力的压缩算法，同时还要有利于快速的求交并集，于是有了Roaring Bitmaps 算法。 另外，Lucene 还会把从磁盘取出来的数据，通过 Roaring bitmaps 处理后，缓存到内存中，Lucene 称之为 filter cache. 6.ElasticSearch中的集群、节点、索引、文档、类型是什么？ 群集 是一个或多个节点（服务器）的集合，它们共同保存您的整个数据，并提供跨所有节点的联合索引和搜索功能。群集由唯一名称标识，默认情况下为“elasticsearch”。此名称很重要，因为如果节点设置为按名称加入群集，则该节点只能是群集的一部分。 节点 是属于集群一部分的单个服务器。它存储数据并参与群集索引和搜索功能。 索引 就像关系数据库中的“数据库”。它有一个定义多种类型的映射。索引是逻辑名称空间，映射到一个或多个主分片，并且可以有零个或多个副本分片。 MySQL =&gt;数据库 ElasticSearch =&gt;索引 文档 类似于关系数据库中的一行。不同之处在于索引中的每个文档可以具有不同的结构（字段），但是对于通用字段应该具有相同的数据类型。 MySQL =&gt; Databases =&gt;Tables =&gt; Columns / Rows ElasticSearch =&gt; Indices =&gt; Types =&gt;具有属性的文档 类型 是索引的逻辑类别/分区，其语义完全取决于用户。 7.ElasticSearch中的分片是什么? 在大多数环境中，每个节点都在单独的盒子或虚拟机上运行。 索引 - 在Elasticsearch中，索引是文档的集合。 分片 -因为Elasticsearch是一个分布式搜索引擎，所以索引通常被分割成分布在多个节点上的被称为分片的元素。 Segment -每个shard（分片）包含多个segment（段），每一个segment都是一个倒排索引 在查询的时，会把所有的segment查询结果汇总归并后最为最终的分片查询结果返回 1.segment是不可变的，物理上你并不能从中删除信息，所以在删除文档的时候，是在文档上面打上一个删除的标记，然后在执行段合并的时候，进行删除 2.索引segment段的个数越多，搜索性能越低且消耗内存更多 ​ 副本 -一个索引被分解成碎片以便于分发和扩展。副本是分片的副本。 ​ 分析器 -在ElasticSearch中索引数据时，数据由为索引定义的Analyzer在内部进行转换。 分析器由一个Tokenizer和零个或多个TokenFilter组成。编译器可以在一个或多个CharFilter之前。分析模块允许您在逻辑名称下注册分析器，然后可以在映射定义或某些API中引用它们。Elasticsearch附带了许多可以随时使用的预建分析器。或者，您可以组合内置的字符过滤器，编译器和过滤器器来创建自定义分析器。 ​ 编译器 -编译器用于将字符串分解为术语或标记流。一个简单的编译器可能会将字符串拆分为任何遇到空格或标点的地方。Elasticsearch有许多内置标记器，可用于构建自定义分析器。 8.详细描述一下Elasticsearch索引文档的过程。 协调节点默认使用文档ID参与计算（也支持通过routing），以便为路由提供合适的分片。 shard = hash(document_id) % (num_of_primary_shards) 当分片所在的节点接收到来自协调节点的请求后，会将请求写入到Memory Buffer，然后定时（默认是每隔1秒）写入到Filesystem Cache，这个从Momery Buffer到Filesystem Cache的过程就叫做refresh； 当然在某些情况下，存在Momery Buffer和Filesystem Cache的数据可能会丢失，ES是通过translog的机制来保证数据的可靠性的。其实现机制是接收到请求后，同时也会写入到translog中，当Filesystem cache中的数据写入到磁盘中时，才会清除掉，这个过程叫做flush； 在flush过程中，内存中的缓冲将被清除，内容被写入一个新段，段的fsync将创建一个新的提交点，并将内容刷新到磁盘，旧的translog将被删除并开始一个新的translog。 flush触发的时机是定时触发（默认30分钟）或者translog变得太大（默认为512M）时； 补充：关于Lucene的Segement： Lucene索引是由多个段组成，段本身是一个功能齐全的倒排索引。 段是不可变的，允许Lucene将新的文档增量地添加到索引中，而不用从头重建索引。 对于每一个搜索请求而言，索引中的所有段都会被搜索，并且每个段会消耗CPU的时钟周、文件句柄和内存。这意味着段的数量越多，搜索性能会越低。 为了解决这个问题，Elasticsearch会合并小段到一个较大的段，提交新的合并段到磁盘，并删除那些旧的小段。 9.详细描述一下Elasticsearch更新和删除文档的过程 删除和更新也都是写操作，但是Elasticsearch中的文档是不可变的，因此不能被删除或者改动以展示其变更； 磁盘上的每个段都有一个相应的.del文件。当删除请求发送后，文档并没有真的被删除，而是在.del文件中被标记为删除。该文档依然能匹配查询，但是会在结果中被过滤掉。当段合并时，在.del文件中被标记为删除的文档将不会被写入新段。 在新的文档被创建时，Elasticsearch会为该文档指定一个版本号，当执行更新时，旧版本的文档在.del文件中被标记为删除，新版本的文档被索引到一个新段。旧版本的文档依然能匹配查询，但是会在结果中被过滤掉。 10.详细描述一下Elasticsearch搜索的过程 搜索被执行成一个两阶段过程，我们称之为 Query Then Fetch； 在初始查询阶段时，查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的大小为 from + size 的优先队列。PS：在搜索的时候是会查询Filesystem Cache的，但是有部分数据还在Memory Buffer，所以搜索是近实时的。 每个分片返回各自优先队列中 所有文档的 ID 和排序值 给协调节点，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。 接下来就是 取回阶段，协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。每个分片加载并 丰富 文档，如果有需要的话，接着返回文档给协调节点。一旦所有的文档都被取回了，协调节点返回结果给客户端。 补充：Query Then Fetch的搜索类型在文档相关性打分的时候参考的是本分片的数据，这样在文档数量较少的时候可能不够准确，DFS Query Then Fetch增加了一个预查询的处理，询问Term和Document frequency，这个评分更准确，但是性能会变差。 11.Elasticsearch对于大数据量（上亿量级）的聚合如何实现？ Elasticsearch 提供的首个近似聚合是cardinality 度量。它提供一个字段的基数，即该字段的distinct或者unique值的数目。它是基于HLL算法的。HLL 会先对我们的输入作哈希运算，然后根据哈希运算的结果中的 bits 做概率估算从而得到基数。其特点是：可配置的精度，用来控制内存的使用（更精确 ＝ 更多内存）；小的数据集精度是非常高的；我们可以通过配置参数，来设置去重需要的固定内存使用量。无论数千还是数十亿的唯一值，内存使用量只与你配置的精确度相关。 12.在并发情况下，Elasticsearch如果保证读写一致？ 可以通过版本号使用乐观并发控制，以确保新版本不会被旧版本覆盖，由应用层来处理具体的冲突； 另外对于写操作，一致性级别支持quorum/one/all，默认为quorum，即只有当大多数分片可用时才允许写操作。但即使大多数可用，也可能存在因为网络等原因导致写入副本失败，这样该副本被认为故障，分片将会在一个不同的节点上重建。 对于读操作，可以设置replication为sync(默认)，这使得操作在主分片和副本分片都完成后才会返回；如果设置replication为async时，也可以通过设置搜索请求参数_preference为primary来查询主分片，确保文档是最新版本。 ","link":"https://tinaxiawuhao.github.io/post/w_tIMVx00/"},{"title":"elasticSearch查询语句","content":" 原文：https://distributedbytes.timojo.com/2016/07/23-useful-elasticsearch-example-queries.html 作者：Tim Ojo 为了演示不同类型的 ElasticSearch 的查询，我们将使用以下字段搜索书籍文档的集合（ title（标题）, authors（作者）, summary（摘要）, publish_date（发布日期）和 num_reviews（浏览数））。 在这之前，首先我们应该先创建一个新的索引（index），并批量导入一些文档： 创建索引： PUT /bookdb_index { &quot;settings&quot;: { &quot;number_of_shards&quot;: 1 }} 批量上传文档： POST /bookdb_index/book/_bulk { &quot;index&quot;: { &quot;_id&quot;: 1 }} { &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;authors&quot;: [&quot;clinton gormley&quot;, &quot;zachary tong&quot;], &quot;summary&quot; : &quot;A distibuted real-time search and analytics engine&quot;, &quot;publish_date&quot; : &quot;2015-02-07&quot;, &quot;num_reviews&quot;: 20, &quot;publisher&quot;: &quot;oreilly&quot; } { &quot;index&quot;: { &quot;_id&quot;: 2 }} { &quot;title&quot;: &quot;Taming Text: How to Find, Organize, and Manipulate It&quot;, &quot;authors&quot;: [&quot;grant ingersoll&quot;, &quot;thomas morton&quot;, &quot;drew farris&quot;], &quot;summary&quot; : &quot;organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization&quot;, &quot;publish_date&quot; : &quot;2013-01-24&quot;, &quot;num_reviews&quot;: 12, &quot;publisher&quot;: &quot;manning&quot; } { &quot;index&quot;: { &quot;_id&quot;: 3 }} { &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;authors&quot;: [&quot;radu gheorge&quot;, &quot;matthew lee hinman&quot;, &quot;roy russo&quot;], &quot;summary&quot; : &quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms&quot;, &quot;publish_date&quot; : &quot;2015-12-03&quot;, &quot;num_reviews&quot;: 18, &quot;publisher&quot;: &quot;manning&quot; } { &quot;index&quot;: { &quot;_id&quot;: 4 }} { &quot;title&quot;: &quot;Solr in Action&quot;, &quot;authors&quot;: [&quot;trey grainger&quot;, &quot;timothy potter&quot;], &quot;summary&quot; : &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;publish_date&quot; : &quot;2014-04-05&quot;, &quot;num_reviews&quot;: 23, &quot;publisher&quot;: &quot;manning&quot; } 例子 1.基本比对查询 执行基本的全文本（匹配）查询有两种方式：使用Search Lite API（它希望所有搜索参数都作为URL的一部分传入），或使用完整的JSON请求正文（允许您使用完整的Elasticsearch DSL)。 这是一个基本的匹配查询，它在所有字段中搜索字符串“ guide” GET /bookdb_index/book/_search?q=guide [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.28168046, &quot;_source&quot;: { &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;authors&quot;: [ &quot;clinton gormley&quot;, &quot;zachary tong&quot; ], &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot;, &quot;num_reviews&quot;: 20, &quot;publisher&quot;: &quot;manning&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.24144039, &quot;_source&quot;: { &quot;title&quot;: &quot;Solr in Action&quot;, &quot;authors&quot;: [ &quot;trey grainger&quot;, &quot;timothy potter&quot; ], &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot;, &quot;num_reviews&quot;: 23, &quot;publisher&quot;: &quot;manning&quot; } } ] 该查询的完整版本如下所示，其产生的结果与上述搜索精简版相同。 { &quot;query&quot;: { &quot;multi_match&quot; : { &quot;query&quot; : &quot;guide&quot;, &quot;fields&quot; : [&quot;_all&quot;] } } } 使用multi_match关键字代替关键字match是对多个字段运行相同查询的便捷快捷方式。该fields属性指定要查询的字段，在这种情况下，我们要查询文档中的所有字段。 这两个API均允许您指定要搜索的字段。例如，要在标题字段中搜索带有“in action”字样的图书，请执行以下操作： GET /bookdb_index/book/_search?q=title:in action [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.6259885, &quot;_source&quot;: { &quot;title&quot;: &quot;Solr in Action&quot;, &quot;authors&quot;: [ &quot;trey grainger&quot;, &quot;timothy potter&quot; ], &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot;, &quot;num_reviews&quot;: 23, &quot;publisher&quot;: &quot;manning&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.5975345, &quot;_source&quot;: { &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;authors&quot;: [ &quot;radu gheorge&quot;, &quot;matthew lee hinman&quot;, &quot;roy russo&quot; ], &quot;summary&quot;: &quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms&quot;, &quot;publish_date&quot;: &quot;2015-12-03&quot;, &quot;num_reviews&quot;: 18, &quot;publisher&quot;: &quot;manning&quot; } } ] 但是，全身DSL在创建更复杂的查询（如我们将在后面看到）以及指定您希望如何返回结果方面给您更大的灵活性。在下面的示例中，我们指定要返回的结果数，开始的偏移量（用于分页），要返回的文档字段以及术语突出显示。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;match&quot; : { &quot;title&quot; : &quot;in action&quot; } }, &quot;size&quot;: 2, &quot;from&quot;: 0, &quot;_source&quot;: [ &quot;title&quot;, &quot;summary&quot;, &quot;publish_date&quot; ], &quot;highlight&quot;: { &quot;fields&quot; : { &quot;title&quot; : {} } } } [Results] &quot;hits&quot;: { &quot;total&quot;: 2, &quot;max_score&quot;: 0.9105287, &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.9105287, &quot;_source&quot;: { &quot;summary&quot;: &quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms&quot;, &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;publish_date&quot;: &quot;2015-12-03&quot; }, &quot;highlight&quot;: { &quot;title&quot;: [ &quot;Elasticsearch &lt;em&gt;in&lt;/em&gt; &lt;em&gt;Action&lt;/em&gt;&quot; ] } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.9105287, &quot;_source&quot;: { &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;title&quot;: &quot;Solr in Action&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot; }, &quot;highlight&quot;: { &quot;title&quot;: [ &quot;Solr &lt;em&gt;in&lt;/em&gt; &lt;em&gt;Action&lt;/em&gt;&quot; ] } } ] } 注意：对于多字查询，该match查询使您可以指定是否使用and运算符而不是默认or运算符。您还可以指定minimum_should_match选项来调整返回结果的相关性。可以在这里找到详细信息。 2.多字段搜索 正如我们已经看到的，要在搜索中查询多个文档字段（例如，在标题和摘要中搜索相同的查询字符串），则可以使用该multi_match查询。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;multi_match&quot; : { &quot;query&quot; : &quot;elasticsearch guide&quot;, &quot;fields&quot;: [&quot;title&quot;, &quot;summary&quot;] } } } [Results] &quot;hits&quot;: { &quot;total&quot;: 3, &quot;max_score&quot;: 0.9448582, &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.9448582, &quot;_source&quot;: { &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;authors&quot;: [ &quot;clinton gormley&quot;, &quot;zachary tong&quot; ], &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot;, &quot;num_reviews&quot;: 20, &quot;publisher&quot;: &quot;manning&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.17312013, &quot;_source&quot;: { &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;authors&quot;: [ &quot;radu gheorge&quot;, &quot;matthew lee hinman&quot;, &quot;roy russo&quot; ], &quot;summary&quot;: &quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms&quot;, &quot;publish_date&quot;: &quot;2015-12-03&quot;, &quot;num_reviews&quot;: 18, &quot;publisher&quot;: &quot;manning&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.14965448, &quot;_source&quot;: { &quot;title&quot;: &quot;Solr in Action&quot;, &quot;authors&quot;: [ &quot;trey grainger&quot;, &quot;timothy potter&quot; ], &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot;, &quot;num_reviews&quot;: 23, &quot;publisher&quot;: &quot;manning&quot; } } ] } 请注意，命中数字3相匹配，因为在摘要中找到了“指南”一词。 3.提高字段重要性 由于我们正在多个字段中进行搜索，因此我们可能希望提高特定字段中的得分。在以下人为设计的示例中，我们将摘要字段的得分提高了3倍，以提高摘要字段的重要性，这反过来又会增加文档_id 4的相关性。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;multi_match&quot; : { &quot;query&quot; : &quot;elasticsearch guide&quot;, &quot;fields&quot;: [&quot;title&quot;, &quot;summary^3&quot;] } }, &quot;_source&quot;: [&quot;title&quot;, &quot;summary&quot;, &quot;publish_date&quot;] } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.31495273, &quot;_source&quot;: { &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.14965448, &quot;_source&quot;: { &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;title&quot;: &quot;Solr in Action&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.13094766, &quot;_source&quot;: { &quot;summary&quot;: &quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms&quot;, &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;publish_date&quot;: &quot;2015-12-03&quot; } } ] 注意 ：Boosting不仅仅意味着计算出的分数会乘以Boosting系数。实际应用的升压值经过归一化和一些内部优化。有关增强工作原理的更多信息，请参见 Elasticsearch指南。 4.布尔查询 AND / OR / NOT运算符可用于微调我们的搜索查询，以提供更相关或更具体的结果。这是在搜索API中作为bool查询实现的。该bool查询接受一个must参数（等同于AND），一个must_not参数（等同于NOT）和一个should参数（等同于OR）。例如，如果我要搜索书名中带有“ Elasticsearch”或“ Solr”字样的书，则AND由“克林顿·戈姆利”（clinton gormley）创作，而不由“ radu gheorge”（radu gheorge）创作： POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: { &quot;bool&quot; : { &quot;should&quot;: [ { &quot;match&quot;: { &quot;title&quot;: &quot;Elasticsearch&quot; }}, { &quot;match&quot;: { &quot;title&quot;: &quot;Solr&quot; }} ] } }, &quot;must&quot;: { &quot;match&quot;: { &quot;authors&quot;: &quot;clinton gormely&quot; }}, &quot;must_not&quot;: { &quot;match&quot;: {&quot;authors&quot;: &quot;radu gheorge&quot; }} } } } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.3672021, &quot;_source&quot;: { &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;authors&quot;: [ &quot;clinton gormley&quot;, &quot;zachary tong&quot; ], &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot;, &quot;num_reviews&quot;: 20, &quot;publisher&quot;: &quot;oreilly&quot; } } ] 注意：如您所见，布尔查询可以包装任何其他查询类型，包括其他布尔查询，以创建任意复杂或深度嵌套的查询。 5.模糊查询 可以在“匹配”和“多匹配”查询中启用模糊匹配，以捕获拼写错误。模糊程度是根据距原始单词的Levenshtein距离指定的。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;multi_match&quot; : { &quot;query&quot; : &quot;comprihensiv guide&quot;, &quot;fields&quot;: [&quot;title&quot;, &quot;summary&quot;], &quot;fuzziness&quot;: &quot;AUTO&quot; } }, &quot;_source&quot;: [&quot;title&quot;, &quot;summary&quot;, &quot;publish_date&quot;], &quot;size&quot;: 1 } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.5961596, &quot;_source&quot;: { &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;title&quot;: &quot;Solr in Action&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot; } } ] 注意：模糊度值&quot;AUTO&quot;等于指定2术语长度大于5时的值。但是，设置80％的人类拼写错误的编辑距离为1，并将模糊度设置为1可以改善整体搜索性能。有关更多信息，请参见《Elasticsearch最终指南》的“错别字和拼写错误”一章。 6.通配符查询 通配符查询使您可以指定要匹配的模式，而不是整个术语。?匹配任何字符并*匹配零个或多个字符。例如，要查找所有作者姓名以字母“ t”开头的记录 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;wildcard&quot; : { &quot;authors&quot; : &quot;t*&quot; } }, &quot;_source&quot;: [&quot;title&quot;, &quot;authors&quot;], &quot;highlight&quot;: { &quot;fields&quot; : { &quot;authors&quot; : {} } } } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: { &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;authors&quot;: [ &quot;clinton gormley&quot;, &quot;zachary tong&quot; ] }, &quot;highlight&quot;: { &quot;authors&quot;: [ &quot;zachary &lt;em&gt;tong&lt;/em&gt;&quot; ] } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: { &quot;title&quot;: &quot;Taming Text: How to Find, Organize, and Manipulate It&quot;, &quot;authors&quot;: [ &quot;grant ingersoll&quot;, &quot;thomas morton&quot;, &quot;drew farris&quot; ] }, &quot;highlight&quot;: { &quot;authors&quot;: [ &quot;&lt;em&gt;thomas&lt;/em&gt; morton&quot; ] } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: { &quot;title&quot;: &quot;Solr in Action&quot;, &quot;authors&quot;: [ &quot;trey grainger&quot;, &quot;timothy potter&quot; ] }, &quot;highlight&quot;: { &quot;authors&quot;: [ &quot;&lt;em&gt;trey&lt;/em&gt; grainger&quot;, &quot;&lt;em&gt;timothy&lt;/em&gt; potter&quot; ] } } ] 7.正则表达式查询 正则表达式查询使您可以指定比通配符查询更复杂的模式。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;regexp&quot; : { &quot;authors&quot; : &quot;t[a-z]*y&quot; } }, &quot;_source&quot;: [&quot;title&quot;, &quot;authors&quot;], &quot;highlight&quot;: { &quot;fields&quot; : { &quot;authors&quot; : {} } } } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: { &quot;title&quot;: &quot;Solr in Action&quot;, &quot;authors&quot;: [ &quot;trey grainger&quot;, &quot;timothy potter&quot; ] }, &quot;highlight&quot;: { &quot;authors&quot;: [ &quot;&lt;em&gt;trey&lt;/em&gt; grainger&quot;, &quot;&lt;em&gt;timothy&lt;/em&gt; potter&quot; ] } } ] 8.匹配词组查询 匹配词组查询要求查询字符串中的所有术语都存在于文档中，并按照查询字符串中指定的顺序并且彼此接近。默认情况下，术语必须彼此完全平行，但是您可以指定一个slop值，该值指示在仍将文档视为匹配项时允许相隔多远的术语。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;multi_match&quot; : { &quot;query&quot;: &quot;search engine&quot;, &quot;fields&quot;: [&quot;title&quot;, &quot;summary&quot;], &quot;type&quot;: &quot;phrase&quot;, &quot;slop&quot;: 3 } }, &quot;_source&quot;: [ &quot;title&quot;, &quot;summary&quot;, &quot;publish_date&quot; ] } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.22327082, &quot;_source&quot;: { &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;title&quot;: &quot;Solr in Action&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.16113183, &quot;_source&quot;: { &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot; } } ] 注意：在上面的示例中，对于非短语类型查询，文档的_id 1得分通常更高，并且_id 4由于其字段长度较短而出现在文档的前面。但是，在进行短语查询时，会考虑到术语的接近度，因此文档_id 4得分会更高。 9.匹配词组前缀 匹配词组前缀查询可在查询时提供“按需输入”或“穷人”版本的自动完成功能，而无需以任何方式准备数据。像match_phrase查询一样，它接受一个slop参数以使单词顺序和相对位置的刚性降低一些。我还接受该max_expansions参数来限制匹配项的数量，以降低资源强度。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;match_phrase_prefix&quot; : { &quot;summary&quot;: { &quot;query&quot;: &quot;search en&quot;, &quot;slop&quot;: 3, &quot;max_expansions&quot;: 10 } } }, &quot;_source&quot;: [ &quot;title&quot;, &quot;summary&quot;, &quot;publish_date&quot; ] } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.5161346, &quot;_source&quot;: { &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;title&quot;: &quot;Solr in Action&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.37248808, &quot;_source&quot;: { &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot; } } ] 注意：“按类型查询时搜索”会降低性能。更好的解决方案是按类型进行索引时间搜索。请查看完成提示API或使用Edge-Ngram过滤器以获取更多信息。 10.请求参数 该query_string查询提供了一种以简洁的速记语法执行多重匹配查询，布尔查询，增强查询，模糊匹配，通配符，正则表达式和范围查询的方法。在下面的示例中，我们对术语“ saerch算法”执行模糊搜索，其中作者之一是“ grant ingersoll”或“ tom morton”。我们搜索所有字段，但对摘要字段加2。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;query_string&quot; : { &quot;query&quot;: &quot;(saerch~1 algorithm~1) AND (grant ingersoll) OR (tom morton)&quot;, &quot;fields&quot;: [&quot;_all&quot;, &quot;summary^2&quot;] } }, &quot;_source&quot;: [ &quot;title&quot;, &quot;summary&quot;, &quot;authors&quot; ], &quot;highlight&quot;: { &quot;fields&quot; : { &quot;summary&quot; : {} } } } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.14558059, &quot;_source&quot;: { &quot;summary&quot;: &quot;organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization&quot;, &quot;title&quot;: &quot;Taming Text: How to Find, Organize, and Manipulate It&quot;, &quot;authors&quot;: [ &quot;grant ingersoll&quot;, &quot;thomas morton&quot;, &quot;drew farris&quot; ] }, &quot;highlight&quot;: { &quot;summary&quot;: [ &quot;organize text using approaches such as full-text &lt;em&gt;search&lt;/em&gt;, proper name recognition, clustering, tagging, information extraction, and summarization&quot; ] } } ] 11.简单查询字符串 该simple_query_string查询是该查询的一种版本query_string，它更适合在暴露给用户的单个搜索框中使用。它分别用+ / | /-代替了AND / OR / NOT的使用，并且丢弃了查询的无效部分，而不是在用户犯错时抛出异常。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;simple_query_string&quot; : { &quot;query&quot;: &quot;(saerch~1 algorithm~1) + (grant ingersoll) | (tom morton)&quot;, &quot;fields&quot;: [&quot;_all&quot;, &quot;summary^2&quot;] } }, &quot;_source&quot;: [ &quot;title&quot;, &quot;summary&quot;, &quot;authors&quot; ], &quot;highlight&quot;: { &quot;fields&quot; : { &quot;summary&quot; : {} } } } 12.术语查询 以上示例是全文搜索的示例。有时，我们对结构化搜索更感兴趣，在结构化搜索中我们希望找到完全匹配并返回结果。在term与terms查询帮助我们在这里。在下面的示例中，我们正在搜索Manning Publications出版的索引中的所有书籍。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;term&quot; : { &quot;publisher&quot;: &quot;manning&quot; } }, &quot;_source&quot; : [&quot;title&quot;,&quot;publish_date&quot;,&quot;publisher&quot;] } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1.2231436, &quot;_source&quot;: { &quot;publisher&quot;: &quot;manning&quot;, &quot;title&quot;: &quot;Taming Text: How to Find, Organize, and Manipulate It&quot;, &quot;publish_date&quot;: &quot;2013-01-24&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1.2231436, &quot;_source&quot;: { &quot;publisher&quot;: &quot;manning&quot;, &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;publish_date&quot;: &quot;2015-12-03&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1.2231436, &quot;_source&quot;: { &quot;publisher&quot;: &quot;manning&quot;, &quot;title&quot;: &quot;Solr in Action&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot; } } ] 可以使用terms关键字代替并传递搜索词数组来指定多个词。 { &quot;query&quot;: { &quot;terms&quot; : { &quot;publisher&quot;: [&quot;oreilly&quot;, &quot;packt&quot;] } } } 13.字词查询-排序 术语查询结果（与任何其他查询结果一样）可以轻松地进行排序。也允许多级排序 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;term&quot; : { &quot;publisher&quot;: &quot;manning&quot; } }, &quot;_source&quot; : [&quot;title&quot;,&quot;publish_date&quot;,&quot;publisher&quot;], &quot;sort&quot;: [ { &quot;publish_date&quot;: {&quot;order&quot;:&quot;desc&quot;}}, { &quot;title&quot;: { &quot;order&quot;: &quot;desc&quot; }} ] } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: null, &quot;_source&quot;: { &quot;publisher&quot;: &quot;manning&quot;, &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;publish_date&quot;: &quot;2015-12-03&quot; }, &quot;sort&quot;: [ 1449100800000, &quot;in&quot; ] }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: null, &quot;_source&quot;: { &quot;publisher&quot;: &quot;manning&quot;, &quot;title&quot;: &quot;Solr in Action&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot; }, &quot;sort&quot;: [ 1396656000000, &quot;solr&quot; ] }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: null, &quot;_source&quot;: { &quot;publisher&quot;: &quot;manning&quot;, &quot;title&quot;: &quot;Taming Text: How to Find, Organize, and Manipulate It&quot;, &quot;publish_date&quot;: &quot;2013-01-24&quot; }, &quot;sort&quot;: [ 1358985600000, &quot;to&quot; ] } ] 14.范围查询 另一个结构化查询示例是范围查询。在此示例中，我们搜索2015年出版的图书。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;range&quot; : { &quot;publish_date&quot;: { &quot;gte&quot;: &quot;2015-01-01&quot;, &quot;lte&quot;: &quot;2015-12-31&quot; } } }, &quot;_source&quot; : [&quot;title&quot;,&quot;publish_date&quot;,&quot;publisher&quot;] } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: { &quot;publisher&quot;: &quot;oreilly&quot;, &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: { &quot;publisher&quot;: &quot;manning&quot;, &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;publish_date&quot;: &quot;2015-12-03&quot; } } ] 注意：范围查询适用于日期，数字和字符串类型字段。 15.过滤查询 筛选查询允许您筛选查询结果。对于我们的示例，我们正在查询标题或摘要中带有“ Elasticsearch”一词的图书，但我们希望将搜索结果过滤为仅包含20条或更多评论的图书。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;filtered&quot;: { &quot;query&quot; : { &quot;multi_match&quot;: { &quot;query&quot;: &quot;elasticsearch&quot;, &quot;fields&quot;: [&quot;title&quot;,&quot;summary&quot;] } }, &quot;filter&quot;: { &quot;range&quot; : { &quot;num_reviews&quot;: { &quot;gte&quot;: 20 } } } } }, &quot;_source&quot; : [&quot;title&quot;,&quot;summary&quot;,&quot;publisher&quot;, &quot;num_reviews&quot;] } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.5955761, &quot;_source&quot;: { &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;publisher&quot;: &quot;oreilly&quot;, &quot;num_reviews&quot;: 20, &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot; } } ] 注意：筛选查询并不要求存在要对其进行筛选的查询。如果未指定match_all查询，则运行查询，该查询基本上返回索引中的所有文档，然后对其进行过滤。实际上，首先运行过滤器，以减少需要查询的表面积。此外，过滤器在首次使用后会被缓存，这使其性能非常好。 更新：已过滤的查询已从即将推出的Elasticsearch 5.0中删除，以支持bool查询。这是与上面相同的示例，改写为使用bool查询。返回的结果完全相同。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot; : { &quot;multi_match&quot;: { &quot;query&quot;: &quot;elasticsearch&quot;, &quot;fields&quot;: [&quot;title&quot;,&quot;summary&quot;] } }, &quot;filter&quot;: { &quot;range&quot; : { &quot;num_reviews&quot;: { &quot;gte&quot;: 20 } } } } }, &quot;_source&quot; : [&quot;title&quot;,&quot;summary&quot;,&quot;publisher&quot;, &quot;num_reviews&quot;] } 在下面的示例中，这也适用于多个过滤器。 16.多个过滤器 可以通过使用过滤器来组合多个过滤bool器。在下一个示例中，过滤器确定返回的结果必须至少具有20条评论，不得在2015年之前发布，而应由oreilly发布。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;filtered&quot;: { &quot;query&quot; : { &quot;multi_match&quot;: { &quot;query&quot;: &quot;elasticsearch&quot;, &quot;fields&quot;: [&quot;title&quot;,&quot;summary&quot;] } }, &quot;filter&quot;: { &quot;bool&quot;: { &quot;must&quot;: { &quot;range&quot; : { &quot;num_reviews&quot;: { &quot;gte&quot;: 20 } } }, &quot;must_not&quot;: { &quot;range&quot; : { &quot;publish_date&quot;: { &quot;lte&quot;: &quot;2014-12-31&quot; } } }, &quot;should&quot;: { &quot;term&quot;: { &quot;publisher&quot;: &quot;oreilly&quot; } } } } } }, &quot;_source&quot; : [&quot;title&quot;,&quot;summary&quot;,&quot;publisher&quot;, &quot;num_reviews&quot;, &quot;publish_date&quot;] } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.5955761, &quot;_source&quot;: { &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;publisher&quot;: &quot;oreilly&quot;, &quot;num_reviews&quot;: 20, &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot; } } ] 17.功能评分：字段值因子 在某些情况下，您可能希望将文档中特定字段的值纳入相关性分数的计算中。在您希望根据文档的受欢迎程度提高其相关性的情况下，这是典型的情况。在我们的示例中，我们希望增加受欢迎的书籍（根据评论数判断）。使用field_value_factor功能评分可以做到这一点。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;multi_match&quot; : { &quot;query&quot; : &quot;search engine&quot;, &quot;fields&quot;: [&quot;title&quot;, &quot;summary&quot;] } }, &quot;field_value_factor&quot;: { &quot;field&quot; : &quot;num_reviews&quot;, &quot;modifier&quot;: &quot;log1p&quot;, &quot;factor&quot; : 2 } } }, &quot;_source&quot;: [&quot;title&quot;, &quot;summary&quot;, &quot;publish_date&quot;, &quot;num_reviews&quot;] } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.44831306, &quot;_source&quot;: { &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;num_reviews&quot;: 20, &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.3718407, &quot;_source&quot;: { &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;num_reviews&quot;: 23, &quot;title&quot;: &quot;Solr in Action&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.046479136, &quot;_source&quot;: { &quot;summary&quot;: &quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms&quot;, &quot;num_reviews&quot;: 18, &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;publish_date&quot;: &quot;2015-12-03&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.041432835, &quot;_source&quot;: { &quot;summary&quot;: &quot;organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization&quot;, &quot;num_reviews&quot;: 12, &quot;title&quot;: &quot;Taming Text: How to Find, Organize, and Manipulate It&quot;, &quot;publish_date&quot;: &quot;2013-01-24&quot; } } ] 注意1：我们本来可以运行常规multi_match查询并按num_reviews字段进行排序，但是这样就失去了进行相关性评分的好处。 注意2：还有许多其他参数可以调整对原始相关性得分的增强效果，例如“修饰符”，“因子”，“ boost_mode”等。这些参数在Elasticsearch指南中进行了详细探讨。 18.作用分值：衰减功能 假设您不想以某个字段的值递增，而希望拥有理想的目标值，并且希望该增益因子随着距离该值的增加而衰减。这通常在基于纬度/经度，价格或日期等数字字段的提升中很有用。在我们精心设计的示例中，我们正在搜索理想情况下于2014年6月左右出版的“搜索引擎”书籍。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;multi_match&quot; : { &quot;query&quot; : &quot;search engine&quot;, &quot;fields&quot;: [&quot;title&quot;, &quot;summary&quot;] } }, &quot;functions&quot;: [ { &quot;exp&quot;: { &quot;publish_date&quot; : { &quot;origin&quot;: &quot;2014-06-15&quot;, &quot;offset&quot;: &quot;7d&quot;, &quot;scale&quot; : &quot;30d&quot; } } } ], &quot;boost_mode&quot; : &quot;replace&quot; } }, &quot;_source&quot;: [&quot;title&quot;, &quot;summary&quot;, &quot;publish_date&quot;, &quot;num_reviews&quot;] } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.27420625, &quot;_source&quot;: { &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;num_reviews&quot;: 23, &quot;title&quot;: &quot;Solr in Action&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.005920768, &quot;_source&quot;: { &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;num_reviews&quot;: 20, &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.000011564, &quot;_source&quot;: { &quot;summary&quot;: &quot;organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization&quot;, &quot;num_reviews&quot;: 12, &quot;title&quot;: &quot;Taming Text: How to Find, Organize, and Manipulate It&quot;, &quot;publish_date&quot;: &quot;2013-01-24&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.0000059171475, &quot;_source&quot;: { &quot;summary&quot;: &quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms&quot;, &quot;num_reviews&quot;: 18, &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;publish_date&quot;: &quot;2015-12-03&quot; } } ] 19.功能评分：脚本评分 如果内置的评分功能无法满足您的需求，则可以选择指定Groovy脚本进行评分。在我们的示例中，我们希望指定一个脚本，该脚本在决定要考虑评论数量的因素之前，先考虑publish_date。较新的书可能没有那么多的评论，因此不应因此受到惩罚。 评分脚本如下所示： publish_date = doc['publish_date'].value num_reviews = doc['num_reviews'].value if (publish_date &gt; Date.parse('yyyy-MM-dd', threshold).getTime()) { my_score = Math.log(2.5 + num_reviews) } else { my_score = Math.log(1 + num_reviews) } return my_score 要动态使用评分脚本，我们使用script_score参数 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;multi_match&quot; : { &quot;query&quot; : &quot;search engine&quot;, &quot;fields&quot;: [&quot;title&quot;, &quot;summary&quot;] } }, &quot;functions&quot;: [ { &quot;script_score&quot;: { &quot;params&quot; : { &quot;threshold&quot;: &quot;2015-07-30&quot; }, &quot;script&quot;: &quot;publish_date = doc['publish_date'].value; num_reviews = doc['num_reviews'].value; if (publish_date &gt; Date.parse('yyyy-MM-dd', threshold).getTime()) { return log(2.5 + num_reviews) }; return log(1 + num_reviews);&quot; } } ] } }, &quot;_source&quot;: [&quot;title&quot;, &quot;summary&quot;, &quot;publish_date&quot;, &quot;num_reviews&quot;] } [Results] &quot;hits&quot;: { &quot;total&quot;: 4, &quot;max_score&quot;: 0.8463001, &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.8463001, &quot;_source&quot;: { &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;num_reviews&quot;: 20, &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.7067348, &quot;_source&quot;: { &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;num_reviews&quot;: 23, &quot;title&quot;: &quot;Solr in Action&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.08952084, &quot;_source&quot;: { &quot;summary&quot;: &quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms&quot;, &quot;num_reviews&quot;: 18, &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;publish_date&quot;: &quot;2015-12-03&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.07602123, &quot;_source&quot;: { &quot;summary&quot;: &quot;organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization&quot;, &quot;num_reviews&quot;: 12, &quot;title&quot;: &quot;Taming Text: How to Find, Organize, and Manipulate It&quot;, &quot;publish_date&quot;: &quot;2013-01-24&quot; } } ] } 注意1：要使用动态脚本，必须为config/elasticsearch.yaml文件中的Elasticsearch实例启用动态脚本。也可以使用存储在Elasticsearch服务器上的脚本。查阅Elasticsearch参考文档了解更多信息。 注意2 ：JSON无法包含嵌入的换行符，因此分号用于分隔语句。 ","link":"https://tinaxiawuhao.github.io/post/AdeCmc8BU/"},{"title":"docker安装ELK","content":"Docker部署ElasticSearch 搜索ElasticSearch镜像 docker search elasticsearch 拉取镜像 拉取镜像的时候，可以指定版本，如果不指定，默认使用latest。 docker pull elasticsearch:7.12.0 查看镜像 docker images docker 启动 elasticsearch # --name : 为 elasticsearch 容器起个别名 # -e : 指定为单节点集群模式 # -i：表示运行容器 # -t：表示容器启动后会进入其命令行。加入这两个参数后，容器创建就能登录进去。即分配一个伪终端。 # -v：表示目录映射关系（前者是宿主机目录，后者是映射到宿主机上的目录），可以使用多个－v做多个目录或文件映射。注意：最好做目录映射，在宿主机上做修改，然后共享到容器上。 # -d：在run后面加上-d参数,则会创建一个守护式容器在后台运行（这样创建容器后不会自动登录容器，如果只加-i -t两个参数，创建后就会自动进去容器）。 # -p：表示端口映射，前者是宿主机端口，后者是容器内的映射端口。可以使用多个-p做多个端口映射 docker run -di --name elasticsearch -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; elasticsearch:7.12.0 elasticsearch配置 创建elasticsearch滚动策略 # 定义审计日志管理策略 curl -X PUT &quot;${host}/_ilm/policy/audit_policy&quot; -H 'Content-Type: application/json' -d' { &quot;policy&quot;: { &quot;phases&quot;: { &quot;hot&quot;: { &quot;actions&quot;: { &quot;rollover&quot;: { &quot;max_size&quot;: &quot;30GB&quot;, &quot;max_age&quot;: &quot;180d&quot; } } }, &quot;delete&quot;: { &quot;min_age&quot;: &quot;180d&quot;, &quot;actions&quot;: { &quot;delete&quot;: {} } } } } } 热数据最大30G,最多180天，数据最少保持180天后删除 创建索引模板 # 导出日志索引模板 curl -X PUT &quot;${host}/_template/export_log_index_template&quot; -H 'Content-Type: application/json' -d' { &quot;index_patterns&quot;: [ &quot;export_log_index*&quot; ], &quot;settings&quot;: { &quot;number_of_shards&quot;: 1, &quot;number_of_replicas&quot;: 1, &quot;index.lifecycle.name&quot;: &quot;audit_policy&quot;, &quot;index.lifecycle.rollover_alias&quot;: &quot;export_log_index&quot;, &quot;index.max_result_window&quot;: &quot;100000&quot; }, &quot;mappings&quot;: { &quot;properties&quot;: { &quot;applicationSide&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;exportComment&quot;: { &quot;type&quot;: &quot;keyword&quot; }, &quot;exportFileSize&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;exportType&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;id&quot;: { &quot;type&quot;: &quot;keyword&quot; }, &quot;operationTime&quot;: { &quot;type&quot;: &quot;date&quot;, &quot;store&quot;: true, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss&quot; }, &quot;operationUser&quot;: { &quot;type&quot;: &quot;keyword&quot; }, &quot;operationUserName&quot;: { &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;fields&quot;: { &quot;keyword&quot;: { &quot;type&quot;: &quot;keyword&quot; } } }, &quot;remarks&quot;: { &quot;type&quot;: &quot;keyword&quot; }, &quot;successful&quot;: { &quot;type&quot;: &quot;boolean&quot; } } } } 创建索引 # 创建导出日志索引，按日期命名 curl -X PUT &quot;${host}/%3Cexport_log_index-%7Bnow%2Fd%7D-1%3E&quot; -H 'Content-Type: application/json' -d' { &quot;aliases&quot;: { &quot;export_log_index&quot;: { &quot;is_write_index&quot;: true } } } docker 安装 kibana 拉取镜像 拉取镜像的时候，需要注意的是, kibana 的版本最好与 elasticsearch 保持一致, 避免发生不必要的错误 docker pull kibana:7.12.0 查看镜像 docker images docker 启动 kibana # -e : 指定环境变量配置, 提供汉化 # --like : 建立两个容器之间的关联, kibana 关联到 es docker run -di --name kibana --link elasticsearch:elasticsearch -e &quot;I18N_LOCALE=zh-CN&quot; -p 5601:5601 kibana:7.12.0 # kibana 的汉化我感觉做的并不好 # 如果不习惯汉化, 可以把条件去除 docker run -di --name kibana --link elasticsearch:elasticsearch -p 5601:5601 kibana:7.12.0 Docker 安装 Logstash 拉取镜像 拉取镜像的时候，需要注意的是, Logstash 的版本最好与 elasticsearch 保持一致, 避免发生不必要的错误 docker pull logstash:7.12.0 查看镜像 docker images 文件映射 在本机建立配置文件和目录,用来存放所有配置的映射 /usr/local/logstash/config/logstash.yml /usr/local/logstash/conf.d/ logstash.yml (文件内容) path.config: /usr/share/logstash/conf.d/*.conf path.logs: /var/log/logstash conf.d/configuration.conf (文件内容) Filebeat和elasticsearch的交互 input { beats { port =&gt; 5044 codec =&gt; &quot;json&quot; } } output { elasticsearch { hosts =&gt; [&quot;elasticsearch:9200&quot;]， index =&gt; &quot;export_log_index&quot; } stdout { codec =&gt; rubydebug } } mysql和elasticsearch的交互 input { jdbc { jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot; jdbc_connection_string =&gt; &quot;jdbc:mysql://localhost:3306/db_example&quot; jdbc_user =&gt; root jdbc_password =&gt; ymruan123 #启用追踪，如果为true，则需要指定tracking_column use_column_value =&gt; true #指定追踪的字段， tracking_column =&gt; &quot;last_updated&quot; #追踪字段的类型，目前只有数字(numeric)和时间类型(timestamp)，默认是数字类型 tracking_column_type =&gt; &quot;numeric&quot; #记录最后一次运行的结果 record_last_run =&gt; true #上面运行结果的保存位置 last_run_metadata_path =&gt; &quot;jdbc-position.txt&quot; statement =&gt; &quot;SELECT * FROM user where last_updated &gt;:sql_last_value;&quot; schedule =&gt; &quot; * * * * * *&quot; } } output { elasticsearch { document_id =&gt; &quot;%{id}&quot; document_type =&gt; &quot;_doc&quot; index =&gt; &quot;export_log_index&quot; hosts =&gt; [&quot;http://localhost:9200&quot;] } stdout{ codec =&gt; rubydebug } } kafka和elasticsearch的交互 input { kafka{ topics =&gt; &quot;topic_export&quot; #kafka中topic名称，记得创建该topic group_id =&gt; &quot;group_export&quot; #默认为“logstash” codec =&gt; &quot;json&quot; #与Shipper端output配置项一致 consumer_threads =&gt; 1 #消费线程数，集群中所有logstash相加最好等于 topic 分区数 bootstrap_servers =&gt; &quot;kafka:9092&quot; decorate_events =&gt; true #在输出消息的时候回输出自身的信息，包括：消费消息的大小、topic来源以及consumer的group信息。 type =&gt; &quot;topic_export&quot; tags =&gt; [&quot;canal&quot;] # 标签，额外使用该参数可以在elastci中创建不同索引 } } filter { # 把默认的data字段重命名为message字段，方便在elastic中显示 mutate { rename =&gt; [&quot;data&quot;, &quot;message&quot;] } # 还可以使用其他的处理方式，在此就不再列出来了 } output { elasticsearch { hosts =&gt; [&quot;http://172.17.107.187:9203&quot;, &quot;http://172.17.107.187:9201&quot;,&quot;http://172.17.107.187:9202&quot;] index =&gt; &quot;export_log_index&quot; # decorate_events=true的作用，可以使用metadata中的数据 #user =&gt; &quot;elastic&quot; #password =&gt; &quot;escluter123456&quot; } } logback和和elasticsearch的交互 引入依赖 &lt;dependency&gt; &lt;groupId&gt;net.logstash.logback&lt;/groupId&gt; &lt;artifactId&gt;logstash-logback-encoder&lt;/artifactId&gt; &lt;version&gt;4.10&lt;/version&gt; &lt;/dependency&gt; 参数配置 input { # 应用日志 tcp{ type =&gt; &quot;app&quot; mode =&gt; &quot;server&quot; host =&gt; &quot;0.0.0.0&quot; port =&gt; 4560 codec =&gt; json } } output { elasticsearch { hosts =&gt; &quot;http://127.0.0.1:9200&quot; index =&gt; &quot;export_log_index&quot; } } 应用日志入口端口为4560，需要配置java客户端logstash入口 &lt;!-- 这个是控制台日志输出格式 方便调试对比--&gt; &lt;appender name=&quot;console&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder&gt; &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss} %contextName %-5level %logger{50} -%msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!--开启tcp格式的logstash传输，通过TCP协议连接Logstash--&gt; &lt;appender name=&quot;STASH&quot; class=&quot;net.logstash.logback.appender.LogstashTcpSocketAppender&quot;&gt; &lt;destination&gt;127.0.0.1:9600&lt;/destination&gt; &lt;encoder class=&quot;net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder&quot;&gt; &lt;!--中文序列化--&gt; &lt;jsonFactoryDecorator class=&quot;net.logstash.logback.decorate.CharacterEscapesJsonFactoryDecorator&quot;&gt; &lt;escape&gt; &lt;targetCharacterCode&gt;10&lt;/targetCharacterCode&gt; &lt;escapeSequence&gt;\\u2028&lt;/escapeSequence&gt; &lt;/escape&gt; &lt;/jsonFactoryDecorator&gt; &lt;providers&gt; &lt;pattern&gt; &lt;pattern&gt; &lt;!--{ &quot;timestamp&quot;:&quot;%date{ISO8601}&quot;, &quot;user&quot;:&quot;test&quot;, &quot;message&quot;:&quot;[%d{yyyy-MM-dd HH:mm:ss.SSS}][%p][%t][%l{80}|%L]%m&quot;}%n }--&gt; { &quot;timestamp&quot;: &quot;%date{\\&quot;yyyy-MM-dd' 'HH:mm:ss,SSSZ\\&quot;}&quot;, &quot;level&quot;: &quot;%level&quot;, &quot;thread&quot;: &quot;%thread&quot;, &quot;class_name&quot;: &quot;%class&quot;, &quot;line_number&quot;: &quot;%line&quot;, &quot;message&quot;: &quot;%message&quot;, &quot;stack_trace&quot;: &quot;%exception{5}&quot;, &quot;req_id&quot;: &quot;%X{reqId}&quot;, &quot;elapsed_time&quot;: &quot;#asLong{%X{elapsedTime}}&quot; } &lt;/pattern&gt; &lt;/pattern&gt; &lt;/providers&gt; &lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符--&gt; &lt;/encoder&gt; &lt;keepAliveDuration&gt;5 minutes&lt;/keepAliveDuration&gt; &lt;/appender&gt; &lt;root level=&quot;INFO&quot;&gt; &lt;appender-ref ref=&quot;STASH&quot;/&gt; &lt;appender-ref ref=&quot;console&quot;/&gt; &lt;/root&gt; docker 启动 Logstash #docker容器互访 运行容器的时候加上参数link docker run -id -p 5044:5044 --name logstash --link elasticsearch --link beats -v /usr/local/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml -v /usr/local/logstash/conf.d/:/usr/share/logstash/conf.d/ logstash:7.4.1 Docker 安装 Filebeat 拉取镜像 拉取镜像的时候，需要注意的是, Filebeat 的版本最好与 elasticsearch 保持一致, 避免发生不必要的错误 docker pull store/elastic/filebeat:7.12.0 查看镜像 docker images 文件映射 下载默认官方配置文件 wget https://raw.githubusercontent.com/elastic/beats/7.12/deploy/docker/filebeat.docker.yml 注意：文件放在宿主机/data/elk/filebeat目录下 打开配置文件 vim filebeat.docker.yml，内容如下： # 日志输入配置 filebeat.inputs: - type: log enabled: true paths: # 需要收集的日志所在的位置，可使用通配符进行配置 #- /data/elk/*.log - /logs/*/*.log #日志输出配置(采用 logstash 收集日志，5044为logstash端口) output.logstash: hosts: ['192.168.12.183:5044'] docker运行Filebeat docker run --name filebeat --user=root -d --net somenetwork --volume=&quot;/usr/local/filebeat/log/nginx/:/var/log/nginx/&quot; --volume=&quot;/data/elk/filebeat/filebeat.docker.yml:/usr/share/filebeat/filebeat.yml&quot; --volume=&quot;/var/lib/docker/containers:/var/lib/docker/containers:ro&quot; --volume=&quot;/var/run/docker.sock:/var/run/docker.sock:ro&quot; store/elastic/filebeat:7.12.0 ","link":"https://tinaxiawuhao.github.io/post/3D5Sr6L0c/"},{"title":"springBoot,mybatis,druid整合","content":"springBoot,mybatis,druid单数据源整合 项目框架 pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.4.5&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.example&lt;/groupId&gt; &lt;artifactId&gt;datasources&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;datasources&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.45&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; application.yml spring: datasource: username: root password: 123456 url: jdbc:mysql://localhost:3306/datatest?characterEncoding=utf-8&amp;useSSl=false driver-class-name: com.mysql.jdbc.Driver type: com.alibaba.druid.pool.DruidDataSource druid: initialSize: 10 # 初始化时建立物理连接的个数。初始化发生在显示调用init方法，或者第一次getConnection时 minIdle: 10 # 最小连接池数量 maxActive: 200 # 最大连接池数量 maxWait: 60000 # 获取连接时最大等待时间，单位毫秒。配置了maxWait之后，缺省启用公平锁，并发效率会有所下降，如果需要可以通过配置 timeBetweenEvictionRunsMillis: 60000 # 关闭空闲连接的检测时间间隔.Destroy线程会检测连接的间隔时间，如果连接空闲时间大于等于minEvictableIdleTimeMillis则关闭物理连接。 minEvictableIdleTimeMillis: 300000 # 连接的最小生存时间.连接保持空闲而不被驱逐的最小时间 validationQuery: SELECT 1 FROM DUAL # 验证数据库服务可用性的sql.用来检测连接是否有效的sql 因数据库方言而差, 例如 oracle 应该写成 SELECT 1 FROM DUAL testWhileIdle: true # 申请连接时检测空闲时间，根据空闲时间再检测连接是否有效.建议配置为true，不影响性能，并且保证安全性。申请连接的时候检测，如果空闲时间大于timeBetweenEvictionRun testOnBorrow: false # 申请连接时直接检测连接是否有效.申请连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能。 testOnReturn: false # 归还连接时检测连接是否有效.归还连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能。 poolPreparedStatements: true # 开启PSCache maxPoolPreparedStatementPerConnectionSize: 20 #设置PSCache值 connectionErrorRetryAttempts: 3 # 连接出错后再尝试连接三次 breakAfterAcquireFailure: true # 数据库服务宕机自动重连机制 timeBetweenConnectErrorMillis: 300000 # 连接出错后重试时间间隔 asyncInit: true # 异步初始化策略 remove-abandoned: true # 是否自动回收超时连接 remove-abandoned-timeout: 1800 # 超时时间(以秒数为单位) transaction-query-timeout: 6000 # 事务超时时间 filters: stat,wall,log4j2 connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=500 #mybatis是独立节点，需要单独配置 mybatis-plus: mapper-locations: classpath*:mapper/*.xml type-aliases-package: com.example.datasources.entity configuration: map-underscore-to-camel-case: true DruidConfig @Configuration public class DruidConfig { @Bean public ServletRegistrationBean statViewServlet(){ ServletRegistrationBean&lt;StatViewServlet&gt; bean = new ServletRegistrationBean&lt;StatViewServlet&gt;( new StatViewServlet(), &quot;/druid/*&quot; ); Map&lt;String,String&gt; iniParms=new HashMap&lt;&gt;( ); iniParms.put( &quot;loginUsername&quot;,&quot;admin&quot; );//登录druid的用户名 iniParms.put( &quot;loginPassword&quot;,&quot;123456&quot; );//登录druid的密码 iniParms.put(&quot;allow&quot;,&quot;&quot;);//默认允许所有 //iniParms.put( &quot;deny&quot;,&quot;192.168.***.***&quot; );//拒绝的ip地址 bean.setInitParameters( iniParms ); return bean; } @Bean public FilterRegistrationBean webStatFilter(){ FilterRegistrationBean bean= new FilterRegistrationBean(); bean.setFilter(new WebStatFilter()); Map&lt;String,String&gt; iniParms=new HashMap&lt;&gt;(); iniParms.put( &quot;excliusions&quot;, &quot;*.js,*.gif,*.jpg,*.bmp,*.png,*.css,*.ico,/druid/*&quot;);//使静态文件访问，还有/druid/* 的访问不被拦截 bean.setInitParameters( iniParms ); bean.setUrlPatterns( Arrays.asList(&quot;/*&quot;)); return bean; } } 访问Druid http://localhost:8080/druid/login.html springBoot,mybatis,druid多数据源整合 项目架构 pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.4.5&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.example&lt;/groupId&gt; &lt;artifactId&gt;datasources&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;datasources&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.45&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; application.yml spring: datasource: #使用druid连接池 type: com.alibaba.druid.pool.DruidDataSource # 自定义的主数据源配置信息 primary: datasource: #druid相关配置 druid: #监控统计拦截的filters filters: stat driverClassName: com.mysql.jdbc.Driver #配置基本属性 url: jdbc:mysql://127.0.0.1:3306/primary_database?useUnicode=true&amp;characterEncoding=UTF-8&amp;allowMultiQueries=true&amp;autoReconnect=true&amp;useSSL=false username: root password: 123456 #配置初始化大小/最小/最大 initialSize: 1 minIdle: 1 maxActive: 20 #获取连接等待超时时间 maxWait: 60000 #间隔多久进行一次检测，检测需要关闭的空闲连接 timeBetweenEvictionRunsMillis: 60000 #一个连接在池中最小生存的时间 minEvictableIdleTimeMillis: 300000 validationQuery: SELECT 'x' testWhileIdle: true testOnBorrow: false testOnReturn: false #打开PSCache，并指定每个连接上PSCache的大小。oracle设为true，mysql设为false。分库分表较多推荐设置为false poolPreparedStatements: false maxPoolPreparedStatementPerConnectionSize: 20 # 自定义的从数据源配置信息 back: datasource: #druid相关配置 druid: #监控统计拦截的filters filters: stat driverClassName: com.mysql.jdbc.Driver #配置基本属性 url: jdbc:mysql://127.0.0.1:3306/back_database?useUnicode=true&amp;characterEncoding=UTF-8&amp;allowMultiQueries=true&amp;autoReconnect=true&amp;useSSL=false username: root password: 123456 #配置初始化大小/最小/最大 initialSize: 1 minIdle: 1 maxActive: 20 #获取连接等待超时时间 maxWait: 60000 #间隔多久进行一次检测，检测需要关闭的空闲连接 timeBetweenEvictionRunsMillis: 60000 #一个连接在池中最小生存的时间 minEvictableIdleTimeMillis: 300000 validationQuery: SELECT 'x' testWhileIdle: true testOnBorrow: false testOnReturn: false #打开PSCache，并指定每个连接上PSCache的大小。oracle设为true，mysql设为false。分库分表较多推荐设置为false poolPreparedStatements: false maxPoolPreparedStatementPerConnectionSize: 20 DruidConfig @Configuration public class DruidConfig { @Bean public ServletRegistrationBean statViewServlet(){ ServletRegistrationBean&lt;StatViewServlet&gt; bean = new ServletRegistrationBean&lt;StatViewServlet&gt;( new StatViewServlet(), &quot;/druid/*&quot; ); Map&lt;String,String&gt; iniParms=new HashMap&lt;&gt;( ); iniParms.put( &quot;loginUsername&quot;,&quot;admin&quot; );//登录druid的用户名 iniParms.put( &quot;loginPassword&quot;,&quot;123456&quot; );//登录druid的密码 iniParms.put(&quot;allow&quot;,&quot;&quot;);//默认允许所有 //iniParms.put( &quot;deny&quot;,&quot;192.168.***.***&quot; );//拒绝的ip地址 bean.setInitParameters( iniParms ); return bean; } @Bean public FilterRegistrationBean webStatFilter(){ FilterRegistrationBean bean= new FilterRegistrationBean(); bean.setFilter(new WebStatFilter()); Map&lt;String,String&gt; iniParms=new HashMap&lt;&gt;(); iniParms.put( &quot;excliusions&quot;, &quot;*.js,*.gif,*.jpg,*.bmp,*.png,*.css,*.ico,/druid/*&quot;);//使静态文件访问，还有/druid/* 的访问不被拦截 bean.setInitParameters( iniParms ); bean.setUrlPatterns( Arrays.asList(&quot;/*&quot;)); return bean; } } PrimaryDataBaseConfig /** * @Description: 主数据源配置类 */ @Data @Configuration // 前缀为primary.datasource.druid的配置信息 @ConfigurationProperties(prefix = &quot;primary.datasource.druid&quot;) @MapperScan(basePackages = PrimaryDataBaseConfig.PACKAGE, sqlSessionFactoryRef = &quot;primarySqlSessionFactory&quot;) public class PrimaryDataBaseConfig { /** * dao层的包路径 */ static final String PACKAGE = &quot;com.example.datasources.dao.primary&quot;; /** * mapper文件的相对路径 */ private static final String MAPPER_LOCATION = &quot;classpath:mapper/primary/*.xml&quot;; private String filters; private String url; private String username; private String password; private String driverClassName; private int initialSize; private int minIdle; private int maxActive; private long maxWait; private long timeBetweenEvictionRunsMillis; private long minEvictableIdleTimeMillis; private String validationQuery; private boolean testWhileIdle; private boolean testOnBorrow; private boolean testOnReturn; private boolean poolPreparedStatements; private int maxPoolPreparedStatementPerConnectionSize; // 主数据源使用@Primary注解进行标识 @Primary @Bean(name = &quot;primaryDataSource&quot;) public DataSource primaryDataSource() throws SQLException { DruidDataSource druid = new DruidDataSource(); // 监控统计拦截的filters druid.setFilters(filters); // 配置基本属性 druid.setDriverClassName(driverClassName); druid.setUsername(username); druid.setPassword(password); druid.setUrl(url); //初始化时建立物理连接的个数 druid.setInitialSize(initialSize); //最大连接池数量 druid.setMaxActive(maxActive); //最小连接池数量 druid.setMinIdle(minIdle); //获取连接时最大等待时间，单位毫秒。 druid.setMaxWait(maxWait); //间隔多久进行一次检测，检测需要关闭的空闲连接 druid.setTimeBetweenEvictionRunsMillis(timeBetweenEvictionRunsMillis); //一个连接在池中最小生存的时间 druid.setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis); //用来检测连接是否有效的sql druid.setValidationQuery(validationQuery); //建议配置为true，不影响性能，并且保证安全性。 druid.setTestWhileIdle(testWhileIdle); //申请连接时执行validationQuery检测连接是否有效 druid.setTestOnBorrow(testOnBorrow); druid.setTestOnReturn(testOnReturn); //是否缓存preparedStatement，也就是PSCache，oracle设为true，mysql设为false。分库分表较多推荐设置为false druid.setPoolPreparedStatements(poolPreparedStatements); // 打开PSCache时，指定每个连接上PSCache的大小 druid.setMaxPoolPreparedStatementPerConnectionSize(maxPoolPreparedStatementPerConnectionSize); return druid; } // 创建该数据源的事务管理 @Primary @Bean(name = &quot;primaryTransactionManager&quot;) public DataSourceTransactionManager primaryTransactionManager() throws SQLException { return new DataSourceTransactionManager(primaryDataSource()); } // 创建Mybatis的连接会话工厂实例 @Primary @Bean(name = &quot;primarySqlSessionFactory&quot;) public SqlSessionFactory primarySqlSessionFactory(@Qualifier(&quot;primaryDataSource&quot;) DataSource primaryDataSource) throws Exception { final SqlSessionFactoryBean sessionFactory = new SqlSessionFactoryBean(); sessionFactory.setDataSource(primaryDataSource); // 设置数据源bean sessionFactory.setMapperLocations(new PathMatchingResourcePatternResolver() .getResources(PrimaryDataBaseConfig.MAPPER_LOCATION)); // 设置mapper文件路径 return sessionFactory.getObject(); } } BackDataBaseConfig /** * @Description: 从数据源配置类 */ @Data @Configuration @ConfigurationProperties(prefix = &quot;back.datasource.druid&quot;) @MapperScan(basePackages = BackDataBaseConfig.PACKAGE, sqlSessionFactoryRef = &quot;backSqlSessionFactory&quot;) public class BackDataBaseConfig { /** * dao层的包路径 */ static final String PACKAGE = &quot;com.example.datasources.dao.back&quot;; /** * mapper文件的相对路径 */ private static final String MAPPER_LOCATION = &quot;classpath:mapper/back/*.xml&quot;; private String filters; private String url; private String username; private String password; private String driverClassName; private int initialSize; private int minIdle; private int maxActive; private long maxWait; private long timeBetweenEvictionRunsMillis; private long minEvictableIdleTimeMillis; private String validationQuery; private boolean testWhileIdle; private boolean testOnBorrow; private boolean testOnReturn; private boolean poolPreparedStatements; private int maxPoolPreparedStatementPerConnectionSize; @Bean(name = &quot;backDataSource&quot;) public DataSource backDataSource() throws SQLException { DruidDataSource druid = new DruidDataSource(); // 监控统计拦截的filters druid.setFilters(filters); // 配置基本属性 druid.setDriverClassName(driverClassName); druid.setUsername(username); druid.setPassword(password); druid.setUrl(url); //初始化时建立物理连接的个数 druid.setInitialSize(initialSize); //最大连接池数量 druid.setMaxActive(maxActive); //最小连接池数量 druid.setMinIdle(minIdle); //获取连接时最大等待时间，单位毫秒。 druid.setMaxWait(maxWait); //间隔多久进行一次检测，检测需要关闭的空闲连接 druid.setTimeBetweenEvictionRunsMillis(timeBetweenEvictionRunsMillis); //一个连接在池中最小生存的时间 druid.setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis); //用来检测连接是否有效的sql druid.setValidationQuery(validationQuery); //建议配置为true，不影响性能，并且保证安全性。 druid.setTestWhileIdle(testWhileIdle); //申请连接时执行validationQuery检测连接是否有效 druid.setTestOnBorrow(testOnBorrow); druid.setTestOnReturn(testOnReturn); //是否缓存preparedStatement，也就是PSCache，oracle设为true，mysql设为false。分库分表较多推荐设置为false druid.setPoolPreparedStatements(poolPreparedStatements); // 打开PSCache时，指定每个连接上PSCache的大小 druid.setMaxPoolPreparedStatementPerConnectionSize(maxPoolPreparedStatementPerConnectionSize); return druid; } @Bean(name = &quot;backTransactionManager&quot;) public DataSourceTransactionManager backTransactionManager() throws SQLException { return new DataSourceTransactionManager(backDataSource()); } @Bean(name = &quot;backSqlSessionFactory&quot;) public SqlSessionFactory backSqlSessionFactory(@Qualifier(&quot;backDataSource&quot;) DataSource backDataSource) throws Exception { final SqlSessionFactoryBean sessionFactory = new SqlSessionFactoryBean(); sessionFactory.setDataSource(backDataSource); sessionFactory.setMapperLocations(new PathMatchingResourcePatternResolver() .getResources(BackDataBaseConfig.MAPPER_LOCATION)); return sessionFactory.getObject(); } } 访问Druid http://localhost:8080/druid/login.html springBoot,mybatis,druid主从备份 项目架构 pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.4.5&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.example&lt;/groupId&gt; &lt;artifactId&gt;datasources&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;datasources&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.45&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; application.yml spring: datasource: #使用druid连接池 type: com.alibaba.druid.pool.DruidDataSource # 自定义的主数据源配置信息 primary: datasource: #druid相关配置 druid: #监控统计拦截的filters filters: stat driverClassName: com.mysql.jdbc.Driver #配置基本属性 url: jdbc:mysql://127.0.0.1:3306/primary_database?useUnicode=true&amp;characterEncoding=UTF-8&amp;allowMultiQueries=true&amp;autoReconnect=true&amp;useSSL=false username: root password: 123456 #配置初始化大小/最小/最大 initialSize: 1 minIdle: 1 maxActive: 20 #获取连接等待超时时间 maxWait: 60000 #间隔多久进行一次检测，检测需要关闭的空闲连接 timeBetweenEvictionRunsMillis: 60000 #一个连接在池中最小生存的时间 minEvictableIdleTimeMillis: 300000 validationQuery: SELECT 'x' testWhileIdle: true testOnBorrow: false testOnReturn: false #打开PSCache，并指定每个连接上PSCache的大小。oracle设为true，mysql设为false。分库分表较多推荐设置为false poolPreparedStatements: false maxPoolPreparedStatementPerConnectionSize: 20 # 自定义的从数据源配置信息 back: datasource: #druid相关配置 druid: #监控统计拦截的filters filters: stat driverClassName: com.mysql.jdbc.Driver #配置基本属性 url: jdbc:mysql://127.0.0.1:3306/back_database?useUnicode=true&amp;characterEncoding=UTF-8&amp;allowMultiQueries=true&amp;autoReconnect=true&amp;useSSL=false username: root password: 123456 #配置初始化大小/最小/最大 initialSize: 1 minIdle: 1 maxActive: 20 #获取连接等待超时时间 maxWait: 60000 #间隔多久进行一次检测，检测需要关闭的空闲连接 timeBetweenEvictionRunsMillis: 60000 #一个连接在池中最小生存的时间 minEvictableIdleTimeMillis: 300000 validationQuery: SELECT 'x' testWhileIdle: true testOnBorrow: false testOnReturn: false #打开PSCache，并指定每个连接上PSCache的大小。oracle设为true，mysql设为false。分库分表较多推荐设置为false poolPreparedStatements: false maxPoolPreparedStatementPerConnectionSize: 20 DruidConfig @Configuration public class DruidConfig { public final static String MAPPER_XML_PATH = &quot;classpath:mapper/*.xml&quot;; @ConfigurationProperties(prefix = &quot;master.datasource.druid&quot;) @Bean(name = &quot;masterDataSource&quot;) public DataSource masterDataSource() { return new DruidDataSource(); } @Bean public PlatformTransactionManager txManager(DataSource dynamicDataSource) { return new DataSourceTransactionManager(dynamicDataSource); } @ConfigurationProperties(prefix = &quot;slave.datasource.druid&quot;) @Bean public DataSource slaveDataSource(){ return new DruidDataSource(); } @Bean public DynamicDataSource dynamicDataSource(){ DynamicDataSource dynamicDataSource=new DynamicDataSource(); Map&lt;Object,Object&gt; map=new HashMap&lt;&gt;(); map.put(DbUtil.master,masterDataSource()); map.put(DbUtil.slave,slaveDataSource()); dynamicDataSource.setDefaultTargetDataSource(masterDataSource()); dynamicDataSource.setTargetDataSources(map); return dynamicDataSource; } @Bean public SqlSessionFactoryBean sqlSessionFactoryBean(DataSource dynamicDataSource) throws IOException { SqlSessionFactoryBean sqlSessionFactory = new SqlSessionFactoryBean(); sqlSessionFactory.setDataSource(dynamicDataSource); sqlSessionFactory.setMapperLocations(new PathMatchingResourcePatternResolver().getResources(MAPPER_XML_PATH)); return sqlSessionFactory; } @Bean public SqlSessionTemplate sqlSessionTemplate(SqlSessionFactoryBean sqlSessionFactoryBean) throws Exception { SqlSessionTemplate sqlSessionTemplate = new SqlSessionTemplate(sqlSessionFactoryBean.getObject()); return sqlSessionTemplate; } @Bean public ServletRegistrationBean statViewServlet(){ ServletRegistrationBean&lt;StatViewServlet&gt; bean = new ServletRegistrationBean&lt;StatViewServlet&gt;( new StatViewServlet(), &quot;/druid/*&quot; ); Map&lt;String,String&gt; iniParms=new HashMap&lt;&gt;( ); iniParms.put( &quot;loginUsername&quot;,&quot;admin&quot; );//登录druid的用户名 iniParms.put( &quot;loginPassword&quot;,&quot;123456&quot; );//登录druid的密码 iniParms.put(&quot;allow&quot;,&quot;&quot;);//默认允许所有 //iniParms.put( &quot;deny&quot;,&quot;192.168.***.***&quot; );//拒绝的ip地址 bean.setInitParameters( iniParms ); return bean; } @Bean public FilterRegistrationBean webStatFilter(){ FilterRegistrationBean bean= new FilterRegistrationBean(); bean.setFilter(new WebStatFilter()); Map&lt;String,String&gt; iniParms=new HashMap&lt;&gt;(); iniParms.put( &quot;excliusions&quot;, &quot;*.js,*.gif,*.jpg,*.bmp,*.png,*.css,*.ico,/druid/*&quot;);//使静态文件访问，还有/druid/* 的访问不被拦截 bean.setInitParameters( iniParms ); bean.setUrlPatterns( Arrays.asList(&quot;/*&quot;)); return bean; } }@Configuration public class DruidConfig { @Bean public ServletRegistrationBean statViewServlet(){ ServletRegistrationBean&lt;StatViewServlet&gt; bean = new ServletRegistrationBean&lt;StatViewServlet&gt;( new StatViewServlet(), &quot;/druid/*&quot; ); Map&lt;String,String&gt; iniParms=new HashMap&lt;&gt;( ); iniParms.put( &quot;loginUsername&quot;,&quot;admin&quot; );//登录druid的用户名 iniParms.put( &quot;loginPassword&quot;,&quot;123456&quot; );//登录druid的密码 iniParms.put(&quot;allow&quot;,&quot;&quot;);//默认允许所有 //iniParms.put( &quot;deny&quot;,&quot;192.168.***.***&quot; );//拒绝的ip地址 bean.setInitParameters( iniParms ); return bean; } @Bean public FilterRegistrationBean webStatFilter(){ FilterRegistrationBean bean= new FilterRegistrationBean(); bean.setFilter(new WebStatFilter()); Map&lt;String,String&gt; iniParms=new HashMap&lt;&gt;(); iniParms.put( &quot;excliusions&quot;, &quot;*.js,*.gif,*.jpg,*.bmp,*.png,*.css,*.ico,/druid/*&quot;);//使静态文件访问，还有/druid/* 的访问不被拦截 bean.setInitParameters( iniParms ); bean.setUrlPatterns( Arrays.asList(&quot;/*&quot;)); return bean; } } MasterDataSource /** * 自定义主数据库注解 */ @Target(ElementType.METHOD) @Retention(RetentionPolicy.RUNTIME) public @interface MasterDataSource { String value() default &quot;&quot;; } DatabaseAOP /** * aop从dao层判断使用哪个数据库 * @MasterDataSource标识使用主数据库 * 不标识使用从数据库 */ @Aspect @Component public class DatabaseAOP { @Pointcut(value = &quot;execution(* com.example.datasources.dao..*.*(..))&quot;) public void pointCut() { } @Before(&quot;pointCut()&quot;) public void before(JoinPoint joinPoint) { MethodSignature methodSignature = (MethodSignature) joinPoint.getSignature(); Method method = methodSignature.getMethod(); boolean isExist = method.isAnnotationPresent(MasterDataSource.class); if (!isExist) { DbUtil.setDb(DbUtil.slave); return; } DbUtil.setDb(DbUtil.master); } } DbUtil /** * 存储当前线程使用数据库标识 */ public class DbUtil { public static String master=&quot;master&quot;; public static String slave=&quot;slave&quot;; private static final ThreadLocal&lt;String&gt; threadLocal=new ThreadLocal(); public static void setDb(String db){ threadLocal.set(db); } public static String getDb(){ return threadLocal.get(); } } DynamicDataSource /** * spring的jdbc提供了动态数据源的入口 * 继承AbstractRoutingDataSource覆盖determineCurrentLookupKey()方法 返回当前使用数据库 */ @Slf4j public class DynamicDataSource extends AbstractRoutingDataSource { @Override protected Object determineCurrentLookupKey() { log.info(&quot;当前使用数据库：{}&quot;, DbUtil.getDb()); return DbUtil.getDb(); } } CityMapper /** * springBoot启动入口DatasourcesApplication注解@MapperScan(basePackages = &quot;com.example.datasources.dao&quot;) * 起到和@Mapper一样的作用 * 添加了@Mapper注解之后这个接口在编译时会生成相应的实现类，不再需要写mapper映射文件，可以按下方@Select(&quot;select * from city where city_name like CONCAT('%', #{cityName},'%')&quot;)实现功能，@Autowired注解也需要通过@Mapper注解实现接口的动态代理实现类 */ @Mapper @Component public interface CityMapper { /** * @MasterDataSource指定使用主数据库新增 * 不指定默认使用从数据库 */ @MasterDataSource void insertCity(City city); /** * 根据城市名称，查询城市信息 * * @param cityName 城市名 * 默认使用从数据库 */ //@Select(&quot;select * from city where city_name like CONCAT('%', #{cityName},'%')&quot;) List&lt;City&gt; selectByName(@Param(&quot;cityName&quot;) String cityName); } CityMapper.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace=&quot;com.example.datasources.dao.CityMapper&quot;&gt; &lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;com.example.datasources.entity.City&quot;&gt; &lt;id column=&quot;id&quot; jdbcType=&quot;INTEGER&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;city_name&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;cityName&quot;/&gt; &lt;/resultMap&gt; &lt;sql id=&quot;Base_Column_List&quot;&gt; id,city_name &lt;/sql&gt; &lt;insert id=&quot;insertCity&quot; parameterType=&quot;com.example.datasources.entity.City&quot;&gt; INSERT into city (id,city_name) VALUES (#{id,jdbcType=INTEGER}, #{cityName,jdbcType=VARCHAR}); &lt;/insert&gt; &lt;select id=&quot;selectByName&quot; resultMap=&quot;BaseResultMap&quot;&gt; select &lt;include refid=&quot;Base_Column_List&quot;/&gt; from city where city_name like CONCAT('%', #{cityName},'%') &lt;/select&gt; &lt;/mapper&gt; ","link":"https://tinaxiawuhao.github.io/post/Ari4Sf8A0/"},{"title":"SpringBoot基础","content":"一、SpringBoot简介 1.1 原有Spring优缺点分析 1.1.1 Spring的优点分析 Spring是Java企业版（Java Enterprise Edition，JEE，也称J2EE）的轻量级代替品。无需开发重量级的Enterprise JavaBean（EJB），Spring为企业级Java开发提供了一种相对简单的方法，通过依赖注入和面向切面编程，用简单的Java对象（Plain Old Java Object，POJO）实现了EJB的功能。 1.1.2 Spring的缺点分析 虽然Spring的组件代码是轻量级的，但它的配置却是重量级的。一开始，Spring用XML配置，而且是很多XML配置。Spring 2.5引入了基于注解的组件扫描，这消除了大量针对应用程序自身组件的显式XML配置。Spring 3.0引入了基于Java的配置，这是一种类型安全的可重构配置方式，可以代替XML。 所有这些配置都代表了开发时的损耗。因为在思考Spring特性配置和解决业务问题之间需要进行思维切换，所以编写配置挤占了编写应用程序逻辑的时间。和所有框架一样，Spring实用，但与此同时它要求的回报也不少。 除此之外，项目的依赖管理也是一件耗时耗力的事情。在环境搭建时，需要分析要导入哪些库的坐标，而且还需要分析导入与之有依赖关系的其他库的坐标，一旦选错了依赖的版本，随之而来的不兼容问题就会严重阻碍项目的开发进度。 1.2 SpringBoot的概述 1.2.1 SpringBoot解决上述Spring的缺点 SpringBoot对上述Spring的缺点进行的改善和优化，基于约定优于配置的思想，可以让开发人员不必在配置与逻辑业务之间进行思维的切换，全身心的投入到逻辑业务的代码编写中，从而大大提高了开发的效率，一定程度上缩短了项目周期。 1.2.2 SpringBoot的特点 为基于Spring的开发提供更快的入门体验 开箱即用，没有代码生成，也无需XML配置。同时也可以修改默认值来满足特定的需求 提供了一些大型项目中常见的非功能性特性，如嵌入式服务器、安全、指标，健康检测、外部配置等 SpringBoot不是对Spring功能上的增强，而是提供了一种快速使用Spring的方式 1.2.3 SpringBoot的核心功能 起步依赖 起步依赖本质上是一个Maven项目对象模型（Project Object Model，POM），定义了对其他库的传递依赖，这些东西加在一起即支持某项功能。 简单的说，起步依赖就是将具备某种功能的坐标打包到一起，并提供一些默认的功能。 自动配置 Spring Boot的自动配置是一个运行时（更准确地说，是应用程序启动时）的过程，考虑了众多因素，才决定Spring配置应该用哪个，不该用哪个。该过程是Spring自动完成的。 ​ 注意：起步依赖和自动配置的原理剖析会在第三章《SpringBoot原理分析》进行详细讲解 二、SpringBoot快速入门 2.1 代码实现 2.1.1 创建Maven工程 使用idea工具创建一个maven工程，该工程为普通的java工程即可 2.1.2 添加SpringBoot的起步依赖 SpringBoot要求，项目要继承SpringBoot的起步依赖spring-boot-starter-parent &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;/parent&gt; SpringBoot要集成SpringMVC进行Controller的开发，所以项目要导入web的启动依赖 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 2.1.3 编写SpringBoot引导类 要通过SpringBoot提供的引导类起步SpringBoot才可以进行访问 package com.itheima; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class MySpringBootApplication { public static void main(String[] args) { SpringApplication.run(MySpringBootApplication.class); } } 2.1.4 编写Controller 在引导类MySpringBootApplication同级包或者子级包中创建QuickStartController package com.itheima.controller; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.ResponseBody; @Controller public class QuickStartController { @RequestMapping(&quot;/quick&quot;) @ResponseBody public String quick(){ return &quot;springboot 访问成功!&quot;; } } 2.1.5 测试 执行SpringBoot起步类的主方法，控制台打印日志如下： . ____ _ __ _ _ /\\\\ / ___'_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.0.1.RELEASE) 2018-05-08 14:29:59.714 INFO 5672 --- [ main] com.itheima.MySpringBootApplication : Starting MySpringBootApplication on DESKTOP-RRUNFUH with PID 5672 (C:\\Users\\muzimoo\\IdeaProjects\\IdeaTest\\springboot_quick\\target\\classes started by muzimoo in C:\\Users\\muzimoo\\IdeaProjects\\IdeaTest) ... ... ... o.s.w.s.handler.SimpleUrlHandlerMapping : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler] 2018-05-08 14:30:03.126 INFO 5672 --- [ main] o.s.j.e.a.AnnotationMBeanExporter : Registering beans for JMX exposure on startup 2018-05-08 14:30:03.196 INFO 5672 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8080 (http) with context path '' 2018-05-08 14:30:03.206 INFO 5672 --- [ main] com.itheima.MySpringBootApplication : Started MySpringBootApplication in 4.252 seconds (JVM running for 5.583) 通过日志发现，Tomcat started on port(s): 8080 (http) with context path '' tomcat已经起步，端口监听8080，web应用的虚拟工程名称为空 打开浏览器访问url地址为：http://localhost:8080/quick 2.2 快速入门解析 2.2.2 SpringBoot代码解析 @SpringBootApplication：标注SpringBoot的启动类，该注解具备多种功能（后面详细剖析） SpringApplication.run(MySpringBootApplication.class) 代表运行SpringBoot的启动类，参数为SpringBoot启动类的字节码对象 2.2.3 SpringBoot工程热部署 我们在开发中反复修改类、页面等资源，每次修改后都是需要重新启动才生效，这样每次启动都很麻烦，浪费了大量的时间，我们可以在修改代码后不重启就能生效，在 pom.xml 中添加如下配置就可以实现这样的功能，我们称之为热部署。 &lt;!--热部署配置--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt; 注意：IDEA进行SpringBoot热部署失败原因 出现这种情况，并不是热部署配置问题，其根本原因是因为Intellij IEDA默认情况下不会自动编译，需要对IDEA进行自动编译的设置，如下： 然后 Shift+Ctrl+Alt+/，选择Registry 2.2.4 使用idea快速创建SpringBoot项目 通过idea快速创建的SpringBoot项目的pom.xml中已经导入了我们选择的web的起步依赖的坐标 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;springboot_quick2&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;springboot_quick2&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;9&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; 可以使用快速入门的方式创建Controller进行访问，此处不再赘述 三、SpringBoot原理分析 3.1 起步依赖原理分析 3.1.1 分析spring-boot-starter-parent 按住Ctrl点击pom.xml中的spring-boot-starter-parent，跳转到了spring-boot-starter-parent的pom.xml，xml配置如下（只摘抄了部分重点配置）： &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;relativePath&gt;../../spring-boot-dependencies&lt;/relativePath&gt; &lt;/parent&gt; 按住Ctrl点击pom.xml中的spring-boot-starter-dependencies，跳转到了spring-boot-starter-dependencies的pom.xml，xml配置如下（只摘抄了部分重点配置）： &lt;properties&gt; &lt;activemq.version&gt;5.15.3&lt;/activemq.version&gt; &lt;antlr2.version&gt;2.7.7&lt;/antlr2.version&gt; &lt;appengine-sdk.version&gt;1.9.63&lt;/appengine-sdk.version&gt; &lt;artemis.version&gt;2.4.0&lt;/artemis.version&gt; &lt;aspectj.version&gt;1.8.13&lt;/aspectj.version&gt; &lt;assertj.version&gt;3.9.1&lt;/assertj.version&gt; &lt;atomikos.version&gt;4.0.6&lt;/atomikos.version&gt; &lt;bitronix.version&gt;2.1.4&lt;/bitronix.version&gt; &lt;build-helper-maven-plugin.version&gt;3.0.0&lt;/build-helper-maven-plugin.version&gt; &lt;byte-buddy.version&gt;1.7.11&lt;/byte-buddy.version&gt; ... ... ... &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-test&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; ... ... ... &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.jetbrains.kotlin&lt;/groupId&gt; &lt;artifactId&gt;kotlin-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${kotlin.version}&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.jooq&lt;/groupId&gt; &lt;artifactId&gt;jooq-codegen-maven&lt;/artifactId&gt; &lt;version&gt;${jooq.version}&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;/plugin&gt; ... ... ... &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;/build&gt; 从上面的spring-boot-starter-dependencies的pom.xml中我们可以发现，一部分坐标的版本、依赖管理、插件管理已经定义好，所以我们的SpringBoot工程继承spring-boot-starter-parent后已经具备版本锁定等配置了。所以起步依赖的作用就是进行依赖的传递。 3.1.2 分析spring-boot-starter-web 按住Ctrl点击pom.xml中的spring-boot-starter-web，跳转到了spring-boot-starter-web的pom.xml，xml配置如下（只摘抄了部分重点配置）： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot; xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starters&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;name&gt;Spring Boot Web Starter&lt;/name&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-json&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate.validator&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt; &lt;version&gt;6.0.9.Final&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;5.0.5.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;5.0.5.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 从上面的spring-boot-starter-web的pom.xml中我们可以发现，spring-boot-starter-web就是将web开发要使用的spring-web、spring-webmvc等坐标进行了“打包”，这样我们的工程只要引入spring-boot-starter-web起步依赖的坐标就可以进行web开发了，同样体现了依赖传递的作用。 3.2 自动配置原理解析 按住Ctrl点击查看启动类MySpringBootApplication上的注解@SpringBootApplication @SpringBootApplication public class MySpringBootApplication { public static void main(String[] args) { SpringApplication.run(MySpringBootApplication.class); } } 注解@SpringBootApplication的源码 @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) }) public @interface SpringBootApplication { /** * Exclude specific auto-configuration classes such that they will never be applied. * @return the classes to exclude */ @AliasFor(annotation = EnableAutoConfiguration.class) Class&lt;?&gt;[] exclude() default {}; ... ... ... } @SpringBootConfiguration：等同与@Configuration，既标注该类是Spring的一个配置类 @EnableAutoConfiguration：SpringBoot自动配置功能开启 按住Ctrl点击查看注解@EnableAutoConfiguration @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @AutoConfigurationPackage @Import(AutoConfigurationImportSelector.class) public @interface EnableAutoConfiguration { ... ... ... } @Import(AutoConfigurationImportSelector.class) 导入了AutoConfigurationImportSelector类 按住Ctrl点击查看AutoConfigurationImportSelector源码 public String[] selectImports(AnnotationMetadata annotationMetadata) { ... ... ... List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes); configurations = removeDuplicates(configurations); Set&lt;String&gt; exclusions = getExclusions(annotationMetadata, attributes); checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations = filter(configurations, autoConfigurationMetadata); fireAutoConfigurationImportEvents(configurations, exclusions); return StringUtils.toStringArray(configurations); } protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) { List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames( getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()); return configurations; } SpringFactoriesLoader.loadFactoryNames 方法的作用就是从META-INF/spring.factories文件中读取指定类对应的类名称列表 spring.factories 文件中有关自动配置的配置信息如下： ... ... ... org.springframework.boot.autoconfigure.web.reactive.function.client.WebClientAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.DispatcherServletAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.ServletWebServerFactoryAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.error.ErrorMvcAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.HttpEncodingAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.MultipartAutoConfiguration,\\ ... ... ... 上面配置文件存在大量的以Configuration为结尾的类名称，这些类就是存有自动配置信息的类，而SpringApplication在获取这些类名后再加载 我们以ServletWebServerFactoryAutoConfiguration为例来分析源码： @Configuration @AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE) @ConditionalOnClass(ServletRequest.class) @ConditionalOnWebApplication(type = Type.SERVLET) @EnableConfigurationProperties(ServerProperties.class) @Import({ ServletWebServerFactoryAutoConfiguration.BeanPostProcessorsRegistrar.class, ServletWebServerFactoryConfiguration.EmbeddedTomcat.class, ServletWebServerFactoryConfiguration.EmbeddedJetty.class, ServletWebServerFactoryConfiguration.EmbeddedUndertow.class }) public class ServletWebServerFactoryAutoConfiguration { ... ... ... } @EnableConfigurationProperties(ServerProperties.class) 代表加载ServerProperties服务器配置属性类 进入ServerProperties.class源码如下： @ConfigurationProperties(prefix = &quot;server&quot;, ignoreUnknownFields = true) public class ServerProperties { /** * Server HTTP port. */ private Integer port; /** * Network address to which the server should bind. */ private InetAddress address; ... ... ... } prefix = &quot;server&quot; 表示SpringBoot配置文件中的前缀，SpringBoot会将配置文件中以server开始的属性映射到该类的字段中。映射关系如下： 四、SpringBoot的配置文件 4.1 SpringBoot配置文件类型 4.1.1 SpringBoot配置文件类型和作用 SpringBoot是基于约定的，所以很多配置都有默认值，但如果想使用自己的配置替换默认配置的话，就可以使用application.properties或者application.yml（application.yaml）进行配置。 SpringBoot默认会从Resources目录下加载application.properties或application.yml（application.yaml）文件 其中，application.properties文件是键值对类型的文件，之前一直在使用，所以此处不在对properties文件的格式进行阐述。除了properties文件外，SpringBoot还可以使用yml文件进行配置，下面对yml文件进行讲解。 4.1.2 application.yml配置文件 4.1.2.1 yml配置文件简介 YML文件格式是YAML (YAML Aint Markup Language)编写的文件格式，YAML是一种直观的能够被电脑识别的的数据数据序列化格式，并且容易被人类阅读，容易和脚本语言交互的，可以被支持YAML库的不同的编程语言程序导入，比如： C/C++, Ruby, Python, Java, Perl, C#, PHP等。YML文件是以数据为核心的，比传统的xml方式更加简洁。 YML文件的扩展名可以使用.yml或者.yaml。 4.1.2.2 yml配置文件的语法 4.1.2.2.1 配置普通数据 语法： key: value 示例代码： name: haohao 注意：value之前有一个空格 4.1.2.2.2 配置对象数据 语法： ​ key: ​ key1: value1 ​ key2: value2 ​ 或者： ​ key: {key1: value1,key2: value2} 示例代码： person: name: haohao age: 31 addr: beijing #或者 person: {name: haohao,age: 31,addr: beijing} 注意：key1前面的空格个数不限定，在yml语法中，相同缩进代表同一个级别 4.1.2.2.3 配置数组（List、Set）数据 语法： ​ key: ​ - value1 ​ - value2 或者： ​ key: [value1,value2] 示例代码： city: - beijing - tianjin - shanghai - chongqing #或者 city: [beijing,tianjin,shanghai,chongqing] #集合中的元素是对象形式 student: - name: zhangsan age: 18 score: 100 - name: lisi age: 28 score: 88 - name: wangwu age: 38 score: 90 注意：value1与之间的 - 之间存在一个空格 4.1.3 SpringBoot配置信息的查询 上面提及过，SpringBoot的配置文件，主要的目的就是对配置信息进行修改的，但在配置时的key从哪里去查询呢？我们可以查阅SpringBoot的官方文档,文档URL：https://docs.spring.io/spring-boot/docs/2.0.1.RELEASE/reference/htmlsingle/#common-application-properties 常用的配置摘抄如下： # QUARTZ SCHEDULER (QuartzProperties) spring.quartz.jdbc.initialize-schema=embedded # Database schema initialization mode. spring.quartz.jdbc.schema=classpath:org/quartz/impl/jdbcjobstore/tables_@@platform@@.sql # Path to the SQL file to use to initialize the database schema. spring.quartz.job-store-type=memory # Quartz job store type. spring.quartz.properties.*= # Additional Quartz Scheduler properties. # ---------------------------------------- # WEB PROPERTIES # ---------------------------------------- # EMBEDDED SERVER CONFIGURATION (ServerProperties) server.port=8080 # Server HTTP port. server.servlet.context-path= # Context path of the application. server.servlet.path=/ # Path of the main dispatcher servlet. # HTTP encoding (HttpEncodingProperties) spring.http.encoding.charset=UTF-8 # Charset of HTTP requests and responses. Added to the &quot;Content-Type&quot; header if not set explicitly. # JACKSON (JacksonProperties) spring.jackson.date-format= # Date format string or a fully-qualified date format class name. For instance, `yyyy-MM-dd HH:mm:ss`. # SPRING MVC (WebMvcProperties) spring.mvc.servlet.load-on-startup=-1 # Load on startup priority of the dispatcher servlet. spring.mvc.static-path-pattern=/** # Path pattern used for static resources. spring.mvc.view.prefix= # Spring MVC view prefix. spring.mvc.view.suffix= # Spring MVC view suffix. # DATASOURCE (DataSourceAutoConfiguration &amp; DataSourceProperties) spring.datasource.driver-class-name= # Fully qualified name of the JDBC driver. Auto-detected based on the URL by default. spring.datasource.password= # Login password of the database. spring.datasource.url= # JDBC URL of the database. spring.datasource.username= # Login username of the database. # JEST (Elasticsearch HTTP client) (JestProperties) spring.elasticsearch.jest.password= # Login password. spring.elasticsearch.jest.proxy.host= # Proxy host the HTTP client should use. spring.elasticsearch.jest.proxy.port= # Proxy port the HTTP client should use. spring.elasticsearch.jest.read-timeout=3s # Read timeout. spring.elasticsearch.jest.username= # Login username. 我们可以通过配置application.poperties 或者 application.yml 来修改SpringBoot的默认配置 例如： application.properties文件 server.port=8888 server.servlet.context-path=demo application.yml文件 server: port: 8888 servlet: context-path: /demo 4.2 配置文件与配置类的属性映射方式 4.2.1 使用注解@Value映射 我们可以通过@Value注解将配置文件中的值映射到一个Spring管理的Bean的字段上 例如： application.properties配置如下： person: name: zhangsan age: 18 或者，application.yml配置如下： person: name: zhangsan age: 18 实体Bean代码如下： @Controller public class QuickStartController { @Value(&quot;${person.name}&quot;) private String name; @Value(&quot;${person.age}&quot;) private Integer age; @RequestMapping(&quot;/quick&quot;) @ResponseBody public String quick(){ return &quot;springboot 访问成功! name=&quot;+name+&quot;,age=&quot;+age; } } 浏览器访问地址：http://localhost:8080/quick 结果如下： 4.2.2 使用注解@ConfigurationProperties映射 通过注解@ConfigurationProperties(prefix=&quot;配置文件中的key的前缀&quot;)可以将配置文件中的配置自动与实体进行映射 application.properties配置如下： person: name: zhangsan age: 18 或者，application.yml配置如下： person: name: zhangsan age: 18 实体Bean代码如下： @Controller @ConfigurationProperties(prefix = &quot;person&quot;) public class QuickStartController { private String name; private Integer age; @RequestMapping(&quot;/quick&quot;) @ResponseBody public String quick(){ return &quot;springboot 访问成功! name=&quot;+name+&quot;,age=&quot;+age; } public void setName(String name) { this.name = name; } public void setAge(Integer age) { this.age = age; } } 浏览器访问地址：http://localhost:8080/quick 结果如下： 注意：使用@ConfigurationProperties方式可以进行配置文件与实体字段的自动映射，但需要字段必须提供set方法才可以，而使用@Value注解修饰的字段不需要提供set方法 五、SpringBoot与整合其他技术 5.1 SpringBoot整合Mybatis 5.1.1 添加Mybatis的起步依赖 &lt;!--mybatis起步依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt; &lt;/dependency&gt; 5.1.2 添加数据库驱动坐标 &lt;!-- MySQL连接驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; 5.1.3 添加数据库连接信息 在application.properties中添加数据量的连接信息 #DB Configuration: spring.datasource.driverClassName=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&amp;characterEncoding=utf8 spring.datasource.username=root spring.datasource.password=root 5.1.4 创建user表 在test数据库中创建user表 -- ---------------------------- -- Table structure for `user` -- ---------------------------- DROP TABLE IF EXISTS `user`; CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `username` varchar(50) DEFAULT NULL, `password` varchar(50) DEFAULT NULL, `name` varchar(50) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8; -- ---------------------------- -- Records of user -- ---------------------------- INSERT INTO `user` VALUES ('1', 'zhangsan', '123', '张三'); INSERT INTO `user` VALUES ('2', 'lisi', '123', '李四'); 5.1.5 创建实体Bean public class User { // 主键 private Long id; // 用户名 private String username; // 密码 private String password; // 姓名 private String name; //此处省略getter和setter方法 .. .. } 5.1.6 编写Mapper @Mapper public interface UserMapper { public List&lt;User&gt; queryUserList(); } 注意：@Mapper标记该类是一个mybatis的mapper接口，可以被spring boot自动扫描到spring上下文中 5.1.7 配置Mapper映射文件 在src\\main\\resources\\mapper路径下加入UserMapper.xml配置文件&quot; &lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot; ?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot; &gt; &lt;mapper namespace=&quot;com.itheima.mapper.UserMapper&quot;&gt; &lt;select id=&quot;queryUserList&quot; resultType=&quot;user&quot;&gt; select * from user &lt;/select&gt; &lt;/mapper&gt; 5.1.8 在application.properties中添加mybatis的信息 #spring集成Mybatis环境 #pojo别名扫描包 mybatis.type-aliases-package=com.itheima.domain #加载Mybatis映射文件 mybatis.mapper-locations=classpath:mapper/*Mapper.xml 5.1.9 编写测试Controller @Controller public class MapperController { @Autowired private UserMapper userMapper; @RequestMapping(&quot;/queryUser&quot;) @ResponseBody public List&lt;User&gt; queryUser(){ List&lt;User&gt; users = userMapper.queryUserList(); return users; } } 5.1.10 测试 5.2 SpringBoot整合Junit 5.2.1 添加Junit的起步依赖 &lt;!--测试的起步依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; 5.2.2 编写测试类 package com.itheima.test; import com.itheima.MySpringBootApplication; import com.itheima.domain.User; import com.itheima.mapper.UserMapper; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.test.context.junit4.SpringRunner; import java.util.List; @RunWith(SpringRunner.class) @SpringBootTest(classes = MySpringBootApplication.class) public class MapperTest { @Autowired private UserMapper userMapper; @Test public void test() { List&lt;User&gt; users = userMapper.queryUserList(); System.out.println(users); } } 其中， SpringRunner继承自SpringJUnit4ClassRunner，使用哪一个Spring提供的测试测试引擎都可以 public final class SpringRunner extends SpringJUnit4ClassRunner @SpringBootTest的属性指定的是引导类的字节码对象 5.2.3 控制台打印信息 5.3 SpringBoot整合Spring Data JPA 5.3.1 添加Spring Data JPA的起步依赖 &lt;!-- springBoot JPA的起步依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; 5.3.2 添加数据库驱动依赖 &lt;!-- MySQL连接驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; 5.3.3 在application.properties中配置数据库和jpa的相关属性 #DB Configuration: spring.datasource.driverClassName=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&amp;characterEncoding=utf8 spring.datasource.username=root spring.datasource.password=root #JPA Configuration: spring.jpa.database=MySQL spring.jpa.show-sql=true spring.jpa.generate-ddl=true spring.jpa.hibernate.ddl-auto=update spring.jpa.hibernate.naming_strategy=org.hibernate.cfg.ImprovedNamingStrategy 5.3.4 创建实体配置实体 @Entity public class User { // 主键 @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; // 用户名 private String username; // 密码 private String password; // 姓名 private String name; //此处省略setter和getter方法... ... } 5.3.5 编写UserRepository public interface UserRepository extends JpaRepository&lt;User,Long&gt;{ public List&lt;User&gt; findAll(); } 5.3.6 编写测试类 @RunWith(SpringRunner.class) @SpringBootTest(classes=MySpringBootApplication.class) public class JpaTest { @Autowired private UserRepository userRepository; @Test public void test(){ List&lt;User&gt; users = userRepository.findAll(); System.out.println(users); } } 5.3.7 控制台打印信息 注意：如果是jdk9，执行报错如下： 原因：jdk缺少相应的jar 解决方案：手动导入对应的maven坐标，如下： &lt;!--jdk9需要导入如下坐标--&gt; &lt;dependency&gt; &lt;groupId&gt;javax.xml.bind&lt;/groupId&gt; &lt;artifactId&gt;jaxb-api&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; 5.4 SpringBoot整合Redis 5.4.1 添加redis的起步依赖 &lt;!-- 配置使用redis启动器 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; 5.4.2 配置redis的连接信息 #Redis spring.redis.host=127.0.0.1 spring.redis.port=6379 5.4.3 注入RedisTemplate测试redis操作 @RunWith(SpringRunner.class) @SpringBootTest(classes = SpringbootJpaApplication.class) public class RedisTest { @Autowired private UserRepository userRepository; @Autowired private RedisTemplate&lt;String, String&gt; redisTemplate; @Test public void test() throws JsonProcessingException { //从redis缓存中获得指定的数据 String userListData = redisTemplate.boundValueOps(&quot;user.findAll&quot;).get(); //如果redis中没有数据的话 if(null==userListData){ //查询数据库获得数据 List&lt;User&gt; all = userRepository.findAll(); //转换成json格式字符串 ObjectMapper om = new ObjectMapper(); userListData = om.writeValueAsString(all); //将数据存储到redis中，下次在查询直接从redis中获得数据，不用在查询数据库 redisTemplate.boundValueOps(&quot;user.findAll&quot;).set(userListData); System.out.println(&quot;===============从数据库获得数据===============&quot;); }else{ System.out.println(&quot;===============从redis缓存中获得数据===============&quot;); } System.out.println(userListData); } } ","link":"https://tinaxiawuhao.github.io/post/RosSxJXYD/"},{"title":"SpringBoot自动装配原理","content":"SpringBoot自动配置 从代码里看项目SpringBoot的项目启动类只有一个注解@SpringBootApplication和一个run方法。 @SpringBootApplication public class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } } 直接看@SpringBootApplication的代码： @Target({ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan( excludeFilters = {@Filter( type = FilterType.CUSTOM, classes = {TypeExcludeFilter.class} ), @Filter( type = FilterType.CUSTOM, classes = {AutoConfigurationExcludeFilter.class} )} ) public @interface SpringBootApplication { @AliasFor( annotation = EnableAutoConfiguration.class, attribute = &quot;exclude&quot; ) Class&lt;?&gt;[] exclude() default {}; @AliasFor( annotation = EnableAutoConfiguration.class, attribute = &quot;excludeName&quot; ) String[] excludeName() default {}; @AliasFor( annotation = ComponentScan.class, attribute = &quot;basePackages&quot; ) String[] scanBasePackages() default {}; @AliasFor( annotation = ComponentScan.class, attribute = &quot;basePackageClasses&quot; ) Class&lt;?&gt;[] scanBasePackageClasses() default {}; } @SpringBootApplication：包含了@SpringBootConfiguration（打开是@Configuration），@EnableAutoConfiguration，@ComponentScan注解。 @Configuration JavaConfig形式的Spring Ioc容器的配置类使用的那个@Configuration，SpringBoot社区推荐使用基于JavaConfig的配置形式，所以，这里的启动类标注了@Configuration之后，本身其实也是一个IoC容器的配置类。 对比一下传统XML方式和config配置方式的区别： XML声明和定义配置方式： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-3.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-3.0.xsd &quot;&gt; &lt;bean id=&quot;app&quot; class=&quot;com...&quot; /&gt; 用一个过滤器举例，JavaConfig的配置方式是这样： @Configuration public class DruidConfiguration { @Bean public FilterRegistrationBean statFilter(){ //创建过滤器 FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(new WebStatFilter()); //设置过滤器过滤路径 filterRegistrationBean.addUrlPatterns(&quot;/*&quot;); //忽略过滤的形式 filterRegistrationBean.addInitParameter(&quot;exclusions&quot;,&quot;*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid/*&quot;); return filterRegistrationBean; } } 任何一个标注了@Configuration的Java类定义都是一个JavaConfig配置类。 任何一个标注了@Bean的方法，其返回值将作为一个bean定义注册到Spring的IoC容器，方法名将默认成该bean定义的id。 @ComponentScan @ComponentScan对应XML配置中的元素，@ComponentScan的功能其实就是自动扫描并加载符合条件的组件（比如@Component和@Repository等）或者bean定义，最终将这些bean定义加载到IoC容器中。 我们可以通过basePackages等属性来细粒度的定制@ComponentScan自动扫描的范围，如果不指定，则默认Spring框架实现会从声明@ComponentScan所在类的package进行扫描。 注：所以SpringBoot的启动类最好是放在root package下，因为默认不指定basePackages。 @EnableAutoConfiguration （核心内容）看英文意思就是自动配置，概括一下就是，借助@Import的帮助，将所有符合自动配置条件的bean定义加载到IoC容器。 @Target({ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @AutoConfigurationPackage @Import({EnableAutoConfigurationImportSelector.class}) public @interface EnableAutoConfiguration { String ENABLED_OVERRIDE_PROPERTY = &quot;spring.boot.enableautoconfiguration&quot;; Class&lt;?&gt;[] exclude() default {}; String[] excludeName() default {}; } 里面最关键的是@Import(EnableAutoConfigurationImportSelector.class)，借助EnableAutoConfigurationImportSelector，@EnableAutoConfiguration可以帮助SpringBoot应用将所有符合条件的@Configuration配置都加载到当前SpringBoot创建并使用的IoC容器。该配置模块的主要使用到了SpringFactoriesLoader。 SpringFactoriesLoader详解 SpringFactoriesLoader为Spring工厂加载器，该对象提供了loadFactoryNames方法，入参为factoryClass和classLoader即需要传入工厂类名称和对应的类加载器，方法会根据指定的classLoader，加载该类加器搜索路径下的指定文件，即spring.factories文件，传入的工厂类为接口，而文件中对应的类则是接口的实现类，或最终作为实现类。 public abstract class SpringFactoriesLoader { private static final Log logger = LogFactory.getLog(SpringFactoriesLoader.class); public static final String FACTORIES_RESOURCE_LOCATION = &quot;META-INF/spring.factories&quot;; public SpringFactoriesLoader() { } public static &lt;T&gt; List&lt;T&gt; loadFactories(Class&lt;T&gt; factoryClass, ClassLoader classLoader) { Assert.notNull(factoryClass, &quot;'factoryClass' must not be null&quot;); ClassLoader classLoaderToUse = classLoader; if (classLoader == null) { classLoaderToUse = SpringFactoriesLoader.class.getClassLoader(); } List&lt;String&gt; factoryNames = loadFactoryNames(factoryClass, classLoaderToUse); if (logger.isTraceEnabled()) { logger.trace(&quot;Loaded [&quot; + factoryClass.getName() + &quot;] names: &quot; + factoryNames); } List&lt;T&gt; result = new ArrayList(factoryNames.size()); Iterator var5 = factoryNames.iterator(); while(var5.hasNext()) { String factoryName = (String)var5.next(); result.add(instantiateFactory(factoryName, factoryClass, classLoaderToUse)); } AnnotationAwareOrderComparator.sort(result); return result; } public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryClass, ClassLoader classLoader) { String factoryClassName = factoryClass.getName(); try { Enumeration&lt;URL&gt; urls = classLoader != null ? classLoader.getResources(&quot;META-INF/spring.factories&quot;) : ClassLoader.getSystemResources(&quot;META-INF/spring.factories&quot;); ArrayList result = new ArrayList(); while(urls.hasMoreElements()) { URL url = (URL)urls.nextElement(); Properties properties = PropertiesLoaderUtils.loadProperties(new UrlResource(url)); String factoryClassNames = properties.getProperty(factoryClassName); result.addAll(Arrays.asList(StringUtils.commaDelimitedListToStringArray(factoryClassNames))); } return result; } catch (IOException var8) { throw new IllegalArgumentException(&quot;Unable to load [&quot; + factoryClass.getName() + &quot;] factories from location [&quot; + &quot;META-INF/spring.factories&quot; + &quot;]&quot;, var8); } } private static &lt;T&gt; T instantiateFactory(String instanceClassName, Class&lt;T&gt; factoryClass, ClassLoader classLoader) { try { Class&lt;?&gt; instanceClass = ClassUtils.forName(instanceClassName, classLoader); if (!factoryClass.isAssignableFrom(instanceClass)) { throw new IllegalArgumentException(&quot;Class [&quot; + instanceClassName + &quot;] is not assignable to [&quot; + factoryClass.getName() + &quot;]&quot;); } else { Constructor&lt;?&gt; constructor = instanceClass.getDeclaredConstructor(); ReflectionUtils.makeAccessible(constructor); return constructor.newInstance(); } } catch (Throwable var5) { throw new IllegalArgumentException(&quot;Unable to instantiate factory class: &quot; + factoryClass.getName(), var5); } } } 所以文件中一般为如下图这种一对多的类名集合，获取到这些实现类的类名后，loadFactoryNames方法返回类名集合，方法调用方得到这些集合后，再通过反射获取这些类的类对象、构造方法，最终生成实例。 下图有助于我们形象理解自动配置流程（盗个图） AutoConfigurationImportSelector 继续上面讲的AutoConfigurationImportSelector.class。该类主要关注selectImports方法 public String[] selectImports(AnnotationMetadata annotationMetadata) { if (!this.isEnabled(annotationMetadata)) { return NO_IMPORTS; } else { try { AutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader.loadMetadata(this.beanClassLoader); AnnotationAttributes attributes = this.getAttributes(annotationMetadata); List&lt;String&gt; configurations = this.getCandidateConfigurations(annotationMetadata, attributes); configurations = this.removeDuplicates(configurations); configurations = this.sort(configurations, autoConfigurationMetadata); Set&lt;String&gt; exclusions = this.getExclusions(annotationMetadata, attributes); this.checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations = this.filter(configurations, autoConfigurationMetadata); this.fireAutoConfigurationImportEvents(configurations, exclusions); return (String[])configurations.toArray(new String[configurations.size()]); } catch (IOException var6) { throw new IllegalStateException(var6); } } } 该方法在springboot启动流程——bean实例化前被执行，返回要实例化的类信息列表。如果获取到类信息，spring可以通过类加载器将类加载到jvm中，现在我们已经通过spring-boot的starter依赖方式依赖了我们需要的组件，那么这些组件的类信息在select方法中就可以被获取到。 protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) { List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames(this.getSpringFactoriesLoaderFactoryClass(), this.getBeanClassLoader()); Assert.notEmpty(configurations, &quot;No auto configuration classes found in META-INF/spring.factories. If you are using a custom packaging, make sure that file is correct.&quot;); return configurations; } 方法中的getCandidateConfigurations方法，其返回一个自动配置类的类名列表，方法调用了loadFactoryNames方法，查看该方法 public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryClass, ClassLoader classLoader) { String factoryClassName = factoryClass.getName(); try { Enumeration&lt;URL&gt; urls = classLoader != null ? classLoader.getResources(&quot;META-INF/spring.factories&quot;) : ClassLoader.getSystemResources(&quot;META-INF/spring.factories&quot;); ArrayList result = new ArrayList(); while(urls.hasMoreElements()) { URL url = (URL)urls.nextElement(); Properties properties = PropertiesLoaderUtils.loadProperties(new UrlResource(url)); String factoryClassNames = properties.getProperty(factoryClassName); result.addAll(Arrays.asList(StringUtils.commaDelimitedListToStringArray(factoryClassNames))); } return result; } catch (IOException var8) { throw new IllegalArgumentException(&quot;Unable to load [&quot; + factoryClass.getName() + &quot;] factories from location [&quot; + &quot;META-INF/spring.factories&quot; + &quot;]&quot;, var8); } } 自动配置器会跟根据传入的factoryClass.getName()到项目系统路径下所有的spring.factories文件中找到相应的key，从而加载里面的类。我们就选取这个mybatis-spring-boot-autoconfigure下的spring.factories文件 Auto Configure org.springframework.boot.autoconfigure.EnableAutoConfiguration= org.mybatis.spring.boot.autoconfigure.MybatisAutoConfiguration 进入org.mybatis.spring.boot.autoconfigure.MybatisAutoConfiguration中，又是一堆注解 @org.springframework.context.annotation.Configuration @ConditionalOnClass({SqlSessionFactory.class, SqlSessionFactoryBean.class}) @ConditionalOnBean({DataSource.class}) @EnableConfigurationProperties({MybatisProperties.class}) @AutoConfigureAfter({DataSourceAutoConfiguration.class}) public class MybatisAutoConfiguration { private static final Logger logger = LoggerFactory.getLogger(MybatisAutoConfiguration.class); private final MybatisProperties properties; private final Interceptor[] interceptors; private final ResourceLoader resourceLoader; private final DatabaseIdProvider databaseIdProvider; private final List&lt;ConfigurationCustomizer&gt; configurationCustomizers; @Configuration是一个通过注解标注的springBean， @ConditionalOnClass({ SqlSessionFactory.class, SqlSessionFactoryBean.class})这个注解的意思是：当存在SqlSessionFactory.class, SqlSessionFactoryBean.class这两个类时才解析MybatisAutoConfiguration配置类,否则不解析这一个配置类。我们需要mybatis为我们返回会话对象，就必须有会话工厂相关类 @CondtionalOnBean(DataSource.class)只处理已经被声明为bean的dataSource @ConditionalOnMissingBean(MapperFactoryBean.class)这个注解的意思是如果容器中不存在name指定的bean则创建bean注入，否则不执行以上配置可以保证sqlSessionFactory、sqlSessionTemplate、dataSource等mybatis所需的组件均可被自动配置，@Configuration注解已经提供了Spring的上下文环境，所以以上组件的配置方式与Spring启动时通过mybatis.xml文件进行配置起到一个效果。 只要一个基于SpringBoot项目的类路径下存在SqlSessionFactory.class, SqlSessionFactoryBean.class，并且容器中已经注册了dataSourceBean，就可以触发自动化配置，意思说我们只要在maven的项目中加入了mybatis所需要的若干依赖，就可以触发自动配置，但引入mybatis原生依赖的话，每集成一个功能都要去修改其自动化配置类，那就得不到开箱即用的效果了。所以Spring-boot为我们提供了统一的starter可以直接配置好相关的类，触发自动配置所需的依赖(mybatis)如下： &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; 因为maven依赖的传递性，我们只要依赖starter就可以依赖到所有需要自动配置的类，实现开箱即用的功能。也体现出Springboot简化了Spring框架带来的大量XML配置以及复杂的依赖管理，让开发人员可以更加关注业务逻辑的开发。 再贴个盗的图SpringBoot的启动结构图 ","link":"https://tinaxiawuhao.github.io/post/YkvDN4zR3/"},{"title":"mysql概述六","content":"数据库优化 为什么要优化 系统的吞吐量瓶颈往往出现在数据库的访问速度上 随着应用程序的运行，数据库的中的数据会越来越多，处理时间会相应变慢 数据是存放在磁盘上的，读写速度无法和内存相比 优化原则：减少系统瓶颈，减少资源占用，增加系统的反应速度。 数据库结构优化 一个好的数据库设计方案对于数据库的性能往往会起到事半功倍的效果。需要考虑数据冗余、查询和更新的速度、字段的数据类型是否合理等多方面的内容。 将字段很多的表分解成多个表 对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。因为当一个表的数据量很大时，会由于使用频率低的字段的存在而变慢。 增加中间表 对于需要经常联合查询的表，可以建立中间表以提高查询效率。通过建立中间表，将需要通过联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询。 增加冗余字段 设计数据表时应尽量遵循范式理论的规约，尽可能的减少冗余字段，让数据库设计看起来精致、优雅。但是，合理的加入冗余字段可以提高查询速度。表的规范化程度越高，表和表之间的关系越多，需要连接查询的情况也就越多，性能也就越差。 注意：冗余字段的值在一个表中修改了，就要想办法在其他表中更新，否则就会导致数据不一致的问题。 MySQL数据库cpu飙满的话怎么处理 当 cpu 飙满时，先用操作系统命令 top 命令观察是不是 mysqld 占用导致的，如果不是，找出占用高的进程，并进行相关处理。 如果是 mysqld 造成的， show processlist，看看里面跑的 session 情况，是不是有消耗资源的 sql 在运行。找出消耗高的 sql，看看执行计划是否准确， index 是否缺失，或者实在是数据量太大造成。 一般来说，肯定要 kill 掉这些线程(同时观察 cpu 使用率是否下降)，等进行相应的调整(比如说加索引、改 sql、改内存参数)之后，再重新跑这些 SQL。 也有可能是每个 sql 消耗资源并不多，但是突然之间，有大量的 session 连进来导致 cpu 飙升，这种情况就需要跟应用一起来分析为何连接数会激增，再做出相应的调整，比如说限制连接数等 大表优化 当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下： 限定数据的范围： 务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内。； 读/写分离： 经典的数据库拆分方案，主库负责写，从库负责读； 缓存： 使用MySQL的缓存，另外对重量级、更新少的数据可以考虑使用应用级别的缓存； 还有就是通过分库分表的方式进行优化，主要有垂直分表和水平分表 垂直分表： 根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。 简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。 如下图所示，这样来说大家应该就更容易理解了。 垂直拆分的优点： 可以使得行数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。 垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂； 适用场景 1、如果一个表中某些列常用，另外一些列不常用 2、可以使数据行变小，一个数据页能存储更多数据，查询时减少I/O次数 缺点 有些分表的策略基于应用层的逻辑算法，一旦逻辑算法改变，整个分表逻辑都会改变，扩展性较差 对于应用层来说，逻辑算法增加开发成本 管理冗余列，查询所有数据需要join操作 水平分表： 保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。 水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。 水品拆分可以支持非常大的数据量。需要注意的一点是:分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 水平拆分最好分库 。 水平拆分能够 支持非常大的数据量存储，应用端改造也少，但 分片事务难以解决 ，跨界点Join性能较差，逻辑复杂。 《Java工程师修炼之道》的作者推荐 尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度 ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络I/O。 适用场景 1、表中的数据本身就有独立性，例如表中分表记录各个地区的数据或者不同时期的数据，特别是有些数据常用，有些不常用。 2、需要把数据存放在多个介质上。 水平切分的缺点 1、给应用增加复杂度，通常查询时需要多个表名，查询所有数据都需UNION操作 2、在许多数据库应用中，这种复杂度会超过它带来的优点，查询时会增加读一个索引层的磁盘次数 数据库分片两种常见方案 客户端代理： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 Sharding-JDBC 、阿里的TDDL是两种比较常用的实现。 中间件代理： 在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。 我们现在谈的 Mycat 、360的Atlas、网易的DDB等等都是这种架构的实现。 分库分表后面临的问题 事务支持 分库分表后，就成了分布式事务了。如果依赖数据库本身的分布式事务管理功能去执行事务，将付出高昂的性能代价； 如果由应用程序去协助控制，形成程序逻辑上的事务，又会造成编程方面的负担。 跨库join 只要是进行切分，跨节点Join的问题是不可避免的。但是良好的设计和切分却可以减少此类情况的发生。解决这一问题的普遍做法是分两次查询实现。在第一次查询的结果集中找出关联数据的id,根据这些id发起第二次请求得到关联数据。 分库分表方案产品 跨节点的count,order by,group by以及聚合函数问题 这些是一类问题，因为它们都需要基于全部数据集合进行计算。多数的代理都不会自动处理合并工作。解决方案：与解决跨节点join问题的类似，分别在各个节点上得到结果后在应用程序端进行合并。和join不同的是每个结点的查询可以并行执行，因此很多时候它的速度要比单一大表快很多。但如果结果集很大，对应用程序内存的消耗是一个问题。 数据迁移，容量规划，扩容等问题 来自淘宝综合业务平台团队，它利用对2的倍数取余具有向前兼容的特性（如对4取余得1的数对2取余也是1）来分配数据，避免了行级别的数据迁移，但是依然需要进行表级别的迁移，同时对扩容规模和分表数量都有限制。总得来说，这些方案都不是十分的理想，多多少少都存在一些缺点，这也从一个侧面反映出了Sharding扩容的难度。 ID问题 一旦数据库被切分到多个物理结点上，我们将不能再依赖数据库自身的主键生成机制。一方面，某个分区数据库自生成的ID无法保证在全局上是唯一的；另一方面，应用程序在插入数据之前需要先获得ID,以便进行SQL路由. 一些常见的主键生成策略 UUID 使用UUID作主键是最简单的方案，但是缺点也是非常明显的。由于UUID非常的长，除占用大量存储空间外，最主要的问题是在索引上，在建立索引和基于索引进行查询时都存在性能问题。 Twitter的分布式自增ID算法Snowflake 在分布式系统中，需要生成全局UID的场合还是比较多的，twitter的snowflake解决了这种需求，实现也还是很简单的，除去配置信息，核心代码就是毫秒级时间41位 机器ID 10位 毫秒内序列12位。 跨分片的排序分页 般来讲，分页时需要按照指定字段进行排序。当排序字段就是分片字段的时候，我们通过分片规则可以比较容易定位到指定的分片，而当排序字段非分片字段的时候，情况就会变得比较复杂了。为了最终结果的准确性，我们需要在不同的分片节点中将数据进行排序并返回，并将不同分片返回的结果集进行汇总和再次排序，最后再返回给用户。如下图所示： MySQL复制原理以及流程 主从复制：将主数据库中的DDL和DML操作通过二进制日志（BINLOG）传输到从数据库上，然后将这些日志重新执行（重做）；从而使得从数据库的数据与主数据库保持一致。 主从复制的作用 主数据库出现问题，可以切换到从数据库。 可以进行数据库层面的读写分离。 可以在从数据库上进行日常备份。 MySQL主从复制解决的问题 数据分布：随意开始或停止复制，并在不同地理位置分布数据备份 负载均衡：降低单个服务器的压力 高可用和故障切换：帮助应用程序避免单点失败 升级测试：可以用更高版本的MySQL作为从库 MySQL主从复制工作原理 在主库上把数据更高记录到二进制日志 从库将主库的日志复制到自己的中继日志 从库读取中继日志的事件，将其重放到从库数据中 基本原理流程，3个线程以及之间的关联 主：binlog线程——记录下所有改变了数据库数据的语句，放进master上的binlog中； 从：io线程——在使用start slave 之后，负责从master上拉取 binlog 内容，放进自己的relay log中； 从：sql执行线程——执行relay log中的语句； 复制过程 Binary log：主数据库的二进制日志 Relay log：从服务器的中继日志 第一步：master在每个事务更新数据完成之前，将该操作记录串行地写入到binlog文件中。 第二步：salve开启一个I/O Thread，该线程在master打开一个普通连接，主要工作是binlog dump process。如果读取的进度已经跟上了master，就进入睡眠状态并等待master产生新的事件。I/O线程最终的目的是将这些事件写入到中继日志中。 第三步：SQL Thread会读取中继日志，并顺序执行该日志中的SQL事件，从而与主数据库中的数据保持一致。 读写分离解决方案 读写分离是依赖于主从复制，而主从复制又是为读写分离服务的。因为主从复制要求slave不能写只能读（如果对slave执行写操作，那么show slave status将会呈现Slave_SQL_Running=NO，此时你需要按照前面提到的手动同步一下slave）。 方案一 使用mysql-proxy代理 优点：直接实现读写分离和负载均衡，不用修改代码，master和slave用一样的帐号，mysql官方不建议实际生产中使用 缺点：降低性能， 不支持事务 方案二 使用AbstractRoutingDataSource+aop+annotation在dao层决定数据源。 如果采用了mybatis， 可以将读写分离放在ORM层，比如mybatis可以通过mybatis plugin拦截sql语句，所有的insert/update/delete都访问master库，所有的select 都访问salve库，这样对于dao层都是透明。 plugin实现时可以通过注解或者分析语句是读写方法来选定主从库。不过这样依然有一个问题， 也就是不支持事务， 所以我们还需要重写一下DataSourceTransactionManager， 将read-only的事务扔进读库， 其余的有读有写的扔进写库。 方案三 使用AbstractRoutingDataSource+aop+annotation在service层决定数据源，可以支持事务. 缺点：类内部方法通过this.xx()方式相互调用时，aop不会进行拦截，需进行特殊处理。 备份计划，mysqldump以及xtranbackup的实现原理 (1)备份计划 视库的大小来定，一般来说 100G 内的库，可以考虑使用 mysqldump 来做，因为 mysqldump更加轻巧灵活，备份时间选在业务低峰期，可以每天进行都进行全量备份(mysqldump 备份出来的文件比较小，压缩之后更小)。 100G 以上的库，可以考虑用 xtranbackup 来做，备份速度明显要比 mysqldump 要快。一般是选择一周一个全备，其余每天进行增量备份，备份时间为业务低峰期。 (2)备份恢复时间 物理备份恢复快，逻辑备份恢复慢 这里跟机器，尤其是硬盘的速率有关系，以下列举几个仅供参考 20G的2分钟（mysqldump） 80G的30分钟(mysqldump) 111G的30分钟（mysqldump) 288G的3小时（xtra) 3T的4小时（xtra) 逻辑导入时间一般是备份时间的5倍以上 (3)备份恢复失败如何处理 首先在恢复之前就应该做足准备工作，避免恢复的时候出错。比如说备份之后的有效性检查、权限检查、空间检查等。如果万一报错，再根据报错的提示来进行相应的调整。 (4)mysqldump和xtrabackup实现原理 mysqldump mysqldump 属于逻辑备份。加入–single-transaction 选项可以进行一致性备份。后台进程会先设置 session 的事务隔离级别为 RR(SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ)，之后显式开启一个事务(START TRANSACTION /*!40100 WITH CONSISTENTSNAPSHOT */)，这样就保证了该事务里读到的数据都是事务事务时候的快照。之后再把表的数据读取出来。如果加上–master-data=1 的话，在刚开始的时候还会加一个数据库的读锁(FLUSH TABLES WITH READ LOCK),等开启事务后，再记录下数据库此时 binlog 的位置(showmaster status)，马上解锁，再读取表的数据。等所有的数据都已经导完，就可以结束事务 Xtrabackup xtrabackup 属于物理备份，直接拷贝表空间文件，同时不断扫描产生的 redo 日志并保存下来。最后完成innodb 的备份后，会做一个 flush engine logs的操作(老版本在有 bug，在5.6 上不做此操作会丢数据)，确保所有的redo log 都已经落盘(涉及到事务的两阶段提交概念，因为 xtrabackup 并不拷贝 binlog，所以必须保证所有的 redo log 都落盘，否则可能会丢最后一组提交事务的数据)。这个时间点就是 innodb 完成备份的时间点，数据文件虽然不是一致性的，但是有这段时间的 redo 就可以让数据文件达到一致性(恢复的时候做的事情)。然后还需要flush tables with read lock，把 myisam 等其他引擎的表给备份出来，备份完后解锁。这样就做到了完美的热备。 数据表损坏的修复方式 使用 myisamchk 来修复，具体步骤： 1）修复前将mysql服务停止。 2）打开命令行方式，然后进入到mysql的/bin目录。 3）执行myisamchk –recover 数据库所在路径/*.MYI 使用repair table 或者 OPTIMIZE table命令来修复，REPAIR TABLE table_name 修复表OPTIMIZE TABLE table_name 优化表 REPAIR TABLE 用于修复被破坏的表。 OPTIMIZE TABLE 用于回收闲置的数据库空间，当表上的数据行被删除时，所占据的磁盘空间并没有立即被回收，使用了OPTIMIZE TABLE命令后这些空间将被回收，并且对磁盘上的数据行进行重排（注意：是磁盘上，而非数据库） ","link":"https://tinaxiawuhao.github.io/post/jytwMqgV3/"},{"title":"mysql概述五","content":"SQL优化 对于低性能的SQL语句的定位，最重要也是最有效的方法就是使用执行计划，MySQL提供了explain命令来查看语句的执行计划。 我们知道，不管是哪种数据库，或者是哪种数据库引擎，在对一条SQL语句进行执行的过程中都会做很多相关的优化，对于查询语句，最重要的优化方式就是使用索引。 而执行计划，就是显示数据库引擎对于SQL语句的执行的详细情况，其中包含了是否使用索引，使用什么索引，使用的索引的相关信息等。 执行计划包含的信息 id 有一组数字组成。表示一个查询中各个子查询的执行顺序; id相同执行顺序由上至下。 id不同，id值越大优先级越高，越先被执行。 id为null时表示一个结果集，不需要使用它查询，常出现在包含union等查询语句中。 select_type 每个子查询的查询类型，一些常见的查询类型。 id select_type description 1 SIMPLE 不包含任何子查询或union等查询 2 PRIMARY 包含子查询最外层查询就显示为 PRIMARY 3 SUBQUERY 在select或 where字句中包含的查询 4 DERIVED from字句中包含的查询 5 UNION 出现在union后的查询语句中 6 UNION RESULT 从UNION中获取结果集，例如上文的第三个例子 type(非常重要，可以看到有没有走索引) 访问类型 ALL 扫描全表数据 index 遍历索引 range 索引范围查找 index_subquery 在子查询中使用 ref unique_subquery 在子查询中使用 eq_ref ref_or_null 对Null进行索引的优化的 ref fulltext 使用全文索引 ref 使用非唯一索引查找数据 eq_ref 在join查询中使用PRIMARY KEY or UNIQUE NOT NULL索引关联。 possible_keys 可能使用的索引，注意不一定会使用。查询涉及到的字段上若存在索引，则该索引将被列出来。当该列为 NULL时就要考虑当前的SQL是否需要优化了。 key 显示MySQL在查询中实际使用的索引，若没有使用索引，显示为NULL。 TIPS:查询中若使用了覆盖索引(覆盖索引：索引的数据覆盖了需要查询的所有数据)，则该索引仅出现在key列表中 key_length 索引长度 ref 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 rows 返回估算的结果集数目，并不是一个准确的值。 extra 的信息非常丰富，常见的有： Using index 使用覆盖索引 Using where 使用了用where子句来过滤结果集 Using filesort 使用文件排序，使用非索引列进行排序时出现，非常消耗性能，尽量优化。 Using temporary 使用了临时表 sql优化的目标可以参考阿里开发手册 【推荐】SQL性能优化的目标：至少要达到 range 级别，要求是ref级别，如果可以是consts最好。 说明： 1） consts 单表中最多只有一个匹配行（主键或者唯一索引），在优化阶段即可读取到数据。 2） ref 指的是使用普通的索引（normal index）。 3） range 对索引进行范围检索。 反例：explain表的结果，type=index，索引物理文件全扫描，速度非常慢，这个index级别比较range还低，与全表扫描是小巫见大巫。 SQL执行顺序 mysql执行sql的顺序从 From 开始，以下是执行的顺序流程 FROM FROM table1 left join table2 on 将table1和table2中的数据产生笛卡尔积，生成Temp1 JOIN JOIN table2 所以先是确定表，再确定关联条件 ON ON table1.column = table2.columu 确定表的绑定条件 由Temp1产生中间表Temp2 WHERE 对中间表Temp2产生的结果进行过滤 产生中间表Temp3 GROUP BY 对中间表Temp3进行分组，产生中间表Temp4 HAVING 对分组后的记录进行聚合 产生中间表Temp5 SELECT 对中间表Temp5进行列筛选，产生中间表 Temp6 DISTINCT 对中间表 Temp6进行去重，产生中间表 Temp7 ORDER BY 对Temp7中的数据进行排序，产生中间表Temp8 LIMIT 对中间表Temp8进行分页，产生中间表Temp9 SQL的生命周期 与服务器建立连接，客户端发送一条查询给服务器 服务器先检查查询缓存，如果命中了缓存则立刻返回存储在缓存中的结果，否则执行下一步 服务端进行sql解析，预处理，再由查询优化器生成对应的查询执行计划 mysql根据优化器提供的执行计划，调用存储引擎的api来执行查询 通过步骤一的连接，发送结果到客户端 关掉连接，释放资源 优化大表数据查询 优化shema、sql语句+索引； 加缓存，memcached, redis； 主从复制，读写分离； 垂直拆分，根据你模块的耦合度，将一个大的系统分为多个小的系统，也就是分布式系统； 水平切分，针对数据量大的表，这一步最麻烦，最能考验技术水平，要选择一个合理的sharding key, 为了有好的查询效率，表结构也要改动，做一定的冗余，应用也要改，sql中尽量带sharding key，将数据定位到限定的表上去查，而不是扫描全部的表； 超大分页怎么处理 超大的分页一般从两个方向上来解决. 数据库层面,这也是我们主要集中关注的(虽然收效没那么大),类似于select * from table where age &gt; 20 limit 1000000,10这种查询其实也是有可以优化的余地的. 这条语句需要load1000000数据然后基本上全部丢弃,只取10条当然比较慢. 当时我们可以修改为select * from table where id in (select id from table where age &gt; 20 limit 1000000,10).这样虽然也load了一百万的数据,但是由于索引覆盖,要查询的所有字段都在索引中,所以速度会很快. 同时如果ID连续的好,我们还可以select * from table where id &gt; 1000000 limit 10,效率也是不错的,优化的可能性有许多种,但是核心思想都一样,就是减少load的数据. 从需求的角度减少这种请求…主要是不做类似的需求(直接跳转到几百万页之后的具体某一页.只允许逐页查看或者按照给定的路线走,这样可预测,可缓存)以及防止ID泄漏且连续被人恶意攻击. 解决超大分页,其实主要是靠缓存,可预测性的提前查到内容,缓存至redis等k-V数据库中,直接返回即可. 在阿里巴巴《Java开发手册》中,对超大分页的解决办法是类似于上面提到的第一种. 【推荐】利用延迟关联或者子查询优化超多分页场景。 说明：MySQL并不是跳过offset行，而是取offset+N行，然后返回放弃前offset行，返回N行，那当offset特别大的时候，效率就非常的低下，要么控制返回的总页数，要么对超过特定阈值的页数进行SQL改写。 正例：先快速定位需要获取的id段，然后再关联： SELECT a.* FROM 表1 a, (select id from 表1 where 条件 LIMIT 100000,20 ) b where a.id=b.id mysql 分页 LIMIT 子句可以被用于强制 SELECT 语句返回指定的记录数。LIMIT 接受一个或两个数字参数。参数必须是一个整数常量。如果给定两个参数，第一个参数指定第一个返回记录行的偏移量，第二个参数指定返回记录行的最大数目。初始记录行的偏移量是 0(而不是 1) mysql&gt; SELECT * FROM table LIMIT 5,10; // 检索记录行 6-15 为了检索从某一个偏移量到记录集的结束所有的记录行，可以指定第二个参数为 -1： mysql&gt; SELECT * FROM table LIMIT 95,-1; // 检索记录行 96-last. 如果只给定一个参数，它表示返回最大的记录行数目： mysql&gt; SELECT * FROM table LIMIT 5; //检索前 5 个记录行 换句话说，LIMIT n 等价于 LIMIT 0,n。 慢查询日志 用于记录执行时间超过某个临界值的SQL日志，用于快速定位慢查询，为我们的优化做参考。 开启慢查询日志 配置项：slow_query_log 可以使用show variables like ‘slov_query_log’查看是否开启，如果状态值为OFF，可以使用set GLOBAL slow_query_log = on来开启，它会在datadir下产生一个xxx-slow.log的文件。 设置临界时间 配置项：long_query_time 查看：show VARIABLES like 'long_query_time'，单位秒 设置：set long_query_time=0.5 实操时应该从长时间设置到短的时间，即将最慢的SQL优化掉 查看日志，一旦SQL超过了我们设置的临界时间就会被记录到xxx-slow.log中 慢查询处理 在业务系统中，除了使用主键进行的查询，其他的会在测试库上测试其耗时，慢查询的统计主要由运维在做，会定期将业务中的慢查询反馈给我们。 慢查询的优化首先要搞明白慢的原因是什么？ 是查询条件没有命中索引？是load了不需要的数据列？还是数据量太大？ 所以优化也是针对这三个方向来的， 首先分析语句，看看是否load了额外的数据，可能是查询了多余的行并且抛弃掉了，可能是加载了许多结果中并不需要的列，对语句进行分析以及重写。 分析语句的执行计划，然后获得其使用索引的情况，之后修改语句或者修改索引，使得语句可以尽可能的命中索引。 如果对语句的优化已经无法进行，可以考虑表中的数据量是否太大，如果是的话可以进行横向或者纵向的分表。 主键必要性 主键是数据库确保数据行在整张表唯一性的保障，即使业务上本张表没有主键，也建议添加一个自增长的ID列作为主键。设定了主键之后，在后续的删改查的时候可能更加快速以及确保操作数据范围安全。 主键使用自增ID还是UUID 推荐使用自增ID，不要使用UUID。 因为在InnoDB存储引擎中，主键索引是作为聚簇索引存在的，也就是说，主键索引的B+树叶子节点上存储了主键索引以及全部的数据(按照顺序)，如果主键索引是自增ID，那么只需要不断向后排列即可，如果是UUID，由于到来的ID与原来的大小不确定，会造成非常多的数据插入，数据移动，然后导致产生很多的内存碎片，进而造成插入性能的下降。 总之，在数据量大一些的情况下，用自增主键性能会好一些。 由于主键是聚簇索引，如果没有主键，InnoDB会选择一个唯一键来作为聚簇索引，如果没有唯一键，会生成一个隐式的主键。 字段为什么要求定义为not null null值会占用更多的字节，且会在程序中造成很多与预期不符的情况。 如果要存储用户的密码散列，应该使用什么字段进行存储？ 密码散列，盐，用户身份证号等固定长度的字符串应该使用char而不是varchar来存储，这样可以节省空间且提高检索效率。 优化查询过程中的数据访问 访问数据太多导致查询性能下降 确定应用程序是否在检索大量超过需要的数据，可能是太多行或列 确认MySQL服务器是否在分析大量不必要的数据行 避免犯如下SQL语句错误 查询不需要的数据。解决办法：使用limit解决 多表关联返回全部列。解决办法：指定列名 总是返回全部列。解决办法：避免使用SELECT * 重复查询相同的数据。解决办法：可以缓存数据，下次直接读取缓存 是否在扫描额外的记录。解决办法： 使用explain进行分析，如果发现查询需要扫描大量的数据，但只返回少数的行，可以通过如下技巧去优化： 使用索引覆盖扫描，把所有的列都放到索引中，这样存储引擎不需要回表获取对应行就可以返回结果。 改变数据库和表的结构，修改数据表范式 重写SQL语句，让优化器可以以更优的方式执行查询。 优化长难的查询语句 一个复杂查询还是多个简单查询 MySQL内部每秒能扫描内存中上百万行数据，相比之下，响应数据给客户端就要慢得多 使用尽可能小的查询是好的，但是有时将一个大的查询分解为多个小的查询是很有必要的。 切分查询 将一个大的查询分为多个小的相同的查询 一次性删除1000万的数据要比一次删除1万，暂停一会的方案更加损耗服务器开销。 分解关联查询，让缓存的效率更高。 执行单个查询可以减少锁的竞争。 在应用层做关联更容易对数据库进行拆分。 查询效率会有大幅提升。 较少冗余记录的查询。 优化特定类型的查询语句 count(*)会忽略所有的列，直接统计所有列数，不要使用count(列名) MyISAM中，没有任何where条件的count(*)非常快。 当有where条件时，MyISAM的count统计不一定比其它引擎快。 可以使用explain查询近似值，用近似值替代count(*) 增加汇总表 使用缓存 优化关联查询 确定ON或者USING子句中是否有索引。 确保GROUP BY和ORDER BY只有一个表中的列，这样MySQL才有可能使用索引。 优化子查询 用关联查询替代 优化GROUP BY和DISTINCT 这两种查询可以使用索引来优化，是最有效的优化方法 关联查询中，使用标识列分组的效率更高 如果不需要ORDER BY，进行GROUP BY时加ORDER BY NULL，MySQL不会再进行文件排序。 WITH ROLLUP超级聚合，可以挪到应用程序处理 优化LIMIT分页 LIMIT偏移量大的时候，查询效率较低 可以记录上次查询的最大ID，下次查询时直接根据该ID来查询 优化UNION查询 UNION ALL的效率高于UNION 优化WHERE子句 对于此类问题，先说明如何定位低效SQL语句，然后根据SQL语句可能低效的原因做排查，先从索引着手，如果索引没有问题，考虑以上几个方面，数据访问的问题，长难查询句的问题还是一些特定类型优化的问题，逐一回答。 SQL语句优化的一些方法？ 1.对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。 2.应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如： select id from t where num is null -- 可以在num上设置默认值0，确保表中num列没有null值，然后这样查询： select id from t where num=0 3.应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则引擎将放弃使用索引而进行全表扫描。 4.应尽量避免在 where 子句中使用or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如： select id from t where num=10 or num=20 -- 可以这样查询： select id from t where num=10 union all select id from t where num=20 5.in 和 not in 也要慎用，否则会导致全表扫描，如： select id from t where num in(1,2,3) -- 对于连续的数值，能用 between 就不要用 in 了： select id from t where num between 1 and 3 6.下面的查询也将导致全表扫描： select id from t where name like ‘%李%’ -- 若要提高效率，可以考虑全文检索。 7.如果在 where 子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然 而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描： select id from t where num=@num -- 可以改为强制查询使用索引： select id from t with(index(索引名)) where num=@num 8.应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如： select id from t where num/2=100 -- 应改为: select id from t where num=100*2 9.应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如： select id from t where substring(name,1,3)=’abc’ -- name以abc开头的id应改为: select id from t where name like ‘abc%’ 10.不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。 ","link":"https://tinaxiawuhao.github.io/post/lbX34toJL/"},{"title":"mysql概述四","content":"视图 为了提高复杂SQL语句的复用性和表操作的安全性，MySQL数据库管理系统提供了视图特性。所谓视图，本质上是一种虚拟表，在物理上是不存在的，其内容与真实的表相似，包含一系列带有名称的列和行数据。但是，视图并不在数据库中以储存的数据值形式存在。行和列数据来自定义视图的查询所引用基本表，并且在具体引用视图时动态生成。 视图使开发者只关心感兴趣的某些特定数据和所负责的特定任务，只能看到视图中所定义的数据，而不是视图所引用表中的数据，从而提高了数据库中数据的安全性。 特点 视图的特点如下: 视图的列可以来自不同的表，是表的抽象和在逻辑意义上建立的新关系。 视图是由基本表(实表)产生的表(虚表)。 视图的建立和删除不影响基本表。 对视图内容的更新(添加，删除和修改)直接影响基本表。 当视图来自多个基本表时，不允许添加和删除数据。 视图的操作包括创建视图，查看视图，删除视图和修改视图。 使用场景 视图根本用途：简化sql查询，提高开发效率。如果说还有另外一个用途那就是兼容老的表结构。 下面是视图的常见使用场景： 重用SQL语句； 简化复杂的SQL操作。在编写查询后，可以方便的重用它而不必知道它的基本查询细节； 使用表的组成部分而不是整个表； 保护数据。可以给用户授予表的特定部分的访问权限而不是整个表的访问权限； 更改数据格式和表示。视图可返回与底层表的表示和格式不同的数据。 优点 查询简单化。视图能简化用户的操作 数据安全性。视图使用户能以多种角度看待同一数据，能够对机密数据提供安全保护 逻辑数据独立性。视图对重构数据库提供了一定程度的逻辑独立性 缺点 性能。数据库必须把视图的查询转化成对基本表的查询，如果这个视图是由一个复杂的多表查询所定义，那么，即使是视图的一个简单查询，数据库也把它变成一个复杂的结合体，需要花费一定的时间。 修改限制。当用户试图修改视图的某些行时，数据库必须把它转化为对基本表的某些行的修改。事实上，当从视图中插入或者删除时，情况也是这样。对于简单视图来说，这是很方便的，但是，对于比较复杂的视图，可能是不可修改的 这些视图有如下特征： 有UNIQUE等集合操作符的视图。 有GROUP BY子句的视图。 有诸如AVG\\SUM\\MAX等聚合函数的视图。 使用DISTINCT关键字的视图。 连接表的视图（其中有些例外） 游标 游标是系统为用户开设的一个数据缓冲区，存放SQL语句的执行结果，每个游标区都有一个名字。用户可以通过游标逐一获取记录并赋给主变量，交由主语言进一步处理。 存储过程与函数 存储过程是一个预编译的SQL语句，优点是允许模块化的设计，就是说只需要创建一次，以后在该程序中就可以调用多次。如果某次操作需要执行多次SQL，使用存储过程比单纯SQL语句执行要快。 优点 存储过程是预编译过的，执行效率高。 存储过程的代码直接存放于数据库中，通过存储过程名直接调用，减少网络通讯。 安全性高，执行存储过程需要有一定权限的用户。 存储过程可以重复使用，减少数据库开发人员的工作量。 缺点 1）调试麻烦，但是用 PL/SQL Developer 调试很方便！弥补这个缺点。 2）移植问题，数据库端代码当然是与数据库相关的。但是如果是做工程型项目，基本不存在移植问题。 3）重新编译问题，因为后端代码是运行前编译的，如果带有引用关系的对象发生改变时，受影响的存储过程、包将需要重新编译（不过也可以设置成运行时刻自动编译）。 4）如果在一个程序系统中大量的使用存储过程，到程序交付使用的时候随着用户需求的增加会导致数据结构的变化，接着就是系统的相关问题了，最后如果用户想维护该系统可以说是很难很难、而且代价是空前的，维护起来更麻烦。 触发器 触发器是用户定义在关系表上的一类由事件驱动的特殊的存储过程。触发器是指一段代码，当触发某个事件时，自动执行这些代码。 使用场景 可以通过数据库中的相关表实现级联更改。 实时监控某张表中的某个字段的更改而需要做出相应的处理。 例如可以生成某些业务的编号。 注意不要滥用，否则会造成数据库及应用程序的维护困难。 大家需要牢记以上基础知识点，重点是理解数据类型CHAR和VARCHAR的差异，表存储引擎InnoDB和MyISAM的区别。 MySQL中都有哪些触发器 在MySQL数据库中有如下六种触发器： Before Insert After Insert Before Update After Update Before Delete After Delete SQL语句 SQL语句分类 数据定义语言DDL（Data Ddefinition Language）CREATE，DROP，ALTER 主要为以上操作 即对逻辑结构等有操作的，其中包括表结构，视图和索引。 数据查询语言DQL（Data Query Language）SELECT 这个较为好理解 即查询操作，以select关键字。各种简单查询，连接查询等 都属于DQL。 数据操纵语言DML（Data Manipulation Language）INSERT，UPDATE，DELETE 主要为以上操作 即对数据进行操作的，对应上面所说的查询操作 DQL与DML共同构建了多数初级程序员常用的增删改查操作。而查询是较为特殊的一种 被划分到DQL中。 数据控制功能DCL（Data Control Language）GRANT，REVOKE，COMMIT，ROLLBACK 主要为以上操作 即对数据库安全性完整性等有操作的，可以简单的理解为权限控制等。 超键、候选键、主键、外键 超键：在关系中能唯一标识元组的属性集称为关系模式的超键。一个属性可以为作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。 候选键：是最小超键，即没有冗余元素的超键。 主键：数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（Null）。 外键：在一个表中存在的另一个表的主键称此表的外键。 SQL 约束 NOT NULL: 用于控制字段的内容一定不能为空（NULL）。 UNIQUE: 控件字段内容不能重复，一个表允许有多个 Unique 约束。 PRIMARY KEY: 也是用于控件字段内容不能重复，但它在一个表只允许出现一个。 FOREIGN KEY: 用于预防破坏表之间连接的动作，也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。 CHECK: 用于控制字段的值范围。 六种关联查询 交叉连接（CROSS JOIN） 内连接（INNER JOIN） 外连接（LEFT JOIN/RIGHT JOIN） 联合查询（UNION与UNION ALL） 全连接（FULL JOIN） 交叉连接（CROSS JOIN） SELECT * FROM A,B(,C)或者SELECT * FROM A CROSS JOIN B (CROSS JOIN C)#没有任何关联条件，结果是笛卡尔积，结果集会很大，没有意义，很少使用内连接（INNER JOIN）SELECT * FROM A,B WHERE A.id=B.id或者SELECT * FROM A INNER JOIN B ON A.id=B.id多表中同时符合某种条件的数据记录的集合，INNER JOIN可以缩写为JOIN 内连接分为三类 等值连接：ON A.id=B.id 不等值连接：ON A.id &gt; B.id 自连接：SELECT * FROM A T1 INNER JOIN A T2 ON T1.id=T2.pid 外连接（LEFT JOIN/RIGHT JOIN） 左外连接：LEFT OUTER JOIN, 以左表为主，先查询出左表，按照ON后的关联条件匹配右表，没有匹配到的用NULL填充，可以简写成LEFT JOIN 右外连接：RIGHT OUTER JOIN, 以右表为主，先查询出右表，按照ON后的关联条件匹配左表，没有匹配到的用NULL填充，可以简写成RIGHT JOIN 联合查询（UNION与UNION ALL） SELECT * FROM A UNION SELECT * FROM B UNION ... 就是把多个结果集集中在一起，UNION前的结果为基准，需要注意的是联合查询的列数要相等，相同的记录行会合并 如果使用UNION ALL，不会合并重复的记录行 效率 UNION ALL 高于 UNION 全连接（FULL JOIN） MySQL不支持全连接 可以使用LEFT JOIN 和UNION和RIGHT JOIN联合使用 SELECT * FROM A LEFT JOIN B ON A.id=B.id UNIONSELECT * FROM A RIGHT JOIN B ON A.id=B.id 表连接展示 有2张表，1张R、1张S，R表有ABC三列，S表有CD两列，表中各有三条记录。 R表 A B C a1 b1 c1 a2 b2 c2 a3 b3 c3 S表 C D c1 d1 c2 d2 c4 d3 交叉连接(笛卡尔积): select r.*,s.* from r,s A B C C D a1 b1 c1 c1 d1 a2 b2 c2 c1 d1 a3 b3 c3 c1 d1 a1 b1 c1 c2 d2 a2 b2 c2 c2 d2 a3 b3 c3 c2 d2 a1 b1 c1 c4 d3 a2 b2 c2 c4 d3 a3 b3 c3 c4 d3 内连接结果： select r.*,s.* from r inner join s on r.c=s.c A B C C D a1 b1 c1 c1 d1 a2 b2 c2 c2 d2 左连接结果： select r.*,s.* from r left join s on r.c=s.c A B C C D a1 b1 c1 c1 d1 a2 b2 c2 c2 d2 a3 b3 c3 右连接结果： select r.*,s.* from r right join s on r.c=s.c A B C C D a1 b1 c1 c1 d1 a2 b2 c2 c2 d2 c4 d3 全表连接的结果（MySql不支持，Oracle支持）： select r.*,s.* from r full join s on r.c=s.c A B C C D a1 b1 c1 c1 d1 a2 b2 c2 c2 d2 a3 b3 c3 c4 d3 子查询 条件：一条SQL语句的查询结果做为另一条查询语句的条件或查询结果 嵌套：多条SQL语句嵌套使用，内部的SQL查询语句称为子查询。 子查询的三种情况 子查询是单行单列的情况：结果集是一个值，父查询使用：=、 &lt;、 &gt; 等运算符 -- 查询工资最高的员工是谁？ select * from employee where salary=(select max(salary) from employee); 子查询是多行单列的情况：结果集类似于一个数组，父查询使用：in 运算符 -- 查询工资最高的员工是谁？ select * from employee where salary=(select max(salary) from employee); 子查询是多行多列的情况：结果集类似于一张虚拟表，不能用于where条件，用于select子句中做为子表 -- 1) 查询出2011年以后入职的员工信息 -- 2) 查询所有的部门信息，与上面的虚拟表中的信息比对，找出所有部门ID相等的员工。 SELECT * FROM dept d, ( SELECT * FROM employee WHERE join_date &gt; '2011-1-1' ) e WHERE e.dept_id = d.id; -- 使用表连接： SELECT d.*, e.* FROM dept d INNER JOIN employee e ON d.id = e.dept_id WHERE e.join_date &gt; '2011-1-1' mysql中 in 和 exists 区别 mysql中的in语句是把外表和内表作hash 连接，而exists语句是对外表作loop循环，每次loop循环再对内表进行查询。一直大家都认为exists比in语句的效率要高，这种说法其实是不准确的。这个是要区分环境的。 如果查询的两个表大小相当，那么用in和exists差别不大。 如果两个表中一个较小，一个是大表，则子查询表大的用exists，子查询表小的用in。 not in 和not exists：如果查询语句使用了not in，那么内外表都进行全表扫描，没有用到索引；而not extsts的子查询依然能用到表上的索引。所以无论那个表大，用not exists都比not in要快。 varchar与char的区别 char的特点 char表示定长字符串，长度是固定的； 如果插入数据的长度小于char的固定长度时，则用空格填充； 因为长度固定，所以存取速度要比varchar快很多，甚至能快50%，但正因为其长度固定，所以会占据多余的空间，是空间换时间的做法； 对于char来说，最多能存放的字符个数为255，和编码无关 varchar的特点 varchar表示可变长字符串，长度是可变的； 插入的数据是多长，就按照多长来存储； varchar在存取方面与char相反，它存取慢，因为长度不固定，但正因如此，不占据多余的空间，是时间换空间的做法； 对于varchar来说，最多能存放的字符个数为65532 总之，结合性能角度（char更快）和节省磁盘空间角度（varchar更小），具体情况还需具体来设计数据库才是妥当的做法。 varchar(50)中50的涵义 最多存放50个字符，varchar(50)和(200)存储hello所占空间一样，但后者在排序时会消耗更多内存，因为order by col采用fixed_length计算col长度(memory引擎也一样)。在早期 MySQL 版本中， 50 代表字节数，现在代表字符数。 int(20)中20的涵义 是指显示字符的长度。20表示最大显示宽度为20，但仍占4字节存储，存储范围不变；不影响内部存储，只是影响带 zerofill 定义的 int 时，前面补多少个 0，易于报表展示 mysql为什么这么设计字段 对大多数应用没有意义，只是规定一些工具用来显示字符的个数；int(1)和int(20)存储和计算均一样； mysql中int(10)和char(10)以及varchar(10)的区别 int(10)的10表示显示的数据的长度，不是存储数据的大小；chart(10)和varchar(10)的10表示存储数据的大小，即表示存储多少个字符。 int(10) 10位的数据长度 9999999999，占32个字节，int型4位 char(10) 10位固定字符串，不足补空格 最多10个字符 varchar(10) 10位可变字符串，不足补空格 最多10个字符 char(10)表示存储定长的10个字符，不足10个就用空格补齐，占用更多的存储空间 varchar(10)表示存储10个变长的字符，存储多少个就是多少个，空格也按一个字符存储，这一点是和char(10)的空格不同的，char(10)的空格表示占位不算一个字符 FLOAT和DOUBLE的区别 FLOAT类型数据可以存储至多8位十进制数，并在内存中占4字节。 DOUBLE类型数据可以存储至多18位十进制数，并在内存中占8字节。 drop、delete与truncate的区别 三者都表示删除，但是三者有一些差别： Delete Truncate Drop 类型 属于DML 属于DDL 属于DDL 回滚 可回滚 不可回滚 不可回滚 删除内容 表结构还在，删除表的全部或者一部分数据行 表结构还在，删除表中的所有数据 从数据库中删除表，所有的数据行，索引和权限也会被删除 删除速度 删除速度慢，需要逐行删除 删除速度快 删除速度最快 因此，在不再需要一张表的时候，用drop；在想删除部分数据行时候，用delete；在保留表而删除所有数据的时候用truncate。 UNION与UNION ALL的区别 如果使用UNION ALL，不会合并重复的记录行 效率 UNION 高于 UNION ALL ","link":"https://tinaxiawuhao.github.io/post/QKdNIC7pG/"},{"title":"mysql概述三","content":"事务 事务是一个不可分割的数据库操作序列，也是数据库并发控制的基本单位，其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态。事务是逻辑上的一组操作，要么都执行，要么都不执行。 事务最经典也经常被拿出来说例子就是转账了。 假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：将小明的余额减少1000元，将小红的余额增加1000元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。 事物的四大特性(ACID) 关系性数据库需要遵循ACID规则，具体内容如下： 原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性： 执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的； 隔离性： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 脏读？幻读？不可重复读？ 脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。 不可重复读(Non-repeatable read):在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新了原有的数据。 幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。 事务的隔离级别 为了达到事务的四大特性，数据库定义了4种不同的事务隔离级别，由低到高依次为Read uncommitted、Read committed、Repeatable read、Serializable，这四个级别可以逐个解决脏读、不可重复读、幻读这几类问题。 隔离级别 脏读 不可重复读 幻影读 READ-UNCOMMITTED √ √ √ READ-COMMITTED × √ √ REPEATABLE-READ × × √ SERIALIZABLE × × × SQL 标准定义了四个隔离级别： READ-UNCOMMITTED(读未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 这里需要注意的是：Mysql 默认采用的 REPEATABLE_READ(可重复读)隔离级别 Oracle 默认采用的 READ_COMMITTED(读已提交)隔离级别 事务隔离机制的实现基于锁机制和并发调度。其中并发调度使用的是MVVC（多版本并发控制），通过保存修改的旧版本信息来支持并发一致性读和回滚等特性。 因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是READ-COMMITTED(读已提交):，但是你要知道的是InnoDB 存储引擎默认使用 **REPEATABLE-READ(可重读)**并不会有任何性能损失。 InnoDB 存储引擎在 分布式事务 的情况下一般会用到**SERIALIZABLE(可串行化)**隔离级别。 锁 当数据库有并发事务的时候，可能会产生数据的不一致，这时候需要一些机制来保证访问的次序，锁机制就是这样的一个机制。 就像酒店的房间，如果大家随意进出，就会出现多人抢夺同一个房间的情况，而在房间上装上锁，申请到钥匙的人才可以入住并且将房间锁起来，其他人只有等他使用完毕才可以再次使用。 隔离级别与锁的关系 在Read Uncommitted级别下，读取数据不需要加共享锁，这样就不会跟被修改的数据上的排他锁冲突 在Read Committed级别下，读操作需要加共享锁，但是在语句执行完以后释放共享锁； 在Repeatable Read级别下，读操作需要加共享锁，但是在事务提交之前并不释放共享锁，也就是必须等待事务执行完毕以后才释放共享锁。 SERIALIZABLE 是限制性最强的隔离级别，因为该级别锁定整个范围的键，并一直持有锁，直到事务完成。 按照锁的粒度分数据库锁 在关系型数据库中，可以按照锁的粒度把数据库锁分为行级锁(INNODB引擎)、表级锁(MYISAM引擎)和页级锁(BDB引擎 )。 MyISAM和InnoDB存储引擎使用的锁 MyISAM采用表级锁(table-level locking)。 InnoDB支持行级锁(row-level locking)和表级锁，默认为行级锁 行级锁，表级锁和页级锁对比 行级锁 行级锁是Mysql中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁 和 排他锁。 特点：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 表级锁 表级锁是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MYISAM与INNODB都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。 特点：开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。 页级锁 页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折中的页级，一次锁定相邻的一组记录。 特点：开销和加锁时间介于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般 按照锁的类别分数据库锁 从锁的类别上来讲，有共享锁和排他锁。 共享锁: 又叫做读锁。 当用户要进行数据的读取时，对数据加上共享锁。共享锁可以同时加上多个。 排他锁: 又叫做写锁。 当用户要进行数据的写入时，对数据加上排他锁。排他锁只可以加一个，他和其他的排他锁，共享锁都相斥。 用上面的例子来说就是用户的行为有两种，一种是来看房，多个用户一起看房是可以接受的。 一种是真正的入住一晚，在这期间，无论是想入住的还是想看房的都不可以。 锁的粒度取决于具体的存储引擎，InnoDB实现了行级锁，页级锁，表级锁。 他们的加锁开销从大到小，并发能力也是从大到小。 MySQL中InnoDB引擎的行锁实现 答：InnoDB是基于索引来完成行锁 例: select * from tab_with_index where id = 1 for update; for update 可以根据条件来完成行锁锁定，并且 id 是有索引键的列，如果 id 不是索引键那么InnoDB将完成表锁，并发将无从谈起 InnoDB存储引擎的锁的算法有三种 Record lock：单个行记录上的锁 Gap lock：间隙锁，锁定一个范围，不包括记录本身 Next-key lock：record+gap 锁定一个范围，包含记录本身 相关知识点： innodb对于行的查询使用next-key lock Next-locking keying为了解决Phantom Problem幻读问题 当查询的索引含有唯一属性时，将next-key lock降级为record key Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生 有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） A. 将事务隔离级别设置为RC B. 将参数innodb_locks_unsafe_for_binlog设置为1 死锁 死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，从而导致恶性循环的现象。 常见的解决死锁的方法 如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。 在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率； 对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率； 如果业务处理不好可以用分布式事务锁或者使用乐观锁 数据库的乐观锁和悲观锁 数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。乐观并发控制（乐观锁）和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。在查询完数据的时候就把事务锁起来，直到提交事务。实现方式：使用数据库中的锁机制 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。在修改数据的时候把事务锁起来，通过version的方式来进行锁定。实现方式：乐一般会使用版本号机制或CAS算法实现。 两种锁的使用场景 从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。 但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 ","link":"https://tinaxiawuhao.github.io/post/5ppmKteEb/"},{"title":"mysql概述二","content":"索引 索引是一种特殊的文件(InnoDB数据表上的索引是表空间的一个组成部分)，它们包含着对数据表里所有记录的引用指针。 索引是一种数据结构。数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用B树及其变种B+树。 更通俗的说，索引就相当于目录。为了方便查找书中的内容，通过对内容建立索引形成目录。索引是一个文件，它是要占据物理空间的。 索引的基本原理 索引用来快速地寻找那些具有特定值的记录。如果没有索引，一般来说执行查询时遍历整张表。 索引的原理很简单，就是把无序的数据变成有序的查询 把创建了索引的列的内容进行排序 对排序结果生成倒排表 在倒排表内容上拼上数据地址链 在查询的时候，先拿到倒排表内容，再取出数据地址链，从而拿到具体数据 索引优缺点 优点 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 缺点 时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增/改/删的执行效率； 空间方面：索引需要占物理空间。 使用场景（重点） where 上图中，根据id查询记录，因为id字段仅建立了主键索引，因此此SQL执行可选的索引只有主键索引，如果有多个，最终会选一个较优的作为检索的依据。 -- 增加一个没有建立索引的字段 alter table innodb1 add sex char(1); -- 按sex检索时可选的索引为null EXPLAIN SELECT * from innodb1 where sex='男'; 可以尝试在一个字段未建立索引时，根据该字段查询的效率，然后对该字段建立索引alter table 表名 add index(字段名)，同样的SQL执行的效率，你会发现查询效率会有明显的提升（数据量越大越明显）。 order by 当我们使用order by将查询结果按照某个字段排序时，如果该字段没有建立索引，那么执行计划会将查询出的所有数据使用外部排序（将数据从硬盘分批读取到内存使用内部排序，最后合并排序结果），这个操作是很影响性能的，因为需要将查询涉及到的所有数据从磁盘中读到内存（如果单条数据过大或者数据量过多都会降低效率），更无论读到内存之后的排序了。 但是如果我们对该字段建立索引alter table 表名 add index(字段名)，那么由于索引本身是有序的，因此直接按照索引的顺序和映射关系逐条取出数据即可。而且如果分页的，那么只用取出索引表某个范围内的索引对应的数据，而不用像上述那取出所有数据进行排序再返回某个范围内的数据。（从磁盘取数据是最影响性能的） join 对join语句匹配关系（on）涉及的字段建立索引能够提高效率 索引覆盖 如果要查询的字段都建立过索引，那么引擎会直接在索引表中查询而不会访问原始数据（否则只要有一个字段没有建立索引就会做全表扫描），这叫索引覆盖。因此我们需要尽可能的在select后只写必要的查询字段，以增加索引覆盖的几率。 这里值得注意的是不要想着为每个字段建立索引，因为优先使用索引的优势就在于其体积小。 索引类型 主键索引: 数据列不允许重复，不允许为NULL，一个表只能有一个主键。 唯一索引: 数据列不允许重复，允许为NULL值，一个表允许多个列创建唯一索引。 可以通过 ALTER TABLE table_name ADD UNIQUE (column); 创建唯一索引 可以通过 ALTER TABLE table_name ADD UNIQUE (column1,column2); 创建唯一组合索引 普通索引: 基本的索引类型，没有唯一性的限制，允许为NULL值。 可以通过ALTER TABLE table_name ADD INDEX index_name (column);创建普通索引 可以通过ALTER TABLE table_name ADD INDEX index_name(column1, column2, column3);创建组合索引 全文索引： 是目前搜索引擎使用的一种关键技术。 可以通过ALTER TABLE table_name ADD FULLTEXT (column);创建全文索引 索引设计的原则 适合索引的列是出现在where子句中的列，或者连接子句中指定的列 基数较小的类，索引效果较差，没有必要在此列建立索引 使用短索引，如果对长字符串列进行索引，应该指定一个前缀长度，这样能够节省大量索引空间 不要过度索引。索引需要额外的磁盘空间，并降低写操作的性能。在修改表内容的时候，索引会进行更新甚至重构，索引列越多，这个时间就会越长。所以只保持需要的索引有利于查询即可。 创建索引的原则（重中之重） 索引虽好，但也不是无限制的使用，最好符合一下几个原则 1） 最左前缀匹配原则，组合索引非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 2）较频繁作为查询条件的字段才去创建索引 3）更新频繁字段不适合创建索引 4）若是不能有效区分数据的列不适合做索引列(如性别，男女未知，最多也就三种，区分度实在太低) 5）尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。 6）定义有外键的数据列一定要建立索引。 7）对于那些查询中很少涉及的列，重复值比较多的列不要建立索引。 8）对于定义为text、image和bit的数据类型的列不要建立索引。 创建索引的三种方式 第一种方式：在执行CREATE TABLE时创建索引 CREATE TABLE user_index2 ( id INT auto_increment PRIMARY KEY, first_name VARCHAR ( 16 ), last_name VARCHAR ( 16 ), id_card VARCHAR ( 18 ), information text, KEY NAME ( first_name, last_name ), FULLTEXT KEY ( information ), UNIQUE KEY ( id_card ) ); 第二种方式：使用ALTER TABLE命令去增加索引 //ALTER TABLE用来创建普通索引、UNIQUE索引或PRIMARY KEY索引。 ALTER TABLE table_name ADD INDEX index_name (column_list); 其中table_name是要增加索引的表名，column_list指出对哪些列进行索引，多列时各列之间用逗号分隔。 索引名index_name可自己命名，缺省时，MySQL将根据第一个索引列赋一个名称。另外，ALTER TABLE允许在单个语句中更改多个表，因此可以在同时创建多个索引。 第三种方式：使用CREATE INDEX命令创建 //CREATE INDEX可对表增加普通索引或UNIQUE索引。（但是，不能创建PRIMARY KEY索引） CREATE INDEX index_name ON table_name (column_list); 删除索引 根据索引名删除普通索引、唯一索引、全文索引：alter table 表名 drop KEY 索引名 ALTER TABLE user_index DROP KEY NAME; ALTER TABLE user_index DROP KEY id_card; ALTER TABLE user_index DROP KEY information; 删除主键索引：alter table 表名 drop primary key（因为主键只有一个）。这里值得注意的是，如果主键自增长，那么不能直接执行此操作（自增长依赖于主键索引）： 需要取消自增长再行删除： alter table user_index -- 重新定义字段 MODIFY id int, drop PRIMARY KEY 但通常不会删除主键，因为设计主键一定与业务逻辑无关。 创建索引时需要注意什么？ 非空字段：应该指定列为NOT NULL，除非你想存储NULL。在mysql中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用0、一个特殊的值或者一个空串代替空值； 取值离散大的字段：（变量各个取值之间的差异程度）的列放到联合索引的前面，可以通过count()函数查看字段的差异值，返回值越大说明字段的唯一值越多字段的离散程度高； 索引字段越小越好：数据库的数据存储以页为单位一页存储的数据越多一次IO操作获取的数据越大效率越高。 使用索引查询一定能提高查询的性能吗？为什么 通常，通过索引查询数据比全表扫描要快。但是我们也必须注意到它的代价。 索引需要空间来存储，也需要定期维护， 每当有记录在表中增减或索引列被修改时，索引本身也会被修改。 这意味着每条记录的INSERT，DELETE，UPDATE将为此多付出4，5 次的磁盘I/O。 因为索引需要额外的存储空间和处理，那些不必要的索引反而会使查询反应时间变慢。使用索引查询不一定能提高查询性能，索引范围查询(INDEX RANGE SCAN)适用于两种情况: 基于一个范围的检索，一般查询返回结果集小于表中记录数的30% 基于非唯一性索引的检索 百万级别或以上的数据如何删除 关于索引：由于索引需要额外的维护成本，因为索引文件是单独存在的文件,所以当我们对数据的增加,修改,删除,都会产生额外的对索引文件的操作,这些操作需要消耗额外的IO,会降低增/改/删的执行效率。所以，在我们删除数据库百万级别数据的时候，查询MySQL官方手册得知删除数据的速度和创建的索引数量是成正比的。 所以我们想要删除百万数据的时候可以先删除索引（此时大概耗时三分多钟） 然后删除其中无用数据（此过程需要不到两分钟） 删除完成后重新创建索引(此时数据较少了)创建索引也非常快，约十分钟左右。 与之前的直接删除绝对是要快速很多，更别说万一删除中断,一切删除会回滚。那更是坑了。 前缀索引 语法：index(field(10))，使用字段值的前10个字符建立索引，默认是使用字段的全部内容建立索引。 前提：前缀的标识度高。比如密码就适合建立前缀索引，因为密码几乎各不相同。 实操的难度：在于前缀截取的长度。 我们可以利用select count(*)/count(distinct left(password,prefixLen));，通过从调整prefixLen的值（从1自增）查看不同前缀长度的一个平均匹配度，接近1时就可以了（表示一个密码的前prefixLen个字符几乎能确定唯一一条记录） 什么是最左前缀原则？什么是最左匹配原则 顾名思义，就是最左优先，在创建多列索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边。 最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 =和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式 索引算法 索引算法有 BTree算法和Hash算法 BTree算法 BTree是最常用的mysql数据库索引算法，也是mysql默认的算法。因为它不仅可以被用在=,&gt;,&gt;=,&lt;,&lt;=和between这些比较操作符上，而且还可以用于like操作符，只要它的查询条件是一个不以通配符开头的常量， 例如： -- 只要它的查询条件是一个不以通配符开头的常量 select * from user where name like 'jack%'; -- 如果一通配符开头，或者没有使用常量，则不会使用索引，例如： select * from user where name like '%jack'; Hash算法 Hash Hash索引只能用于对等比较，例如=,&lt;=&gt;（相当于=）操作符。由于是一次定位数据，不像BTree索引需要从根节点到枝节点，最后才能访问到页节点这样多次IO访问，所以检索效率远高于BTree索引。 索引的数据结构（b树，hash） 索引的数据结构和具体存储引擎的实现有关，在MySQL中使用较多的索引有Hash索引，B+树索引等，而我们经常使用的InnoDB存储引擎的默认索引实现为：B+树索引。对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引。 B树索引 mysql通过存储引擎取数据，基本上90%的人用的就是InnoDB了，按照实现方式分，InnoDB的索引类型目前只有两种：BTREE（B树）索引和HASH索引。B树索引是Mysql数据库中使用最频繁的索引类型，基本所有存储引擎都支持BTree索引。通常我们说的索引不出意外指的就是（B树）索引（实际是用B+树实现的，因为在查看表索引时SHOW INDEX FROM &lt;表名&gt; [ FROM &lt;数据库名&gt;]，mysql一律打印BTREE，所以简称为B树索引） 查询方式： 主键索引区:PI(关联保存的数据的地址)按主键查询, 普通索引区:si(关联的id的地址,然后再到达上面的地址)。所以按主键查询,速度最快 B+tree性质： 1.）n棵子tree的节点包含n个关键字，不用来保存数据而是保存数据的索引。 2.）所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 3.）所有的非终端结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字。 4.）B+ 树中，数据对象的插入和删除仅在叶节点上进行。 5.）B+树有2个头指针，一个是树的根节点，一个是最小关键码的叶节点。 哈希索引 简要说下，类似于数据结构中简单实现的HASH表（散列表）一样，当我们在mysql中用哈希索引时，主要就是通过Hash算法（常见的Hash算法有直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的Hash值，与这条数据的行指针一并存入Hash表的对应位置；如果发生Hash碰撞（两个不同关键字的Hash值相同），则在对应Hash键下以链表形式存储。当然这只是简略模拟图。 B树和B+树的区别 在B树中，你可以将键和值存放在内部节点和叶子节点；但在B+树中，内部节点都是键，没有值，叶子节点同时存放键和值。 B+树的叶子节点有一条链相连，而B树的叶子节点各自独立。 使用B树的好处 B树可以在内部节点同时存储键和值，因此，把频繁访问的数据放在靠近根节点的地方将会大大提高热点数据的查询效率。这种特性使得B树在特定数据重复多次查询的场景中更加高效。 使用B+树的好处 由于B+树的内部节点只存放键，不存放值，因此，一次读取，可以在内存页中获取更多的键，有利于更快地缩小查找范围。 B+树的叶节点由一条链相连，因此，当需要进行一次全数据遍历的时候，B+树只需要使用O(logN)时间找到最小的一个节点，然后通过链进行O(N)的顺序遍历即可。而B树则需要对树的每一层进行遍历，这会需要更多的内存置换次数，因此也就需要花费更多的时间 Hash索引和B+树对比 首先要知道Hash索引和B+树索引的底层实现原理： hash索引底层就是hash表，进行查找时，调用一次hash函数就可以获取到相应的键值，之后进行回表查询获得实际数据。B+树底层实现是多路平衡查找树。对于每一次的查询都是从根节点出发，查找到叶子节点方可以获得所查键值，然后根据查询判断是否需要回表查询数据。 那么可以看出他们有以下的不同： hash索引进行等值查询更快(一般情况下)，但是却无法进行范围查询。 因为在hash索引中经过hash函数建立索引之后，索引的顺序与原顺序无法保持一致，不能支持范围查询。而B+树的的所有节点皆遵循(左节点小于父节点，右节点大于父节点，多叉树也类似)，天然支持范围。 hash索引不支持使用索引进行排序，原理同上。 hash索引不支持模糊查询以及多列索引的最左前缀匹配。原理也是因为hash函数的不可预测。AAAA和AAAAB的索引没有相关性。 hash索引任何时候都避免不了回表查询数据，而B+树在符合某些条件(聚簇索引，覆盖索引等)的时候可以只通过索引完成查询。 hash索引虽然在等值查询上较快，但是不稳定。性能不可预测，当某个键值存在大量重复的时候，发生hash碰撞，此时效率可能极差。而B+树的查询效率比较稳定，对于所有的查询都是从根节点到叶子节点，且树的高度较低。 因此，在大多数情况下，直接选择B+树索引可以获得稳定且较好的查询速度。而不需要使用hash索引。 数据库为什么使用B+树而不是B树 B树只适合随机检索，而B+树同时支持随机检索和顺序检索； B+树空间利用率更高，可减少I/O次数，磁盘读写代价更低。一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗。B+树的内部结点并没有指向关键字具体信息的指针，只是作为索引使用，其内部结点比B树小，盘块能容纳的结点中关键字数量更多，一次性读入内存中可以查找的关键字也就越多，相对的，IO读写次数也就降低了。而IO读写次数是影响索引检索效率的最大因素； B+树的查询效率更加稳定。B树搜索有可能会在非叶子结点结束，越靠近根节点的记录查找时间越短，只要找到关键字即可确定记录的存在，其性能等价于在关键字全集内做一次二分查找。而在B+树中，顺序检索比较明显，随机检索时，任何关键字的查找都必须走一条从根节点到叶节点的路，所有关键字的查找路径长度相同，导致每一个关键字的查询效率相当。 B-树在提高了磁盘IO性能的同时并没有解决元素遍历的效率低下的问题。B+树的叶子节点使用指针顺序连接在一起，只要遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作。 增删文件（节点）时，效率更高。因为B+树的叶子节点包含所有关键字，并以有序的链表结构存储，这样可很好提高增删效率。 B+树在满足聚簇索引和覆盖索引的时候不需要回表查询数据 在B+树的索引中，叶子节点可能存储了当前的key值，也可能存储了当前的key值以及整行的数据，这就是聚簇索引和非聚簇索引。 在InnoDB中，只有主键索引是聚簇索引，如果没有主键，则挑选一个唯一键建立聚簇索引。如果没有唯一键，则隐式的生成一个键来建立聚簇索引。 当查询使用聚簇索引时，在对应的叶子节点，可以获取到整行数据，因此不用再次进行回表查询。 什么是聚簇索引？何时使用聚簇索引与非聚簇索引 聚簇索引：将数据存储与索引放到了一块，找到索引也就找到了数据 非聚簇索引：将数据存储于索引分开结构，索引结构的叶子节点指向了数据的对应行，myisam通过key_buffer把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在key buffer命中时，速度慢的原因 澄清一个概念：innodb中，在聚簇索引之上创建的索引称之为辅助索引，辅助索引访问数据总是需要二次查找，非聚簇索引都是辅助索引，像复合索引、前缀索引、唯一索引，辅助索引叶子节点存储的不再是行的物理位置，而是主键值 何时使用聚簇索引与非聚簇索引 非聚簇索引一定会回表查询吗？ 不一定，这涉及到查询语句所要求的字段是否全部命中了索引，如果全部命中了索引，那么就不必再进行回表查询。 举个简单的例子，假设我们在员工表的年龄上建立了索引，那么当进行select age from employee where age &lt; 20的查询时，在索引的叶子节点上，已经包含了age信息，不会再次进行回表查询。 联合索引 MySQL可以使用多个字段同时建立一个索引，叫做联合索引。 在联合索引中，如果想要命中索引，需要按照建立索引时的字段顺序挨个使用，否则无法命中索引。 具体原因为: MySQL使用索引时需要索引有序，假设现在建立了&quot;name，age，school&quot;的联合索引，那么索引的排序为: 先按照name排序，如果name相同，则按照age排序，如果age的值也相等，则按照school进行排序。 当进行查询时，此时索引仅仅按照name严格有序，因此必须首先使用name字段进行等值查询，之后对于匹配到的列而言，其按照age字段严格有序，此时可以使用age字段用做索引查找，以此类推。因此在建立联合索引的时候应该注意索引列的顺序，一般情况下，将查询需求频繁或者字段选择性高的列放在前面。此外可以根据特例的查询或者表结构进行单独的调整。 漫画：什么是B-树？https://tinaxiawuhao.github.io/post/LhRxiTagh/ 漫画：什么是B+树？https://tinaxiawuhao.github.io/post/jVTlR3ljT/ ","link":"https://tinaxiawuhao.github.io/post/QzFPAEzVT/"},{"title":"mysql概述一","content":"为什么要使用数据库 数据保存在内存 优点： 存取速度快 缺点： 数据不能永久保存 数据保存在文件 优点： 数据永久保存 缺点：1）速度比内存操作慢，频繁的IO操作。 ​ 2）查询数据不方便 数据保存在数据库 1）数据永久保存 2）使用SQL语句，查询方便效率高。 3）管理数据方便 SQL 结构化查询语言(Structured Query Language)简称SQL，是一种数据库查询语言。 作用：用于存取数据、查询、更新和管理关系数据库系统。 MySQL MySQL是一个关系型数据库管理系统，由瑞典MySQL AB 公司开发，属于 Oracle 旗下产品。MySQL 是最流行的关系型数据库管理系统之一，在 WEB 应用方面，MySQL是最好的 RDBMS (Relational Database Management System，关系数据库管理系统) 应用软件之一。在Java企业级开发中非常常用，因为 MySQL 是开源免费的，并且方便扩展。 数据库三大范式是什么 第一范式：每个列都不可以再拆分。 第二范式：在第一范式的基础上，非主键列完全依赖于主键，而不能是依赖于主键的一部分。 第三范式：在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键。 在设计数据库结构的时候，要尽量遵守三范式，如果不遵守，必须有足够的理由。比如性能。事实上我们经常会为了性能而妥协数据库的设计。 mysql有关权限的表 MySQL服务器通过权限表来控制用户对数据库的访问，权限表存放在mysql数据库里，由mysql_install_db脚本初始化。这些权限表分别user，db，table_priv，columns_priv和host。下面分别介绍一下这些表的结构和内容： user权限表：记录允许连接到服务器的用户帐号信息，里面的权限是全局级的。 db权限表：记录各个帐号在各个数据库上的操作权限。 table_priv权限表：记录数据表级的操作权限。 columns_priv权限表：记录数据列级的操作权限。 host权限表：配合db权限表对给定主机上数据库级操作权限作更细致的控制。这个权限表不受GRANT和REVOKE语句的影响。 MySQL的binlog有几种录入格式？分别有什么区别？ 有三种格式，statement，row和mixed。 statement模式下，每一条会修改数据的sql都会记录在binlog中。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。由于sql的执行是有上下文的，因此在保存的时候需要保存相关的信息，同时还有一些使用了函数之类的语句无法被记录复制。 row级别下，不记录sql语句上下文相关信息，仅保存哪条记录被修改。记录单元为每一行的改动，基本是可以全部记下来但是由于很多操作，会导致大量行的改动(比如alter table)，因此这种模式的文件保存的信息太多，日志量太大。 mixed，一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。 此外，新版的MySQL中对row级别也做了一些优化，当表结构发生变化的时候，会记录语句而不是逐行记录。 数据类型 分类 类型名称 说明 整数类型 tinyInt 很小的整数(8位二进制) smallint 小的整数(16位二进制) mediumint 中等大小的整数(24位二进制) int(integer) 普通大小的整数(32位二进制) 小数类型 float 单精度浮点数 double 双精度浮点数 decimal(m,d) 压缩严格的定点数 日期类型 year YYYY 1901~2155 time HH:MM:SS -838:59:59~838:59:59 date YYYY-MM-DD 1000-01-01~9999-12-3 datetime YYYY-MM-DD HH:MM:SS 1000-01-01 00:00:00~ 9999-12-31 23:59:59 timestamp YYYY-MM-DD HH:MM:SS 19700101 00:00:01 UTC~2038-01-19 03:14:07UTC 文本、二进制类型 CHAR(M) M为0~255之间的整数 VARCHAR(M) M为0~65535之间的整数 TINYBLOB 允许长度0~255字节 BLOB 允许长度0~65535字节 MEDIUMBLOB 允许长度0~167772150字节 LONGBLOB 允许长度0~4294967295字节 TINYTEXT 允许长度0~255字节 TEXT 允许长度0~65535字节 MEDIUMTEXT 允许长度0~167772150字节 LONGTEXT 允许长度0~4294967295字节 VARBINARY(M) 允许长度0~M个字节的变长字节字符串 BINARY(M) 允许长度0~M个字节的定长字节字符串 1、整数类型，包括TINYINT、SMALLINT、MEDIUMINT、INT、BIGINT，分别表示1字节、2字节、3字节、4字节、8字节整数。任何整数类型都可以加上UNSIGNED属性，表示数据是无符号的，即非负整数。 长度：整数类型可以被指定长度，例如：INT(11)表示长度为11的INT类型。长度在大多数场景是没有意义的，它不会限制值的合法范围，只会影响显示字符的个数，而且需要和UNSIGNED ZEROFILL属性配合使用才有意义。 例子，假定类型设定为INT(5)，属性为UNSIGNED ZEROFILL，如果用户插入的数据为12的话，那么数据库实际存储数据为00012。 2、实数类型，包括FLOAT、DOUBLE、DECIMAL。 DECIMAL可以用于存储比BIGINT还大的整型，能存储精确的小数。 而FLOAT和DOUBLE是有取值范围的，并支持使用标准的浮点进行近似计算。 计算时FLOAT和DOUBLE相比DECIMAL效率更高一些，DECIMAL你可以理解成是用字符串进行处理。 3、字符串类型，包括VARCHAR、CHAR、TEXT、BLOB VARCHAR用于存储可变长字符串，它比定长类型更节省空间。 VARCHAR使用额外1或2个字节存储字符串长度。列长度小于255字节时，使用1字节表示，否则使用2字节表示。 VARCHAR存储的内容超出设置的长度时，内容会被截断。 CHAR是定长的，根据定义的字符串长度分配足够的空间。 CHAR会根据需要使用空格进行填充方便比较。 CHAR适合存储很短的字符串，或者所有值都接近同一个长度。 CHAR存储的内容超出设置的长度时，内容同样会被截断。 使用策略： 对于经常变更的数据来说，CHAR比VARCHAR更好，因为CHAR不容易产生碎片。 对于非常短的列，CHAR比VARCHAR在存储空间上更有效率。 使用时要注意只分配需要的空间，更长的列排序时会消耗更多内存。 尽量避免使用TEXT/BLOB类型，查询时会使用临时表，导致严重的性能开销。 4、枚举类型（ENUM），把不重复的数据存储为一个预定义的集合。 有时可以使用ENUM代替常用的字符串类型。 ENUM存储非常紧凑，会把列表值压缩到一个或两个字节。 ENUM在内部存储时，其实存的是整数。 尽量避免使用数字作为ENUM枚举的常量，因为容易混乱。 排序是按照内部存储的整数 5、日期和时间类型，尽量使用timestamp，空间效率高于datetime， 用整数保存时间戳通常不方便处理。 如果需要存储微妙，可以使用bigint存储。 MySQL存储引擎 存储引擎Storage engine：MySQL中的数据、索引以及其他对象是如何存储的，是一套文件系统的实现。 常用的存储引擎有以下： Innodb引擎：Innodb引擎提供了对数据库ACID事务的支持。并且还提供了行级锁和外键的约束。它的设计的目标就是处理大数据容量的数据库系统。 MyIASM引擎(原本Mysql的默认引擎)：不提供事务的支持，也不支持行级锁和外键。 MEMORY引擎：所有的数据都在内存中，数据的处理速度快，但是安全性不高。 MyISAM与InnoDB区别 MyISAM Innodb 存储结构 每张表被存放在三个文件：frm-表格定义、MYD(MYData)-数据文件、MYI(MYIndex)-索引文件 所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB 存储空间 MyISAM可被压缩，存储空间较小 InnoDB的表需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引 可移植性、备份及恢复 由于MyISAM的数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作 免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump，在数据量达到几十G的时候就相对痛苦了 文件格式 数据和索引是分别存储的，数据.MYD，索引.MYI 数据和索引是集中存储的，.ibd 记录存储顺序 按记录插入顺序保存 按主键大小有序插入 外键 不支持 支持 事务 不支持 支持 锁支持（锁是避免资源争用的一个机制，MySQL锁对用户几乎是透明的） 表级锁定 行级锁定、表级锁定，锁定力度小并发能力高 SELECT MyISAM更优 INSERT、UPDATE、DELETE InnoDB更优 select count(*) myisam更快，因为myisam内部维护了一个计数器，可以直接调取。 索引的实现方式 B+树索引，myisam 是堆表 B+树索引，Innodb 是索引组织表 哈希索引 不支持 支持 全文索引 支持 不支持 MyISAM索引与InnoDB索引的区别？ InnoDB索引是聚簇索引，MyISAM索引是非聚簇索引。 InnoDB的主键索引的叶子节点存储着行数据，因此主键索引非常高效。 MyISAM索引的叶子节点存储的是行数据地址，需要再寻址一次才能得到数据。 InnoDB非主键索引的叶子节点存储的是主键和其他带索引的列数据，因此查询时做到覆盖索引会非常高效。 InnoDB引擎的4大特性 插入缓冲（Insert Buffer/Change Buffer) 提升插入性能，change buffering是insert buffer的加强，insert buffer只针对insert有效，change buffering对insert、delete、update(delete+insert)、purge都有效 只对于非聚集索引（非唯一）的插入和更新有效，对于每一次的插入不是写到索引页中，而是先判断插入的非聚集索引页是否在缓冲池中，如果在则直接插入；若不在，则先放到Insert Buffer 中，再按照一定的频率进行合并操作，再写回disk。这样通常能将多个插入合并到一个操作中，目的还是为了减少随机IO带来性能损耗。 使用插入缓冲的条件： * 非聚集索引 * 非唯一索引 Change buffer是作为buffer pool中的一部分存在。Innodb_change_buffering参数缓存所对应的操作：(update会被认为是delete+insert) innodb_change_buffering，设置的值有：inserts、deletes、purges、changes（inserts和deletes）、all（默认）、none。 all: 默认值，缓存insert, delete, purges操作 none: 不缓存 inserts: 缓存insert操作 deletes: 缓存delete操作 changes: 缓存insert和delete操作 purges: 缓存后台执行的物理删除操作 可以通过参数控制其使用的大小： innodb_change_buffer_max_size，默认是25%，即缓冲池的1/4。最大可设置为50%。当MySQL实例中有大量的修改操作时，要考虑增大innodb_change_buffer_max_size 上面提过在一定频率下进行合并，那所谓的频率是什么条件？ 1）辅助索引页被读取到缓冲池中。正常的select先检查Insert Buffer是否有该非聚集索引页存在，若有则合并插入。 2）辅助索引页没有可用空间。空间小于1/32页的大小，则会强制合并操作。 3）Master Thread 每秒和每10秒的合并操作。 二次写(double write) Doublewrite缓存是位于系统表空间的存储区域，用来缓存InnoDB的数据页从innodb buffer pool中flush之后并写入到数据文件之前，所以当操作系统或者数据库进程在数据页写磁盘的过程中崩溃，Innodb可以在doublewrite缓存中找到数据页的备份而用来执行crash恢复。数据页写入到doublewrite缓存的动作所需要的IO消耗要小于写入到数据文件的消耗，因为此写入操作会以一次大的连续块的方式写入 在应用（apply）重做日志前，用户需要一个页的副本，当写入失效发生时，先通过页的副本来还原该页，再进行重做，这就是double write doublewrite组成： 内存中的doublewrite buffer,大小2M。 物理磁盘上共享表空间中连续的128个页，即2个区（extend），大小同样为2M。 对缓冲池的脏页进行刷新时，不是直接写磁盘，而是会通过memcpy()函数将脏页先复制到内存中的doublewrite buffer，之后通过doublewrite 再分两次，每次1M顺序地写入共享表空间的物理磁盘上，在这个过程中，因为doublewrite页是连续的，因此这个过程是顺序写的，开销并不是很大。在完成doublewrite页的写入后，再将doublewrite buffer 中的页写入各个 表空间文件中，此时的写入则是离散的。如果操作系统在将页写入磁盘的过程中发生了崩溃，在恢复过程中，innodb可以从共享表空间中的doublewrite中找到该页的一个副本，将其复制到表空间文件，再应用重做日志。 自适应哈希索引(ahi) Adaptive Hash index属性使得InnoDB更像是内存数据库。该属性通过innodb_adapitve_hash_index开启，也可以通过—skip-innodb_adaptive_hash_index参数 Innodb存储引擎会监控对表上二级索引的查找，如果发现某二级索引被频繁访问，二级索引成为热数据，建立哈希索引可以带来速度的提升 经常访问的二级索引数据会自动被生成到hash索引里面去(最近连续被访问三次的数据)，自适应哈希索引通过缓冲池的B+树构造而来，因此建立的速度很快。 哈希（hash）是一种非常快的等值查找方法，在一般情况下这种查找的时间复杂度为O(1),即一般仅需要一次查找就能定位数据。而B+树的查找次数，取决于B+树的高度，在生产环境中，B+树的高度一般3-4层，故需要3-4次的查询。 innodb会监控对表上个索引页的查询。如果观察到建立哈希索引可以带来速度提升，则自动建立哈希索引，称之为自适应哈希索引（Adaptive Hash Index，AHI）。 AHI有一个要求，就是对这个页的连续访问模式必须是一样的。 例如对于（a,b）访问模式情况： where a = xxx where a = xxx and b = xxx 特点 1、无序，没有树高 2、降低对二级索引树的频繁访问资源，索引树高&lt;=4，访问索引：访问树、根节点、叶子节点 3、自适应 缺陷 1、hash自适应索引会占用innodb buffer pool； 2、自适应hash索引只适合搜索等值的查询，如select * from table where index_col='xxx'，而对于其他查找类型，如范围查找，是不能使用的； 3、极端情况下，自适应hash索引才有比较大的意义，可以降低逻辑读。 预读(read ahead) InnoDB使用两种预读算法来提高I/O性能：线性预读（linear read-ahead）和随机预读（randomread-ahead） 为了区分这两种预读的方式，我们可以把线性预读放到以extent为单位，而随机预读放到以extent中的page为单位。线性预读着眼于将下一个extent提前读取到buffer pool中，而随机预读着眼于将当前extent中的剩余的page提前读取到buffer pool中。 线性预读（linear read-ahead） 线性预读方式有一个很重要的变量控制是否将下一个extent预读到buffer pool中，通过使用配置参数innodb_read_ahead_threshold，可以控制Innodb执行预读操作的时间。如果一个extent中的被顺序读取的page超过或者等于该参数变量时，Innodb将会异步的将下一个extent读取到buffer pool中，innodb_read_ahead_threshold可以设置为0-64的任何值，默认值为56，值越高，访问模式检查越严格 例如，如果将值设置为48，则InnoDB只有在顺序访问当前extent中的48个pages时才触发线性预读请求，将下一个extent读到内存中。如果值为8，InnoDB触发异步预读，即使程序段中只有8页被顺序访问。你可以在MySQL配置文件中设置此参数的值，或者使用SET GLOBAL需要该SUPER权限的命令动态更改该参数。 在没有该变量之前，当访问到extent的最后一个page的时候，Innodb会决定是否将下一个extent放入到buffer pool中。 随机预读（randomread-ahead） 随机预读方式则是表示当同一个extent中的一些page在buffer pool中发现时，Innodb会将该extent中的剩余page一并读到buffer pool中，由于随机预读方式给Innodb code带来了一些不必要的复杂性，同时在性能也存在不稳定性，在5.5中已经将这种预读方式废弃。要启用此功能，请将配置变量设置innodb_random_read_ahead为ON。 存储引擎选择 如果没有特别的需求，使用默认的Innodb即可。 MyISAM：以读写插入为主的应用程序，比如博客系统、新闻门户网站。 Innodb：更新（删除）操作频率也高，或者要保证数据的完整性；并发量高，支持事务和外键。比如OA自动化办公系统。 MySQL查询过程 ​ MySQL查询优化实质说白了就是遵循一些原则让MySQL的优化器能够按照预想的合理方式运行，从而达到不同的业务目标需要实现的效果。下图展示了MySQL的查询过程。 客户端/服务端通信协议 MySQL客户端/服务端通信协议是&quot;半双工&quot;的：在任意时刻，要么是服务器向客户端发送数据，要么是客户端向服务器发送数据，这两个动作不能同时发生。一旦一端开始发送消息，另一端要接收完整个消息才能响应它，所以我们无法也无须将一个消息切成小块独立发送，也没有办法进行流量控制。 客户端用一个单独的数据包将查询请求发送给服务器，所以当查询语句很长的时候，需要设置max_allowed_packet参数。但是需要注意的是，如果查询实在是太大，服务端会拒绝接收更多数据并抛出异常。 与之相反的是，服务器响应给用户的数据通常会很多，由多个数据包组成。但是当服务器响应客户端请求时，客户端必须完整的接收整个返回结果，而不能简单的只取前面几条结果，然后让服务器停止发送。因而在实际开发中，尽量保持查询简单且只返回必需的数据，减小通信间数据包的大小和数量是一个非常好的习惯，这也是查询中尽量避免使用SELECT *以及加上LIMIT限制的原因之一。 查询缓存 在解析一个查询语句前，如果查询缓存是打开的，那么MySQL会检查这个查询语句是否命中查询缓存中的数据。如果当前查询恰好命中查询缓存，在检查一次用户权限后直接返回缓存中的结果。这种情况下，查询不会被解析，也不会生成执行计划，更不会执行。 MySQL将缓存存放在一个引用表（类似于HashMap的数据结构）中，通过哈希值索引，这个哈希值通过查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息计算得来。所以两个查询在任何字符上的不同（例如：空格、注释），都会导致缓存不会命中。 如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、mysql库中的系统表，其查询结果都不会被缓存。比如函数NOW()或者CURRENT_DATE()会因为不同的查询时间，返回不同的查询结果，再比如包含CURRENT_USER或者CONNECION_ID()的查询语句会因为不同的用户而返回不同的结果，将这样的查询结果缓存起来没有任何的意义。 MySQL的查询缓存系统会跟踪查询中涉及的每个表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效。正因为如此，在任何的写操作时，MySQL必须将对应表的所有缓存都设置为失效。如果查询缓存非常大或者碎片很多，这个操作就可能带来很大的系统消耗，甚至导致系统僵死一会儿。而且查询缓存对系统的额外消耗也不仅仅在写操作，读操作也不例外。如果查询结果被缓存，那么执行完成后，会将结果存入缓存，也会带来额外的系统消耗。基于此，我们知道并不是什么情况下查询缓存都会提高系统性能，缓存和失效都会带来额外消耗，只有当缓存带来的资源节约大于其本身消耗的资源时，才会给系统带来性能提升。如果系统确实存在一些性能问题，可以尝试打开查询缓存，并在数据库设计上做一些优化，比如： 1）多个小表代替一个大表（注意不要过度设计） 2）批量插入代替循环单条插入，降低磁盘IO次数 3）合理控制缓存空间大小，一般来说其大小设置为几十兆比较合适 4）可以通过SQL_CACHE和SQL_NO_CACHE来控制某个查询语句是否需要进行缓存 语法解析和预处理 MySQL通过关键字将SQL语句进行解析并生成一颗对应的解析树。这个过程解析器主要通过语法规则来验证和解析，比如SQL中是否使用了错误的关键字或者关键字的顺序是否正确等等。预处理则会根据MySQL规则进一步检查解析树是否合法，比如检查要查询的数据表和数据列是否存在等等。 查询优化 经过前面的步骤生成的语法树被认为是合法的了，并且由优化器将其转化成查询计划。多数情况下一条查询可以有很多种执行方式，最后都返回相应的结果。优化器的作用就是找到这其中最好的执行计划。MySQL使用基于成本的优化器，它尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个。在MySQL可以通过查询当前会话的last_query_cost的值来得到其计算当前查询的成本。 有非常多的原因会导致MySQL选择错误的执行计划，比如统计信息不准确、不会考虑不受其控制的操作成本（用户自定义函数、存储过程）、MySQL认为的最优跟我们想的不一样，我们希望执行时间尽可能短，但MySQL只选择它认为成本小的，但成本小有的时候并不是我们的预期。 查询执行引擎 在完成解析和优化阶段以后，MySQL会生成对应的执行计划，查询执行引擎根据执行计划给出的指令逐步执行得出结果。整个执行过程的大部分操作均是通过调用存储引擎实现的接口来完成，这些接口被称为handler API。查询过程中的每一张表由一个handler实例表示。实际上，MySQL在查询优化阶段就为每一张表创建了一个handler实例，优化器可以根据这些实例的接口来获取表的相关信息，包括表的所有列名、索引统计信息等。存储引擎接口提供了非常丰富的功能，但其底层仅有几十个接口，这些接口像搭积木一样完成了一次查询的大部分操作。 返回结果给客户端 查询执行的最后一个阶段就是将结果返回给客户端。即使查询不到数据，MySQL仍然会返回这个查询的相关信息，比如该查询影响到的行数以及执行时间等等。如果查询缓存被打开且这个查询可以被缓存，MySQL也会将结果存放到缓存中。结果集返回客户端是一个增量且逐步返回的过程。有可能MySQL在生成第一条结果时，就开始向客户端逐步返回结果集了。这样服务端就无须存储太多结果而消耗过多内存，也可以让客户端第一时间获得返回结果。需要注意的是，结果集中的每一行都会以一个满足客户端/服务器通信协议的数据包发送，再通过TCP协议进行传输，在传输过程中，可能对MySQL的数据包进行缓存然后批量发送。 ","link":"https://tinaxiawuhao.github.io/post/Nq-4WaiGL/"},{"title":"redis概述五","content":"分布式问题 Redis实现分布式锁 Redis为单进程单线程模式，采用队列模式将并发访问变成串行访问，且多客户端对Redis的连接并不存在竞争关系Redis中可以使用SETNX命令实现分布式锁。 当且仅当 key 不存在，将 key 的值设为 value。 若给定的 key 已经存在，则 SETNX 不做任何动作 SETNX 是『SET if Not eXists』(如果不存在，则 SET)的简写。 返回值：设置成功，返回 1 。设置失败，返回 0 。 使用SETNX完成同步锁的流程及事项如下： 使用SETNX命令获取锁，若返回0（key已存在，锁已存在）则获取失败，反之获取成功 为了防止获取锁后程序出现异常，导致其他线程/进程调用SETNX命令总是返回0而进入死锁状态，需要为该key设置一个“合理”的过期时间 释放锁，使用DEL命令将锁数据删除 package com.example.redisstudy.template; import org.apache.commons.lang3.StringUtils; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.data.redis.core.StringRedisTemplate; import org.springframework.stereotype.Component; import java.util.concurrent.TimeUnit; /** * @Description //直接使用Redis进行分布式锁 * 这是简易版本 如果要使用Redis原生锁记得加过期时间，防止死锁 最好使用Redisson操作简单更加方便 * @Date * @Author wuhao **/ @Component public class RedisLockCommon { @Autowired private StringRedisTemplate stringRedisTemplate; private Integer EXPIRE_TIME=3000; /** * Redis加锁的操作 * * @param key * @param value * @return */ public Boolean tryLock(String key, String value) { if (stringRedisTemplate.opsForValue().setIfAbsent(key, value, EXPIRE_TIME, TimeUnit.SECONDS)) { return true; } return false; } /** * Redis解锁的操作 * * @param key * @param value */ public void unlock(String key, String value) { String currentValue = stringRedisTemplate.opsForValue().get(key); try { if (StringUtils.isNotEmpty(currentValue) &amp;&amp; currentValue.equals(value)) { stringRedisTemplate.opsForValue().getOperations().delete(key); } } catch (Exception e) { } } } 如何解决 Redis 的并发竞争 Key 问题 所谓 Redis 的并发竞争 Key 的问题也就是多个系统同时对一个 key 进行操作，但是最后执行的顺序和我们期望的顺序不同，这样也就导致了结果的不同！ 推荐一种方案：分布式锁（zookeeper 和 redis 都可以实现分布式锁）。（如果不存在 Redis 的并发竞争 Key 问题，不要使用分布式锁，这样会影响性能） 基于zookeeper临时有序节点可以实现的分布式锁。大致思想为：每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。完成业务流程后，删除对应的子节点释放锁。 在实践中，当然是从以可靠性为主。所以首推Zookeeper。 参考：https://www.jianshu.com/p/8bddd381de06 分布式Redis是前期做还是后期规模上来了再做好？为什么？ 既然Redis是如此的轻量（单实例只使用1M内存），为防止以后的扩容，最好的办法就是一开始就启动较多实例。即便你只有一台服务器，你也可以一开始就让Redis以分布式的方式运行，使用分区，在同一台服务器上启动多个实例。 一开始就多设置几个Redis实例，例如32或者64个实例，对大多数用户来说这操作起来可能比较麻烦，但是从长久来看做这点牺牲是值得的。 这样的话，当你的数据不断增长，需要更多的Redis服务器时，你需要做的就是仅仅将Redis实例从一台服务迁移到另外一台服务器而已（而不用考虑重新分区的问题）。一旦你添加了另一台服务器，你需要将你一半的Redis实例从第一台机器迁移到第二台机器。 什么是 RedLock Redis 官方站提出了一种权威的基于 Redis 实现分布式锁的方式名叫 Redlock，此种方式比原先的单节点的方法更安全。它可以保证以下特性： 安全特性：互斥访问，即永远只有一个 client 能拿到锁 避免死锁：最终 client 都可能拿到锁，不会出现死锁的情况，即使原本锁住某资源的 client crash 了或者出现了网络分区 容错性：只要大部分 Redis 节点存活就可以正常提供服务 package com.example.redisstudy.template; import org.springframework.data.redis.connection.RedisConnection; import org.springframework.data.redis.connection.RedisConnectionFactory; import org.springframework.data.redis.connection.ReturnType; import org.springframework.data.redis.core.RedisConnectionUtils; import org.springframework.data.redis.core.StringRedisTemplate; import org.springframework.stereotype.Repository; import java.nio.charset.Charset; import java.util.UUID; import java.util.concurrent.TimeUnit; @Repository public class RedisLock { /** * 解锁脚本，原子操作 */ private static final String unlockScript = &quot;if redis.call(\\&quot;get\\&quot;,KEYS[1]) == ARGV[1]\\n&quot; + &quot;then\\n&quot; + &quot; return redis.call(\\&quot;del\\&quot;,KEYS[1])\\n&quot; + &quot;else\\n&quot; + &quot; return 0\\n&quot; + &quot;end&quot;; private StringRedisTemplate redisTemplate; public RedisLock(StringRedisTemplate redisTemplate) { this.redisTemplate = redisTemplate; } /** * 加锁，有阻塞 * @param name * @param expire * @param timeout * @return */ public String lock(String name, long expire, long timeout){ long startTime = System.currentTimeMillis(); String token; do{ token = tryLock(name, expire); if(token == null) { if((System.currentTimeMillis()-startTime) &gt; (timeout-50)) break; try { Thread.sleep(50); //try 50 per sec } catch (InterruptedException e) { e.printStackTrace(); return null; } } }while(token==null); return token; } /** * 加锁，无阻塞 * @param name * @param expire * @return */ public String tryLock(String name, long expire) { String token = UUID.randomUUID().toString(); if (redisTemplate.opsForValue().setIfAbsent(name, token, expire, TimeUnit.SECONDS)) { return token; } return null; } /** * 解锁 * @param name * @param token * @return */ public boolean unlock(String name, String token) { byte[][] keysAndArgs = new byte[2][]; keysAndArgs[0] = name.getBytes(Charset.forName(&quot;UTF-8&quot;)); keysAndArgs[1] = token.getBytes(Charset.forName(&quot;UTF-8&quot;)); RedisConnectionFactory factory = redisTemplate.getConnectionFactory(); RedisConnection conn = factory.getConnection(); try { Long result = (Long)conn.scriptingCommands().eval(unlockScript.getBytes(Charset.forName(&quot;UTF-8&quot;)), ReturnType.INTEGER, 1, keysAndArgs); if(result!=null &amp;&amp; result&gt;0) return true; }finally { RedisConnectionUtils.releaseConnection(conn, factory); } return false; } } 缓存雪崩 缓存雪崩是指缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。 解决方案 缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生。 一般并发量不是特别多的时候，使用最多的解决方案是加锁排队。 给每一个缓存数据增加相应的缓存标记，记录缓存的是否失效，如果缓存标记失效，则更新数据缓存。 缓存穿透 缓存穿透是指缓存和数据库中都没有的数据，导致所有的请求都落到数据库上，造成数据库短时间内承受大量请求而崩掉。 解决方案 接口层增加校验，如用户鉴权校验，id做基础校验，id&lt;=0的直接拦截； 从缓存取不到的数据，在数据库中也没有取到，这时也可以将key-value对写为key-null，缓存有效时间可以设置短点，如30秒（设置太长会导致正常情况也没法使用）。这样可以防止攻击用户反复用同一个id暴力攻击 采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的 bitmap 中，一个一定不存在的数据会被这个 bitmap 拦截掉，从而避免了对底层存储系统的查询压力 附加 对于空间的利用到达了一种极致，那就是Bitmap和布隆过滤器(Bloom Filter)。 Bitmap： 典型的就是哈希表 缺点是，Bitmap对于每个元素只能记录1bit信息，如果还想完成额外的功能，恐怕只能靠牺牲更多的空间、时间来完成了。 布隆过滤器（推荐） 就是引入了k(k&gt;1)个相互独立的哈希函数，保证在给定的空间、误判率下，完成元素判重的过程。 它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。 Bloom-Filter算法的核心思想就是利用多个不同的Hash函数来解决“冲突”。 Hash存在一个冲突（碰撞）的问题，用同一个Hash得到的两个URL的值有可能相同。为了减少冲突，我们可以多引入几个Hash，如果通过其中的一个Hash值我们得出某元素不在集合中，那么该元素肯定不在集合中。只有在所有的Hash函数告诉我们该元素在集合中时，才能确定该元素存在于集合中。这便是Bloom-Filter的基本思想。 Bloom-Filter一般用于在大数据量的集合中判定某元素是否存在。 缓存击穿 缓存击穿是指缓存中没有但数据库中有的数据（一般是缓存时间到期），这时由于并发用户特别多，同时读缓存没读到数据，又同时去数据库去取数据，引起数据库压力瞬间增大，造成过大压力。和缓存雪崩不同的是，缓存击穿指并发查同一条数据，缓存雪崩是不同数据都过期了，很多数据都查不到从而查数据库。 解决方案 设置热点数据永远不过期。 加互斥锁，互斥锁 缓存预热 缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！ 解决方案 直接写个缓存刷新页面，上线时手工操作一下； 数据量不大，可以在项目启动的时候自动进行加载； 定时刷新缓存； 缓存降级 当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。 缓存降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）。 在进行降级之前要对系统进行梳理，看看系统是不是可以丢卒保帅；从而梳理出哪些必须誓死保护，哪些可降级；比如可以参考日志级别设置预案： 一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级； 警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警； 错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级； 严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。 服务降级的目的，是为了防止Redis服务故障，导致数据库跟着一起发生雪崩问题。因此，对于不重要的缓存数据，可以采取服务降级策略，例如一个比较常见的做法就是，Redis出现问题，不去数据库查询，而是直接返回默认值给用户。 热点数据和冷数据 热点数据，缓存才有价值 对于冷数据而言，大部分数据可能还没有再次访问到就已经被挤出内存，不仅占用内存，而且价值不大。频繁修改的数据，看情况考虑使用缓存 对于热点数据，比如我们的某IM产品，生日祝福模块，当天的寿星列表，缓存以后可能读取数十万次。再举个例子，某导航产品，我们将导航信息，缓存以后可能读取数百万次。 数据更新前至少读取两次，缓存才有意义。这个是最基本的策略，如果缓存还没有起作用就失效了，那就没有太大价值了。 那存不存在，修改频率很高，但是又不得不考虑缓存的场景呢？有！比如，这个读取接口对数据库的压力很大，但是又是热点数据，这个时候就需要考虑通过缓存手段，减少数据库的压力，比如我们的某助手产品的，点赞数，收藏数，分享数等是非常典型的热点数据，但是又不断变化，此时就需要将数据同步保存到Redis缓存，减少数据库压力。 缓存热点key 缓存中的一个Key(比如一个促销商品)，在某个时间点过期的时候，恰好在这个时间点对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。 解决方案 对缓存查询加锁，如果KEY不存在，就加锁，然后查DB入缓存，然后解锁；其他进程如果发现有锁就等待，然后等解锁后返回数据或者进入DB查询 常用工具 Redis支持的Java客户端都有哪些？官方推荐用哪个？ Redisson、Jedis、lettuce等等，官方推荐使用Redisson。 Redis和Redisson有什么关系？ Redisson是一个高级的分布式协调Redis客服端，能帮助用户在分布式环境中轻松实现一些Java的对象 (Bloom filter, BitSet, Set, SetMultimap, ScoredSortedSet, SortedSet, Map, ConcurrentMap, List, ListMultimap, Queue, BlockingQueue, Deque, BlockingDeque, Semaphore, Lock, ReadWriteLock, AtomicLong, CountDownLatch, Publish / Subscribe, HyperLogLog)。 Jedis与Redisson对比有什么优缺点？ Jedis是Redis的Java实现的客户端，其API提供了比较全面的Redis命令的支持；Redisson实现了分布式和可扩展的Java数据结构，和Jedis相比，功能较为简单，不支持字符串操作，不支持排序、事务、管道、分区等Redis特性。Redisson的宗旨是促进使用者对Redis的关注分离，从而让使用者能够将精力更集中地放在处理业务逻辑上。 Redisson实现分布式锁 package com.example.redisstudy.template; import org.redisson.Redisson; import org.redisson.api.RLock; import org.redisson.api.RedissonClient; import org.redisson.config.Config; import org.springframework.beans.factory.annotation.Value; import org.springframework.stereotype.Component; /** * @author wuhao * @desc ... * @date 2020-12-09 14:57:30 */ @Component public class RedissonLock { @Value(&quot;${spring.redis.address}&quot;) public String address; public RLock lock(){ Config config = new Config(); config.useSingleServer().setAddress(address); // config.useSingleServer().setPassword(&quot;redis1234&quot;); final RedissonClient client = Redisson.create(config); RLock lock = client.getLock(&quot;redis:lock&quot;); return lock; } } 如何保证缓存与数据库双写时的数据一致性？ 你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？ 一般来说，就是如果你的系统不是严格要求缓存+数据库必须一致性的话，缓存可以稍微的跟数据库偶尔有不一致的情况，最好不要做这个方案，读请求和写请求串行化，串到一个内存队列里去，这样就可以保证一定不会出现不一致的情况 串行化之后，就会导致系统的吞吐量会大幅度的降低，用比正常情况下多几倍的机器去支撑线上的一个请求。 还有一种方式就是可能会暂时产生不一致的情况，但是发生的几率特别小，就是先更新数据库，然后再删除缓存。 问题场景 描述 解决 先写缓存，再写数据库，缓存写成功，数据库写失败 缓存写成功，但写数据库失败或者响应延迟，则下次读取（并发读）缓存时，就出现脏读 这个写缓存的方式，本身就是错误的，需要改为先写数据库，把旧缓存置为失效；读取数据的时候，如果缓存不存在，则读取数据库再写缓存 先写数据库，再写缓存，数据库写成功，缓存写失败 写数据库成功，但写缓存失败，则下次读取（并发读）缓存时，则读不到数据 缓存使用时，假如读缓存失败，先读数据库，再回写缓存的方式实现 需要缓存异步刷新 指数据库操作和写缓存不在一个操作步骤中，比如在分布式场景下，无法做到同时写缓存或需要异步刷新（补救措施）时候 确定哪些数据适合此类场景，根据经验值确定合理的数据不一致时间，用户数据刷新的时间间隔 Redis常见性能问题和解决方案？ Master最好不要做任何持久化工作，包括内存快照和AOF日志文件，特别是不要启用内存快照做持久化。如果采用了主从架构，那么建议必须开启 master node 的持久化，不建议用 slave node 作为 master node 的数据热备，因为那样的话，如果你关掉 master 的持久化，可能在 master 宕机重启的时候数据是空的，然后可能一经过复制， slave node 的数据也丢了。另外，master 的各种备份方案，也需要做。万一本地的所有文件丢失了，从备份中挑选一份 rdb 去恢复 master，这样才能确保启动的时候，是有数据的，即使采用了后续讲解的高可用机制，slave node 可以自动接管 master node，但也可能 sentinel 还没检测到 master failure，master node 就自动重启了，还是可能导致上面所有的 slave node 数据被清空。 如果数据比较关键，某个Slave开启AOF备份数据，策略为每秒同步一次。 为了主从复制的速度和连接的稳定性，Slave和Master最好在同一个局域网内。 尽量避免在压力较大的主库上增加从库 Master调用BGREWRITEAOF重写AOF文件，AOF在重写的时候会占大量的CPU和内存资源，导致服务load过高，出现短暂服务暂停现象。 为了Master的稳定性，主从复制不要用图状结构，用单向链表结构更稳定，即主从关系为：Master&lt;–Slave1&lt;–Slave2&lt;–Slave3…，这样的结构也方便解决单点故障问题，实现Slave对Master的替换，也即，如果Master挂了，可以立马启用Slave1做Master，其他不变。 ","link":"https://tinaxiawuhao.github.io/post/Sjb64Uy0k/"},{"title":"redis概述四","content":"集群方案 1，Redis 主从架构 单机的 redis，能够承载的 QPS 大概就在上万到几万不等。对于缓存来说，一般都是用来支撑读高并发的。因此架构做成主从(master-slave)架构，一主多从，主负责写，并且将数据复制到其它的 slave 节点，从节点负责读。所有的读请求全部走从节点。这样也可以很轻松实现水平扩容，支撑读高并发。 redis replication -&gt; 主从架构 -&gt; 读写分离 -&gt; 水平扩容支撑读高并发 redis replication 的核心机制 redis 采用异步方式复制数据到 slave 节点，不过 redis2.8 开始，slave node 会周期性地确认自己每次复制的数据量； 一个 master node 是可以配置多个 slave node 的； slave node 也可以连接其他的 slave node； slave node 做复制的时候，不会 block master node 的正常工作； slave node 在做复制的时候，也不会 block 对自己的查询操作，它会用旧的数据集来提供服务；但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了； slave node 主要用来进行横向扩容，做读写分离，扩容的 slave node 可以提高读的吞吐量。 注意，如果采用了主从架构，那么建议必须开启 master node 的持久化，不建议用 slave node 作为 master node 的数据热备，因为那样的话，如果你关掉 master 的持久化，可能在 master 宕机重启的时候数据是空的，然后可能一经过复制， slave node 的数据也丢了。 另外，master 的各种备份方案，也需要做。万一本地的所有文件丢失了，从备份中挑选一份 rdb 去恢复 master，这样才能确保启动的时候，是有数据的，即使采用了后续讲解的高可用机制，slave node 可以自动接管 master node，但也可能 sentinel 还没检测到 master failure，master node 就自动重启了，还是可能导致上面所有的 slave node 数据被清空。 redis 主从复制的核心原理 当启动一个 slave node 的时候，它会发送一个 PSYNC 命令给 master node。 如果这是 slave node 初次连接到 master node，那么会触发一次 full resynchronization 全量复制。此时 master 会启动一个后台线程，开始生成一份 RDB 快照文件， 同时还会将从客户端 client 新收到的所有写命令缓存在内存中。RDB 文件生成完毕后， master 会将这个 RDB 发送给 slave，slave 会先写入本地磁盘，然后再从本地磁盘加载到内存中， 接着 master 会将内存中缓存的写命令发送到 slave，slave 也会同步这些数据。 slave node 如果跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 仅会复制给 slave 部分缺少的数据。 过程原理 当从库和主库建立MS关系后，会向主数据库发送SYNC命令 主库接收到SYNC命令后会开始在后台保存快照(RDB持久化过程)，并将期间接收到的写命令缓存起来 当快照完成后，主Redis会将快照文件和所有缓存的写命令发送给从Redis 从Redis接收到后，会载入快照文件并且执行收到的缓存的命令 之后，主Redis每当接收到写命令时就会将命令发送从Redis，从而保证数据的一致 缺点 所有的slave节点数据的复制和同步都由master节点来处理，会照成master节点压力太大，使用主从从结构来解决 2，哨兵模式 哨兵的介绍 sentinel，中文名是哨兵。哨兵是 redis 集群机构中非常重要的一个组件，主要有以下功能： 集群监控：负责监控 redis master 和 slave 进程是否正常工作。 消息通知：如果某个 redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员。 故障转移：如果 master node 挂掉了，会自动转移到 slave node 上。 配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址。 哨兵用于实现 redis 集群的高可用，本身也是分布式的，作为一个哨兵集群去运行，互相协同工作。 故障转移时，判断一个 master node 是否宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题。 即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的，因为如果一个作为高可用机制重要组成部分的故障转移系统本身是单点的，那就很坑爹了。 哨兵的核心知识 哨兵至少需要 3 个实例，来保证自己的健壮性。 哨兵 + redis 主从的部署架构，是不保证数据零丢失的，只能保证 redis 集群的高可用性。 对于哨兵 + redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练。 分布式和哨兵 Redis主从复制模式下， 一旦主节点出现了故障不可达， 需要人工干预进行故障转移， 无论对于Redis的应用方还是运维方都带来了很大的不便。对于应用方来说无法及时感知到主节点的变化， 必然会造成一定的写数据丢失和读数据错误， 甚至可能造成应用方服务不可用。 对于Redis的运维方来说， 整个故障转移的过程是需要人工来介入的， 故障转移实时性和准确性上都无法得到保障。考虑到这点， 有些公司把上述流程自动化了， 但是仍然存在如下问题： 第一， 判断节点不可达的机制是否健全和标准。 第二， 如果有多个从节点， 怎样保证只有一个被晋升为主节点。 第三，通知客户端新的主节点机制是否足够健壮。 Redis Sentinel正是用于解决这些问题 Redis Sentinel是一个分布式架构， 其中包含若干个Sentinel节点和Redis数据节点， 每个Sentinel节点会对数据节点和其余Sentinel节点进行监控， 当它发现节点不可达时， 会对节点做下线标识。 如果被标识的是主节点， 它还会和其他Sentinel节点进行“协商”， 当大多数Sentinel节点都认为主节点不可达时， 它们会选举出一个Sentinel节点来完成自动故障转移的工作， 同时会将这个变化实时通知给Redis应用方。 整个过程完全是自动的， 不需要人工来介入， 所以这套方案很有效地解决了Redis的高可用问题 哨兵的监控与选举 哨兵的定时监控 任务1：每个哨兵节点每10秒会向主节点和从节点发送info命令获取最拓扑结构图，哨兵配置时只要配置对主节点的监控即可，通过向主节点发送info，获取从节点的信息，并当有新的从节点加入时可以马上感知到 任务2：每个哨兵节点每隔2秒会向redis数据节点的指定频道上发送该哨兵节点对于主节点的判断以及当前哨兵节点的信息，同时每个哨兵节点也会订阅该频道，来了解其它哨兵节点的信息及对主节点的判断，其实就是通过消息publish和subscribe来完成的 任务3：每隔1秒每个哨兵会向主节点、从节点及其余哨兵节点发送一次ping命令做一次心跳检测，这个也是哨兵用来判断节点是否正常的重要依据 主观下线：所谓主观下线，就是单个sentinel认为某个服务下线（有可能是接收不到订阅，之间的网络不通等等原因）。 sentinel会以每秒一次的频率向所有与其建立了命令连接的实例（master，从服务，其他sentinel）发ping命令，通过判断ping回复是有效回复，还是无效回复来判断实例时候在线（对该sentinel来说是“主观在线”）。 sentinel配置文件中的down-after-milliseconds设置了判断主观下线的时间长度，如果实例在down-after-milliseconds毫秒内，返回的都是无效回复，那么sentinel回认为该实例已（主观）下线，修改其flags状态为SRI_S_DOWN。如果多个sentinel监视一个服务，有可能存在多个sentinel的down-after-milliseconds配置不同，这个在实际生产中要注意。 客观下线：当主观下线的节点是主节点时，此时该哨兵3节点会通过指令sentinel is-masterdown-by-addr寻求其它哨兵节点对主节点的判断，如果其他的哨兵也认为主节点主观线下了，则当认为主观下线的票数超过了quorum（选举）个数，此时哨兵节点则认为该主节点确实有问题，这样就客观下线了，大部分哨兵节点都同意下线操作，也就说是客观下线 哨兵lerder选举流程 如果主节点被判定为客观下线之后，就要选取一个哨兵节点来完成后面的故障转移工作，选举出一个leader的流程如下: a)每个在线的哨兵节点都可以成为领导者，当它确认（比如哨兵3）主节点下线时，会向其它哨兵发is-master-down-by-addr命令，征求判断并要求将自己设置为领导者，由领导者处理故障转移； b)当其它哨兵收到此命令时，可以同意或者拒绝它成为领导者； c)如果哨兵3发现自己在选举的票数大于等于num(sentinels)/2+1时，将成为领导者，如果没有超过，继续选举………… 自动故障转移机制 在从节点中选择新的主节点 sentinel状态数据结构中保存了主服务的所有从服务信息，领头sentinel按照如下的规则从从服务列表中挑选出新的主服务 过滤掉主观下线的节点 选择slave-priority最高的节点，如果由则返回没有就继续选择 选择出复制偏移量最大的系节点，因为复制便宜量越大则数据复制的越完整，如果由就返回了，没有就继续 选择run_id最小的节点 更新主从状态 通过slaveof no one命令，让选出来的从节点成为主节点；并通过slaveof命令让其他节点成为其从节点。 将已下线的主节点设置成新的主节点的从节点，当其回复正常时，复制新的主节点，变成新的主节点的从节点 同理，当已下线的服务重新上线时，sentinel会向其发送slaveof命令，让其成为新主的从 3，官方Redis Cluster 方案(服务端路由查询) 简介 Redis Cluster是一种服务端Sharding技术，3.0版本开始正式提供。Redis Cluster并没有使用一致性hash，而是采用slot(槽)的概念，一共分成16384个槽。将请求发送到任意节点，接收到请求的节点会将查询请求发送到正确的节点上执行 方案说明 通过哈希的方式，将数据分片，每个节点均分存储一定哈希槽(哈希值)区间的数据，默认分配了16384 个槽位 每份数据分片会存储在多个互为主从的多节点上 数据写入先写主节点，再同步到从节点(支持配置为阻塞同步) 同一分片多个节点间的数据不保持一致性 读取数据时，当客户端操作的key没有分配在该节点上时，redis会返回转向指令，指向正确的节点 扩容时时需要需要把旧节点的数据迁移一部分到新节点 在 redis cluster 架构下，每个 redis 要放开两个端口号，比如一个是 6379，另外一个就是 加1w 的端口号，比如 16379。 16379 端口号是用来进行节点间通信的，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus 用了另外一种二进制的协议，gossip 协议，用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间。 节点间的内部通信机制 基本通信原理 集群元数据的维护有两种方式：集中式、Gossip 协议。redis cluster 节点间采用 gossip 协议进行通信。 分布式寻址算法 hash 算法（大量缓存重建） 使用特定的数据， 如Redis的键或用户ID， 再根据节点数量N使用公式：hash（key） %N计算出哈希值， 用来决定数据映射到哪一个节点上。 这种方案存在一个问题： 当节点数量变化时， 如扩容或收缩节点， 数据节点映射关系需要重新计算， 会导致数据的重新迁移。 这种方式的突出优点是简单性， 常用于数据库的分库分表规则， 一般采用预分区的方式， 提前根据数据量规划好分区数， 比如划分为512或1024张表， 保证可支撑未来一段时间的数据量， 再根据负载情况将表迁移到其他数据库中。 扩容时通常采用翻倍扩容， 避免数据映射全部被打乱导致全量迁移的情况， 如图所示。 一致性 hash 算法（自动缓存迁移）+ 虚拟节点（自动负载均衡） 一致性哈希分区（Distributed Hash Table） 实现思路是为系统中每个节点分配一个token， 范围一般在0~2^32， 这些token构成一个哈希环。 数据读写执行节点查找操作时， 先根据key计算hash值， 然后顺时针找到第一个大于等于该哈希值的token节点， 如图所示。 这种方式相比节点取余最大的好处在于加入和删除节点只影响哈希环中相邻的节点， 对其他节点无影响。 但一致性哈希分区存在几个问题： 加减节点会造成哈希环中部分数据无法命中， 需要手动处理或者忽略这部分数据， 因此一致性哈希常用于缓存场景。 当使用少量节点时， 节点变化将大范围影响哈希环中数据映射， 因此这种方式不适合少量数据节点的分布式方案。 普通的一致性哈希分区在增减节点时需要增加一倍或减去一半节点才能保证数据和负载的均衡。 redis cluster 的 hash slot 算法 虚拟槽分区巧妙地使用了哈希空间， 使用分散度良好的哈希函数把所有数据映射到一个固定范围的整数集合中， 整数定义为槽（slot）。 这个范围一般远远大于节点数， 比如Redis Cluster槽范围是0~16383。 槽是集群内数据管理和迁移的基本单位。 采用大范围槽的主要目的是为了方便数据拆分和集群扩展。 每个节点会负责一定数量的槽， 如图所示。 Redis Cluser采用虚拟槽分区， 所有的键根据哈希函数映射到0~16383整数槽内， 计算公式： slot=CRC16（key） &amp;16383。 每一个节点负责维护一部分槽以及槽所映射的键值数据， 如图所示 Redis虚拟槽分区的特点： 解耦数据和节点之间的关系， 简化了节点扩容和收缩难度。 节点自身维护槽的映射关系， 不需要客户端或者代理服务维护槽分区元数据。 支持节点、 槽、 键之间的映射查询， 用于数据路由、 在线伸缩等场景 优点 无中心架构，支持动态扩容，对业务透明 具备Sentinel的监控和自动Failover(故障转移)能力 客户端不需要连接集群所有节点，连接集群中任何一个可用节点即可 高性能，客户端直连redis服务，免去了proxy代理的损耗 缺点 运维也很复杂，数据迁移需要人工干预 只能使用0号数据库 不支持批量操作(pipeline管道操作) 分布式逻辑和存储模块耦合等 集群相关提问 Redis集群的主从复制模型是怎样的？ 为了使在部分节点失败或者大部分节点无法通信的情况下集群仍然可用，所以集群使用了主从复制模型，每个节点都会有N-1个复制品 生产环境中的 redis 是怎么部署的？ redis cluster，10 台机器，5 台机器部署了 redis 主实例，另外 5 台机器部署了 redis 的从实例，每个主实例挂了一个从实例，5 个节点对外提供读写服务，每个节点的读写高峰qps可能可以达到每秒 5 万，5 台机器最多是 25 万读写请求/s。 机器是什么配置？32G 内存+ 8 核 CPU + 1T 磁盘，但是分配给 redis 进程的是10g内存，一般线上生产环境，redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。 5 台机器对外提供读写，一共有 50g 内存。 因为每个主实例都挂了一个从实例，所以是高可用的，任何一个主实例宕机，都会自动故障迁移，redis 从实例会自动变成主实例继续提供读写服务。 你往内存里写的是什么数据？每条数据的大小是多少？商品数据，每条数据是 10kb。100 条数据是 1mb，10 万条数据是 1g。常驻内存的是 200 万条商品数据，占用内存是 20g，仅仅不到总内存的 50%。目前高峰期每秒就是 3500 左右的请求量。 其实大型的公司，会有基础架构的 team 负责缓存集群的运维。 说说Redis哈希槽的概念？ Redis集群没有使用一致性hash,而是引入了哈希槽的概念，Redis集群有16384个哈希槽，每个key通过CRC16校验后对16384取模来决定放置哪个槽，slot=CRC16（key） &amp;16383，集群的每个节点负责一部分hash槽。 Redis集群会有写操作丢失吗？为什么？ Redis并不能保证数据的强一致性，这意味这在实际中集群在特定的条件下可能会丢失写操作。 以下情况可能导致写操作丢失： 过期 key 被清理 最大内存不足，导致 Redis 自动清理部分 key 以节省空间 主库故障后自动重启，从库自动同步 单独的主备方案，网络不稳定触发哨兵的自动切换主从节点，切换期间会有数据丢失 Redis集群之间是如何复制的？ 在从节点执行slaveof命令后， 复制过程便开始运作， 下面详细介绍建立复制的完整流程， 如图所示。 从图中可以看出复制过程大致分为6个过程： 保存主节点（master） 信息。执行slaveof后从节点只保存主节点的地址信息便直接返回， 这时建立复制流程还没有开始， 在从节点6380执行info replication可以看到如下信息： master_host:127.0.0.1 master_port:6379 master_link_status:down 从统计信息可以看出， 主节点的ip和port被保存下来， 但是主节点的连接状态（master_link_status） 是下线状态。 执行slaveof后Redis会打印如下日志： SLAVE OF 127.0.0.1:6379 enabled (user request from 'id=65 addr=127.0.0.1:58090 fd=5 name= age=11 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=slaveof') 通过该日志可以帮助运维人员定位发送slaveof命令的客户端， 方便追踪和发现问题。 从节点（slave） 内部通过每秒运行的定时任务维护复制相关逻辑，当定时任务发现存在新的主节点后， 会尝试与该节点建立网络连接， 如图所示。 从节点会建立一个socket套接字， 例如图6-8中从节点建立了一个端口为24555的套接字， 专门用于接受主节点发送的复制命令。 从节点连接成功后 打印如下日志： \\* Connecting to MASTER 127.0.0.1:6379 \\* MASTER &lt;-&gt; SLAVE sync started 如果从节点无法建立连接， 定时任务会无限重试直到连接成功或者执行slaveof no one取消复制， 如图所示。 关于连接失败， 可以在从节点执行info replication查看master_link_down_since_seconds指标， 它会记录与主节点连接失败的系统时 间。 从节点连接主节点失败时也会每秒打印如下日志， 方便运维人员发现问题： \\# Error condition on socket for SYNC: {socket_error_reason} 发送ping命令。 连接建立成功后从节点发送ping请求进行首次通信， ping请求主要目的如下： ·检测主从之间网络套接字是否可用。 ·检测主节点当前是否可接受处理命令。 如果发送ping命令后， 从节点没有收到主节点的pong回复或者超时， 比如网络超时或者主节点正在阻塞无法响应命令， 从节点会断开复制连接， 下 次定时任务会发起重连， 如图所示。 从节点发送的ping命令成功返回， Redis打印如下日志， 并继续后续复制流程： Master replied to PING, replication can continue... 权限验证。 如果主节点设置了requirepass参数， 则需要密码验证，从节点必须配置masterauth参数保证与主节点相同的密码才能通过验证； 如果验证失败复制将终止， 从节点重新发起复制流程。 同步数据集。 主从复制连接正常通信后， 对于首次建立复制的场景， 主节点会把持有的数据全部发送给从节点， 这部分操作是耗时最长的步骤。 Redis在2.8版本以后采用新复制命令psync进行数据同步， 原来的sync命令依然支持， 保证新旧版本的兼容性。 新版同步划分两种情况： 全量同步和部分同步. 命令持续复制。 当主节点把当前的数据同步给从节点后， 便完成了复制的建立流程。 接下来主节点会持续地把写命令发送给从节点， 保证主从数据一致性。 Redis集群最大节点个数是多少？ 16384个 Redis集群如何选择数据库？ Redis集群目前无法做数据库选择，默认在0数据库。 ","link":"https://tinaxiawuhao.github.io/post/FWuQD6Kxu/"},{"title":"redis概述三","content":"Redis持久化 持久化就是把内存的数据写到磁盘中去，防止服务宕机了内存数据丢失。 Redis 的持久化机制是什么？各自的优缺点？ Redis 提供两种持久化机制 RDB（默认） 和 AOF 机制: RDB： RDB是Redis DataBase缩写快照 RDB是Redis默认的持久化方式。按照一定的时间将内存的数据以快照的形式保存到硬盘中，对应产生的数据文件为dump.rdb。通过配置文件中的save参数来定义快照的周期。 bgsave是主流的触发RDB持久化方式， 根据下图了解它的运作流程 执行bgsave命令， Redis父进程判断当前是否存在正在执行的子进程， 如RDB/AOF子进程， 如果存在bgsave命令直接返回。 父进程执行fork操作创建子进程， fork操作过程中父进程会阻塞， 通过info stats命令查看latest_fork_usec选项， 可以获取最近一个fork操作的耗时， 单位为微秒。 父进程fork完成后， bgsave命令返回“Background saving started”信息并不再阻塞父进程， 可以继续响应其他命令。 子进程创建RDB文件， 根据父进程内存生成临时快照文件， 完成后对原有文件进行原子替换。 执行lastsave命令可以获取最后一次生成RDB的时间， 对应info统计的rdb_last_save_time选项。 进程发送信号给父进程表示完成， 父进程更新统计信息， 具体见info Persistence下的rdb_*相关选 优点： 1、只有一个文件 dump.rdb，方便持久化。 2、容灾性好，一个文件可以保存到安全的磁盘。 3、性能最大化，fork 子进程来完成写操作，让主进程继续处理命令，所以是 IO 最大化。使用单独子进程来进行持久化，主进程不会进行任何 IO 操作，保证了 redis 的高性能 4.相对于数据集大时，比 AOF 的启动效率更高。 缺点： 1、数据安全性低。RDB 是间隔一段时间进行持久化，如果持久化之间 redis 发生故障，会发生数据丢失。所以这种方式更适合数据要求不严谨的时候 2、AOF（Append-only file)持久化方式： 是指所有的命令行记录以 redis 命令请 求协议的格式完全持久化存储)保存为 aof 文件。 AOF： AOF持久化(即Append Only File持久化)，则是将Redis执行的每次写命令记录到单独的日志文件中，当重启Redis会重新将持久化的日志中文件恢复数据。 当两种方式同时开启时，数据恢复Redis会优先选择AOF恢复。 开启AOF功能需要设置配置： appendonly yes， 默认不开启。 AOF文件名通过appendfilename配置设置， 默认文件名是appendonly.aof。 保存路径同RDB持久化方式一致， 通过dir配置指定。 AOF的工作流程操作： 命令写入（append） 、 文件同步（sync） 、 文件重写（rewrite） 、 重启加载 （load） ， 如图所示 流程如下： 所有的写入命令会追加到aof_buf（ 缓冲区） 中。 AOF缓冲区根据对应的策略向硬盘做同步操作。 随着AOF文件越来越大， 需要定期对AOF文件进行重写， 达到压缩的目的。 当Redis服务器重启时， 可以加载AOF文件进行数据恢复。 aof重写 AOF重新运作流程 流程说明： 执行AOF重写请求。 如果当前进程正在执行AOF重写， 请求不执行并返回如下响应：ERR Background append only file rewriting already in progress 如果当前进程正在执行bgsave操作， 重写命令延迟到bgsave完成之后再执行， 返回如下响应：Background append only file rewriting scheduled 父进程执行fork创建子进程， 开销等同于bgsave过程。 3.1. 主进程fork操作完成后， 继续响应其他命令。 所有修改命令依然写入AOF缓冲区并根据appendfsync策略同步到硬盘， 保证原有AOF机制正确性。 3.2. 由于fork操作运用写时复制技术， 子进程只能共享fork操作时的内存数据。 由于父进程依然响应命令， Redis使用“AOF重写缓冲区”保存这部分新数据， 防止新AOF文件生成期间丢失这部分数据。 子进程根据内存快照， 按照命令合并规则写入到新的AOF文件。 每次批量写入硬盘数据量由配置aof-rewrite-incremental-fsync控制， 默认为32MB， 防止单次刷盘数据过多造成硬盘阻塞。 5.1. 新AOF文件写入完成后， 子进程发送信号给父进程， 父进程更新统计信息， 具体见info persistence下的aof_*相关统计。 5.2. 父进程把AOF重写缓冲区的数据写入到新的AOF文件。 5.3. 使用新AOF文件替换老文件， 完成AOF重写。 AOF追加阻塞 当开启AOF持久化时， 常用的同步硬盘的策略是everysec， 用于平衡性能和数据安全性。 对于这种方式， Redis使用另一条线程每秒执行fsync同步硬盘。 当系统硬盘资源繁忙时， 会造成Redis主线程阻塞， 如图所示。 使用everysec做刷盘策略的流程 阻塞流程分析： 1） 主线程负责写入AOF缓冲区。 2） AOF线程负责每秒执行一次同步磁盘操作， 并记录最近一次同步时间。 3） 主线程负责对比上次AOF同步时间： ·如果距上次同步成功时间在2秒内， 主线程直接返回。 ·如果距上次同步成功时间超过2秒， 主线程将会阻塞， 直到同步操作完成。 通过对AOF阻塞流程可以发现两个问题： 1） everysec配置最多可能丢失2秒数据， 不是1秒。 2） 如果系统fsync缓慢， 将会导致Redis主线程阻塞影响效率。 AOF阻塞问题定位： 1） 发生AOF阻塞时， Redis输出如下日志， 用于记录AOF fsync阻塞导致拖慢Redis服务的行为： Asynchronous AOF fsync is taking too long (disk is busy). Writing the AOF buffer without waiting for fsync to complete, this may slow down Redis 2） 每当发生AOF追加阻塞事件发生时， 在info Persistence统计中，aof_delayed_fsync指标会累加， 查看这个指标方便定位AOF阻塞问题。 3） AOF同步最多允许2秒的延迟， 当延迟发生时说明硬盘存在高负载问题， 可以通过监控工具如iotop， 定位消耗硬盘IO资源的进程。优化AOF追加阻塞问题主要是优化系统硬盘负载 优点： 1、数据安全，aof 持久化可以配置 appendfsync 属性。 配置为always时， 每次写入都要同步AOF文件， 在一般的SATA硬盘上， Redis只能支持大约几百TPS写入， 显然跟Redis高性能特性背道而驰，不建议配置。 配置为no， 由于操作系统每次同步AOF文件的周期不可控， 而且会加大每次同步硬盘的数据量， 虽然提升了性能， 但数据安全性无法保证。 配置为everysec， 是建议的同步策略， 也是默认配置， 做到兼顾性能和数据安全性。 理论上只有在系统突然宕机的情况下丢失1秒的数据 2、通过 append 模式写文件，即使中途服务器宕机，可以通过 redis-check-aof 工具解决数据一致性问题。 3、AOF 机制的 rewrite 模式。AOF 文件没被 rewrite 之前（文件过大时会对命令 进行合并重写），可以删除其中的某些命令（比如误操作的 flushall）) 缺点： 1、AOF 文件比 RDB 文件大，且恢复速度慢。 2、数据集大的时候，比 rdb 启动效率低。 比较 AOF文件比RDB更新频率高，优先使用AOF还原数据。 AOF比RDB更安全也更大 RDB性能比AOF好 如果两个都配了优先加载AOF 如何选择合适的持久化方式 一般来说， 如果想达到足以媲美PostgreSQL的数据安全性，你应该同时使用两种持久化功能。在这种情况下，当 Redis 重启的时候会优先载入AOF文件来恢复原始的数据，因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整。 如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失，那么你可以只使用RDB持久化。 有很多用户都只使用AOF持久化，但并不推荐这种方式，因为定时生成RDB快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比AOF恢复的速度要快，除此之外，使用RDB还可以避免AOF程序的bug。 如果你只希望你的数据在服务器运行的时候存在，你也可以不使用任何持久化方式。 Redis持久化数据和缓存怎么做扩容？ 如果Redis被当做缓存使用，使用一致性哈希实现动态扩容缩容。 如果Redis被当做一个持久化存储使用，必须使用固定的keys-to-nodes映射关系，节点的数量一旦确定不能变化。否则的话(即Redis节点需要动态变化的情况），必须使用可以在运行时进行数据再平衡的一套系统，而当前只有Redis集群可以做到这样。 Redis的过期键的删除策略 我们都知道，Redis是key-value数据库，我们可以设置Redis中缓存的key的过期时间。Redis的过期策略就是指当Redis中缓存的key过期了，Redis如何处理。 过期策略通常有以下三种： 定时过期：每个设置过期时间的key都需要创建一个定时器，到过期时间就会立即清除。该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的CPU资源去处理过期的数据，从而影响缓存的响应时间和吞吐量。 惰性过期：只有当访问一个key时，才会判断该key是否已过期，过期则清除。该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存。 定期过期：每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果。 (expires字典会保存所有设置了过期时间的key的过期时间数据，其中，key是指向键空间中的某个键的指针，value是该键的毫秒精度的UNIX时间戳表示的过期时间。键空间是指该Redis集群中保存的所有键。) Redis中同时使用了惰性过期和定期过期两种过期策略。 redis内存 redis内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略。 Redis的内存淘汰策略有哪些 Redis的内存淘汰策略是指在Redis的用于缓存的内存不足时，怎么处理需要新写入且需要申请额外空间的数据。 全局的键空间选择性移除 noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。（这个是最常用的） allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。 allkeys-lfu：从所有键中驱逐使用频率最少的键 设置过期时间的键空间选择性移除 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。 volatile-lfu：从所有配置了过期时间的键中驱逐使用频率最少的键 总结 Redis的内存淘汰策略的选取并不会影响过期的key的处理。内存淘汰策略用于处理内存不足时的需要申请额外空间的数据；过期策略用于处理过期的缓存数据。 Redis主要消耗什么物理资源？ 内存。 Redis的内存用完了会发生什么？ 如果达到设置的上限，Redis的写命令会返回错误信息（但是读命令还可以正常返回。）或者你可以配置内存淘汰机制，当Redis达到内存上限时会冲刷掉旧的内容。 Redis如何做内存优化？ 可以好好利用Hash,list,sorted set,set等集合类型数据，因为通常情况下很多小的Key-Value可以用更紧凑的方式存放到一起。尽可能使用散列表（hashes），散列表（是说散列表里面存储的数少）使用的内存非常小，所以你应该尽可能的将你的数据模型抽象到一个散列表里面。比如你的web系统中有一个用户对象，不要为这个用户的名称，姓氏，邮箱，密码设置单独的key，而是应该把这个用户的所有信息存储到一张散列表里面 Redis线程模型 Redis基于Reactor模式开发了网络事件处理器，这个处理器被称为文件事件处理器（file event handler）。它的组成结构为4部分：多个套接字、IO多路复用程序、文件事件分派器、事件处理器。因为文件事件分派器队列的消费是单线程的，所以Redis才叫单线程模型。 文件事件处理器使用 I/O 多路复用（multiplexing）程序来同时监听多个套接字， 并根据套接字目前执行的任务来为套接字关联不同的事件处理器。 当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作时， 与操作相对应的文件事件就会产生， 这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。 虽然文件事件处理器以单线程方式运行， 但通过使用 I/O 多路复用程序来监听多个套接字， 文件事件处理器既实现了高性能的网络通信模型， 又可以很好地与 redis 服务器中其他同样以单线程方式运行的模块进行对接， 这保持了 Redis 内部单线程设计的简单性。 参考：https://www.cnblogs.com/barrywxx/p/8570821.html 事务 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断.事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。 Redis事务的概念 Redis 事务的本质是通过MULTI、EXEC、DISCARD、WATCH等一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。 总结说：redis事务就是一次性、顺序性、排他性的执行一个队列中的一系列命令。 Redis事务的三个阶段 事务开始 MULTI 命令入队 事务执行 EXEC 事务执行过程中，如果服务端收到有EXEC、DISCARD、WATCH、MULTI之外的请求，将会把请求放入队列中排队 Redis事务相关命令 Redis事务功能是通过MULTI、EXEC、DISCARD和WATCH 四个原语实现的 Redis会将一个事务中的所有命令序列化，然后按顺序执行。 redis 不支持回滚，“Redis 在事务失败时不进行回滚，而是继续执行余下的命令”， 所以 Redis 的内部可以保持简单且快速。 如果在一个事务中的命令出现错误，那么所有的命令都不会执行； 如果在一个事务中出现运行错误，那么正确的命令会被执行。 WATCH 命令是一个乐观锁，可以为 Redis 事务提供 check-and-set （CAS）行为。 可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行，监控一直持续到EXEC命令。 MULTI命令用于开启一个事务，它总是返回OK。 MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行。 EXEC：执行所有事务块内的命令。返回事务块内所有命令的返回值，按命令执行的先后顺序排列。 当操作被打断时，返回空值 nil 。 通过调用DISCARD，客户端可以清空事务队列，并放弃执行事务， 并且客户端会从事务状态中退出。 UNWATCH命令可以取消watch对所有key的监控。 事务管理（ACID）概述 原子性（Atomicity） 原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 一致性（Consistency） 事务前后数据的完整性必须保持一致。 隔离性（Isolation） 多个事务并发执行时，一个事务的执行不应影响其他事务的执行 持久性（Durability） 持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对其有任何影响 Redis的事务总是具有ACID中的隔离性，不具有隔离级别，其他特性是不支持的。当服务器运行在AOF持久化模式下，并且appendfsync选项的值为always时，事务也具有持久性。 Redis事务支持隔离性吗 Redis 是单进程程序，并且它保证在执行事务时，不会对事务进行中断，事务可以运行直到执行完所有事务队列中的命令为止。因此，Redis 的事务是总是带有隔离性的，因为是单线程不具有隔离级别。 Redis事务保证原子性吗，支持回滚吗 Redis中，单条命令是原子性执行的，但事务不保证原子性，且没有回滚。事务中任意命令执行失败，其余的命令仍会被执行。 Redis事务其他实现 基于Lua脚本，Redis可以保证脚本内的命令一次性、按顺序地执行， 其同时也不提供事务运行错误的回滚，执行过程中如果部分命令运行错误，剩下的命令还是会继续运行完 基于中间标记变量，通过另外的标记变量来标识事务是否执行完成，读取数据时先读取该标记变量判断是否事务执行完成。但这样会需要额外写代码实现，比较繁琐 ","link":"https://tinaxiawuhao.github.io/post/WAG-fYk_l/"},{"title":"redis概述二","content":"Redis有哪些数据类型 Redis主要有5种数据类型，包括String，List，Set，Zset，Hash，满足大部分的使用要求 数据类型 可以存储的值 操作 应用场景 STRING 字符串、整数或者浮点数 对整个字符串或者字符串的其中一部分执行操作对整数和浮点数执行自增或者自减操作 做简单的键值对缓存 LIST 列表 从两端压入或者弹出元素对单个或者多个元素进行修剪，只保留一个范围内的元素 存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类的数据 SET 无序集合 添加、获取、移除单个元素检查一个元素是否存在于集合中计算交集、并集、差集从集合里面随机获取元素 交集、并集、差集的操作，比如交集，可以把两个人的粉丝列表整一个交集 HASH 包含键值对的无序散列表 添加、获取、移除单个键值对获取所有键值对检查某个键是否存在 结构化的数据，比如一个对象 ZSET 有序集合 添加、获取、删除元素根据分值范围或者成员来获取元素计算一个键的排名 去重但可以排序，如获取排名前几名的用户 Redis数据详解 String 设置值 //设置值 set key value [ex seconds] [px milliseconds] [nx|xx] 127.0.0.1:6379&gt; set hello world set命令有几个选项： ·ex seconds： 为键设置秒级过期时间。 ·px milliseconds： 为键设置毫秒级过期时间。 ·nx： 键必须不存在， 才可以设置成功， 用于添加。 ·xx： 与nx相反， 键必须存在， 才可以设置成功， 用于更新。 // 因为键hello已存在， 所以setnx失败， 返回结果为0 127.0.0.1:6379&gt; setnx hello redis (integer) 0 //因为键hello已存在， 所以set xx成功， 返回结果为OK： 127.0.0.1:6379&gt; set hello jedis xx OK setnx和setxx在实际使用中有什么应用场景吗？ 以setnx命令为例子， 由于Redis的单线程命令处理机制， 如果有多个客户端同时执行setnx key value，根据setnx的特性只有一个客户端能设置成功， setnx可以作为分布式锁的一种实现方案， Redis官方给出了使用setnx实现分布式锁的方法： http://redis.io/topics/distlock。 获取值 get key //下面操作获取键hello的值： 127.0.0.1:6379&gt; get hello &quot;world&quot; //如果要获取的键不存在， 则返回nil（空） ： 127.0.0.1:6379&gt; get not_exist_key (nil) 批量设置值 mset key value [key value ...] //下面操作通过mset命令一次性设置4个键值对： 127.0.0.1:6379&gt; mset a 1 b 2 c 3 d 4 OK 批量获取值 mget key [key ...] //下面操作批量获取了键a、 b、 c、 d的值： 127.0.0.1:6379&gt; mget a b c d 1) &quot;1&quot; 2) &quot;2&quot; 3) &quot;3&quot; 4) &quot;4&quot; 如果有些键不存在， 那么它的值为nil（空） ， 结果是按照传入键的顺序返回： 127.0.0.1:6379&gt; mget a b c f 1) &quot;1&quot; 2) &quot;2&quot; 3) &quot;3&quot; 4) (nil) 计数 incr key //incr命令用于对值做自增操作， 返回结果分为三种情况： //·值不是整数， 返回错误。 //·值是整数， 返回自增后的结果。 //·键不存在， 按照值为0自增， 返回结果为1。 //例如对一个不存在的键执行incr操作后， 返回结果是1： 127.0.0.1:6379&gt; exists key (integer) 0 127.0.0.1:6379&gt; incr key (integer) 1 //再次对键执行incr命令， 返回结果是2： 127.0.0.1:6379&gt; incr key (integer) 2 //如果值不是整数， 那么会返回错误： 127.0.0.1:6379&gt; set hello world OK 127.0.0.1:6379&gt; incr hello (error) ERR value is not an integer or out of range //除了incr命令， Redis提供了decr(自减)、incrby(自增指定数字)、decrby(自减指定数字)、incrbyfloat(自增浮点数)： decr key incrby key increment decrby key decrement incrbyfloat key increment //很多存储系统和编程语言内部使用CAS机制实现计数功能， 会有一定的CPU开销， 但在Redis中完全不存在这个问题， 因为Redis是单线程架构， 任何命令到了Redis服务端都要顺序执行 String使用场景 缓存功能 计数 共享Session 限速 List 添加 //从右边插入元素 rpush key value [value ...] //下面代码从右向左插入元素c、 b、 a： 127.0.0. 1:6379&gt; rpush listkey c b a (integer) 3 //lrange0-1命令可以从左到右获取列表的所有元素： 127.0.0.1:6379&gt; lrange listkey 0 -1 1) &quot;c&quot; 2) &quot;b&quot; 3) &quot;a&quot; //从左边插入元素 lpush key value [value ...] 使用方法和rpush相同， 只不过从左侧插入。 //向某个元素前或者后插入元素 linsert key before|after pivot value //linsert命令会从列表中找到等于pivot的元素， 在其前(before)或者后(after)插入一个新的元素value， 例如下面操作会在列表的元素b前插入java： 127.0.0.1:6379&gt; linsert listkey before b java (integer) 4 //返回结果为4， 代表当前命令的长度， 当前列表变为： 127.0.0.1:6379&gt; lrange listkey 0 -1 1) &quot;c&quot; 2) &quot;java&quot; 3) &quot;b&quot; 4) &quot;a&quot; 查找 //获取指定范围内的元素列表 lrange key start end //lrange操作会获取列表指定索引范围所有的元素。 索引下标有两个特点： 第一， 索引下标从左到右分别是0到N-1， 但是从右到左分别是-1到-N。 //第二， lrange中的end选项包含了自身， 这个和很多编程语言不包含end不太相同， 例如想获取列表的第2到第4个元素， 可以执行如下操作： 127.0.0.1:6379&gt; lrange listkey 1 3 1) &quot;java&quot; 2) &quot;b&quot; 3) &quot;a&quot; //获取列表指定索引下标的元素 lindex key index //例如当前列表最后一个元素为a： 127.0.0.1:6379&gt; lindex listkey -1 &quot;a&quot; // 获取列表长度 llen key //例如， 下面示例当前列表长度为4： 127.0.0.1:6379&gt; llen listkey (integer) 4 删除 //从列表左侧弹出元素 lpop key //如下操作将列表最左侧的元素c会被弹出， 弹出后列表变为java、 b、a： 127.0.0.1:6379&gt;t lpop listkey &quot;c&quot; 127.0.0.1:6379&gt; lrange listkey 0 -1 1) &quot;java&quot; 2) &quot;b&quot; 3) &quot;a&quot; //从列表右侧弹出 rpop key 它的使用方法和lpop是一样的， 只不过从列表右侧弹出 //删除指定元素 lrem key count value //lrem命令会从列表中找到等于value的元素进行删除， 根据count的不同分为三种情况： //·count&gt;0， 从左到右， 删除最多count个元素。 //·count&lt;0， 从右到左， 删除最多count绝对值个元素。 //·count=0， 删除所有。 //例如向列表从左向右插入5个a， 那么当前列表变为“a a a a a java b a”， //下面操作将从列表左边开始删除4个为a的元素： 127.0.0.1:6379&gt; lrem listkey 4 a (integer) 4 127.0.0.1:6379&gt; lrange listkey 0 -1 1) &quot;a&quot; 2) &quot;java&quot; 3) &quot;b&quot; 4) &quot;a&quot; //按照索引范围修剪列表 ltrim key start end //例如， 下面操作会只保留列表listkey第2个到第4个元素： 127.0.0.1:6379&gt; ltrim listkey 1 3 OK 127.0.0.1:6379&gt; lrange listkey 0 -1 1) &quot;java&quot; 2) &quot;b&quot; 3) &quot;a&quot; 修改 //修改指定索引下标的元素： lset key index newValue //下面操作会将列表listkey中的第3个元素设置为python： 127.0.0.1:6379&gt; lset listkey 2 python OK 127.0.0.1:6379&gt; lrange listkey 0 -1 1) &quot;java&quot; 2) &quot;b&quot; 3) &quot;python&quot; 阻塞操作 //阻塞式弹出如下： blpop key [key ...] timeout brpop key [key ...] timeout //blpop和brpop是lpop和rpop的阻塞版本， 它们除了弹出方向不同， 使用方法基本相同， 所以下面以brpop命令进行说明， brpop命令包含两个参数： //·key[key...]： 多个列表的键。 //·timeout： 阻塞时间（单位： 秒） 。 //列表为空： 如果timeout=3， 那么客户端要等到3秒后返回， 如果timeout=0， 那么客户端一直阻塞等下去： 127.0.0.1:6379&gt; brpop list:test 3 (nil) (3.10s) 127.0.0.1:6379&gt; brpop list:test 0 //...阻塞... //如果此期间添加了数据element1， 客户端立即返回： 127.0.0.1:6379&gt; brpop list:test 3 1) &quot;list:test&quot; 2) &quot;element1&quot; (2.06s) //列表不为空： 客户端会立即返回。 127.0.0.1:6379&gt; brpop list:test 0 1) &quot;list:test&quot; 2) &quot;element1&quot; //在使用brpop时， 有两点需要注意。 //第一点， 如果是多个键， 那么brpop会从左至右遍历键， 一旦有一个键能弹出元素， 客户端立即返回： 127.0.0.1:6379&gt; brpop list:1 list:2 list:3 0 //..阻塞.. //此时另一个客户端分别向list： 2和list： 3插入元素： client-lpush&gt; lpush list:2 element2 (integer) 1 client-lpush&gt; lpush list:3 element3 (integer) 1 //客户端会立即返回list： 2中的element2， 因为list： 2最先有可以弹出的元素： 127.0.0.1:6379&gt; brpop list:1 list:2 list:3 0 1) &quot;list:2&quot; 2) &quot;element2_1&quot; //第二点， 如果多个客户端对同一个键执行brpop， 那么最先执行brpop命令的客户端可以获取到弹出的值。 //客户端1： client-1&gt; brpop list:test 0 //...阻塞... //客户端2： client-2&gt; brpop list:test 0 //...阻塞... //客户端3： client-3&gt; brpop list:test 0 //...阻塞... //此时另一个客户端lpush一个元素到list： test列表中： client-lpush&gt; lpush list:test element (integer) 1 //那么客户端1最会获取到元素， 因为客户端1最先执行brpop， 而客户端2和客户端3继续阻塞： client&gt; brpop list:test 0 1) &quot;list:test&quot; 2) &quot;element&quot; List使用场景 lpush+lpop=Stack（ 栈） lpush+rpop=Queue（ 队列） lpush+ltrim=Capped Collection（ 有限集合） lpush+brpop=Message Queue（ 消息队列） Set 集合内操作 添加元素 //sadd key element [element ...] //返回结果为添加成功的元素个数， 例如： 127.0.0.1:6379&gt; exists myset (integer) 0 127.0.0.1:6379&gt; sadd myset a b c (integer) 3 127.0.0.1:6379&gt; sadd myset a b (integer) 0 删除元素 //srem key element [element ...] //返回结果为成功删除元素个数， 例如： 127.0.0.1:6379&gt; srem myset a b (integer) 2 127.0.0.1:6379&gt; srem myset hello (integer) 0 计算元素个数 //scard key //scard的时间复杂度为O（1） ， 它不会遍历集合所有元素， 而是直接用Redis内部的变量， 例如： 127.0.0.1:6379&gt; scard myset (integer) 1 判断元素是否在集合中 //sismember key element //如果给定元素element在集合内返回1， 反之返回0， 例如： 127.0.0.1:6379&gt; sismember myset c (integer) 1 随机从集合返回指定个数元素 //srandmember key [count] //[count]是可选参数， 如果不写默认为1， 例如： 127.0.0.1:6379&gt; srandmember myset 2 1) &quot;a&quot; 2) &quot;c&quot; 127.0.0.1:6379&gt; srandmember myset &quot;d&quot; 从集合随机弹出元素 //spop key //spop操作可以从集合中随机弹出一个元素， 例如下面代码是一次spop后， 集合元素变为&quot;d b a&quot;： 123 127.0.0.1:6379&gt; spop myset &quot;c&quot; 127.0.0.1:6379&gt; smembers myset 1) &quot;d&quot; 2) &quot;b&quot; 3) &quot;a&quot; //需要注意的是Redis从3.2版本开始， spop也支持[count]参数。 //srandmember和spop都是随机从集合选出元素， 两者不同的是spop命令执行后， 元素会从集合中删除， 而srandmember不会。 获取所有元素 //smembers key //下面代码获取集合myset所有元素， 并且返回结果是无序的： 127.0.0.1:6379&gt; smembers myset 1) &quot;d&quot; 2) &quot;b&quot; 3) &quot;a&quot; //smembers和lrange、 hgetall都属于比较重的命令， 如果元素过多存在阻塞Redis的可能性， 这时候可以使用sscan来完成， 有关sscan命令2.7节会介绍。 集合间操作 //现在有两个集合， 它们分别是user： 1： follow和user： 2： follow： 127.0.0.1:6379&gt; sadd user:1:follow it music his sports (integer) 4 127.0.0.1:6379&gt; sadd user:2:follow it news ent sports (integer) 4 124 求多个集合的交集 //sinter key [key ...] //例如下面代码是求user： 1： follow和user： 2： follow两个集合的交集，返回结果是sports、 it： 127.0.0.1:6379&gt; sinter user:1:follow user:2:follow 1) &quot;sports&quot; 2) &quot;it&quot; 求多个集合的并集 //suinon key [key ...] //例如下面代码是求user： 1： follow和user： 2： follow两个集合的并集，返回结果是sports、 it、 his、 news、 music、 ent： 127.0.0.1:6379&gt; sunion user:1:follow user:2:follow 1) &quot;sports&quot; 2) &quot;it&quot; 3) &quot;his&quot; 4) &quot;news&quot; 5) &quot;music&quot; 6) &quot;ent&quot; 求多个集合的差集 //sdiff key [key ...] //例如下面代码是求user： 1： follow和user： 2： follow两个集合的差集，返回结果是music和his： 127.0.0.1:6379&gt; sdiff user:1:follow user:2:follow 1) &quot;music&quot; 2) &quot;his&quot; 前面三个命令如图所示 将交集、 并集、 差集的结果保存 sinterstore destination key [key ...] suionstore destination key [key ...] sdiffstore destination key [key ...] 集合间的运算在元素较多的情况下会比较耗时， 所以Redis提供了上面三个命令（原命令+store） 将集合间交集、 并集、 差集的结果保存在destination key中， 例如下面操作将user： 1： follow和user： 2： follow两个集合的交集结果保存在user： 1_2： inter中， user： 1_2： inter本身也是集合类 型： 127.0.0.1:6379&gt; sinterstore user:1_2:inter user:1:follow user:2:follow (integer) 2 127.0.0.1:6379&gt; type user:1_2:inter set 127.0.0.1:6379&gt; smembers user:1_2:inter 1) &quot;it&quot; 2) &quot;sports&quot; Set使用场景 sadd=Tagging（标签） spop/srandmember=Random item（生成随机数， 比如抽奖） sadd+sinter=Social Graph（社交需求） Zset 集合内操作 添加成员 //zadd key score member [score member ...] //下面操作向有序集合user： ranking添加用户tom和他的分数251： 127.0.0.1:6379&gt; zadd user:ranking 251 tom (integer) 1 //返回结果代表成功添加成员的个数： 127.0.0.1:6379&gt; zadd user:ranking 1 kris 91 mike 200 frank 220 tim 250 martin (integer) 5 有关zadd命令有两点需要注意： Redis3.2为zadd命令添加了nx、 xx、 ch、 incr四个选项： nx： member必须不存在， 才可以设置成功， 用于添加。 xx： member必须存在， 才可以设置成功， 用于更新。 ch： 返回此次操作后， 有序集合元素和分数发生变化的个数 incr： 对score做增加， 相当于后面介绍的zincrby。 有序集合相比集合提供了排序字段， 但是也产生了代价， zadd的时间复杂度为O（log（n） ） ， sadd的时间复杂度为O（1） 。 计算成员个数 //zcard key //例如下面操作返回有序集合user： ranking的成员数为5， 和集合类型的scard命令一样， zcard的时间复杂度为O（1） 。 127.0.0.1:6379&gt; zcard user:ranking (integer) 5 计算某个成员的分数 //zscore key member //tom的分数为251， 如果成员不存在则返回nil： 127.0.0.1:6379&gt; zscore user:ranking tom &quot;251&quot; 127.0.0.1:6379&gt; zscore user:ranking test (nil) 计算成员的排名 //zrank key member //zrevrank key member //zrank是从分数从低到高返回排名， zrevrank反之。 例如下面操作中， tom在zrank和zrevrank分别排名第5和第0（排名从0开始计算） 。 127.0.0.1:6379&gt; zrank user:ranking tom (integer) 5 127.0.0.1:6379&gt; zrevrank user:ranking tom (integer) 0 删除成员 //zrem key member [member ...] //下面操作将成员mike从有序集合user： ranking中删除。 127.0.0.1:6379&gt; zrem user:ranking mike (integer) 1 //返回结果为成功删除的个数。 增加成员的分数 //zincrby key increment member //下面操作给tom增加了9分， 分数变为了260分： 127.0.0.1:6379&gt; zincrby user:ranking 9 tom &quot;260&quot; 返回指定排名范围的成员 //zrange key start end [withscores] //zrevrange key start end [withscores] //有序集合是按照分值排名的， zrange是从低到高返回， zrevrange反之。 //下面代码返回排名最低的是三个成员， 如果加上withscores选项， 同时会返回成员的分数： 127.0.0.1:6379&gt; zrange user:ranking 0 2 withscores 1) &quot;kris&quot; 2) &quot;1&quot; 3) &quot;frank&quot; 4) &quot;200&quot; 5) &quot;tim&quot; 6) &quot;220&quot; 127.0.0.1:6379&gt; zrevrange user:ranking 0 2 withscores 1) &quot;tom&quot; 2) &quot;260&quot; 3) &quot;martin&quot; 4) &quot;250&quot; 5) &quot;tim&quot; 6) &quot;220&quot; 返回指定分数范围的成员 //zrangebyscore key min max [withscores] [limit offset count] //zrevrangebyscore key max min [withscores] [limit offset count] //其中zrangebyscore按照分数从低到高返回， zrevrangebyscore反之。 例如下面操作从低到高返回200到221分的成员， withscores选项会同时返回每个成员的分数。 [limit offset count]选项可以限制输出的起始位置和个数： 127.0.0.1:6379&gt; zrangebyscore user:ranking 200 tinf withscores 1) &quot;frank&quot; 2) &quot;200&quot; 3) &quot;tim&quot; 4) &quot;220&quot; 127.0.0.1:6379&gt; zrevrangebyscore user:ranking 221 200 withscores 1) &quot;tim&quot; 2) &quot;220&quot; 3) &quot;frank&quot; 4) &quot;200&quot; //同时min和max还支持开区间（ 小括号） 和闭区间（ 中括号） ， -inf和+inf分别代表无限小和无限大： 127.0.0.1:6379&gt; zrangebyscore user:ranking (200 +inf withscores 1) &quot;tim&quot; 2) &quot;220&quot; 3) &quot;martin&quot; 4) &quot;250&quot; 5) &quot;tom&quot; 6) &quot;260&quot; 返回指定分数范围成员个数 //zcount key min max //下面操作返回200到221分的成员的个数： 127.0.0.1:6379&gt; zcount user:ranking 200 221 (integer) 2 删除指定排名内的升序元素 //zremrangebyrank key start end //下面操作删除第start到第end名的成员： 127.0.0.1:6379&gt; zremrangebyrank user:ranking 0 2 (integer) 3 删除指定分数范围的成员 //zremrangebyscore key min max //下面操作将250分以上的成员全部删除， 返回结果为成功删除的个数： 127.0.0.1:6379&gt; zremrangebyscore user:ranking (250 +inf (integer) 2 集合间操作 将图中的两个有序集合导入到Redis中。 127.0.0.1:6379&gt; zadd user:ranking:1 1 kris 91 mike 200 frank 220 tim 250 martin 251 tom (integer) 6 127.0.0.1:6379&gt; zadd user:ranking:2 8 james 77 mike 625 martin 888 tom (integer) 4 交集 //zinterstore destination numkeys key [key ...] [weights weight [weight ...]][aggregate sum|min|max] //这个命令参数较多， 下面分别进行说明： //·destination： 交集计算结果保存到这个键。 //·numkeys： 需要做交集计算键的个数。 //·key[key...]： 需要做交集计算的键。 //·weights weight[weight...]： 每个键的权重， 在做交集计算时， 每个键中的每个member会将自己分数乘以这个权重， 每个键的权重默认是1。 //·aggregate sum|min|max： 计算成员交集后， 分值可以按照sum（ 和） 、min（ 最小值） 、 max（ 最大值） 做汇总， 默认值是sum。 //下面操作对user： ranking： 1和user： ranking： 2做交集， weights和aggregate使用了默认配置， 可以看到目标键user： ranking： 1_inter_2对分值做了sum操作： 127.0.0.1:6379&gt; zinterstore user:ranking:1_inter_2 2 user:ranking:1 user:ranking:2 (integer) 3 127.0.0.1:6379&gt; zrange user:ranking:1_inter_2 0 -1 withscores 1) &quot;mike&quot; 2) &quot;168&quot; 3) &quot;martin&quot; 4) &quot;875&quot; 5) &quot;tom&quot; 6) &quot;1139&quot; //如果想让user： ranking： 2的权重变为0.5， 并且聚合效果使用max， 可以执行如下操作： 127.0.0.1:6379&gt; zinterstore user:ranking:1_inter_2 2 user:ranking:1 user:ranking:2 weights 1 0.5 aggregate max (integer) 3 127.0.0.1:6379&gt; zrange user:ranking:1_inter_2 0 -1 withscores 1) &quot;mike&quot; 2) &quot;91&quot; 3) &quot;martin&quot; 4) &quot;312.5&quot; 5) &quot;tom&quot; 6) &quot;444&quot; 并集 //zunionstore destination numkeys key [key ...] [weights weight [weight ...]][aggregate sum|min|max] //该命令的所有参数和zinterstore是一致的， 只不过是做并集计算， 例如下面操作是计算user： ranking： 1和user： ranking： 2的并集， weights和 //aggregate使用了默认配置， 可以看到目标键user： ranking： 1_union_2对分值做了sum操作： 127.0.0.1:6379&gt; zunionstore user:ranking:1_union_2 2 user:ranking:1 user:ranking:2 (integer) 7 127.0.0.1:6379&gt; zrange user:ranking:1_union_2 0 -1 withscores 1) &quot;kris&quot; 2) &quot;1&quot; 3) &quot;james&quot; 4) &quot;8&quot; 5) &quot;mike&quot; 6) &quot;168&quot; 7) &quot;frank&quot; 8) &quot;200&quot; 9) &quot;tim&quot; 10) &quot;220&quot; 11) &quot;martin&quot; 12) &quot;875&quot; 13) &quot;tom&quot; 14) &quot;1139&quot; Zset使用场景 有序集合比较典型的使用场景就是排行榜系统。 例如视频网站需要对用户上传的视频做排行榜， 榜单的维度可能是多个方面的： 按照时间、 按照播 放数量、 按照获得的赞数。 本节使用赞数这个维度， 记录每天用户上传视频的排行榜。 主要需要实现以下4个功能。 添加用户赞数 例如用户mike上传了一个视频， 并获得了3个赞， 可以使用有序集合的zadd和zincrby功能： zadd user:ranking:2016_03_15 mike 3 如果之后再获得一个赞， 可以使用zincrby： zincrby user:ranking:2016_03_15 mike 1 取消用户赞数 由于各种原因（例如用户注销、 用户作弊） 需要将用户删除， 此时需要将用户从榜单中删除掉， 可以使用zrem。 例如删除成员tom： zrem user:ranking:2016_03_15 mike 展示获取赞数最多的十个用户 此功能使用zrevrange命令实现： zrevrangebyrank user:ranking:2016_03_15 0 9 展示用户信息以及用户分数 此功能将用户名作为键后缀， 将用户信息保存在哈希类型中， 至于用户的分数和排名可以使用zscore和zrank两个功能： hgetall user:info:tom zscore user:ranking:2016_03_15 mike zrank user:ranking:2016_03_15 mik Hash 设置值 //hset key field value //下面为user： 1添加一对field-value： 127.0.0.1:6379&gt; hset user:1 name tom (integer) 1 //如果设置成功会返回1， 反之会返回0。 此外Redis提供了hsetnx命令， 它们的关系就像set和setnx命令一样， 只不过作用域由键变为field。 获取值 //hget key field //例如， 下面操作获取user： 1的name域（属性） 对应的值： 127.0.0.1:6379&gt; hget user:1 name &quot;tom&quot; //如果键或field不存在， 会返回nil： 127.0.0.1:6379&gt; hget user:2 name (nil) 127.0.0.1:6379&gt; hget user:1 age (nil) 删除field //hdel key field [field ...] //hdel会删除一个或多个field， 返回结果为成功删除field的个数， 例如： 127.0.0.1:6379&gt; hdel user:1 name (integer) 1 127.0.0.1:6379&gt; hdel user:1 age (integer) 0 计算field个数 //hlen key //例如user： 1有3个field： 127.0.0.1:6379&gt; hset user:1 name tom (integer) 1 127.0.0.1:6379&gt; hset user:1 age 23 (integer) 1 127.0.0.1:6379&gt; hset user:1 city tianjin (integer) 1 127.0.0.1:6379&gt; hlen user:1 (integer) 3 批量设置或获取field-value //hmget key field [field ...] //hmset key field value [field value ...] //hmset和hmget分别是批量设置和获取field-value， hmset需要的参数是key和多对field-value， hmget需要的参数是key和多个field。 例如： 127.0.0.1:6379&gt; hmset user:1 name mike age 12 city tianjin OK 127.0.0.1:6379&gt; hmget user:1 name city 1) &quot;mike&quot; 2) &quot;tianjin&quot; 判断field是否存在 //hexists key field //例如， user： 1包含name域， 所以返回结果为1， 不包含时返回0： 127.0.0.1:6379&gt; hexists user:1 name (integer) 1 获取所有field //hkeys key //hkeys命令应该叫hfields更为恰当， 它返回指定哈希键所有的field， 例如： 127.0.0.1:6379&gt; hkeys user:1 1) &quot;name&quot; 2) &quot;age&quot; 3) &quot;city&quot; 获取所有value //hvals key 下面操作获取user： 1全部value： 127.0.0.1:6379&gt; hvals user:1 1) &quot;mike&quot; 2) &quot;12&quot; 3) &quot;tianjin&quot; 获取所有的field-value //hgetall key //下面操作获取user： 1所有的field-value： 127.0.0.1:6379&gt; hgetall user:1 1) &quot;name&quot; 2) &quot;mike&quot; 3) &quot;age&quot; 4) &quot;12&quot; 5) &quot;city&quot; 6) &quot;tianjin&quot; 在使用hgetall时， 如果哈希元素个数比较多， 会存在阻塞Redis的可能。 如果开发人员只需要获取部分field， 可以使用hmget， 如果一定要获取全部field-value， 可以使用hscan命令， 该命令会渐进式遍历哈希类型. hincrby hincrbyfloat //hincrby key field //hincrbyfloat key field //hincrby和hincrbyfloat， 就像incrby和incrbyfloat命令一样， 但是它们的作用域是filed。 计算value的字符串长度（需要Redis3.2以上） //hstrlen key field //例如hget user： 1name的value是tom， 那么hstrlen的返回结果是3： 127.0.0.1:6379&gt; hstrlen user:1 name (integer) 3 Hash使用场景 保存对象信息 Redis 发布订阅 Redis 发布订阅 (pub/sub) 是一种消息通信模式：发送者 (pub) 发送消息，订阅者 (sub) 接收消息。 Redis 客户端可以订阅任意数量的频道。 下图展示了频道 channel1 ， 以及订阅这个频道的三个客户端 —— client2 、 client5 和 client1 之间的关系： 当有新消息通过 PUBLISH 命令发送给频道 channel1 时， 这个消息就会被发送给订阅它的三个客户端： 实例 以下实例演示了发布订阅是如何工作的，需要开启两个 redis-cli 客户端。 在我们实例中我们创建了订阅频道名为 runoobChat: 第一个 redis-cli 客户端 redis 127.0.0.1:6379&gt; SUBSCRIBE runoobChat Reading messages... (press Ctrl-C to quit) &quot;subscribe&quot; &quot;redisChat&quot; (integer) 1 现在，我们先重新开启个 redis 客户端，然后在同一个频道 runoobChat 发布两次消息，订阅者就能接收到消息。 第二个 redis-cli 客户端 redis 127.0.0.1:6379&gt; PUBLISH runoobChat &quot;Redis PUBLISH test&quot; (integer) 1 redis 127.0.0.1:6379&gt; PUBLISH runoobChat &quot;Learn redis by runoob.com&quot; (integer) 1 # 订阅者的客户端会显示如下消息 &quot;message&quot; &quot;runoobChat&quot; &quot;Redis PUBLISH test&quot; &quot;message&quot; &quot;runoobChat&quot; &quot;Learn redis by runoob.com&quot; springboot中实现 一.配置文件 spring: redis: host: 192.168.0.200 port: 6379 password: database: 1 pool.max-active: 8 pool.max-wait: -1 pool.max-idle: 8 pool.min-idle: 0 timeout: 5000 二.maven坐标 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 三.redis消息监听器容器以及redis监听器注入IOC容器 package com.example.demo; import org.springframework.cache.annotation.EnableCaching; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.context.annotation.Scope; import org.springframework.data.redis.connection.RedisConnectionFactory; import org.springframework.data.redis.listener.PatternTopic; import org.springframework.data.redis.listener.RedisMessageListenerContainer; import org.springframework.data.redis.listener.adapter.MessageListenerAdapter; @Configuration @EnableCaching public class RedisConfig{ /** * Redis消息监听器容器 * @param connectionFactory * @return **/ @Bean RedisMessageListenerContainer container(RedisConnectionFactory connectionFactory) { RedisMessageListenerContainer container = new RedisMessageListenerContainer(); container.setConnectionFactory(connectionFactory); //订阅了一个叫pmp和channel 的通道，多通道 container.addMessageListener(listenerAdapter(new RedisPmpSub()),new PatternTopic(&quot;pmp&quot;)); container.addMessageListener(listenerAdapter(new RedisChannelSub()),new PatternTopic(&quot;channel&quot;)); //这个container 可以添加多个 messageListener return container; } /** * 配置消息接收处理类 * @param redisMsg 自定义消息接收类 * @return */ @Bean() @Scope(&quot;prototype&quot;) MessageListenerAdapter listenerAdapter(RedisMsg redisMsg) { //这个地方 是给messageListenerAdapter 传入一个消息接受的处理器，利用反射的方法调用“receiveMessage” //也有好几个重载方法，这边默认调用处理器的方法 叫handleMessage 可以自己到源码里面看 return new MessageListenerAdapter(redisMsg, &quot;receiveMessage&quot;);//注意2个通道调用的方法都要为receiveMessage } } 四.普通的消息处理器接口 package com.example.demo; import org.springframework.stereotype.Component; @Component public interface RedisMsg { public void receiveMessage(String message); } 五.普通的消息处理器POJO package com.example.demo; /** * @Auther: Administrator * @Date: 2018/7/9 11:01 * @Description: */ public class RedisChannelSub implements RedisMsg { @Override public void receiveMessage(String message) { //注意通道调用的方法名要和RedisConfig的listenerAdapter的MessageListenerAdapter参数2相同 System.out.println(&quot;这是RedisChannelSub&quot;+&quot;-----&quot;+message); } } package com.example.demo; public class RedisPmpSub implements RedisMsg{ /** * 接收消息的方法 * @param message 订阅消息 */ public void receiveMessage(String message){ //注意通道调用的方法名要和RedisConfig的listenerAdapter的MessageListenerAdapter参数2相同 System.out.println(message); } } 六.消息发布者 package com.example.demo; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.data.redis.core.StringRedisTemplate; import org.springframework.scheduling.annotation.EnableScheduling; import org.springframework.scheduling.annotation.Scheduled; import org.springframework.stereotype.Component; //定时器 @EnableScheduling @Component public class TestSenderController { @Autowired private StringRedisTemplate stringRedisTemplate; //向redis消息队列index通道发布消息 @Scheduled(fixedRate = 2000) public void sendMessage(){ stringRedisTemplate.convertAndSend(&quot;pmp&quot;,String.valueOf(Math.random())); stringRedisTemplate.convertAndSend(&quot;channel&quot;,String.valueOf(Math.random())); } } 七.启动类 package com.example.demo; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } } 3种高级数据结构 BitMap BitMap，即位图，其实也就是 byte 数组，用二进制表示，只有 0 和 1 两个数字。 如图所示： 重要 API 命令 含义 getbit key offset 对key所存储的字符串值，获取指定偏移量上的位（bit） setbit key offset value 对key所存储的字符串值，设置或清除指定偏移量上的位（bit） 1. 返回值为该位在setbit之前的值 2. value只能取0或1 3. offset从0开始，即使原位图只能10位，offset可以取1000 bitcount key [start end] 获取位图指定范围中位值为1的个数 如果不指定start与end，则取所有 bitop op destKey key1 [key2...] 做多个BitMap的and（交集）、or（并集）、not（非）、xor（异或）操作并将结果保存在destKey中 bitpos key tartgetBit [start end] 计算位图指定范围第一个偏移量对应的的值等于targetBit的位置 1. 找不到返回-1 2. start与end没有设置，则取全部 3. targetBit只能取0或者1 演示 应用场景 统计每日用户的登录数。每一位标识一个用户ID，当某个用户访问我们的网页或执行了某个操作，就在bitmap中把标识此用户的位设置为1。 这里做了一个 使用 set 和 BitMap 存储的对比。 场景1：1 亿用户，5千万独立 数据类型 每个 userid 占用空间 需要存储的用户量 全部内存量 set 32位（假设userid用的是整型，实际很多网站用的是长整型） 50,000,000 32位 * 50,000,000 = 200 MB BitMap 1 位 100,000,000 1 位 * 100,000,000 = 12.5 MB 一天 一个月 一年 set 200M 6G 72G BitMap 12.5M 375M 4.5G 场景2：只有 10 万独立用户 数据类型 每个 userid 占用空间 需要存储的用户量 全部内存量 set 32位（假设userid用的是整型，实际很多网站用的是长整型） 1,000,000 32位 * 1,000,000 = 4 MB BitMap 1 位 100,000,000 1 位 * 100,000,000 = 12.5 MB 通过上面的对比，我们可以看到，如果独立用户数量很多，使用 BitMap 明显更有优势，能节省大量的内存。但如果独立用户数量较少，还是建议使用 set 存储，BitMap 会产生多余的存储开销。 使用经验 type = string，BitMap 是 sting 类型，最大 512 MB。 注意 setbit 时的偏移量，可能有较大耗时 位图不是绝对好。 HyperLogLog HyperLogLog 是基于 HyperLogLog 算法的一种数据结构，该算法可以在极小空间完成独立数量统计。 在本质上还是字符串类型。 重要 API 命令 含义 pfadd key element1 [element2...] 向HyperLogLog中添加元素 pfcount key1 [key2...] 计算HyperLogLog的独立总数 pfmerge destKey key1 [key2...] 合并多个hyperLogLog到destKey中 演示 内存消耗 以百万独立用户为例 内存消耗 1 天 15 KB 1 个月 450 KB 1 年 15KB * 365 = 5 MB 可以看到内存消耗是非常低的，比我们之前学过的 BitMap 还要低得多。 使用经验 Q：既然 HyperLogLog 那么好，那么是不是以后用这个来存储数据就行了呢？ A：这里要考虑两个因素： hyperloglog 的错误率为：0.81%，存储的数据不能百分百准确。 hyperloglog 不能取出单条数据。api 中也没有相关操作。 如果你没有这两个方面的顾虑，那么用 HyperLogLog 来存储大规模数据，还是非常不错的。 GEO Redis 3.2添加新特性 功能：存储经纬度、计算两地距离、范围计算等 基于ZSet实现 删除操作使用 zrem key member GEO 相关命令 1. geoadd key longitude latitude member [lon lat member...] 含义：增加地理位置信息 longitude ：经度 latitude : 纬度 member : 标识信息 2. geopos key member1 [member2...] 含义：获取地理位置信息 3. geodist key member1 member2 [unit] 含义：获取两个地理位置的距离 unit取值范围 m（米，默认） km（千米） mi（英里） ft（英尺） 4. georadius key longitude latitude unit [withcoord] [withdist] [withhash] [COUNT count] [sort] [store key] [storedist key] 含义：以给定的经纬度为中心，返回包含的位置元素当中，与中心距离不超过给定最大距离的所有位置元素。 unit取值范围 m（米） km（千米） mi（英里） ft（英尺） withcoord：将位置元素的经度与纬度也一并返回 withdist：在返回位置元素的同时，将距离也一并返回。距离的单位和用户给定的范围单位保持一致 withhash：以52位的符号整数形式，返回位置元素经过geohash编码的有序集合分值。用于底层应用或调试，实际作用不大。 sort取值范围 asc：根据中心位置，按照从近到远的方式返回位置元素 desc：根据中心位置，按照从远到近的方式返回位置元素 store key：将返回结果而的地理位置信息保存到指定键 storedist key：将返回结果距离中心节点的距离保存到指定键 5. georadiusbymember key member radius unit [withcoord][withdist][withhash][COUNT count][sort][store key][storedist key] 含义：以给定的元素为中心，返回包含的位置元素当中，与中心距离不超过给定最大距离的所有位置元素。 unit取值范围 m（米） km（千米） mi（英里） ft（英尺） withcoord：将位置元素的经度与纬度也一并返回 withdist：在返回位置元素的同时，将距离也一并返回。距离的单位和用户给定的范围单位保持一致 withhash：以52位的符号整数形式，返回位置元素经过geohash编码的有序集合分值。用于底层应用或调试，实际作用不大。 sort取值范围 asc：根据中心位置，按照从近到远的方式返回位置元素 desc：根据中心位置，按照从远到近的方式返回位置元素 store key：将返回结果而的地理位置信息保存到指定键 storedist key：将返回结果距离中心节点的距离保存到指定键 演示 geo 功能是在 redis-3.2 后引入的 127.0.0.1:6381&gt; geoadd cities:locations 116.28 39.55 beijing (integer) 1 127.0.0.1:6381&gt; geoadd cities:locations 117.12 39.08 tianjin 114.29 38.02 shijiazhuang 118.01 39.38 tangshan 115.29 38.51 baoding (integer) 4 127.0.0.1:6381&gt; geopos cities:locations tianjin 1) 1) &quot;117.12000042200088501&quot; 2) &quot;39.0800000535766543&quot; 127.0.0.1:6381&gt; geodist cities:locations tianjin beijing km &quot;89.2061&quot; 127.0.0.1:6379&gt; georadiusbymember cities:locations beijing 150 km 1) &quot;beijing&quot; 2) &quot;tianjin&quot; 3) &quot;tangshan&quot; 4) &quot;baoding&quot;。 ","link":"https://tinaxiawuhao.github.io/post/YNIBE7Yaj/"},{"title":"redis概述一","content":"什么是Redis Redis(Remote Dictionary Server) 是一个使用 C 语言编写的，开源的（BSD许可）高性能非关系型（NoSQL）的键值对数据库。 Redis 可以存储键和五种不同类型的值之间的映射。键的类型只能为字符串，值支持五种数据类型：字符串、列表、集合、散列表、有序集合。 与传统数据库不同的是 Redis 的数据是存在内存中的，所以读写速度非常快，因此 redis 被广泛应用于缓存方向，每秒可以处理超过 10万次读写操作，是已知性能最快的Key-Value DB。另外，Redis 也经常用来做分布式锁。除此之外，Redis 支持事务 、持久化、LUA脚本、LRU驱动事件、多种集群方案。 Redis有哪些优缺点 优点 读写性能优异， Redis能读的速度是110000次/s，写的速度是81000次/s。 支持数据持久化，支持AOF和RDB两种持久化方式。 支持事务，Redis的所有操作都是原子性的，同时Redis还支持对几个操作合并后的原子性执行。 数据结构丰富，除了支持string类型的value外还支持hash、set、zset、list等数据结构。 支持主从复制，主机会自动将数据同步到从机，可以进行读写分离。 缺点 数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。 Redis 不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复。 主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性。 Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。 为什么要用 Redis /为什么要用缓存 主要从“高性能”和“高并发”这两点来看待这个问题。 高性能： 假如用户第一次访问数据库中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据存在缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！ 高并发： 直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。 为什么要用 Redis 而不用 map/guava 做缓存? 缓存分为本地缓存和分布式缓存。以 Java 为例，使用自带的 map 或者 guava 实现的是本地缓存，最主要的特点是轻量以及快速，生命周期随着 jvm 的销毁而结束，并且在多实例的情况下，每个实例都需要各自保存一份缓存，缓存不具有一致性。 使用 redis 或 memcached 之类的称为分布式缓存，在多实例的情况下，各实例共用一份缓存数据，缓存具有一致性。缺点是需要保持 redis 或 memcached服务的高可用，整个程序架构上较为复杂。 Redis为什么这么快 1、完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于 HashMap，HashMap 的优势就是查找和操作的时间复杂度都是O(1)； 2、数据结构简单，对数据操作也简单，Redis 中的数据结构是专门进行设计的； 3、采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗； 4、使用多路 I/O 复用模型，非阻塞 IO； 5、使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis 直接自己构建了 VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求； ","link":"https://tinaxiawuhao.github.io/post/7i0HRwy0e/"},{"title":"多线程基础概念四","content":"Lock 接口(Lock interface)简介 Lock 接口比同步方法和同步块提供了更具扩展性的锁操作。他们允许更灵活的结构，可以具有完全不同的性质，并且可以支持多个相关类的条件对象。 优势 它的优势有： （1）可以使锁更公平 （2）可以使线程在等待锁的时候响应中断 （3）可以让线程尝试获取锁，并在无法获取锁的时候立即返回或者等待一段时间 （4）可以在不同的范围，以不同的顺序获取和释放锁 整体上来说 Lock 是 synchronized 的扩展版，Lock 提供了无条件的、可轮询的(tryLock 方法)、定时的(tryLock 带参方法)、可中断的(lockInterruptibly)、可多条件队列的(newCondition 方法)锁操作。另外 Lock 的实现类基本都支持非公平锁(默认)和公平锁，synchronized 只支持非公平锁，当然，在大部分情况下，非公平锁是高效的选择。 乐观锁和悲观锁 悲观锁：总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。再比如 Java 里面的同步原语 synchronized 关键字的实现也是悲观锁。 乐观锁：顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于 write_condition 机制，其实都是提供的乐观锁。在 Java中 java.util.concurrent.atomic 包下面的原子变量类就是使用了乐观锁的一种实现方式 CAS 实现的。 乐观锁的实现方式： 1、使用版本标识来确定读到的数据与提交时的数据是否一致。提交后修改版本标识，不一致时可以采取丢弃和再次尝试的策略。 2、java 中的 Compare and Swap 即 CAS ，当多个线程尝试使用 CAS 同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。 CAS 操作中包含三个操作数 —— 需要读写的内存位置（V），进行比较的预期原值（A）和拟写入的新值(B)。如果内存位置 V 的值与预期原值 A 相匹配，那么处理器会自动将该位置值更新为新值 B。否则处理器不做任何操作。 CAS CAS 是 compare and swap 的缩写，即我们所说的比较交换。 CAS 是一种基于锁的操作，而且是乐观锁。在 java 中锁分为乐观锁和悲观锁。悲观锁是将资源锁住，等一个之前获得锁的线程释放锁之后，下一个线程才可以访问。而乐观锁采取了一种宽泛的态度，通过某种方式不加锁来处理资源，比如通过给记录加 version 来获取数据，性能较悲观锁有很大的提高。 CAS 操作包含三个操作数 —— 内存位置（V）、预期原值（A）和新值(B)。如果内存地址里面的值和 A 的值是一样的，那么就将内存里面的值更新成 B。CAS是通过无限循环来获取数据的，若果在第一轮循环中，a 线程获取地址里面的值被b 线程修改了，那么 a 线程需要自旋，到下次循环才有可能机会执行。 java.util.concurrent.atomic 包下的类大多是使用 CAS 操作来实现的(AtomicInteger,AtomicBoolean,AtomicLong)。 CAS产生的问题 ABA 问题： 比如说一个线程 one 从内存位置 V 中取出 A，这时候另一个线程 two 也从内存中取出 A，并且 two 进行了一些操作变成了 B，然后 two 又将 V 位置的数据变成 A，这时候线程 one 进行 CAS 操作发现内存中仍然是 A，然后 one 操作成功。尽管线程 one 的 CAS 操作成功，但可能存在潜藏的问题。从 Java1.5 开始 JDK 的 atomic包里提供了一个类AtomicStampedReference 来解决 ABA 问题。 循环时间长开销大： 对于资源竞争严重（线程冲突严重）的情况，CAS 自旋的概率会比较大，从而浪费更多的 CPU 资源，效率低于 synchronized。 只能保证一个共享变量的原子操作： 当对一个共享变量执行操作时，我们可以使用循环 CAS 的方式来保证原子操作，但是对多个共享变量操作时，循环 CAS 就无法保证操作的原子性，这个时候就可以用锁。 公平锁锁和非公平锁 公平锁：多个线程按照申请锁的顺序去获得锁，线程会直接进入队列去排队，永远都是队列的第一位才能得到锁。 优点：所有的线程都能得到资源，不会饿死在队列中。 缺点：吞吐量会下降很多，队列里面除了第一个线程，其他的线程都会阻塞，cpu唤醒阻塞线程的开销会很大。 非公平锁：多个线程去获取锁的时候，会直接去尝试获取，获取不到，再去进入等待队列，如果能获取到，就直接获取到锁。 优点：可以减少CPU唤醒线程的开销，整体的吞吐效率会高点，CPU也不必取唤醒所有线程，会减少唤起线程的数量。 缺点：你们可能也发现了，这样可能导致队列中间的线程一直获取不到锁或者长时间获取不到锁，导致饿死。 伪共享 一、伪共享的定义： 缓存系统中是以缓存行（cache line）为单位存储的，当多线程修改互相独立的变量时，如果这些变量共享同一个缓存行，就会无意中影响彼此的性能，这就是伪共享。 二、CPU缓存机制 CPU 缓存（Cache Memory）是位于 CPU 与内存之间的临时存储器，它的容量比内存小的多但是交换速度却比内存要快得多。 高速缓存的出现主要是为了解决 CPU 运算速度与内存读写速度不匹配的矛盾，因为 CPU 运算速度要比内存读写速度快很多，这样会使 CPU 花费很长时间等待数据到来或把数据写入内存。 在缓存中的数据是内存中的一小部分，但这一小部分是短时间内 CPU 即将访问的，当 CPU 调用大量数据时，就可避开内存直接从缓存中调用，从而加快读取速度。 CPU 和主内存之间有好几层缓存，因为即使直接访问主内存也是非常慢的。如果你正在多次对一块数据做相同的运算，那么在执行运算的时候把它加载到离 CPU 很近的地方就有意义了。 按照数据读取顺序和与 CPU 结合的紧密程度，CPU 缓存可以分为一级缓存，二级缓存，部分高端 CPU 还具有三级缓存。每一级缓存中所储存的全部数据都是下一级缓存的一部分，越靠近 CPU 的缓存越快也越小。所以 L1 缓存很小但很快(译注：L1 表示一级缓存)，并且紧靠着在使用它的 CPU 内核。L2 大一些，也慢一些，并且仍然只能被一个单独的 CPU 核使用。L3 在现代多核机器中更普遍，仍然更大，更慢，并且被单个插槽上的所有 CPU 核共享。最后，你拥有一块主存，由全部插槽上的所有 CPU 核共享。拥有三级缓存的的 CPU，到三级缓存时能够达到 95% 的命中率，只有不到 5% 的数据需要从内存中查询。 多核机器的存储结构如下图所示： 当 CPU 执行运算的时候，它先去 L1 查找所需的数据，再去 L2，然后是 L3，最后如果这些缓存中都没有，所需的数据就要去主内存拿。走得越远，运算耗费的时间就越长。所以如果你在做一些很频繁的事，你要确保数据在 L1 缓存中。 Martin Thompson 给出了一些缓存未命中的消耗数据，如下所示： 三、缓存行 缓存系统中是以缓存行（cache line）为单位存储的。缓存行通常是 64 字节（译注：本文基于 64 字节，其他长度的如 32 字节等不适本文讨论的重点），并且它有效地引用主内存中的一块地址。例如一个 的 long 类型是 8 字节，因此在一个缓存行中可以存 8 个 long 类型的变量。所以，如果你访问一个 long 数组，当数组中的一个值被加载到缓存中，它会额外加载另外 7 个，以致你能非常快地遍历这个数组。事实上，你可以非常快速的遍历在连续的内存块中分配的任意数据结构。而如果你在数据结构中的项在内存中不是彼此相邻的（如链表），你将得不到免费缓存加载所带来的优势，并且在这些数据结构中的每一个项都可能会出现缓存未命中。 如果存在这样的场景，有多个线程操作不同的成员变量，但是相同的缓存行，这个时候会发生什么？。没错，伪共享（False Sharing）问题就发生了！有张 Disruptor 项目的经典示例图，如下： 上图中，一个运行在处理器 core1上的线程想要更新变量 X 的值，同时另外一个运行在处理器 core2 上的线程想要更新变量 Y 的值。但是，这两个频繁改动的变量都处于同一条缓存行。两个线程就会轮番发送 RFO 消息，占得此缓存行的拥有权。当 core1 取得了拥有权开始更新 X，则 core2 对应的缓存行需要设为 I 状态。当 core2 取得了拥有权开始更新 Y，则 core1 对应的缓存行需要设为 I 状态(失效态)。轮番夺取拥有权不但带来大量的 RFO 消息，而且如果某个线程需要读此行数据时，L1 和 L2 缓存上都是失效数据，只有 L3 缓存上是同步好的数据。从前一篇我们知道，读 L3 的数据非常影响性能。更坏的情况是跨槽读取，L3 都要 miss，只能从内存上加载。 表面上 X 和 Y 都是被独立线程操作的，而且两操作之间也没有任何关系。只不过它们共享了一个缓存行，但所有竞争冲突都是来源于共享。 因此，当两个以上CPU都要访问同一个缓存行大小的内存区域时，就会引起冲突，这种情况就叫“共享”。但是，这种情况里面又包含了“其实不是共享”的“伪共享”情况。比如，两个处理器各要访问一个word，这两个word却存在于同一个cache line大小的区域里，这时，从应用逻辑层面说，这两个处理器并没有共享内存，因为他们访问的是不同的内容（不同的word）。但是因为cache line的存在和限制，这两个CPU要访问这两个不同的word时，却一定要访问同一个cache line块，产生了事实上的“共享”。显然，由于cache line大小限制带来的这种“伪共享”是我们不想要的，会浪费系统资源。 四、如何避免伪共享？ 1）让不同线程操作的对象处于不同的缓存行。 可以进行缓存行填充（Padding） 。例如，如果一条缓存行有 64 字节，而 Java 程序的对象头固定占 8 字节(32位系统)或 12 字节( 64 位系统默认开启压缩, 不开压缩为 16 字节)，所以我们只需要填 6 个无用的长整型补上6*8=48字节，让不同的 VolatileLong 对象处于不同的缓存行，就避免了伪共享( 64 位系统超过缓存行的 64 字节也无所谓，只要保证不同线程不操作同一缓存行就可以)。 2）使用编译指示，强制使每一个变量对齐。 强制使对象按照缓存行的边界对齐。例如可以使数据按64位对齐，那么一个缓存行只有一个可操作对象，这样发生伪共享之后，也只是对应缓存行的数据变化，并不影响其他的对象。 无锁队列 import java.util.concurrent.atomic.AtomicInteger; import java.util.concurrent.atomic.AtomicReferenceArray; /** * 用数组实现无锁有界队列 */ public class LockFreeQueue { private AtomicReferenceArray atomicReferenceArray; //代表为空，没有元素 private static final Integer EMPTY = null; //头指针,尾指针 AtomicInteger head,tail; public LockFreeQueue(int size){ atomicReferenceArray = new AtomicReferenceArray(new Integer[size + 1]); head = new AtomicInteger(0); tail = new AtomicInteger(0); } /** * 入队 * @param element * @return */ public boolean add(Integer element){ int index = (tail.get() + 1) % atomicReferenceArray.length(); if( index == head.get() % atomicReferenceArray.length()){ System.out.println(&quot;当前队列已满,&quot;+ element+&quot;无法入队!&quot;); return false; } while(!atomicReferenceArray.compareAndSet(index,EMPTY,element)){ return add(element); } tail.incrementAndGet(); //移动尾指针 System.out.println(&quot;入队成功!&quot; + element); return true; } /** * 出队 * @return */ public Integer poll(){ if(head.get() == tail.get()){ System.out.println(&quot;当前队列为空&quot;); return null; } int index = (head.get() + 1) % atomicReferenceArray.length(); Integer ele = (Integer) atomicReferenceArray.get(index); if(ele == null){ //有可能其它线程也在出队 return poll(); } while(!atomicReferenceArray.compareAndSet(index,ele,EMPTY)){ return poll(); } head.incrementAndGet(); System.out.println(&quot;出队成功!&quot; + ele); return ele; } public void print(){ StringBuffer buffer = new StringBuffer(&quot;[&quot;); for(int i = 0; i &lt; atomicReferenceArray.length() ; i++){ if(i == head.get() || atomicReferenceArray.get(i) == null){ continue; } buffer.append(atomicReferenceArray.get(i) + &quot;,&quot;); } buffer.deleteCharAt(buffer.length() - 1); buffer.append(&quot;]&quot;); System.out.println(&quot;队列内容:&quot; +buffer.toString()); } } 死锁 当线程 A 持有独占锁a，并尝试去获取独占锁 b 的同时，线程 B 持有独占锁 b，并尝试获取独占锁 a 的情况下，就会发生 AB 两个线程由于互相持有对方需要的锁，而发生的阻塞现象，我们称为死锁。 1. 产生死锁的条件和防止死锁 互斥 请求与保持 不可剥夺 循环等待 产生死锁的必要条件： 互斥条件：所谓互斥就是进程在某一时间内独占资源。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不可剥夺条件：进程已获得资源，在末使用完之前，不能强行剥夺。 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。 这四个条件是死锁的必要条件，只要系统发生死锁，这些条件必然成立，而只要上述条件之 一不满足，就不会发生死锁。 理解了死锁的原因，尤其是产生死锁的四个必要条件，就可以最大可能地避免、预防和 解除死锁。 防止死锁可以采用以下的方法： 尽量使用 tryLock(long timeout, TimeUnit unit)的方法(ReentrantLock、ReentrantReadWriteLock)，设置超时时间，超时可以退出防止死锁。 尽量使用 Java. util. concurrent 并发类代替自己手写锁。 尽量降低锁的使用粒度，尽量不要几个功能用同一把锁。 尽量减少同步的代码块。 import lombok.SneakyThrows; import java.util.concurrent.TimeUnit; class HoldLockThread implements Runnable{ private String lockOne; private String lockTwo; public HoldLockThread(String lockOne, String lockTwo) { this.lockOne = lockOne; this.lockTwo = lockTwo; } @Override @SneakyThrows public void run(){ synchronized (lockOne){ System.out.println(Thread.currentThread().getName()+&quot;自己获取&quot;+lockOne+&quot;尝试获取&quot;+lockTwo); TimeUnit.SECONDS.sleep(2L); synchronized (lockTwo){ System.out.println(Thread.currentThread().getName()+&quot;自己获取&quot;+lockTwo+&quot;尝试获取&quot;+lockOne); } } } } public class DeadLockDemo { public static void main(String[] args) { String lockA=&quot;lockA&quot;; String lockB=&quot;lockB&quot;; new Thread(new HoldLockThread(lockA,lockB),&quot;ThreadAAA&quot;).start(); new Thread(new HoldLockThread(lockB,lockA),&quot;ThreadBBB&quot;).start(); } } 2. 死锁与活锁与饥饿的区别 死锁：是指两个或两个以上的进程（或线程）在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。 活锁：任务或者执行者没有被阻塞，由于某些条件没有满足，导致一直重复尝试，失败，尝试，失败。 活锁和死锁的区别在于，处于活锁的实体是在不断的改变状态，这就是所谓的“活”， 而处于死锁的实体表现为等待；活锁有可能自行解开，死锁则不能。 饥饿：一个或者多个线程因为种种原因无法获得所需要的资源，导致一直无法执行的状态。 Java 中导致饥饿的原因： 1、高优先级线程吞噬所有的低优先级线程的 CPU 时间。 2、线程被永久堵塞在一个等待进入同步块的状态，因为其他线程总是能在它之前持续地对该同步块进行访问。 3、线程在等待一个本身也处于永久等待完成的对象(比如调用这个对象的 wait 方法)，因为其他线程总是被持续地获得唤醒。 ReentrantLock(重入锁) ReentrantLock重入锁，是实现Lock接口的一个类，也是在实际编程中使用频率很高的一个锁，支持重入性，表示能够对共享资源能够重复加锁，即当前线程获取该锁再次获取不会被阻塞。 在java关键字synchronized隐式支持重入性，synchronized通过获取自增，释放自减的方式实现重入。与此同时，ReentrantLock还支持公平锁和非公平锁两种方式。那么，要想完完全全的弄懂ReentrantLock的话，主要也就是ReentrantLock同步语义的学习： 重入性的实现原理； 公平锁和非公平锁 1. 重入性的实现原理 要想支持重入性，就要解决两个问题： 1. 在线程获取锁的时候，如果已经获取锁的线程是当前线程的话则直接再次获取成功； 2. 由于锁会被获取n次，那么只有锁在被释放同样的n次之后，该锁才算是完全释放成功。 ReentrantLock支持两种锁：公平锁和非公平锁。何谓公平性，是针对获取锁而言的，如果一个锁是公平的，那么锁的获取顺序就应该符合请求上的绝对时间顺序，满足FIFO。 2. ReentrantLock使用 class Bank{ /** * volatile实现 */ private int count=0; /** * 使用可重入锁 */ private Lock lock=new ReentrantLock(); public void getCount(){ System.out.println(&quot;账户余额为：&quot;+count); } /** * 同步方法实现存钱 * @param money */ public void save(int money){ lock.lock(); try { count+=money; System.out.println(System.currentTimeMillis()+&quot;存进：&quot;+money); } catch (Exception e) { // TODO Auto-generated catch block e.printStackTrace(); }finally { lock.unlock();//释放锁 } } /** * 同步代码块实现取钱 * @param money */ public void remove(int money){ if (count-money&lt;0) { System.err.println(&quot;余额不足。&quot;); return; } lock.lock(); try { count-=money; System.err.println(System.currentTimeMillis()+&quot;取出：&quot;+money); } catch (Exception e) { // TODO Auto-generated catch block e.printStackTrace(); }finally { lock.unlock(); } } } 读写锁ReentrantReadWriteLock 首先明确一下，不是说 ReentrantLock 不好，只是 ReentrantLock 某些时候有局限。如果使用 ReentrantLock，可能本身是为了防止线程 A 在写数据、线程 B 在读数据造成的数据不一致，但这样，如果线程 C 在读数据、线程 D 也在读数据，读数据是不会改变数据的，没有必要加锁，但是还是加锁了，降低了程序的性能。因为这个，才诞生了读写锁 ReadWriteLock。 ReadWriteLock 是一个读写锁接口，读写锁是用来提升并发程序性能的锁分离技术，ReentrantReadWriteLock 是 ReadWriteLock 接口的一个具体实现，实现了读写的分离，读锁是共享的，写锁是独占的，读和读之间不会互斥，读和写、写和读、写和写之间才会互斥，提升了读写的性能。 而读写锁有以下三个重要的特性： （1）公平选择性：支持非公平（默认）和公平的锁获取方式，吞吐量还是非公平优于公平。 （2）重进入：读锁和写锁都支持线程重进入。 并发容器 1. ConcurrentHashMap ConcurrentHashMap是Java中的一个线程安全且高效的HashMap实现。平时涉及高并发如果要用map结构，那第一时间想到的就是它。相对于hashmap来说，ConcurrentHashMap就是线程安全的map，其中利用了锁分段的思想提高了并发度。 那么它到底是如何实现线程安全的？ JDK 1.6版本关键要素： segment继承了ReentrantLock充当锁的角色，为每一个segment提供了线程安全的保障； segment维护了哈希散列表的若干个桶，每个桶由HashEntry构成的链表。 JDK1.8后，ConcurrentHashMap抛弃了原有的Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性。 1.1 ConcurrentHashMap 的并发度 ConcurrentHashMap 把实际 map 划分成若干部分来实现它的可扩展性和线程安全。这种划分是使用并发度获得的，它是 ConcurrentHashMap 类构造函数的一个可选参数，默认值为 16，这样在多线程情况下就能避免争用。 在 JDK8 后，它摒弃了 Segment（锁段）的概念，而是启用了一种全新的方式实现,利用 CAS 算法。同时加入了更多的辅助变量来提高并发度， 1.2 并发容器的实现 何为同步容器：可以简单地理解为通过 synchronized 来实现同步的容器，如果有多个线程调用同步容器的方法，它们将会串行执行。比如Vector，Hashtable，以及 Collections.synchronizedSet，synchronizedList 等方法返回的容器。可以通过查看 Vector，Hashtable等这些同步容器的实现代码，可以看到这些容器实现线程安全的方式就是将它们的状态封装起来，并在需要同步的方法上加上关键字 synchronized。 并发容器使用了与同步容器完全不同的加锁策略来提供更高的并发性和伸缩性，例如在 ConcurrentHashMap 中采用了一种粒度更细的加锁机制，可以称为分段锁，在这种锁机制下，允许任意数量的读线程并发地访问 map，并且执行读操作的线程和写操作的线程也可以并发的访问 map，同时允许一定数量的写操作线程并发地修改 map，所以它可以在并发环境下实现更高的吞吐量。 1.3 同步集合与并发集合区别 同步集合与并发集合都为多线程和并发提供了合适的线程安全的集合，不过并发集合的可扩展性更高。在 Java1.5 之前程序员们只有同步集合来用且在多线程并发的时候会导致争用，阻碍了系统的扩展性。Java5 介绍了并发集合像ConcurrentHashMap，不仅提供线程安全还用锁分离和内部分区等现代技术提高了可扩展性。 1.4 SynchronizedMap 和 ConcurrentHashMap 比较 SynchronizedMap 一次锁住整张表来保证线程安全，所以每次只能有一个线程来访为 map。 ConcurrentHashMap 使用分段锁来保证在多线程下的性能。 ConcurrentHashMap 中则是一次锁住一个桶。ConcurrentHashMap 默认将hash 表分为 16 个桶，诸如 get，put，remove 等常用操作只锁当前需要用到的桶。 这样，原来只能一个线程进入，现在却能同时有 16 个写线程执行，并发性能的提升是显而易见的。 另外 ConcurrentHashMap 使用了一种不同的迭代方式。在这种迭代方式中，当iterator 被创建后集合再发生改变就不再是抛出ConcurrentModificationException，取而代之的是在改变时 new 新的数据从而不影响原有的数据，iterator 完成后再将头指针替换为新的数据 ，这样 iterator线程可以使用原来老的数据，而写线程也可以并发的完成改变。 2. CopyOnWriteArrayList CopyOnWriteArrayList 是一个并发容器。有很多人称它是线程安全的，我认为这句话不严谨，缺少一个前提条件，那就是非复合场景下操作它是线程安全的。 CopyOnWriteArrayList(免锁容器)的好处之一是当多个迭代器同时遍历和修改这个列表时，不会抛出ConcurrentModificationException。在CopyOnWriteArrayList 中，写入将导致创建整个底层数组的副本，而源数组将保留在原地，使得复制的数组在被修改时，读取操作可以安全地执行。 CopyOnWriteArrayList 的使用场景 通过源码分析，我们看出它的优缺点比较明显，所以使用场景也就比较明显。就是合适读多写少的场景。 CopyOnWriteArrayList 的缺点 由于写操作的时候，需要拷贝数组，会消耗内存，如果原数组的内容比较多的情况下，可能导致 young gc 或者 full gc。 不能用于实时读的场景，像拷贝数组、新增元素都需要时间，所以调用一个 set 操作后，读取到数据可能还是旧的，虽然CopyOnWriteArrayList 能做到最终一致性,但是还是没法满足实时性要求。 由于实际使用中可能没法保证 CopyOnWriteArrayList 到底要放置多少数据，万一数据稍微有点多，每次 add/set 都要重新复制数组，这个代价实在太高昂了。在高性能的互联网应用中，这种操作分分钟引起故障。 CopyOnWriteArrayList 的设计思想 读写分离，读和写分开 最终一致性 使用另外开辟空间的思路，来解决并发冲突 3. ThreadLocal ThreadLocal 是一个本地线程副本变量工具类，在每个线程中都创建了一个 ThreadLocalMap 对象，简单说 ThreadLocal 就是一种以空间换时间的做法，每个线程可以访问自己内部 ThreadLocalMap 对象内的 value。通过这种方式，避免资源在多线程间共享。 原理：线程局部变量是局限于线程内部的变量，属于线程自身所有，不在多个线程间共享。Java提供ThreadLocal类来支持线程局部变量，是一种实现线程安全的方式。但是在管理环境下（如 web 服务器）使用线程局部变量的时候要特别小心，在这种情况下，工作线程的生命周期比任何应用变量的生命周期都要长。任何线程局部变量一旦在工作完成后没有释放，Java 应用就存在内存泄露的风险。 经典的使用场景是为每个线程分配一个 JDBC 连接 Connection。这样就可以保证每个线程的都在各自的 Connection 上进行数据库的操作，不会出现 A 线程关了 B线程正在使用的 Connection； 还有 Session 管理 等问题。 ThreadLocal 使用例子： public class TestThreadLocal { //线程本地存储变量 private static final ThreadLocal&lt;Integer&gt; THREAD_LOCAL_NUM = new ThreadLocal&lt;Integer&gt;() { @Override protected Integer initialValue() { return 0; } }; public static void main(String[] args) { for (int i = 0; i &lt;3; i++) {//启动三个线程 Thread t = new Thread() { @Override public void run() { add10ByThreadLocal(); } }; t.start(); } } /** * 线程本地存储变量加 5 */ private static void add10ByThreadLocal() { for (int i = 0; i &lt;5; i++) { Integer n = THREAD_LOCAL_NUM.get(); n += 1; THREAD_LOCAL_NUM.set(n); System.out.println(Thread.currentThread().getName() + &quot; : ThreadLocal num=&quot; + n); } } } 打印结果：启动了 3 个线程，每个线程最后都打印到 “ThreadLocal num=5”，而不是 num 一直在累加直到值等于 15 3.1什么是线程局部变量？ 线程局部变量是局限于线程内部的变量，属于线程自身所有，不在多个线程间共享。Java 提供 ThreadLocal 类来支持线程局部变量，是一种实现线程安全的方式。但是在管理环境下（如 web 服务器）使用线程局部变量的时候要特别小心，在这种情况下，工作线程的生命周期比任何应用变量的生命周期都要长。任何线程局部变量一旦在工作完成后没有释放，Java 应用就存在内存泄露的风险。 3.2 ThreadLocal内存泄漏分析与解决方案 ThreadLocal造成内存泄漏的原因？ ThreadLocalMap 中使用的 key 为 ThreadLocal 的弱引用,而 value 是强引用。所以，如果 ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候，key 会被清理掉，而 value 不会被清理掉。这样一来，ThreadLocalMap 中就会出现key为null的Entry。假如我们不做任何措施的话，value 永远无法被GC 回收，这个时候就可能会产生内存泄露。ThreadLocalMap实现中已经考虑了这种情况，在调用 set()、get()、remove() 方法的时候，会清理掉 key 为 null 的记录。使用完 ThreadLocal方法后 最好手动调用remove()方法 ThreadLocal内存泄漏解决方案？ 每次使用完ThreadLocal，都调用它的remove()方法，清除数据。 在使用线程池的情况下，没有及时清理ThreadLocal，不仅是内存泄漏的问题，更严重的是可能导致业务逻辑出现问题。所以，使用ThreadLocal就跟加锁完要解锁一样，用完就清理。 4. BlockingQueue 阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。 这两个附加的操作是：在队列为空时，获取元素的线程会等待队列变为非空。当队列满时，存储元素的线程会等待队列可用。 阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。 JDK7 提供了 7 个阻塞队列。分别是： ArrayBlockingQueue ：一个由数组结构组成的有界阻塞队列。 LinkedBlockingQueue ：一个由链表结构组成的有界阻塞队列。 PriorityBlockingQueue ：一个支持优先级排序的无界阻塞队列。 DelayQueue：一个使用优先级队列实现的无界阻塞队列。 SynchronousQueue：一个不存储元素的阻塞队列。 LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。 LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列。 Java 5 之前实现同步存取时，可以使用普通的一个集合，然后在使用线程的协作和线程同步可以实现生产者，消费者模式，主要的技术就是用好，wait,notify,notifyAll,sychronized 这些关键字。而在 java 5 之后，可以使用阻塞队列来实现，此方式大大简少了代码量，使得多线程编程更加容易，安全方面也有保障。 BlockingQueue 接口是 Queue 的子接口，它的主要用途并不是作为容器，而是作为线程同步的的工具，因此他具有一个很明显的特性，当生产者线程试图向 BlockingQueue 放入元素时，如果队列已满，则线程被阻塞，当消费者线程试图从中取出一个元素时，如果队列为空，则该线程会被阻塞，正是因为它所具有这个特性，所以在程序中多个线程交替向 BlockingQueue 中放入元素，取出元素，它可以很好的控制线程之间的通信。 阻塞队列使用最经典的场景就是 socket 客户端数据的读取和解析，读取数据的线程不断将数据放入队列，然后解析线程不断从队列取数据解析。 ","link":"https://tinaxiawuhao.github.io/post/b7Jq-wpXZ/"},{"title":"多线程基础概念三","content":"Java内存模型和线程相关 Java中垃圾回收有什么目的？什么时候进行垃圾回收？ 垃圾回收的目的是识别并且丢弃应用不再使用的对象来释放和重用资源。 如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存？ 不会，在本次垃圾回收周期中，这个对象将被标记为可回收的。也就是说并不会立即被垃圾收集器立刻回收，而是在下一次垃圾回收时对象依旧不可达才会释放其占用的内存。 finalize()方法什么时候被调用？析构函数(finalization)的目的是什么？ 1）垃圾回收器（garbage colector）决定回收某对象时，就会运行该对象的finalize()方法； finalize是Object类的一个方法，该方法在Object类中的声明protected void finalize() throws Throwable { } 在垃圾回收器执行时会调用被回收对象的finalize()方法，可以覆盖此方法来实现对其资源的回收。注意：一旦垃圾回收器准备释放对象占用的内存，将首先调用该对象的finalize()方法，并且下一次垃圾回收动作发生时，才真正回收对象占用的内存空间 2）GC本来就是内存回收了，应用还需要在finalization做什么呢？ 答案是大部分时候，什么都不用做(也就是不需要重载)。只有在某些很特殊的情况下，比如你调用了一些native的方法(一般是C写的)，可以要在finaliztion里去调用C的释放函数。 重排序与数据依赖性 为什么代码会重排序？ 在执行程序时，为了提供性能，处理器和编译器常常会对指令进行重排序，但是不能随意重排序，不是你想怎么排序就怎么排序，它需要满足以下两个条件： 在单线程环境下不能改变程序运行的结果； 存在数据依赖关系的不允许重排序 需要注意的是：重排序不会影响单线程环境的执行结果，但是会破坏多线程的执行语义。 as-if-serial规则和happens-before规则的区别 as-if-serial语义保证单线程内程序的执行结果不被改变，happens-before关系保证正确同步的多线程程序的执行结果不被改变。 as-if-serial语义给编写单线程程序的程序员创造了一个幻境：单线程程序是按程序的顺序来执行的。happens-before关系给编写正确同步的多线程程序的程序员创造了一个幻境：正确同步的多线程程序是按happens-before指定的顺序来执行的。 as-if-serial语义和happens-before这么做的目的，都是为了在不改变程序执行结果的前提下，尽可能地提高程序执行的并行度。 synchronized 在 Java 中,synchronized 关键字是用来控制线程同步的,就是在多线程的环境下,控制synchronized 代码段不被多个线程同时执行。synchronized 可以修饰类、方法、变量。 另外，在 Java 早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的 synchronized 效率低的原因。庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了。JDK1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。 synchronized关键字最主要的三种使用方式： 修饰实例方法: 作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁 修饰静态方法: 也就是给当前类加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例对象，是类成员（ static 表明这是该类的一个静态资源，不管new了多少个对象，只有一份）。所以如果一个线程A调用一个实例对象的非静态 synchronized 方法，而线程B需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。 修饰代码块: 指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。 总结： synchronized 关键字加到 static 静态方法和 synchronized(class)代码块上都是是给 Class 类上锁。synchronized 关键字加到实例方法上是给对象实例上锁。尽量不要使用 synchronized(String a) 因为JVM中，字符串常量池具有缓存功能！ 下面我以一个常见的面试题为例讲解一下 synchronized 关键字的具体使用。 面试中面试官经常会说：“单例模式了解吗？来给我手写一下！给我解释一下双重检验锁方式实现单例模式的原理呗！” 双重校验锁实现对象单例（线程安全） public class Singleton { private volatile static Singleton uniqueInstance; private Singleton() { } public static Singleton getUniqueInstance() { //先判断对象是否已经实例过，没有实例化过才进入加锁代码 if (uniqueInstance == null) { //类对象加锁 synchronized (Singleton.class) { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } } } return uniqueInstance; } } 另外，需要注意uniqueInstance 采用 volatile 关键字修饰也是很有必要。 uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行： 为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址 使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。 说一下 synchronized 底层实现原理？ synchronized是Java中的一个关键字，在使用的过程中并没有看到显示的加锁和解锁过程。因此有必要通过javap命令，查看相应的字节码文件。 synchronized 同步语句块的情况 public class SynchronizedDemo { public void method() { synchronized (this) { System.out.println(&quot;synchronized 代码块&quot;); } } } 通过JDK 反汇编指令 javap -c -v SynchronizedDemo 可以看出在执行同步代码块之前之后都有一个monitor字样，其中前面的是monitorenter，后面的是离开monitorexit，不难想象一个线程也执行同步代码块，首先要获取锁，而获取锁的过程就是monitorenter ，在执行完代码块之后，要释放锁，释放锁就是执行monitorexit指令。 为什么会有两个monitorexit呢？ 这个主要是防止在同步代码块中线程因异常退出，而锁没有得到释放，这必然会造成死锁（等待的线程永远获取不到锁）。因此最后一个monitorexit是保证在异常情况下，锁也可以得到释放，避免死锁。 仅有ACC_SYNCHRONIZED这么一个标志，该标记表明线程进入该方法时，需要monitorenter，退出该方法时需要monitorexit。 synchronized可重入的原理 重入锁是指一个线程获取到该锁之后，该线程可以继续获得该锁。底层原理维护一个计数器，当线程获取该锁时，计数器加一，再次获得该锁时继续加一，释放锁时，计数器减一，当计数器值为0时，表明该锁未被任何线程所持有，其它线程可以竞争获取锁。 什么是自旋 很多 synchronized 里面的代码只是一些很简单的代码，执行时间非常快，此时等待的线程都加锁可能是一种不太值得的操作，因为线程阻塞涉及到用户态和内核态切换的问题。既然 synchronized 里面的代码执行得非常快，不妨让等待锁的线程不要被阻塞，而是在 synchronized 的边界做忙循环，这就是自旋。如果做了多次循环发现还没有获得锁，再阻塞，这样可能是一种更好的策略。 锁状态 锁的状态总共有四种：无锁状态、偏向锁、轻量级锁和重量级锁。随着锁的竞争，锁可以从偏向锁升级到轻量级锁，再升级的重量级锁（但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级）。JDK 1.6中默认是开启偏向锁和轻量级锁的，我们也可以通过-XX:-UseBiasedLocking来禁用偏向锁。锁的状态保存在对象的头文件中，以32位的JDK为例： “轻量级”是相对于使用操作系统互斥量来实现的传统锁而言的。但是，首先需要强调一点的是，轻量级锁并不是用来代替重量级锁的，它的本意是在没有多线程竞争的前提下，减少传统的重量级锁使用产生的性能消耗。在解释轻量级锁的执行过程之前，先明白一点，轻量级锁所适应的场景是线程交替执行同步块的情况，如果存在同一时间访问同一锁的情况，就会导致轻量级锁膨胀为重量级锁。 多线程中 synchronized 锁升级的原理是什么？ synchronized 锁升级原理：在锁对象的对象头里面有一个 threadid 字段，在第一次访问的时候threadid 为空，jvm 让其持有偏向锁，并将 threadid 设置为其线程 id，再次进入的时候会先判断 threadid 是否与其线程 id 一致，如果一致则可以直接使用此对象，如果不一致，则升级偏向锁为轻量级锁，通过自旋循环一定次数来获取锁，执行一定次数之后，如果还没有正常获取到要使用的对象，此时就会把锁从轻量级升级为重量级锁，此过程就构成了 synchronized 锁的升级。 锁的升级的目的：锁升级是为了减低了锁带来的性能消耗。在 Java 6 之后优化 synchronized 的实现方式，使用了偏向锁升级为轻量级锁再升级到重量级锁的方式，从而减低了锁带来的性能消耗。 重量级锁、轻量级锁和偏向锁之间转换图 线程 B 怎么知道线程 A 修改了变量 （1）volatile 修饰变量 （2）synchronized 修饰修改变量的方法 （3）wait/notify （4）while 轮询 当一个线程进入一个对象的 synchronized 方法 A 之后，其它线程是否可进入此对象的 synchronized 方法 B？ 不能。其它线程只能访问该对象的非同步方法，同步方法则不能进入。因为非静态方法上的 synchronized 修饰符要求执行方法时要获得对象的锁，如果已经进入A 方法说明对象锁已经被取走，那么试图进入 B 方法的线程就只能在等锁池（注意不是等待池哦）中等待对象的锁。 synchronized、volatile、CAS 比较 （1）synchronized 是悲观锁，属于抢占式，会引起其他线程阻塞。 （2）volatile 提供多线程共享变量可见性和禁止指令重排序优化。 （3）CAS 是基于冲突检测的乐观锁（非阻塞） synchronized 和 Lock 有什么区别？ 首先synchronized是Java内置关键字，在JVM层面，Lock是个Java类； synchronized 可以给类、方法、代码块加锁；而 lock 只能给代码块加锁。 synchronized 不需要手动获取锁和释放锁，使用简单，发生异常会自动释放锁，不会造成死锁；而 lock 需要自己加锁和释放锁，如果使用不当没有 unLock()去释放锁就会造成死锁。 通过 Lock 可以知道有没有成功获取锁，而 synchronized 却无法办到。 synchronized 和 ReentrantLock 区别是什么？ synchronized 是和 if、else、for、while 一样的关键字，ReentrantLock 是类，这是二者的本质区别。既然 ReentrantLock 是类，那么它就提供了比synchronized 更多更灵活的特性，可以被继承、可以有方法、可以有各种各样的类变量 synchronized 早期的实现比较低效，对比 ReentrantLock，大多数场景性能都相差较大，但是在 Java 6 中对 synchronized 进行了非常多的改进。 相同点：两者都是可重入锁 两者都是可重入锁。“可重入锁”概念是：自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果不可锁重入的话，就会造成死锁。同一个线程每次获取锁，锁的计数器都自增1，所以要等到锁的计数器下降为0时才能释放锁。 主要区别如下： ReentrantLock 使用起来比较灵活，但是必须有释放锁的配合动作； ReentrantLock 必须手动获取与释放锁，而 synchronized 不需要手动释放和开启锁； ReentrantLock 只适用于代码块锁，而 synchronized 可以修饰类、方法、变量等。 二者的锁机制其实也是不一样的。ReentrantLock 底层调用的是 Unsafe 的park 方法加锁，synchronized 操作的应该是对象头中 mark word Java中每一个对象都可以作为锁，这是synchronized实现同步的基础： 普通同步方法，锁是当前实例对象 静态同步方法，锁是当前类的class对象 同步方法块，锁是括号里面的对象 尽可能去使用synchronized而不要去使用LOCK 什么概念呢？我和大家打个比方：你叫jdk，你生了一个孩子叫synchronized，后来呢，你领养了一个孩子叫LOCK。起初，LOCK刚来到新家的时候，它很乖，很懂事，各个方面都表现的比synchronized好。你很开心，但是你内心深处又有一点淡淡的忧伤，你不希望你自己亲生的孩子竟然还不如一个领养的孩子乖巧。这个时候，你对亲生的孩子教育更加深刻了，你想证明，你的亲生孩子synchronized并不会比领养的孩子LOCK差。 那如何教育呢？ 在jdk1.6~jdk1.7的时候，你作为爸爸，你给他优化了，具体优化在哪里呢： 线程自旋和适应性自旋 我们知道，java’线程其实是映射在内核之上的，线程的挂起和恢复会极大的影响开销。并且jdk官方人员发现，很多线程在等待锁的时候，在很短的一段时间就获得了锁，所以它们在线程等待的时候，并不需要把线程挂起，而是让他无目的的循环，一般设置10次。这样就避免了线程切换的开销，极大的提升了性能。 而适应性自旋，是赋予了自旋一种学习能力，它并不固定自旋10次一下。他可以根据它前面线程的自旋情况，从而调整它的自旋，甚至是不经过自旋而直接挂起。 锁消除 什么叫锁消除呢？就是把不必要的同步在编译阶段进行移除。 那么有的小伙伴又迷糊了，我自己写的代码我会不知道这里要不要加锁？我加了锁就是表示这边会有同步呀？ 并不是这样，这里所说的锁消除并不一定指代是你写的代码的锁消除，我打一个比方： 在jdk1.5以前，我们的String字符串拼接操作其实底层是StringBuffer来实现的（写一个简单的demo，然后查看class文件中的字节码指令就清楚了），而在jdk1.5之后，那么是用StringBuilder来拼接的。我们考虑前面的情况，比如如下代码： String str1=&quot;qwe&quot;; String str2=&quot;asd&quot;; String str3=str1+str2; 底层实现会变成这样： StringBuffer sb = new StringBuffer(); sb.append(&quot;qwe&quot;); sb.append(&quot;asd&quot;); 我们知道，StringBuffer是一个线程安全的类，也就是说两个append方法都会同步，通过指针逃逸分析（就是变量不会外泄），我们发现在这段代码并不存在线程安全问题，这个时候就会把这个同步锁消除。 锁粗化 在用synchronized的时候，我们都讲究为了避免大开销，尽量同步代码块要小。那么为什么还要加粗呢？ 我们继续以上面的字符串拼接为例，我们知道在这一段代码中，每一个append都需要同步一次，那么我可以把锁粗化到第一个append和最后一个append（这里不要去纠结前面的锁消除，我只是打个比方） 轻量级锁 轻量级锁的加锁过程 （1）在代码进入同步块的时候，如果同步对象锁状态为无锁状态（锁标志位为“01”状态，是否为偏向锁为“0”），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝，官方称之为 Displaced Mark Word。这时候线程堆栈与对象头的状态如图2.1所示。 （2）拷贝对象头中的Mark Word复制到锁记录中。 （3）拷贝成功后，虚拟机将使用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针，并将Lock record里的owner指针指向object mark word。如果更新成功，则执行步骤（3），否则执行步骤（4）。 （4）如果这个更新动作成功了，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位设置为“00”，即表示此对象处于轻量级锁定状态，这时候线程堆栈与对象头的状态如图2.2所示。 （5）如果这个更新操作失败了，虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧，如果是就说明当前线程已经拥有了这个对象的锁，那就可以直接进入同步块继续执行。否则说明多个线程竞争锁，轻量级锁就要膨胀为重量级锁，锁标志的状态值变为“10”，Mark Word中存储的就是指向重量级锁（互斥量）的指针，后面等待锁的线程也要进入阻塞状态。 而当前线程便尝试使用自旋来获取锁，自旋就是为了不让线程阻塞，而采用循环去获取锁的过程。 ​ 图2.1 轻量级锁CAS操作之前堆栈与对象的状态 ​ 图2.2 轻量级锁CAS操作之后堆栈与对象的状态 轻量级锁的解锁过程： （1）通过CAS操作尝试把线程中复制的Displaced Mark Word对象替换当前的Mark Word。 （2）如果替换成功，整个同步过程就完成了。 （3）如果替换失败，说明有其他线程尝试过获取该锁（此时锁已膨胀），那就要在释放锁的同时，唤醒被挂起的线程。 偏向锁 引入偏向锁是为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径，因为轻量级锁的获取及释放依赖多次CAS原子指令，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令（由于一旦出现多线程竞争的情况就必须撤销偏向锁，所以偏向锁的撤销操作的性能损耗必须小于节省下来的CAS原子指令的性能消耗）。上面说过，轻量级锁是为了在线程交替执行同步块时提高性能，而偏向锁则是在只有一个线程执行同步块时进一步提高性能。 1、偏向锁获取过程： （1）访问Mark Word中偏向锁的标识是否设置成1，锁标志位是否为01——确认为可偏向状态。 （2）如果为可偏向状态，则测试线程ID是否指向当前线程，如果是，进入步骤（5），否则进入步骤（3）。 （3）如果线程ID并未指向当前线程，则通过CAS操作竞争锁。如果竞争成功，则将Mark Word中线程ID设置为当前线程ID，然后执行（5）；如果竞争失败，执行（4）。 （4）如果CAS获取偏向锁失败，则表示有竞争。当到达全局安全点（safepoint）时获得偏向锁的线程被挂起，偏向锁升级为轻量级锁，然后被阻塞在安全点的线程继续往下执行同步代码。 （5）执行同步代码。 2、偏向锁的释放： 偏向锁的撤销在上述第四步骤中有提到**。**偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动去释放偏向锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，判断锁对象是否处于被锁定状态，撤销偏向锁后恢复到未锁定（标志位为“01”）或轻量级锁（标志位为“00”）的状态。 volatile 对于可见性，Java 提供了 volatile 关键字来保证可见性和禁止指令重排。 volatile 提供 happens-before 的保证，确保一个线程的修改能对其他线程是可见的。当一个共享变量被 volatile 修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。 从实践角度而言，volatile 的一个重要作用就是和 CAS 结合，保证了原子性，详细的可以参见 java.util.concurrent.atomic 包下的类，比如 AtomicInteger。 volatile 常用于多线程环境下的单次操作(单次读或者单次写)。 Java 中能创建 volatile 数组吗？ 能，Java 中可以创建 volatile 类型数组，不过只是一个指向数组的引用，而不是整个数组。意思是，如果改变引用指向的数组，将会受到 volatile 的保护，但是如果多个线程同时改变数组的元素，volatile 标示符就不能起到之前的保护作用了。 volatile 变量和 atomic 变量有什么不同？ volatile 变量可以确保先行关系，即写操作会发生在后续的读操作之前, 但它并不能保证原子性。例如用 volatile 修饰 count 变量，那么 count++ 操作就不是原子性的。 而 AtomicInteger 类提供的 atomic 方法可以让这种操作具有原子性如getAndIncrement()方法会原子性的进行增量操作把当前值加一，其它数据类型和引用变量也可以进行相似操作。 volatile 能使得一个非原子操作变成原子操作吗？ 关键字volatile的主要作用是使变量在多个线程间可见，但无法保证原子性，对于多个线程访问同一个实例变量需要加锁进行同步。 虽然volatile只能保证可见性不能保证原子性，但用volatile修饰long和double可以保证其操作原子性。 所以从Oracle Java Spec里面可以看到： 对于64位的long和double，如果没有被volatile修饰，那么对其操作可以不是原子的。在操作的时候，可以分成两步，每次对32位操作。 如果使用volatile修饰long和double，那么其读写都是原子操作 对于64位的引用地址的读写，都是原子操作 在实现JVM时，可以自由选择是否把读写long和double作为原子操作 推荐JVM实现为原子操作 volatile 修饰符的有过什么实践？ 单例模式 是否 Lazy 初始化：是 是否多线程安全：是 实现难度：较复杂 描述：对于Double-Check这种可能出现的问题（当然这种概率已经非常小了，但毕竟还是有的嘛~），解决方案是：只需要给instance的声明加上volatile关键字即可volatile关键字的一个作用是禁止指令重排，把instance声明为volatile之后，对它的写操作就会有一个内存屏障（什么是内存屏障？），这样，在它的赋值完成之前，就不用会调用读操作。注意：volatile阻止的不是singleton = newSingleton()这句话内部[1-2-3]的指令重排，而是保证了在一个写操作（[1-2-3]）完成之前，不会调用读操作（if (instance == null)）。 public class Singleton7 { private static volatile Singleton7 instance = null; private Singleton7() {} public static Singleton7 getInstance() { if (instance == null) { synchronized (Singleton7.class) { if (instance == null) { instance = new Singleton7(); } } } return instance; } } synchronized 和 volatile 的区别是什么？ synchronized 表示只有一个线程可以获取作用对象的锁，执行代码，阻塞其他线程。 volatile 表示变量在 CPU 的寄存器中是不确定的，必须从主存中读取。保证多线程环境下变量的可见性；禁止指令重排序。 区别 volatile 是变量修饰符；synchronized 可以修饰类、方法、变量。 volatile 仅能实现变量的修改可见性，不能保证原子性；而 synchronized 则可以保证变量的修改可见性和原子性。 volatile 不会造成线程的阻塞；synchronized 可能会造成线程的阻塞。 volatile标记的变量不会被编译器优化；synchronized标记的变量可以被编译器优化。 volatile关键字是线程同步的轻量级实现，所以volatile性能肯定比synchronized关键字要好。但是volatile关键字只能用于变量而synchronized关键字可以修饰方法以及代码块。synchronized关键字在JavaSE1.6之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的偏向锁和轻量级锁以及其它各种优化之后执行效率有了显著提升，实际开发中使用 synchronized 关键字的场景还是更多一些。 类别 synchronized Lock 存在层次 Java的关键字，在jvm层面上 是一个类 锁的释放 1、已获取锁的线程执行完同步代码，释放锁 2、线程执行发生异常，jvm会让线程释放锁 在finally中必须释放锁，不然容易造成线程死锁 锁的获取 假设A线程获得锁，B线程等待。如果A线程阻塞，B线程会一直等待 分情况而定，Lock有多个锁获取的方式，具体下面会说道，大致就是可以尝试获得锁，线程可以不用一直等待 锁状态 无法判断 可以判断 锁类型 可重入 不可中断 非公平 可重入 可判断 可公平（两者皆可） 性能 少量同步 大量同步 final 不可变对象(Immutable Objects)即对象一旦被创建它的状态（对象的数据，也即对象属性值）就不能改变，反之即为可变对象(Mutable Objects)。 不可变对象的类即为不可变类(Immutable Class)。Java 平台类库中包含许多不可变类，如 String、基本类型的包装类、BigInteger 和 BigDecimal 等。 只有满足如下状态，一个对象才是不可变的； 它的状态不能在创建后再被修改； 所有域都是 final 类型；并且，它被正确创建（创建期间没有发生 this 引用的逸出）。 不可变对象保证了对象的内存可见性，对不可变对象的读取不需要进行额外的同步手段，提升了代码执行效率。 ","link":"https://tinaxiawuhao.github.io/post/c-lkdzfwM/"},{"title":"多线程基础概念二","content":"线程的状态和基本操作 线程的生命周期及五种基本状态 新建(new)：新创建了一个线程对象。 可运行(runnable)：线程对象创建后，当调用线程对象的 start()方法，该线程处于就绪状态，等待被线程调度选中，获取cpu的使用权。 运行(running)：可运行状态(runnable)的线程获得了cpu时间片（timeslice），执行程序代码。注：就绪状态是进入到运行状态的唯一入口，也就是说，线程要想进入运行状态执行，首先必须处于就绪状态中； 阻塞(block)：处于运行状态中的线程由于某种原因，暂时放弃对 CPU的使用权，停止执行，此时进入阻塞状态，直到其进入到就绪状态，才 有机会再次被 CPU 调用以进入到运行状态。 阻塞的情况分三种： (一). 等待阻塞：运行状态中的线程执行 wait()方法，JVM会把该线程放入等待队列(waitting queue)中，使本线程进入到等待阻塞状态； (二). 同步阻塞：线程在获取 synchronized 同步锁失败(因为锁被其它线程所占用)，，则JVM会把该线程放入锁池(lock pool)中，线程会进入同步阻塞状态； (三). 其他阻塞: 通过调用线程的 sleep()或 join()或发出了 I/O 请求时，线程会进入到阻塞状态。当 sleep()状态超时、join()等待线程终止或者超时、或者 I/O 处理完毕时，线程重新转入就绪状态。 死亡(dead)：线程run()、main()方法执行结束，或者因异常退出了run()方法，则该线程结束生命周期。死亡的线程不可再次复生。 Java 中用到的线程调度算法是什么？ 计算机通常只有一个 CPU，在任意时刻只能执行一条机器指令，每个线程只有获得CPU 的使用权才能执行指令。所谓多线程的并发运行，其实是指从宏观上看，各个线程轮流获得 CPU 的使用权，分别执行各自的任务。在运行池中，会有多个处于就绪状态的线程在等待 CPU，JAVA 虚拟机的一项任务就是负责线程的调度，线程调度是指按照特定机制为多个线程分配 CPU 的使用权。 有两种调度模型：分时调度模型和抢占式调度模型。 分时调度模型是指让所有的线程轮流获得 cpu 的使用权，并且平均分配每个线程占用的 CPU 的时间片这个也比较好理解。 Java虚拟机采用抢占式调度模型，是指优先让可运行池中优先级高的线程占用CPU，如果可运行池中的线程优先级相同，那么就随机选择一个线程，使其占用CPU。处于运行状态的线程会一直运行，直至它不得不放弃 CPU。 线程的调度策略 线程调度器选择优先级最高的线程运行，但是，如果发生以下情况，就会终止线程的运行： （1）线程体中调用了 yield 方法让出了对 cpu 的占用权利 （2）线程体中调用了 sleep 方法使线程进入睡眠状态 （3）线程由于 IO 操作受到阻塞 （4）另外一个更高优先级线程出现 （5）在支持时间片的系统中，该线程的时间片用完 什么是线程调度器(Thread Scheduler)和时间分片(Time Slicing )？ 线程调度器是一个操作系统服务，它负责为 Runnable 状态的线程分配 CPU 时间。一旦我们创建一个线程并启动它，它的执行便依赖于线程调度器的实现。 时间分片是指将可用的 CPU 时间分配给可用的 Runnable 线程的过程。分配 CPU 时间可以基于线程优先级或者线程等待的时间。 线程调度并不受到 Java 虚拟机控制，所以由应用程序来控制它是更好的选择（也就是说不要让你的程序依赖于线程的优先级）。 请说出与线程同步以及线程调度相关的方法。 （1） wait()：使一个线程处于等待（阻塞）状态，并且释放所持有的对象的锁； （2）sleep()：使一个正在运行的线程处于睡眠状态，是一个静态方法，调用此方法要处理 InterruptedException 异常； （3）notify()：唤醒一个处于等待状态的线程，当然在调用此方法的时候，并不能确切的唤醒某一个等待状态的线程，而是由 JVM 确定唤醒哪个线程，而且与优先级无关； （4）notityAll()：唤醒所有处于等待状态的线程，该方法并不是将对象的锁给所有线程，而是让它们竞争，只有获得锁的线程才能进入就绪状态； sleep() 和 wait() 有什么区别？ 两者都可以暂停线程的执行 类的不同：sleep() 是 Thread线程类的静态方法，wait() 是 Object类的方法。 是否释放锁：sleep() 不释放锁；wait() 释放锁。 用途不同：Wait 通常被用于线程间交互/通信，sleep 通常被用于暂停执行。 用法不同：wait() 方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 notify() 或者 notifyAll() 方法。sleep() 方法执行完成后，线程会自动苏醒。或者可以使用wait(long timeout)超时后线程会自动苏醒。 你是如何调用 wait() 方法的？使用 if 块还是循环？为什么？ 处于等待状态的线程可能会收到错误警报和伪唤醒，如果不在循环中检查等待条件，程序就会在没有满足结束条件的情况下退出。 wait() 方法应该在循环调用，因为当线程获取到 CPU 开始执行的时候，其他条件可能还没有满足，所以在处理前，循环检测条件是否满足会更好。下面是一段标准的使用 wait 和 notify 方法的代码： synchronized (monitor) { // 判断条件谓词是否得到满足 while(!locked) { // 等待唤醒 monitor.wait(); } // 处理其他的业务逻辑 } 为什么线程通信的方法 wait(), notify()和 notifyAll()被定义在 Object 类里？ Java中，任何对象都可以作为锁，并且 wait()，notify()等方法用于等待对象的锁或者唤醒线程，在 Java 的线程中并没有可供任何对象使用的锁，所以任意对象调用方法一定要定义在Object类中。 wait(), notify()和 notifyAll()这些方法在同步代码块中调用 有的人会说，既然是线程放弃对象锁，那也可以把wait()定义在Thread类里面啊，新定义的线程继承于Thread类，也不需要重新定义wait()方法的实现。然而，这样做有一个非常大的问题，一个线程完全可以持有很多锁，你一个线程放弃锁的时候，到底要放弃哪个锁？当然了，这种设计并不是不能实现，只是管理起来更加复杂。 综上所述，wait()、notify()和notifyAll()方法要定义在Object类中。 为什么 wait(), notify()和 notifyAll()必须在同步方法或者同步块中被调用？ 当一个线程需要调用对象的 wait()方法的时候，这个线程必须拥有该对象的锁，接着它就会释放这个对象锁并进入等待状态直到其他线程调用这个对象上的 notify()方法。同样的，当一个线程需要调用对象的 notify()方法时，它会释放这个对象的锁，以便其他在等待的线程就可以得到这个对象锁。由于所有的这些方法都需要线程持有对象的锁，这样就只能通过同步来实现，所以他们只能在同步方法或者同步块中被调用。 Thread 类中的 yield 方法有什么作用？ 使当前线程从执行状态（运行状态）变为可执行态（就绪状态）。 当前线程到了就绪状态，那么接下来哪个线程会从就绪状态变成执行状态呢？可能是当前线程，也可能是其他线程，看系统的分配了。 为什么 Thread 类的 sleep()和 yield ()方法是静态的？ Thread 类的 sleep()和 yield()方法将在当前正在执行的线程上运行。所以在其他处于等待状态的线程上调用这些方法是没有意义的。这就是为什么这些方法是静态的。它们可以在当前正在执行的线程中工作，并避免程序员错误的认为可以在其他非运行线程调用这些方法。 线程的 sleep()方法和 yield()方法有什么区别？ （1） sleep()方法给其他线程运行机会时不考虑线程的优先级，因此会给低优先级的线程以运行的机会；yield()方法只会给相同优先级或更高优先级的线程以运行的机会； （2） 线程执行 sleep()方法后转入阻塞（blocked）状态，而执行 yield()方法后转入就绪（ready）状态； （3）sleep()方法声明抛出 InterruptedException，而 yield()方法没有声明任何异常； （4）sleep()方法比 yield()方法（跟操作系统 CPU 调度相关）具有更好的可移植性，通常不建议使用yield()方法来控制并发线程的执行。 如何停止一个正在运行的线程？ 在java中有以下3种方法可以终止正在运行的线程： 使用退出标志，使线程正常退出，也就是当run方法完成后线程终止。 使用stop方法强行终止，但是不推荐这个方法，因为stop和suspend及resume一样都是过期作废的方法。 使用interrupt方法中断线程。 Java 中 interrupted 和 isInterrupted 方法的区别？ interrupt：用于中断线程。调用该方法的线程的状态为将被置为”中断”状态。 注意：线程中断仅仅是置线程的中断状态位，不会停止线程。需要用户自己去监视线程的状态为并做处理。支持线程中断的方法（也就是线程中断后会抛出interruptedException 的方法）就是在监视线程的中断状态，一旦线程的中断状态被置为“中断状态”，就会抛出中断异常。 interrupted：是静态方法，查看当前中断信号是true还是false并且清除中断信号。如果一个线程被中断了，第一次调用 interrupted 则返回 true，第二次和后面的就返回 false 了。 isInterrupted：查看当前中断信号是true还是false 什么是阻塞式方法？ 阻塞式方法是指程序会一直等待该方法完成期间不做其他事情，ServerSocket 的accept()方法就是一直等待客户端连接。这里的阻塞是指调用结果返回之前，当前线程会被挂起，直到得到结果之后才会返回。此外，还有异步和非阻塞式方法在任务完成前就返回。 Java 中你怎样唤醒一个阻塞的线程？ 首先 ，wait()、notify() 方法是针对对象的，调用任意对象的 wait()方法都将导致线程阻塞，阻塞的同时也将释放该对象的锁，相应地，调用任意对象的 notify()方法则将随机解除该对象阻塞的线程，但它需要重新获取该对象的锁，直到获取成功才能往下执行； 其次，wait、notify 方法必须在 synchronized 块或方法中被调用，并且要保证同步块或方法的锁对象与调用 wait、notify 方法的对象是同一个，如此一来在调用 wait 之前当前线程就已经成功获取某对象的锁，执行 wait 阻塞后当前线程就将之前获取的对象锁释放。 notify() 和 notifyAll() 有什么区别？ 如果线程调用了对象的 wait()方法，那么线程便会处于该对象的等待池中，等待池中的线程不会去竞争该对象的锁。 notifyAll() 会唤醒所有的线程，notify() 只会唤醒一个线程。 notifyAll() 调用后，会将全部线程由等待池移到锁池，然后参与锁的竞争，竞争成功则继续执行，如果不成功则留在锁池等待锁被释放后再次参与竞争。而 notify()只会唤醒一个线程，具体唤醒哪一个线程由虚拟机控制。 如何在两个线程间共享数据？ 在两个线程间共享变量即可实现共享。 一般来说，共享变量要求变量本身是线程安全的，然后在线程内使用的时候，如果有对共享变量的复合操作，那么也得保证复合操作的线程安全性。 可以通过中断 和 共享变量的方式实现线程间的通讯和协作 比如说最经典的生产者-消费者模型：当队列满时，生产者需要等待队列有空间才能继续往里面放入商品，而在等待的期间内，生产者必须释放对临界资源（即队列）的占用权。因为生产者如果不释放对临界资源的占用权，那么消费者就无法消费队列中的商品，就不会让队列有空间，那么生产者就会一直无限等待下去。因此，一般情况下，当队列满时，会让生产者交出对临界资源的占用权，并进入挂起状态。然后等待消费者消费了商品，然后消费者通知生产者队列有空间了。同样地，当队列空时，消费者也必须等待，等待生产者通知它队列中有商品了。这种互相通信的过程就是线程间的协作。 Java中线程通信协作的最常见的两种方式： syncrhoized加锁的线程的Object类的wait()/notify()/notifyAll() ReentrantLock类加锁的线程的Condition类的await()/signal()/signalAll() 线程间直接的数据交换： 通过管道进行线程间通信：1）字节流；2）字符流 同步方法和同步块，哪个是更好的选择？ Lock lock = new ReentrantLock(); lock. lock(); try { System. out. println(&quot;获得锁&quot;); } catch (Exception e) { // TODO: handle exception } finally { System. out. println(&quot;释放锁&quot;); lock. unlock(); } 同步块更要符合开放调用的原则，只在需要锁住的代码块锁住相应的对象，这样从侧面来说也可以避免死锁。 请知道一条原则：同步的范围越小越好。 什么是线程同步和线程互斥，有哪几种实现方式？ 当一个线程对共享的数据进行操作时，应使之成为一个&quot;原子操作&quot;，即在没有完成相关操作之前，不允许其他线程打断它，否则，就会破坏数据的完整性，必然会得到错误的处理结果，这就是线程的同步。 在多线程应用中，考虑不同线程之间的数据同步和防止死锁。当两个或多个线程之间同时等待对方释放资源的时候就会形成线程之间的死锁。为了防止死锁的发生，需要通过同步来实现线程安全。 线程互斥是指对于共享的进程系统资源，在各单个线程访问时的排它性。当有若干个线程都要使用某一共享资源时，任何时刻最多只允许一个线程去使用，其它要使用该资源的线程必须等待，直到占用资源者释放该资源。线程互斥可以看成是一种特殊的线程同步。 线程间的同步方法大体可分为两类：用户模式和内核模式。顾名思义，内核模式就是指利用系统内核对象的单一性来进行同步，使用时需要切换内核态与用户态，而用户模式就是不需要切换到内核态，只在用户态完成操作。 用户模式下的方法有：原子操作（例如一个单一的全局变量），临界区。 内核模式下的方法有：事件，信号量，互斥量。 实现线程同步的方法 同步代码方法：sychronized 关键字修饰的方法 同步代码块：sychronized 关键字修饰的代码块 使用特殊变量域volatile实现线程同步：volatile关键字为域变量的访问提供了一种免锁机制 使用重入锁实现线程同步：reentrantlock类是可重入、互斥、实现了lock接口的锁，它与sychronized方法具有相同的基本行为和语义 在监视器(Monitor)内部，是如何做线程同步的？程序应该做哪种级别的同步？ 在 java 虚拟机中，每个对象( Object 和 class )通过某种逻辑关联监视器,每个监视器和一个对象引用相关联，为了实现监视器的互斥功能，每个对象都关联着一把锁。 一旦方法或者代码块被 synchronized 修饰，那么这个部分就放入了监视器的监视区域，确保一次只能有一个线程执行该部分的代码，线程在获取锁之前不允许执行该部分的代码 另外 java 还提供了显式监视器( Lock )和隐式监视器( synchronized )两种锁方案 如果你提交任务时，线程池队列已满，这时会发生什么 这里区分一下： （1）如果使用的是无界队列 LinkedBlockingQueue，也就是无界队列的话，没关系，继续添加任务到阻塞队列中等待执行，因为 LinkedBlockingQueue 可以近乎认为是一个无穷大的队列，可以无限存放任务 （2）如果使用的是有界队列比如 ArrayBlockingQueue，任务首先会被添加到ArrayBlockingQueue 中，ArrayBlockingQueue 满了，会根据maximumPoolSize 的值增加线程数量，如果增加了线程数量还是处理不过来，ArrayBlockingQueue 继续满，那么则会使用拒绝策略RejectedExecutionHandler 处理满了的任务，默认是 AbortPolicy 什么叫线程安全？servlet 是线程安全吗? 线程安全是编程中的术语，指某个方法在多线程环境中被调用时，能够正确地处理多个线程之间的共享变量，使程序功能正确完成。 Servlet 不是线程安全的，servlet 是单实例多线程的，当多个线程同时访问同一个方法，是不能保证共享变量的线程安全性的。 Struts2 的 action 是多实例多线程的，是线程安全的，每个请求过来都会 new 一个新的 action 分配给这个请求，请求完成后销毁。 SpringMVC 的 Controller 是线程安全的吗？不是的，和 Servlet 类似的处理流程。 Struts2 好处是不用考虑线程安全问题；Servlet 和 SpringMVC 需要考虑线程安全问题，但是性能可以提升不用处理太多的 gc，可以使用 ThreadLocal 来处理多线程的问题。 在 Java 程序中怎么保证多线程的运行安全？ 方法一：使用安全类，比如 java.util.concurrent 下的类，使用原子类AtomicInteger 方法二：使用自动锁 synchronized。 方法三：使用手动锁 Lock。 手动锁 Java 示例代码如下： Lock lock = new ReentrantLock(); lock. lock(); try { System. out. println(&quot;获得锁&quot;); } catch (Exception e) { // TODO: handle exception } finally { System. out. println(&quot;释放锁&quot;); lock. unlock(); } 你对线程优先级的理解是什么？ 每一个线程都是有优先级的，一般来说，高优先级的线程在运行时会具有优先权，但这依赖于线程调度的实现，这个实现是和操作系统相关的(OS dependent)。我们可以定义线程的优先级，但是这并不能保证高优先级的线程会在低优先级的线程前执行。线程优先级是一个 int 变量(从 1-10)，1 代表最低优先级，10 代表最高优先级。 Java 的线程优先级调度会委托给操作系统去处理，所以与具体的操作系统优先级有关，如非特别需要，一般无需设置线程优先级。 线程类的构造方法、静态块是被哪个线程调用的 这是一个非常刁钻和狡猾的问题。请记住：线程类的构造方法、静态块是被 new这个线程类所在的线程所调用的，而 run 方法里面的代码才是被线程自身所调用的。 如果说上面的说法让你感到困惑，那么我举个例子，假设 Thread2 中 new 了Thread1，main 函数中 new 了 Thread2，那么： （1）Thread2 的构造方法、静态块是 main 线程调用的，Thread2 的 run()方法是Thread2 自己调用的 （2）Thread1 的构造方法、静态块是 Thread2 调用的，Thread1 的 run()方法是Thread1 自己调用的 一个线程运行时发生异常会怎样？ 如果异常没有被捕获该线程将会停止执行。Thread.UncaughtExceptionHandler是用于处理未捕获异常造成线程突然中断情况的一个内嵌接口。当一个未捕获异常将造成线程中断的时候，JVM 会使用 Thread.getUncaughtExceptionHandler()来查询线程的 UncaughtExceptionHandler并将线程和异常作为参数传递给 handler 的 uncaughtException()方法进行处理。 Java 线程数过多会造成什么异常？ 线程的生命周期开销非常高 消耗过多的 CPU 资源如果可运行的线程数量多于可用处理器的数量，那么有线程将会被闲置。大量空闲的线程会占用许多内存，给垃圾回收器带来压力，而且大量的线程在竞争 CPU资源时还将产生其他性能的开销。 降低JVM稳定性 在可创建线程的数量上存在一个限制，这个限制值将随着平台的不同而不同，并且承受着多个因素制约，包括 JVM 的启动参数、Thread 构造函数中请求栈的大小，以及底层操作系统对线程的限制等。如果破坏了这些限制，那么可能抛出OutOfMemoryError 异常。 ","link":"https://tinaxiawuhao.github.io/post/h9DSVN_A0/"},{"title":" 多线程基础概念","content":"什么是线程 是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。 线程安全的问题 并发编程三要素是什么？在 Java 程序中怎么保证多线程的运行安全？ 并发编程三要素（线程的安全性问题体现在）： 原子性：原子，即一个不可再被分割的颗粒。原子性指的是一个或多个操作要么全部执行成功要么全部执行失败。(synchronized,lock,unlock) 可见性：一个线程对共享变量的修改,另一个线程能够立刻看到。（synchronized,volatile,final） 有序性：程序执行的顺序按照代码的先后顺序执行。（处理器可能会对指令进行重排序）（synchronized,volatile） 出现线程安全问题的原因： 线程切换带来的原子性问题 缓存导致的可见性问题 编译优化带来的有序性问题 解决办法： JDK Atomic开头的原子类、synchronized、LOCK，可以解决原子性问题 synchronized、volatile、LOCK，可以解决可见性问题 Happens-Before 规则可以解决有序性问题 并发与并行 并发：多个任务在同一个 CPU 核上，按细分的时间片轮流(交替)执行，从逻辑上来看那些任务是同时执行。 并行：单位时间内，多个处理器或多核处理器同时处理多个任务，是真正意义上的“同时进行”。 串行：有n个任务，由一个线程按顺序执行。由于任务、方法都在一个线程执行所以不存在线程不安全情况，也就不存在临界区的问题。 并发编程（多线程）的优点 早期的CPU是单核的，为了提升计算能力，将多个计算单元整合到一起。形成了多核CPU。多线程就是为了将多核CPU发挥到极致，一边提高性能。 方便进行业务拆分，提升系统并发能力和性能：在特殊的业务场景下，先天的就适合于并发编程。现在的系统动不动就要求百万级甚至千万级的并发量，而多线程并发编程正是开发高并发系统的基础，利用好多线程机制可以大大提高系统整体的并发能力以及性能。面对复杂业务模型，并行程序会比串行程序更适应业务需求，而并发编程更能吻合这种业务拆分 并发编程（多线程）的缺点 上面说了多线程的优点是：为了提高计算性能。那么一定会提高？答案是不一定的。有时候多线程不一定比单线程计算快 多线程会带来额外的开销和复杂的问题 上下文切换 死锁 内存泄漏 上下文切换 时间片是CPU分配给各个线程的时间，因为时间非常短，所以CPU不断通过切换线程，让我们觉得多个线程是同时执行的，时间片一般是几十毫秒。而每次切换时，需要保存当前的状态起来，以便能够进行恢复先前状态，而这个切换时非常损耗性能，过于频繁反而无法发挥出多线程编程的优势。 减少上下文切换可以采用无锁并发编程，CAS算法，使用最少的线程和使用协程。 无锁并发编程：可以参照concurrentHashMap锁分段的思想，不同的线程处理不同段的数据，这样在多线程竞争的条件下，可以减少上下文切换的时间。 CAS算法，利用Atomic下使用CAS算法来更新数据，使用了乐观锁，可以有效的减少一部分不必要的锁竞争带来的上下文切换 使用最少线程：避免创建不需要的线程，比如任务很少，但是创建了很多的线程，这样会造成大量的线程都处于等待状态 协程：在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换 线程死锁 死锁是指两个或两个以上的进程（线程）在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程（线程）称为死锁进程（线程）。 多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。由于线程被无限期地阻塞，因此程序不可能正常终止。 如下图所示，线程 A 持有资源 2，线程 B 持有资源 1，他们同时都想申请对方的资源，所以这两个线程就会互相等待而进入死锁状态。 下面通过一个例子来说明线程死锁： public class DeadLockDemo { private static Object resource1 = new Object();//资源 1 private static Object resource2 = new Object();//资源 2 public static void main(String[] args) { new Thread(() -&gt; { synchronized (resource1) { System.out.println(Thread.currentThread() + &quot;get resource1&quot;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + &quot;waiting get resource2&quot;); synchronized (resource2) { System.out.println(Thread.currentThread() + &quot;get resource2&quot;); } } }, &quot;线程 1&quot;).start(); new Thread(() -&gt; { synchronized (resource2) { System.out.println(Thread.currentThread() + &quot;get resource2&quot;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + &quot;waiting get resource1&quot;); synchronized (resource1) { System.out.println(Thread.currentThread() + &quot;get resource1&quot;); } } }, &quot;线程 2&quot;).start(); } } 输出结果 Thread[线程 1,5,main]get resource1 Thread[线程 2,5,main]get resource2 Thread[线程 1,5,main]waiting get resource2 Thread[线程 2,5,main]waiting get resource1 线程 A 通过 synchronized (resource1) 获得 resource1 的监视器锁，然后通过Thread.sleep(1000)；让线程 A 休眠 1s 为的是让线程 B 得到CPU执行权，然后获取到 resource2 的监视器锁。线程 A 和线程 B 休眠结束了都开始企图请求获取对方的资源，然后这两个线程就会陷入互相等待的状态，这也就产生了死锁。上面的例子符合产生死锁的四个必要条件。 形成死锁的四个必要条件是什么 互斥条件：线程(进程)对于所分配到的资源具有排它性，即一个资源只能被一个线程(进程)占用，直到被该线程(进程)释放 请求与保持条件：一个线程(进程)因请求被占用资源而发生阻塞时，对已获得的资源保持不放。 不剥夺条件：线程(进程)已获得的资源在末使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。 循环等待条件：当发生死锁时，所等待的线程(进程)必定会形成一个环路（类似于死循环），造成永久阻塞 如何避免线程死锁 我们只要破坏产生死锁的四个条件中的其中一个就可以了。 破坏互斥条件 这个条件我们没有办法破坏，因为我们用锁本来就是想让他们互斥的（临界资源需要互斥访问）。 破坏请求与保持条件 一次性申请所有的资源。 破坏不剥夺条件 占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。 破坏循环等待条件 靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。 创建线程的四种方式 创建线程有四种方式： 继承 Thread 类； 实现 Runnable 接口； 实现 Callable 接口； 使用 Executors 工具类创建线程池 继承 Thread 类 步骤 定义Thread 类的子类，并重写该类的run() 方法，该run() 方法的方法体就代表类线程需要完成的任务。因此把run() 方法称为线程执行体。 创建 Thread 子类的实例，即创建线程对象 调用子类实例的star()方法来启动线程 /** * 继承 thread 的内部类，以买票例子 */ public class FirstThread extends Thread{ private int i; private int ticket = 10; @Override public void run() { for (;i&lt;20;i++) { //当继承thread 时，直接使用this 可以获取当前的线程,getName()获取当前线程的名字 if(this.ticket&gt;0){ Log.e(TAG, getName() + &quot;, 卖票：ticket=&quot; + ticket--); } } } } private void starTicketThread(){ Log.d(TAG,&quot;starTicketThread, &quot;+Thread.currentThread().getName()); FirstThread thread1 = new FirstThread(); FirstThread thread2 = new FirstThread(); FirstThread thread3 = new FirstThread(); thread1.start(); thread2.start(); thread3.start(); //开启3个线程进行买票，每个线程都卖了10张，总共就30张票 } 运行结果 注意 ：可以看到 3 个线程输入的 票数变量不连续，注意：ticket 是 FirstThread 的实例属性，而不是局部变量，但是因为程序每次创建线程对象都需要创建一个FirstThread 的对象，所有多个线程不共享该实例的属性。 //使用Lambda表达式，实现多线程 new Thread(() -&gt; { System.out.println(Thread.currentThread().getName() + &quot;新线程创建了！&quot;); } ).start(); 实现 Runnable 接口 步骤 定义Runnable接口实现类SecondThread，并重写run()方法 创建SecondThread实例secondThread，以secondThread作为target创建Thead对象，该Thread对象才是真正的线程对象 调用线程对象的start()方法 /** * 实现 runnable 接口，创建线程类 */ public class SecondThread implements Runnable{ private int i; private int ticket = 100; @Override public void run() { for (;i&lt;20;i++) { //如果线程类实现 runnable 接口 //获取当前的线程，只能用 Thread.currentThread() 获取当前的线程名 Log.d(TAG,Thread.currentThread().getName()+&quot; &quot;+i); if(this.ticket&gt;0){ Log.e(TAG, Thread.currentThread().getName() + &quot;, 卖票：ticket=&quot; + ticket--); } } } } private void starTicketThread2(){ Log.d(TAG,&quot;starTicketThread2, &quot;+Thread.currentThread().getName()); SecondThread secondThread = new SecondThread(); //通过new Thread(target,name)创建新的线程 new Thread(secondThread,&quot;买票人1&quot;).start(); new Thread(secondThread,&quot;买票人2&quot;).start(); new Thread(secondThread,&quot;买票人3&quot;).start(); //虽然是开启了3个线程，但是一共只买了100张票 } 执行结果 注意：可以看到 3 个线程输入的 票数变量是连续的，采用 Runnable 接口的方式创建多个线程可以共享线程类的实例的属性。这是因为在这种方式下，程序所创建的Runnable 对象只是线程的 target ,而多个线程可以共享同一个 target,所以多个线程可以共享同一个线程类（实际上应该是该线程的target 类）的实例属性。 //使用匿名内部类的方式，实现多线程 new Thread(new Runnable() { @Override public void run() { System.out.println(Thread.currentThread().getName() + &quot;新线程创建了！&quot;); } }).start(); 实现 Callable 接口 步骤 创建callable接口的实现类，并实现call() 方法，该call() 方法将作为线程的执行体，且该call() 方法是有返回值的。 创建 callable实现类的实例，使用 FutureTask 类来包装Callable对象，该FutureTask 对象封装 call() 方法的返回值。 使用FutureTask 对象作为Thread对象的target创建并启动新线程。 调用FutureTask对象的get()方法来获取子线程执行结束后的返回值。 /** * 使用callable 来实现线程类 */ public class ThirdThread implements Callable&lt;Integer&gt;{ private int ticket = 20; @Override public Integer call(){ for ( int i = 0;i&lt;10;i++) { //获取当前的线程，只能用 Thread.currentThread() 获取当前的线程名 // Log.d(TAG,Thread.currentThread().getName()+&quot; &quot;+i); if(this.ticket&gt;0){ Log.e(TAG, Thread.currentThread().getName() + &quot;, 卖票：ticket=&quot; + ticket--); } } return ticket; } } private void starCallableThread(){ ThirdThread thirdThread = new ThirdThread(); FutureTask&lt;Integer&gt; task = new FutureTask&lt;Integer&gt;(thirdThread); new Thread(task,&quot;有返回值的线程&quot;).start(); try { Integer integer = task.get(); Log.d(TAG,&quot;starCallableThread, 子线程的返回值=&quot;+integer); } catch (InterruptedException e) { e.printStackTrace(); } catch (ExecutionException e) { e.printStackTrace(); } } 执行结果 注意：注意：Callable的call() 方法允许声明抛出异常，并且允许带有返回值。 程序最后调用FutureTask 对象的get()方法来返回Call(）方法的返回值，导致主线程被阻塞，直到call()方法结束并返回为止。 使用 Executors 工具类创建线程池 Executors提供了一系列工厂方法用于创先线程池，返回的线程池都实现了ExecutorService接口。 主要有newFixedThreadPool，newCachedThreadPool，newSingleThreadExecutor，newScheduledThreadPool，后续详细介绍这四种线程池 public class MyRunnable implements Runnable { @Override public void run() { System.out.println(Thread.currentThread().getName() + &quot; run()方法执行中...&quot;); } } public class SingleThreadExecutorTest { public static void main(String[] args) { ExecutorService executorService = Executors.newSingleThreadExecutor(); MyRunnable runnableTest = new MyRunnable(); for (int i = 0; i &lt; 5; i++) { executorService.execute(runnableTest); } System.out.println(&quot;线程任务开始执行&quot;); executorService.shutdown(); } } 执行结果 说一下 runnable 和 callable 有什么区别？ 相同点 都是接口 都可以编写多线程程序 都采用Thread.start()启动线程 主要区别 Runnable 接口 run 方法无返回值；Callable 接口 call 方法有返回值，是个泛型，和Future、FutureTask配合可以用来获取异步执行的结果 Runnable 接口 run 方法只能抛出运行时异常，且无法捕获处理；Callable 接口 call 方法允许抛出异常，可以获取异常信息 注：Callalbe接口支持返回执行结果，需要调用FutureTask.get()得到，此方法会阻塞主进程的继续往下执行，如果不调用不会阻塞。 线程的 run()和 start()有什么区别？ 每个线程都是通过某个特定Thread对象所对应的方法run()来完成其操作的，run()方法称为线程体。通过调用Thread类的start()方法来启动一个线程。 start() 方法用于启动线程，run() 方法用于执行线程的运行时代码。run() 可以重复调用，而 start() 只能调用一次。 为什么我们调用 start() 方法时会执行 run() 方法，为什么我们不能直接调用 run() 方法？ new 一个 Thread，线程进入了新建状态。调用 start() 方法，会启动一个线程并使线程进入了就绪状态，当分配到时间片后就可以开始运行了。 start() 会执行线程的相应准备工作，然后自动执行 run() 方法的内容，这是真正的多线程工作。 而直接执行 run() 方法，会把 run 方法当成一个 main 线程下的普通方法去执行，并不会在某个线程中执行它，所以这并不是多线程工作。 总结： 调用 start 方法方可启动线程并使线程进入就绪状态，而 run 方法只是 thread 的一个普通方法调用，还是在主线程里执行。 什么是 Callable 和 Future? Callable 接口类似于 Runnable，从名字就可以看出来了，但是 Runnable 不会返回结果，并且无法抛出返回结果的异常，而 Callable 功能更强大一些，被线程执行后，可以返回值，这个返回值可以被 Future 拿到，也就是说，Future 可以拿到异步执行任务的返回值。 Future 接口表示异步任务，是一个可能还没有完成的异步任务的结果。所以说 Callable用于产生结果，Future 用于获取结果。 什么是 FutureTask FutureTask 表示一个异步运算的任务。FutureTask 里面可以传入一个 Callable 的具体实现类，可以对这个异步运算的任务的结果进行等待获取、判断是否已经完成、取消任务等操作。只有当运算完成的时候结果才能取回，如果运算尚未完成 get 方法将会阻塞。一个 FutureTask 对象可以对调用了 Callable 和 Runnable 的对象进行包装，由于 FutureTask 也是Runnable 接口的实现类，所以 FutureTask 也可以放入线程池中。 ","link":"https://tinaxiawuhao.github.io/post/ddmZREDcW/"},{"title":"java基础之一文件操作","content":" 一、抽象类： 字节流： InputStream（读入流） OutputStream(写出流） 字符流：Reader（字符 读入流） Writer （字符写出流） 字节流读取字符数据的问题 : 汉字等字符,往往有多个字节组成,那么到底哪几个字节应该组成一个汉字呢? 字节流不知道.如果我们自己用字节流读取字节数据,然后手动自己转字符,就有可能对规则不了解.从而造成错误. Java中的字符流恰好就可以帮助我们解决这个问题. Java提供的字符流的底层，它自己会根据读取到的字节数据的特点，然后自动的帮助我们把字节转成字符，最后我们就会自然的得到想要的字符数据。 字符流怎么解决这个问题呢? ​ 字符流 = 字节流 + 编码表 编码表其实里面定义了字节与字符的转换规则. 字符流在读取数据的时候,底层还是字节流在读取数据,字符流会自动根据编码规则把字节数据转为字符数据,就不会造成错误! 编码表的介绍 : 说明 : 计算机中保存的数据都是二进制（1010100101001），我们要把生活中的数据保存到计算机中，需要把生活中的数据转成二进制数据，然后才能让计算机来识别生活中的数据，进而对这些数据进行处理。 &quot;补充 :&quot; 要把生活中的数据转成计算机能够识别的数据，就需要定义一个固定的转换关系： 老美他们为了把自己的生活中的数据保存到计算机中，他们发明了一张表，然后在这张表中规定了生活中所有的字符和二进制之间的对应关系：老美的文字和计算机中的二进制的对应关系表：ASCII。 ASCII码表： 它采用的是一个字节表示所有的文字（标点符号，英文字母，其他的特殊符号，数字等）。 生活中的字符 十进制 二进制 A 65 01000001 B 66 01000010 欧洲也定义一张拉丁码表：ISO-8859-1 这个表码表兼容ASCII码表。也采用的一个字节表示字符数据。 ASCII： 一个字节表示一个字符： 0xxx xxxx 规定二进制的最高位是0，其他的7位表示某个字符的编码值。 ISO-8859-1： 一个字节表示一个字符：把整个字节表示字符，xxxx xxxx 全部用来表示字符数据 中国的编码表： GBK编码表采用2个字节表示一个汉字。 GB2312 ： 它识别六七千的文字。兼容ASCII编码表。 GBK： 识别两万多字符。目前主流的编码表. GB18030： 识别更多。 世界计算机协会也制定了一个张国际通用的编码表： ​ Unicode编码表： 它也采用2个字节表示一个字符。 Unicode编码表升级：UTF-8。 ​ UTF-8： 它对字符的编码规律可以使用一个字节表示的字符就使用一个字节。 可以使用两个字节表示的字符就使用两个字节。 可以使用三个字节表示的字符就使用三个字节（基本汉字都使用三个字节）。 可以识别汉字的编码表： GB2312、GBK、Unicode、UTF-8 二、文件操作流 字节流： FileInputStream ，FileOutputStream 字符流： FileReader, FileWriter File file = new File(&quot;D:/test/testIO.java&quot;); InputStream in = new FileInputStream(file); InputStream in = new FileInputStream(&quot;D:/test/testIO.java&quot;); OutputStream out = new FileOutputStream(file); OutputStream out = new FileOutputStream(&quot;D:/test/testIO.java&quot;); Reader reader = new FileReader(&quot;demo01.txt&quot;); FileWriter writer = new FileWriter(&quot;demo02.txt&quot;); Writer writer = new OutputStreamWriter(new FileOutputStream(&quot;demo03_utf8.txt&quot;), &quot;UTF-8&quot;); Reader reader = new InputStreamReader(new FileInputStream(&quot;demo03_utf8.txt&quot;), &quot;UTF-8&quot;); //1.指定要读 的文件目录及名称 File file =new File(&quot;文件路径&quot;); //2.创建文件读入流对象 FileInputStream fis =new FileInputStream(file); //3.定义结束标志,可用字节数组读取 int i =0 ; while((i = fis.read())!=-1){ //i 就是从文件中读取的字节，读完后返回-1 } //4.关闭流 fis.close(); //5.处理异常 //1.指定要写到的文件目录及名称 File file =new File(&quot;文件路径&quot;); //2.创建文件读入流对象 FileOutputStream fos =new FileOutputStream(file); //3.定义结束标志 fos.write(要写出的字节或者字节数组); //4.刷新和关闭流 fos.flush(); fos.close(); //5.处理异常 三、缓冲流： ​ 字节缓冲流：BufferedInputStream,BufferedOutputStream ​ 字符缓冲流：BufferedReader ,BufferedWriter ​ 缓冲流是对流的操作的功能的加强，提高了数据的读写效率。既然缓冲流是对流的功能和读写效率的加强和提高，所以在创建缓冲流的对象时应该要传入要加强的流对象。 //保存其参数，即输入流 in，以便将来使用。创建一个内部缓冲区数组并将 //其存储在 buf 中,该buf的大小默认为8192。 public BufferedInputStream(InputStream in); //创建具有指定缓冲区大小的 BufferedInputStream 并保存其参数， //即输入流 in，以便将来使用。创建一个长度为 size 的内部缓冲区数组并 //将其存储在 buf 中。 public BufferedInputStream(InputStream in,int size); //创建一个新的缓冲输出流，以将数据写入指定的底层输出流。 public BufferedOutputStream(OutputStream out); //创建一个新的缓冲输出流，以将具有指定缓冲区大小的数据写入指定的底层输出流。 public BufferedOutputStream(OutputStream out,int size); BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(&quot;demo08_utf8.txt&quot;), &quot;UTF-8&quot;)); BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(&quot;demo08_utf8.txt&quot;), &quot;UTF-8&quot;)); //1.指定要读 的文件目录及名称 File file =new File(&quot;文件路径&quot;); //2.创建文件读入流对象 FileInputStream fis =new FileInputStream(file); //3.创建缓冲流对象加强fis功能 BufferedInputStream bis =new BufferedInputStream(fis); //4.定义结束标志,可用字节数组读取 int i =0 ; while((i = bis.read())!=-1){ //i 就是从文件中读取的字节，读完后返回-1 } //5.关闭流 bis.close(); //6.处理异常 //1.指定要写到的文件目录及名称 File file =new File(&quot;文件路径&quot;); //2.创建文件读入流对象 FileOutputStream fos =new FileOutputStream(file); //3.创建缓冲流对象加强fos功能 BufferedOutputStream bos=new BufferedOutputStream(fos); //4.向流中写入数据 bos.write(要写出的字节或者字节数组); //5.刷新和关闭流 bos.flush(); bos.close(); //6.处理异常 由以上看出流的操作基本相同，此流与文件流操作是几乎一样的只是将文件流作为参数传入缓冲流的构造方法中堆文件流读写文件的功能进行加强 注1：在字符读入缓冲流BufferedReader 中还提供了读一行的方法 readLine()可以读取一行文本 在字符写出缓冲流BufferedWriter 中还提供了写入一个行分隔符的方法writeLine(),用于写出时换行 注2：此处用到的是设计模式中的装饰模式 //装饰的对象只要是抽象类的子类即可 BufferedInputStream bis =new BufferedInputStream(new FileInputStream(new File(&quot;文件路径&quot;))); 四、转换流： 这类流是用于将字符转换为字节输入输出，用于操作字符文件，属于字符流的子类，所以后缀为reader，writer；前缀inputstream，outputstream； 注 ：要传入字节流作为参赛 InputStreamReader: 字符转换输出流 OutputStreamWriter：字符转换输入流 需求：读取键盘输入的一行文本,再将输入的写到本地磁盘上 //1.获取键盘输入的字节流对象in InputStream in =Stream.in; /*2.用转换流将字节流对象转换为字符流对象，方便调用字符缓冲流的readeLine()方法*/ InputStreamReader isr =new InputStreamReader(in); /*5.创建字符转换输出流对象osw，方便把输入的字符流转换为字节输出到本地文件。*/ OutputStreamWriter osw =new OutputStreamWriter(new FileOutputStream(new File(&quot;文件名&quot;))); /*3.现在isr是字符流，可以作为参数传入字符缓冲流中*/ BufferedReader br =new BufferedReader(isr); /*4.可以调用字符缓冲流br的readLine()方法度一行输入文本*/ String line =null; while((line =br.readLine()){ osw.write(line);//osw是字符流对象，可以直接操作字符串 } //刷新此缓冲的输出流，保证数据全部都能写出 osw.flush(); br.close(); osw.close(); 读取数据 //读取方式一 : 效率低! int content = -1; while ((content = reader.read()) != -1) { System.out.print((char)content); } //读取方式二 : 数组之后的无用数据都会被读取! int len = -1; char[] cbuf = new char[1024]; while ((len = reader.read(cbuf)) != -1) { System.out.println(cbuf); } // 读取方式三 : 将读取的数据内容转换为 `字符串` int len = -1; char[] cbuf = new char[1024]; while ((len = reader.read(cbuf)) != -1) { String str = new String(cbuf, 0, len); System.out.println(str); } &quot;转换流 和 操作流 之间的关系 :&quot; &quot;转换流 :&quot; 1. OutputStreamWriter = OutputStream + 任意编码表 2. InputStreamReader = InputStream + 任意编码表 &quot;操作流 :&quot; 1. FileWriter = OutputStreamWriter + 默认编码表 (GBK) 2. FileReader = InputStreamReader + 默认编码表(GBK) &quot;总结 : 一般我们都使用操作流,当需要指定编码表的时候,才会使用转换流&quot; 拆分文件及还原 public class Test { public static void main(String[] args) throws IOException { int count = judge(); System.out.println(count); BufferedInputStream bis = new BufferedInputStream(new FileInputStream( &quot;somethings\\\\test.flv&quot;)); incision(count, bis, 6); BufferedOutputStream bos = new BufferedOutputStream( new FileOutputStream(&quot;somethings\\\\jzc.flv&quot;)); joint(bos,6); bos.close(); bis.close(); } public static void joint( BufferedOutputStream bos,int n) throws FileNotFoundException, IOException { for (int i = 1; i &lt;= n; i++) { BufferedInputStream bis = new BufferedInputStream( new FileInputStream(&quot;somethings\\\\jzc&quot; + i + &quot;.flv&quot;)); byte[] bys = new byte[1024]; int len = -1; while ((len = bis.read(bys)) != -1) { bos.write(bys, 0, len); } bis.close(); } bos.close(); System.out.println(&quot;拼接完了&quot;); } public static void incision(int count, BufferedInputStream bis, int n) throws FileNotFoundException, IOException { byte[] bys = new byte[1024]; int len = -1; for (int i = 1; i &lt;= n; i++) { int count2 = 0; BufferedOutputStream bos = new BufferedOutputStream( new FileOutputStream(&quot;somethings\\\\jzc&quot; + i + &quot;.flv&quot;)); while ((len = bis.read(bys)) != -1) { count2 += 1024; System.out.println(count2); if (count2 &lt; (count / n)+1024&amp;&amp;count2 &gt; (count / n)-1024) { break; } else { bos.write(bys, 0, len); } } bos.close(); } System.out.println(&quot;切割完了&quot;); } public static int judge() throws FileNotFoundException, IOException { BufferedInputStream bis = new BufferedInputStream(new FileInputStream( &quot;somethings\\\\test.flv&quot;)); byte[] bys = new byte[1024]; int len = -1; int count = 0; while ((len = bis.read(bys)) != -1) { count += 1024; } bis.close(); return count; } } ","link":"https://tinaxiawuhao.github.io/post/FNm9kw7nn/"},{"title":"java基础之一lambda","content":"一、引言 java8最大的特性就是引入Lambda表达式，即函数式编程，可以将行为进行传递。总结就是：使用不可变值与函数，函数对不可变值进行处理，映射成另一个值。 二、java重要的函数式接口 1、什么是函数式接口 函数接口是只有一个抽象方法的接口，用作 Lambda 表达式的类型。使用@FunctionalInterface注解修饰的类，编译器会检测该类是否只有一个抽象方法或接口，否则，会报错。可以有多个默认方法，静态方法。 1.1 java8自带的常用函数式接口。 函数接口 抽象方法 功能 参数 返回类型 示例 Predicate test(T t) 判断真假 T boolean 金刚的身高大于185cm吗？ Consumer accept(T t) 消费消息 T void 输出一个值 Function R apply(T t) 将T映射为R（转换功能） T R 获得student对象的名字 Supplier T get() 生产消息 None T 工厂方法 UnaryOperator T apply(T t) 一元操作 T T 逻辑非（!） BinaryOperator apply(T t, U u) 二元操作 (T，T) (T) 求两个数的乘积（*） public class Test { public static void main(String[] args) { Predicate&lt;Integer&gt; predicate = x -&gt; x &gt; 185; Student student = new Student(&quot;金刚&quot;, 23, 175); System.out.println(&quot;金刚的身高高于185吗？：&quot; + predicate.test(student.getStature())); Consumer&lt;String&gt; consumer = System.out::println; consumer.accept(&quot;消费消息，输出一个值&quot;); Function&lt;Student, String&gt; function = Student::getName; String name = function.apply(student); System.out.println(name); Supplier&lt;Integer&gt; supplier = () -&gt; Integer.valueOf(BigDecimal.TEN.toString()); System.out.println(supplier.get()); UnaryOperator&lt;Boolean&gt; unaryOperator = uglily -&gt; !uglily; Boolean apply2 = unaryOperator.apply(true); System.out.println(apply2); BinaryOperator&lt;Integer&gt; operator = (x, y) -&gt; x * y; Integer integer = operator.apply(2, 3); System.out.println(integer); test(() -&gt; &quot;我是一个演示的函数式接口&quot;); } /** * 演示自定义函数式接口使用 * * @param worker */ public static void test(Worker worker) { String work = worker.work(); System.out.println(work); } public interface Worker { String work(); } } //金刚的身高高于185吗？：false //消费消息，输出一个值 //金刚 //10 //false //6 //我是一个演示的函数式接口 以上演示了lambda接口的使用及自定义一个函数式接口并使用。下面，我们看看java8将函数式接口封装到流中如何高效的帮助我们处理集合。 注意：Student::getName例子中这种编写lambda表达式的方式称为方法引用。格式为ClassName::methodName。 示例：本篇所有示例都基于以下三个类。OutstandingClass：班级；Student：学生；SpecialityEnum：特长。 1.2 惰性求值与及早求值 惰性求值：只描述Stream，操作的结果也是Stream，这样的操作称为惰性求值。 惰性求值可以像建造者模式一样链式使用，最后再使用及早求值得到最终结果。 及早求值：得到最终的结果而不是Stream，这样的操作称为及早求值。 2、常用的流 2.1 collect(Collectors.toList()) 将流转换为list。还有toSet()，toMap()等。及早求值。 public class TestCase { public static void main(String[] args) { List&lt;Student&gt; studentList = Stream.of(new Student(&quot;路飞&quot;, 22, 175), new Student(&quot;红发&quot;, 40, 180), new Student(&quot;白胡子&quot;, 50, 185)).collect(Collectors.toList()); System.out.println(studentList); } } //输出结果 //[Student{name='路飞', age=22, stature=175, specialities=null}, //Student{name='红发', age=40, stature=180, specialities=null}, //Student{name='白胡子', age=50, stature=185, specialities=null}] //转成map public Map&lt;Long, String&gt; getIdNameMap(List&lt;Account&gt; accounts) { return accounts.stream().collect(Collectors.toMap(Account::getId, Account::getUsername)); } //收集成实体本身map public Map&lt;Long, Account&gt; getIdAccountMap(List&lt;Account&gt; accounts) { return accounts.stream().collect(Collectors.toMap(Account::getId, account -&gt; account)); } //account -&gt; account是一个返回本身的lambda表达式，其实还可以使用Function接口中的一个默认方法代替，使整个方法更简洁优雅： public Map&lt;Long, Account&gt; getIdAccountMap(List&lt;Account&gt; accounts) { return accounts.stream().collect(Collectors.toMap(Account::getId, Function.identity())); } //重复key的情况，下面name可能重复 public Map&lt;String, Account&gt; getNameAccountMap(List&lt;Account&gt; accounts) { return accounts.stream().collect(Collectors.toMap(Account::getUsername, Function.identity())); } //toMap有个重载方法，可以传入一个合并的函数来解决key冲突问题 public Map&lt;String, Account&gt; getNameAccountMap(List&lt;Account&gt; accounts) { return accounts.stream().collect(Collectors.toMap(Account::getUsername, Function.identity(), (key1, key2) -&gt; key2)); } //按id分组 Map&lt;Long, List&lt;Account&gt;&gt; map = accounts.stream().collect(Collectors.groupingBy(Account::getId)); //指定具体收集的map public Map&lt;String, Account&gt; getNameAccountMap(List&lt;Account&gt; accounts) { return accounts.stream().collect(Collectors.toMap(Account::getUsername, Function.identity(), (key1, key2) -&gt; key2, LinkedHashMap::new)); } 2.2 filter 顾名思义，起过滤筛选的作用。内部就是Predicate接口。惰性求值。 比如我们筛选出出身高小于180的同学。 public class TestCase { public static void main(String[] args) { List&lt;Student&gt; students = new ArrayList&lt;&gt;(3); students.add(new Student(&quot;路飞&quot;, 22, 175)); students.add(new Student(&quot;红发&quot;, 40, 180)); students.add(new Student(&quot;白胡子&quot;, 50, 185)); List&lt;Student&gt; list = students.stream() .filter(stu -&gt; stu.getStature() &lt; 180) .collect(Collectors.toList()); System.out.println(list); } } //输出结果 //[Student{name='路飞', age=22, stature=175, specialities=null}] 2.3 map 转换功能，内部就是Function接口。惰性求值 public class TestCase { public static void main(String[] args) { List&lt;Student&gt; students = new ArrayList&lt;&gt;(3); students.add(new Student(&quot;路飞&quot;, 22, 175)); students.add(new Student(&quot;红发&quot;, 40, 180)); students.add(new Student(&quot;白胡子&quot;, 50, 185)); List&lt;String&gt; names = students.stream().map(student -&gt; student.getName()) .collect(Collectors.toList()); System.out.println(names); } } //输出结果 //[路飞, 红发, 白胡子] 例子中将student对象转换为String对象，获取student的名字。 2.4 flatMap 将多个Stream合并为一个Stream。惰性求值 public class TestCase { public static void main(String[] args) { List&lt;Student&gt; students = new ArrayList&lt;&gt;(3); students.add(new Student(&quot;路飞&quot;, 22, 175)); students.add(new Student(&quot;红发&quot;, 40, 180)); students.add(new Student(&quot;白胡子&quot;, 50, 185)); List&lt;Student&gt; studentList = Stream.of(students, asList(new Student(&quot;艾斯&quot;, 25, 183), new Student(&quot;雷利&quot;, 48, 176))) .flatMap(students1 -&gt; students1.stream()).collect(Collectors.toList()); System.out.println(studentList); } } //输出结果 //[Student{name='路飞', age=22, stature=175, specialities=null}, //Student{name='红发', age=40, stature=180, specialities=null}, //Student{name='白胡子', age=50, stature=185, specialities=null}, //Student{name='艾斯', age=25, stature=183, specialities=null}, //Student{name='雷利', age=48, stature=176, specialities=null}] 调用Stream.of的静态方法将两个list转换为Stream，再通过flatMap将两个流合并为一个。 2.5 max和min 我们经常会在集合中求最大或最小值，使用流就很方便。及早求值。 public class TestCase { public static void main(String[] args) { List&lt;Student&gt; students = new ArrayList&lt;&gt;(3); students.add(new Student(&quot;路飞&quot;, 22, 175)); students.add(new Student(&quot;红发&quot;, 40, 180)); students.add(new Student(&quot;白胡子&quot;, 50, 185)); Optional&lt;Student&gt; max = students.stream() .max(Comparator.comparing(stu -&gt; stu.getAge())); Optional&lt;Student&gt; min = students.stream() .min(Comparator.comparing(stu -&gt; stu.getAge())); //判断是否有值 if (max.isPresent()) { System.out.println(max.get()); } if (min.isPresent()) { System.out.println(min.get()); } } } //输出结果 //Student{name='白胡子', age=50, stature=185, specialities=null} //Student{name='路飞', age=22, stature=175, specialities=null} max、min接收一个Comparator（例子中使用java8自带的静态函数，只需要传进需要比较值即可。）并且返回一个Optional对象，该对象是java8新增的类，专门为了防止null引发的空指针异常。可以使用max.isPresent()判断是否有值；可以使用max.orElse(new Student())，当值为null时就使用给定值；也可以使用max.orElseGet(() -&gt; new Student());这需要传入一个Supplier的lambda表达式。 2.6 count 统计功能，一般都是结合filter使用，因为先筛选出我们需要的再统计即可。及早求值 public class TestCase { public static void main(String[] args) { List&lt;Student&gt; students = new ArrayList&lt;&gt;(3); students.add(new Student(&quot;路飞&quot;, 22, 175)); students.add(new Student(&quot;红发&quot;, 40, 180)); students.add(new Student(&quot;白胡子&quot;, 50, 185)); long count = students.stream().filter(s1 -&gt; s1.getAge() &lt; 45).count(); System.out.println(&quot;年龄小于45岁的人数是：&quot; + count); } } //输出结果 //年龄小于45岁的人数是：2 2.7 reduce reduce 操作可以实现从一组值中生成一个值。在上述例子中用到的 count 、 min 和 max 方 法，因为常用而被纳入标准库中。事实上，这些方法都是 reduce 操作。及早求值。 public class TestCase { public static void main(String[] args) { Integer reduce = Stream.of(1, 2, 3, 4).reduce(0, (acc, x) -&gt; acc+ x); System.out.println(reduce); } } //输出结果 //10 我们看得reduce接收了一个初始值为0的累加器，依次取出值与累加器相加，最后累加器的值就是最终的结果。 三、高级集合类及收集器 3.1 转换成值 **收集器，一种通用的、从流生成复杂值的结构。**只要将它传给 collect 方法，所有 的流就都可以使用它了。标准类库已经提供了一些有用的收集器，以下示例代码中的收集器都是从 java.util.stream.Collectors 类中静态导入的。 public class CollectorsTest { public static void main(String[] args) { List&lt;Student&gt; students1 = new ArrayList&lt;&gt;(3); students1.add(new Student(&quot;路飞&quot;, 23, 175)); students1.add(new Student(&quot;红发&quot;, 40, 180)); students1.add(new Student(&quot;白胡子&quot;, 50, 185)); OutstandingClass ostClass1 = new OutstandingClass(&quot;一班&quot;, students1); //复制students1，并移除一个学生 List&lt;Student&gt; students2 = new ArrayList&lt;&gt;(students1); students2.remove(1); OutstandingClass ostClass2 = new OutstandingClass(&quot;二班&quot;, students2); //将ostClass1、ostClass2转换为Stream Stream&lt;OutstandingClass&gt; classStream = Stream.of(ostClass1, ostClass2); OutstandingClass outstandingClass = biggestGroup(classStream); System.out.println(&quot;人数最多的班级是：&quot; + outstandingClass.getName()); System.out.println(&quot;一班平均年龄是：&quot; + averageNumberOfStudent(students1)); } /** * 获取人数最多的班级 */ private static OutstandingClass biggestGroup(Stream&lt;OutstandingClass&gt; outstandingClasses) { return outstandingClasses.collect( maxBy(comparing(ostClass -&gt; ostClass.getStudents().size()))) .orElseGet(OutstandingClass::new); } /** * 计算平均年龄 */ private static double averageNumberOfStudent(List&lt;Student&gt; students) { return students.stream().collect(averagingInt(Student::getAge)); } } //输出结果 //人数最多的班级是：一班 //一班平均年龄是：37.666666666666664 maxBy或者minBy就是求最大值与最小值。 3.2 转换成块 常用的流操作是将其分解成两个集合，Collectors.partitioningBy帮我们实现了，接收一个Predicate函数式接口。 将示例学生分为会唱歌与不会唱歌的两个集合。 public class PartitioningByTest { public static void main(String[] args) { //省略List&lt;student&gt; students的初始化 Map&lt;Boolean, List&lt;Student&gt;&gt; listMap = students.stream().collect( Collectors.partitioningBy(student -&gt; student.getSpecialities(). contains(SpecialityEnum.SING))); } } 3.3 数据分组 数据分组是一种更自然的分割数据操作，与将数据分成 ture 和 false 两部分不同，可以使 用任意值对数据分组。Collectors.groupingBy接收一个Function做转换。 如图，我们使用groupingBy将根据进行分组为圆形一组，三角形一组，正方形一组。 例子：根据学生第一个特长进行分组 public class GroupingByTest { public static void main(String[] args) { //省略List&lt;student&gt; students的初始化 Map&lt;SpecialityEnum, List&lt;Student&gt;&gt; listMap = students.stream().collect( Collectors.groupingBy(student -&gt; student.getSpecialities().get(0))); } } Collectors.groupingBy与SQL 中的 group by 操作是一样的。 3.4 字符串拼接 如果将所有学生的名字拼接起来，怎么做呢？通常只能创建一个StringBuilder，循环拼接。使用Stream，使用Collectors.joining()简单容易。 public class JoiningTest { public static void main(String[] args) { List&lt;Student&gt; students = new ArrayList&lt;&gt;(3); students.add(new Student(&quot;路飞&quot;, 22, 175)); students.add(new Student(&quot;红发&quot;, 40, 180)); students.add(new Student(&quot;白胡子&quot;, 50, 185)); String names = students.stream() .map(Student::getName).collect(Collectors.joining(&quot;,&quot;,&quot;[&quot;,&quot;]&quot;)); System.out.println(names); } } //输出结果 //[路飞,红发,白胡子] joining接收三个参数，第一个是分界符，第二个是前缀符，第三个是结束符。也可以不传入参数Collectors.joining()，这样就是直接拼接。 原文地址：https://juejin.cn/post/6844903849753329678#hjava ","link":"https://tinaxiawuhao.github.io/post/7BfPpY68W/"},{"title":"Java基础之—反射","content":"反射是框架设计的灵魂（使用的前提条件：必须先得到代表的字节码的Class，Class类用于表示.class文件（字节码）） 反射的概述 JAVA反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为java语言的反射机制。 要想解剖一个类,必须先要获取到该类的字节码文件对象。而解剖使用的就是Class类中的方法.所以先要获取到每一个字节码文件对应的Class类型的对象. 反射就是把java类中的各种成分映射成一个个的Java对象 例如：一个类有：成员变量、方法、构造方法、包等等信息，利用反射技术可以对一个类进行解剖，把个个组成部分映射成一个个对象。（其实：一个类中这些成员方法、构造方法、在加入类中都有一个类来描述） 如图是类的正常加载过程：Class对象的由来是将class文件读入内存，并为之创建一个Class对象。 其中这个Class对象很特殊。我们先了解一下这个Class类 查看Class类在java中的api详解 Class 类的实例表示正在运行的 Java 应用程序中的类和接口。也就是jvm中有N多的实例每个类都有该Class对象。（包括基本数据类型） Class 没有公共构造方法。Class 对象是在加载类时由 Java 虚拟机以及通过调用类加载器中的defineClass 方法自动构造的。也就是这不需要我们自己去处理创建，JVM已经帮我们创建好了。 没有公共的构造方法，方法共有64个。 反射的使用 获取Class对象的三种方式 1.1 Object ——&gt; getClass(); 1.2 任何数据类型（包括基本数据类型）都有一个“静态”的class属性 1.3 通过Class类的静态方法：forName（String className）(常用) 其中1.1是因为Object类中的getClass方法、因为所有类都继承Object类。从而调用Object类来获取 package fanshe; /** * 获取Class对象的三种方式 * 1 Object ——&gt; getClass(); * 2 任何数据类型（包括基本数据类型）都有一个“静态”的class属性 * 3 通过Class类的静态方法：forName（String className）(常用) * */ public class Fanshe { public static void main(String[] args) { //第一种方式获取Class对象 Student stu1 = new Student();//这一new 产生一个Student对象，一个Class对象。 Class stuClass = stu1.getClass();//获取Class对象 System.out.println(stuClass.getName()); //第二种方式获取Class对象 Class stuClass2 = Student.class; System.out.println(stuClass == stuClass2);//判断第一种方式获取的Class对象和第二种方式获取的是否是同一个 //第三种方式获取Class对象 try { Class stuClass3 = Class.forName(&quot;fanshe.Student&quot;);//注意此字符串必须是真实路径，就是带包名的类路径，包名.类名 System.out.println(stuClass3 == stuClass2);//判断三种方式是否获取的是同一个Class对象 } catch (ClassNotFoundException e) { e.printStackTrace(); } } } 注意：在运行期间，一个类，只有一个Class对象产生。 三种方式常用第三种，第一种对象都有了还要反射干什么。第二种需要导入类的包，依赖太强，不导包就抛编译错误。一般都第三种，一个字符串可以传入也可写在配置文件中等多种方法。 通过反射获取构造方法并使用： package fanshe; public class Student { //---------------构造方法------------------- //（默认的构造方法） Student(String str){ System.out.println(&quot;(默认)的构造方法 s = &quot; + str); } //无参构造方法 public Student(){ System.out.println(&quot;调用了公有、无参构造方法执行了。。。&quot;); } //有一个参数的构造方法 public Student(char name){ System.out.println(&quot;姓名：&quot; + name); } //有多个参数的构造方法 public Student(String name ,int age){ System.out.println(&quot;姓名：&quot;+name+&quot;年龄：&quot;+ age);//这的执行效率有问题，以后解决。 } //受保护的构造方法 protected Student(boolean n){ System.out.println(&quot;受保护的构造方法 n = &quot; + n); } //私有构造方法 private Student(int age){ System.out.println(&quot;私有的构造方法 年龄：&quot;+ age); } } 共有6个构造方法； /* * 通过Class对象可以获取某个类中的：构造方法、成员变量、成员方法；并访问成员； * * 1.获取构造方法： * 1).批量的方法： * public Constructor[] getConstructors()：所有&quot;公有的&quot;构造方法 * public Constructor[] getDeclaredConstructors()：获取所有的构造方法(包括私有、受保护、默认、公有) * * 2).获取单个的方法，并调用： * public Constructor getConstructor(Class... parameterTypes):获取单个的&quot;公有的&quot;构造方法： * public Constructor getDeclaredConstructor(Class... parameterTypes):获取&quot;某个构造方法&quot;可以是私有的，或受保护、默认、公有； * *调用构造方法： * Constructor--&gt;newInstance(Object... initargs) * 使用此 Constructor 对象表示的构造方法来创建该构造方法的声明类的新实例，并用指定的初始化参数初始化该实例。 * 它的返回值是T类型，所以newInstance是创建了一个构造方法的声明类的新实例对象。并为之调用 */ package fanshe; import java.lang.reflect.Constructor; public class Constructors { public static void main(String[] args) throws Exception { //1.加载Class对象 Class clazz = Class.forName(&quot;fanshe.Student&quot;); //2.获取所有公有构造方法 System.out.println(&quot;**********************所有公有构造方法*********************************&quot;); Constructor[] conArray = clazz.getConstructors(); for(Constructor c : conArray){ System.out.println(c); } System.out.println(&quot;************所有的构造方法(包括：私有、受保护、默认、公有)***************&quot;); conArray = clazz.getDeclaredConstructors(); for(Constructor c : conArray){ System.out.println(c); } System.out.println(&quot;*****************获取公有、无参的构造方法*******************************&quot;); Constructor con = clazz.getConstructor(null); //1&gt;、因为是无参的构造方法所以类型是一个null,不写也可以：这里需要的是一个参数的类型，切记是类型 //2&gt;、返回的是描述这个无参构造函数的类对象。 System.out.println(&quot;con = &quot; + con); //调用构造方法 Object obj = con.newInstance(); // System.out.println(&quot;obj = &quot; + obj); // Student stu = (Student)obj; System.out.println(&quot;******************获取私有构造方法，并调用*******************************&quot;); con = clazz.getDeclaredConstructor(char.class); System.out.println(con); //调用构造方法 con.setAccessible(true);//暴力访问(忽略掉访问修饰符) obj = con.newInstance('男'); } } 后台输出： **********************所有公有构造方法********************************* public fanshe.Student(java.lang.String,int) public fanshe.Student(char) public fanshe.Student() ************所有的构造方法(包括：私有、受保护、默认、公有)*************** private fanshe.Student(int) protected fanshe.Student(boolean) public fanshe.Student(java.lang.String,int) public fanshe.Student(char) public fanshe.Student() fanshe.Student(java.lang.String) *****************获取公有、无参的构造方法******************************* con = public fanshe.Student() 调用了公有、无参构造方法执行了。。。 ******************获取私有构造方法，并调用******************************* public fanshe.Student(char) 姓名：男 获取成员变量并调用： package fanshe.field; public class Student { public Student(){ } //**********字段*************// public String name; protected int age; char sex; private String phoneNum; @Override public String toString() { return &quot;Student [name=&quot; + name + &quot;, age=&quot; + age + &quot;, sex=&quot; + sex + &quot;, phoneNum=&quot; + phoneNum + &quot;]&quot;; } } 测试类： /* * 获取成员变量并调用： * * 1.批量的 * 1).Field[] getFields():获取所有的&quot;公有字段&quot; * 2).Field[] getDeclaredFields():获取所有字段，包括：私有、受保护、默认、公有； * 2.获取单个的： * 1).public Field getField(String fieldName):获取某个&quot;公有的&quot;字段； * 2).public Field getDeclaredField(String fieldName):获取某个字段(可以是私有的) * * 设置字段的值： * Field --&gt; public void set(Object obj,Object value): * 参数说明： * 1.obj:要设置的字段所在的对象； * 2.value:要为字段设置的值； * */ package fanshe.field; import java.lang.reflect.Field; public class Fields { public static void main(String[] args) throws Exception { //1.获取Class对象 Class stuClass = Class.forName(&quot;fanshe.field.Student&quot;); //2.获取字段 System.out.println(&quot;************获取所有公有的字段********************&quot;); Field[] fieldArray = stuClass.getFields(); for(Field f : fieldArray){ System.out.println(f); } System.out.println(&quot;************获取所有的字段(包括私有、受保护、默认的)********************&quot;); fieldArray = stuClass.getDeclaredFields(); for(Field f : fieldArray){ System.out.println(f); } System.out.println(&quot;*************获取公有字段**并调用***********************************&quot;); Field f = stuClass.getField(&quot;name&quot;); System.out.println(f); //获取一个对象 Object obj = stuClass.getConstructor().newInstance();//产生Student对象--》Student stu = new Student(); //为字段设置值 f.set(obj, &quot;刘德华&quot;);//为Student对象中的name属性赋值--》stu.name = &quot;刘德华&quot; //验证 Student stu = (Student)obj; System.out.println(&quot;验证姓名：&quot; + stu.name); System.out.println(&quot;**************获取私有字段****并调用********************************&quot;); f = stuClass.getDeclaredField(&quot;phoneNum&quot;); System.out.println(f); f.setAccessible(true);//暴力反射，解除私有限定 f.set(obj, &quot;18888889999&quot;); System.out.println(&quot;验证电话：&quot; + stu); } } 后台输出： ************获取所有公有的字段******************** public java.lang.String fanshe.field.Student.name ************获取所有的字段(包括私有、受保护、默认的)******************** public java.lang.String fanshe.field.Student.name protected int fanshe.field.Student.age char fanshe.field.Student.sex private java.lang.String fanshe.field.Student.phoneNum *************获取公有字段**并调用*********************************** public java.lang.String fanshe.field.Student.name 验证姓名：刘德华 **************获取私有字段****并调用******************************** private java.lang.String fanshe.field.Student.phoneNum 验证电话：Student [name=刘德华, age=0, sex=，phoneNum=18888889999] 获取成员方法并调用 package fanshe.method; public class Student { //**************成员方法***************// public void show1(String s){ System.out.println(&quot;调用了：公有的，String参数的show1(): s = &quot; + s); } protected void show2(){ System.out.println(&quot;调用了：受保护的，无参的show2()&quot;); } void show3(){ System.out.println(&quot;调用了：默认的，无参的show3()&quot;); } private String show4(int age){ System.out.println(&quot;调用了，私有的，并且有返回值的，int参数的show4(): age = &quot; + age); return &quot;abcd&quot;; } } 测试类： /* * 获取成员方法并调用： * * 1.批量的： * public Method[] getMethods():获取所有&quot;公有方法&quot;；（包含了父类的方法也包含Object类） * public Method[] getDeclaredMethods():获取所有的成员方法，包括私有的(不包括继承的) * 2.获取单个的： * public Method getMethod(String name,Class&lt;?&gt;... parameterTypes): * 参数： * name : 方法名； * Class ... : 形参的Class类型对象 * public Method getDeclaredMethod(String name,Class&lt;?&gt;... parameterTypes) * * 调用方法： * Method --&gt; public Object invoke(Object obj,Object... args): * 参数说明： * obj : 要调用方法的对象； * args:调用方式时所传递的实参； ): */ package fanshe.method; import java.lang.reflect.Method; public class MethodClass { public static void main(String[] args) throws Exception { //1.获取Class对象 Class stuClass = Class.forName(&quot;fanshe.method.Student&quot;); //2.获取所有公有方法 System.out.println(&quot;***************获取所有的”公有“方法*******************&quot;); stuClass.getMethods(); Method[] methodArray = stuClass.getMethods(); for(Method m : methodArray){ System.out.println(m); } System.out.println(&quot;***************获取所有的方法，包括私有的*******************&quot;); methodArray = stuClass.getDeclaredMethods(); for(Method m : methodArray){ System.out.println(m); } System.out.println(&quot;***************获取公有的show1()方法*******************&quot;); Method m = stuClass.getMethod(&quot;show1&quot;, String.class); System.out.println(m); //实例化一个Student对象 Object obj = stuClass.getConstructor().newInstance(); m.invoke(obj, &quot;刘德华&quot;); System.out.println(&quot;***************获取私有的show4()方法******************&quot;); m = stuClass.getDeclaredMethod(&quot;show4&quot;, int.class); System.out.println(m); m.setAccessible(true);//解除私有限定 Object result = m.invoke(obj, 20);//需要两个参数，一个是要调用的对象（获取有反射），一个是实参 System.out.println(&quot;返回值：&quot; + result); } } 控制台输出： ***************获取所有的”公有“方法******************* public void fanshe.method.Student.show1(java.lang.String) public final void java.lang.Object.wait(long,int) throws java.lang.InterruptedException public final native void java.lang.Object.wait(long) throws java.lang.InterruptedException public final void java.lang.Object.wait() throws java.lang.InterruptedException public boolean java.lang.Object.equals(java.lang.Object) public java.lang.String java.lang.Object.toString() public native int java.lang.Object.hashCode() public final native java.lang.Class java.lang.Object.getClass() public final native void java.lang.Object.notify() public final native void java.lang.Object.notifyAll() ***************获取所有的方法，包括私有的******************* public void fanshe.method.Student.show1(java.lang.String) private java.lang.String fanshe.method.Student.show4(int) protected void fanshe.method.Student.show2() void fanshe.method.Student.show3() ***************获取公有的show1()方法******************* public void fanshe.method.Student.show1(java.lang.String) 调用了：公有的，String参数的show1(): s = 刘德华 ***************获取私有的show4()方法****************** private java.lang.String fanshe.method.Student.show4(int) 调用了，私有的，并且有返回值的，int参数的show4(): age = 20 返回值：abcd 反射main方法 package fanshe.main; public class Student { public static void main(String[] args) { System.out.println(&quot;main方法执行了。。。&quot;); } } 测试类： package fanshe.main; import java.lang.reflect.Method; /** * 获取Student类的main方法、不要与当前的main方法搞混了 */ public class Main { public static void main(String[] args) { try { //1、获取Student对象的字节码 Class clazz = Class.forName(&quot;fanshe.main.Student&quot;); //2、获取main方法 Method methodMain = clazz.getMethod(&quot;main&quot;, String[].class);//第一个参数：方法名称，第二个参数：方法形参的类型， //3、调用main方法 // methodMain.invoke(null, new String[]{&quot;a&quot;,&quot;b&quot;,&quot;c&quot;}); //第一个参数，对象类型，因为方法是static静态的，所以为null可以，第二个参数是String数组，这里要注意在jdk1.4时是数组，jdk1.5之后是可变参数 //这里拆的时候将 new String[]{&quot;a&quot;,&quot;b&quot;,&quot;c&quot;} 拆成3个对象。。。所以需要将它强转。 methodMain.invoke(null, (Object)new String[]{&quot;a&quot;,&quot;b&quot;,&quot;c&quot;});//方式一 // methodMain.invoke(null, new Object[]{new String[]{&quot;a&quot;,&quot;b&quot;,&quot;c&quot;}});//方式二 } catch (Exception e) { e.printStackTrace(); } } } 控制台输出： main方法执行了。。。 反射方法的其它使用之---通过反射运行配置文件内容 public class Student { public void show(){ System.out.println(&quot;is show()&quot;); } } 配置文件以txt文件为例子（pro.txt）： className = cn.fanshe.Student methodName = show 测试类： import java.io.FileNotFoundException; import java.io.FileReader; import java.io.IOException; import java.lang.reflect.Method; import java.util.Properties; /* * 我们利用反射和配置文件，可以使：应用程序更新时，对源码无需进行任何修改 * 我们只需要将新类发送给客户端，并修改配置文件即可 */ public class Demo { public static void main(String[] args) throws Exception { //通过反射获取Class对象 Class stuClass = Class.forName(getValue(&quot;className&quot;));//&quot;cn.fanshe.Student&quot; //2获取show()方法 Method m = stuClass.getMethod(getValue(&quot;methodName&quot;));//show //3.调用show()方法 m.invoke(stuClass.getConstructor().newInstance()); } //此方法接收一个key，在配置文件中获取相应的value public static String getValue(String key) throws IOException{ Properties pro = new Properties();//获取配置文件的对象 FileReader in = new FileReader(&quot;pro.txt&quot;);//获取输入流 pro.load(in);//将流加载到配置文件对象中 in.close(); return pro.getProperty(key);//返回根据key获取的value值 } } 控制台输出： is show() 需求： 当我们升级这个系统时，不要Student类，而需要新写一个Student2的类时，这时只需要更改pro.txt的文件内容就可以了。代码就一点不用改动 要替换的student2类： public class Student2 { public void show2(){ System.out.println(&quot;is show2()&quot;); } } 配置文件更改为： className = cn.fanshe.Student2 methodName = show2 控制台输出： is show2(); 反射方法的其它使用之---通过反射越过泛型检查 泛型用在编译期，编译过后泛型擦除（消失掉）。所以是可以通过反射越过泛型检查的 import java.lang.reflect.Method; import java.util.ArrayList; /* * 通过反射越过泛型检查 * * 例如：有一个String泛型的集合，怎样能向这个集合中添加一个Integer类型的值？ */ public class Demo { public static void main(String[] args) throws Exception{ ArrayList&lt;String&gt; strList = new ArrayList&lt;&gt;(); strList.add(&quot;aaa&quot;); strList.add(&quot;bbb&quot;); // strList.add(100); //获取ArrayList的Class对象，反向的调用add()方法，添加数据 Class listClass = strList.getClass(); //得到 strList 对象的字节码 对象 //获取add()方法 Method m = listClass.getMethod(&quot;add&quot;, Object.class); //调用add()方法 m.invoke(listClass , 100); //遍历集合 for(Object obj : strList){ System.out.println(obj); } } } 控制台输出： aaa bbb 100 ","link":"https://tinaxiawuhao.github.io/post/NjXzyw7sR/"},{"title":"java基础之一集合","content":"基础数据结构说明 ​ 数组：采用一段连续的存储单元来存储数据。对于指定下标的查找，时间复杂度为O(1)；通过给定值进行查找，需要遍历数组，逐一比对给定关键字和数组元素，时间复杂度为O(n)，当然，对于有序数组，则可采用二分查找，插值查找，斐波那契查找等方式，可将查找复杂度提高为O(logn)；对于一般的插入删除操作，涉及到数组元素的移动，其平均复杂度也为O(n) 线性链表：对于链表的新增，删除等操作（在找到指定操作位置后），仅需处理结点间的引用即可，时间复杂度为O(1)，而查找操作需要遍历链表逐一进行比对，复杂度为O(n) 二叉树：对一棵相对平衡的有序二叉树，对其进行插入，查找，删除等操作，平均复杂度均为O(logn)。 哈希表：相比上述几种数据结构，在哈希表中进行添加，删除，查找等操作，性能十分之高，不考虑哈希冲突的情况下，仅需一次定位即可完成，时间复杂度为O(1)，接下来我们就来看看哈希表是如何实现达到惊艳的常数阶O(1)的。 哈希表具体说明 我们知道，数据结构的物理存储结构只有两种：顺序存储结构和链式存储结构（像栈，队列，树，图等是从逻辑结构去抽象的，映射到内存中，也是这两种物理组织形式），而在上面我们提到过，在数组中根据下标查找某个元素，一次定位就可以达到，哈希表利用了这种特性，哈希表的主干就是数组。 比如我们要新增或查找某个元素，我们通过把当前元素的关键字 通过某个函数映射到数组中的某个位置，通过数组下标一次定位就可完成操作。 存储位置 = f(关键字) 其中，这个函数f一般称为哈希函数，这个函数的设计好坏会直接影响到哈希表的优劣。举个例子，比如我们要在哈希表中执行插入操作： 查找操作同理，先通过哈希函数计算出实际存储地址，然后从数组中对应地址取出即可。 哈希冲突 然而万事无完美，如果两个不同的元素，通过哈希函数得出的实际存储地址相同怎么办？也就是说，当我们对某个元素进行哈希运算，得到一个存储地址，然后要进行插入的时候，发现已经被其他元素占用了，其实这就是所谓的哈希冲突，也叫哈希碰撞。前面我们提到过，哈希函数的设计至关重要，好的哈希函数会尽可能地保证 计算简单和散列地址分布均匀但是，我们需要清楚的是，数组是一块连续的固定长度的内存空间，再好的哈希函数也不能保证得到的存储地址绝对不发生冲突。那么哈希冲突如何解决呢？哈希冲突的解决方案有多种:开放定址法（线性探测）（发生冲突，继续寻找下一块未被占用的存储地址），再散列函数法，链地址法。 R-B Tree简介 R-B Tree，全称是Red-Black Tree，又称为“红黑树”，它一种特殊的二叉查找树。红黑树的每个节点上都有存储位表示节点的颜色，可以是红(Red)或黑(Black)。 红黑树的特性: （1）每个节点或者是黑色，或者是红色。 （2）根节点是黑色。 （3）每个叶子节点（NIL）是黑色。 [注意：这里叶子节点，是指为空(NIL或NULL)的叶子节点！] （4）如果一个节点是红色的，则它的子节点必须是黑色的。 （5）从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。 注意： (01) 特性(3)中的叶子节点，是只为空(NIL或null)的节点。 (02) 特性(5)，确保没有一条路径会比其他路径长出俩倍。因而，红黑树是相对是接近平衡的二叉树。 红黑树示意图如下： 性质 红黑树是每个节点都带有颜色属性的二叉查找树，颜色或红色或黑色。在二叉查找树强制一般要求以外，对于任何有效的红黑树我们增加了如下的额外要求： 【1】性质1. 节点是红色或黑色。 【2】性质2. 根节点是黑色。 【3】性质3 每个叶节点是黑色的。 【4】性质4 每个红色节点的两个子节点都是黑色。(从每个叶子到根的所有路径上不能有两个连续的红色节点) 【5】性质5. 从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。 用途和好处 红黑树和AVL树一样都对插入时间、删除时间和查找时间提供了最好可能的最坏情况担保。这不只是使它们在时间敏感的应用如即时应用(real time application)中有价值，而且使它们有在提供最坏情况担保的其他数据结构中作为建造板块的价值；例如，在计算几何中使用的很多数据结构都可以基于红黑树。 ​ 红黑树在函数式编程中也特别有用，在这里它们是最常用的持久数据结构之一，它们用来构造关联数组和集合，在突变之后它们能保持为以前的版本。除了O(log n)的时间之外，红黑树的持久版本对每次插入或删除需要O(log n)的空间。 ​ 红黑树是 2-3-4树的一种等同。换句话说，对于每个 2-3-4 树，都存在至少一个数据元素是同样次序的红黑树。在 2-3-4 树上的插入和删除操作也等同于在红黑树中颜色翻转和旋转。这使得 2-3-4 树成为理解红黑树背后的逻辑的重要工具，这也是很多介绍算法的教科书在红黑树之前介绍 2-3-4 树的原因，尽管 2-3-4 树在实践中不经常使用。 红黑树的数据结构 public class RBTree&lt;T extends Comparable&lt;T&gt;&gt; { private RBTNode&lt;T&gt; mRoot; // 根结点 private static final boolean RED = false; private static final boolean BLACK = true; public class RBTNode&lt;T extends Comparable&lt;T&gt;&gt; { boolean color; // 颜色 T key; // 关键字(键值) RBTNode&lt;T&gt; left; // 左孩子 RBTNode&lt;T&gt; right; // 右孩子 RBTNode&lt;T&gt; parent; // 父结点 public RBTNode(T key, boolean color, RBTNode&lt;T&gt; parent, RBTNode&lt;T&gt; left, RBTNode&lt;T&gt; right) { this.key = key; this.color = color; this.parent = parent; this.left = left; this.right = right; } } ... } 集合说明 ArrayList实现原理要点概括 ArrayList是List接口的可变数组非同步实现，并允许包括null在内的所有元素。 ArrayList的数据结构如下： 说明：底层的数据结构就是数组，数组元素类型为Object类型，即可以存放所有类型数据。我们对ArrayList类的实例的所有的操作底层都是基于数组的。 该集合是可变长度数组，数组扩容时，会将老数组中的元素重新拷贝一份到新的数组中，每次数组容量增长大约是其容量的1.5倍，这种操作的代价很高。 newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1) 采用了Fail-Fast机制，面对并发的修改时，迭代器很快就会完全失败，而不是冒着在将来某个不确定时间发生任意不确定行为的风险 remove方法会让下标到数组末尾的元素向前移动一个单位，并把最后一位的值置空，方便GC LinkedList实现原理要点概括 LinkedList是List接口的双向链表非同步实现，并允许包括null在内的所有元素。从JDK1.7开始，LinkedList 由双向循环链表改为双向链表 LinkedList数据结构如下 说明：如上图所示，LinkedList底层使用的双向链表结构，有一个头结点和一个尾结点，双向链表意味着我们可以从头开始正向遍历，或者是从尾开始逆向遍历，并且可以针对头部和尾部进行相应的操作。 双向链表节点对应的类Node的实例，Node中包含成员变量：prev，next，item。其中，prev是该节点的上一个节点，next是该节点的下一个节点，item是该节点所包含的值。 public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable { transient int size = 0; /** * Pointer to first node. * Invariant: (first == null &amp;&amp; last == null) || * (first.prev == null &amp;&amp; first.item != null) */ transient Node&lt;E&gt; first; /** * Pointer to last node. * Invariant: (first == null &amp;&amp; last == null) || * (last.next == null &amp;&amp; last.item != null) */ transient Node&lt;E&gt; last; /** * 节点结构 */ private static class Node&lt;E&gt; { E item; Node&lt;E&gt; next; Node&lt;E&gt; prev; Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) { this.item = element; this.next = next; this.prev = prev; } } } 说明：LinkedList的属性非常简单，一个头结点、一个尾结点、一个表示链表中实际元素个数的变量。注意，头结点、尾结点都有transient关键字修饰，这也意味着在序列化时该域是不会序列化的。 它的查找是分两半查找，先判断index是在链表的哪一半，然后再去对应区域查找，这样最多只要遍历链表的一半节点即可找到 HashMap实现原理要点概括 HashMap是基于哈希表的Map接口的非同步实现，允许使用null值和null键，但不保证映射的顺序。 底层使用数组实现，数组中每一项是个单向链表，即数组和链表的结合体；当链表长度大于8时，链表转换为红黑树，这样减少链表查询时间。 HashMap在底层将key-value当成一个整体进行处理，这个整体就是一个Node对象。HashMap底层采用一个Node[]数组来保存所有的key-value对，当需要存储一个Node对象时，会根据key的hash算法来决定其在数组中的存储位置，再根据equals方法决定其在该数组位置上的链表中的存储位置；当需要取出一个Node时，也会根据key的hash算法找到其在数组中的存储位置，再根据equals方法从该位置上的链表中取出该Node。 HashMap解决hash冲突是采用了链地址法，也就是数组+链表的方式， HashMap进行数组扩容需要重新计算扩容后每个元素在数组中的位置，很耗性能 采用了Fail-Fast机制，通过一个modCount值记录修改次数，对HashMap内容的修改都将增加这个值。迭代器初始化过程中会将这个值赋给迭代器的expectedModCount，在迭代过程中，判断modCount跟expectedModCount是否相等，如果不相等就表示已经有其他线程修改了Map，马上抛出异常 重写equals方法需同时重写hashCode方法 public static void main(String []args){ HashMap&lt;Person,String&gt; map = new HashMap&lt;Person, String&gt;(); Person person = new Person(1234,&quot;乔峰&quot;); //put到hashmap中去 map.put(person,&quot;天龙八部&quot;); //get取出，从逻辑上讲应该能输出“天龙八部” System.out.println(&quot;结果:&quot;+map.get(new Person(1234,&quot;乔峰&quot;))); } ​ 如果我们已经对HashMap的原理有了一定了解，这个结果就不难理解了。尽管我们在进行get和put操作的时候，使用的key从逻辑上讲是等值的（通过equals比较是相等的），但由于没有重写hashCode方法，所以put操作时，key(hashcode1)--&gt;hash--&gt;indexFor--&gt;最终索引位置 ，而通过key取出value的时候 key(hashcode2)--&gt;hash--&gt;indexFor--&gt;最终索引位置，由于hashcode1不等于hashcode2，导致没有定位到一个数组位置而返回逻辑上错误的值null（也有可能碰巧定位到一个数组位置，但是也会判断其entry的hash值是否相等。） 所以，在重写equals的方法的时候，必须注意重写hashCode方法，同时还要保证通过equals判断相等的两个对象，调用hashCode方法要返回同样的整数值。而如果equals判断不相等的两个对象，其hashCode可以相同（只不过会发生哈希冲突，应尽量避免）。 Hashtable实现原理要点概括 Hashtable是基于哈希表的Map接口的同步实现，不允许使用null值和null键，Hashtable中的映射不是有序的 底层使用数组实现，数组中每一项是个单链表，即数组和链表的结合体 Hashtable在底层将key-value当成一个整体进行处理，这个整体就是一个Entry对象。Hashtable底层采用一个Entry[]数组来保存所有的key-value对，当需要存储一个Entry对象时，会根据key的hash算法来决定其在数组中的存储位置，在根据equals方法决定其在该数组位置上的链表中的存储位置；当需要取出一个Entry时，也会根据key的hash算法找到其在数组中的存储位置，再根据equals方法从该位置上的链表中取出该Entry。 synchronized是针对整张Hash表的，即每次锁住整张表让线程独占 ConcurrentHashMap实现原理要点概括 ConcurrentHashMap允许多个修改操作并发进行，其关键在于使用了锁分离技术。 它使用了多个锁来控制对hash表的不同段进行的修改，每个段其实就是一个小的hashtable，它们有自己的锁。只要多个并发发生在不同的段上，它们就可以并发进行。 ConcurrentHashMap在底层将key-value当成一个整体进行处理，这个整体就是一个Entry对象。Hashtable底层采用一个Entry[]数组来保存所有的key-value对，当需要存储一个Entry对象时，会根据key的hash算法来决定其在数组中的存储位置，在根据equals方法决定其在该数组位置上的链表中的存储位置；当需要取出一个Entry时，也会根据key的hash算法找到其在数组中的存储位置，再根据equals方法从该位置上的链表中取出该Entry。 与HashMap不同的是，ConcurrentHashMap使用多个子Hash表，也就是段(Segment) ConcurrentHashMap完全允许多个读操作并发进行，读操作并不需要加锁。如果使用传统的技术，如HashMap中的实现，如果允许可以在hash链的中间添加或删除元素，读操作不加锁将得到不一致的数据。 ConcurrentHashMap 1.8为什么要使用CAS+Synchronized取代Segment+ReentrantLock ​ 大家应该都知道ConcurrentHashMap在1.8的时候有了很大的改动,当然,我这里要说的改动不是指链表长度大于8就转为红黑树这种常识,我要说的是ConcurrentHashMap在1.8为什么用CAS+Synchronized取代Segment+ReentrantLock了 ​ 首先,我假设你对CAS,Synchronized,ReentrantLock这些知识很了解,并且知道AQS,自旋锁,偏向锁,轻量级锁,重量级锁这些知识,也知道Synchronized和ReentrantLock在唤醒被挂起线程竞争的时候有什么区别 ​ 首先我们说下1.8以前的ConcurrentHashMap是怎么保证线程并发的,首先在初始化ConcurrentHashMap的时候,会初始化一个Segment数组,容量为16,而每个Segment呢,都继承了ReentrantLock类,也就是说每个Segment类本身就是一个锁,之后Segment内部又有一个table数组,而每个table数组里的索引数据呢,又对应着一个Node链表. ​ 那么这样的好处是什么呢?我先从老版本的添加流程说起吧,由于电脑里没有JDK1.7及以下的版本我没法给你看代码,所以使用文字描述的方式,首先,当我们使用put方法的时候,是对我们的key进行hash拿到一个整型,然后将整型对16取模,拿到对应的Segment,之后调用Segment的put方法,然后上锁,请注意,这里lock()的时候其实是this.lock(),也就是说,每个Segment的锁是分开的 ​ 其中一个上锁不会影响另一个,此时也就代表了我可以有十六个线程进来,而ReentrantLock上锁的时候如果只有一个线程进来,是不会有线程挂起的操作的,也就是说只需要在AQS里使用CAS改变一个state的值为1,此时就能对代码进行操作,这样一来,我们等于将并发量/16了. 好,说完了老版本的ConcurrentHashMap,我们再说说新版本的,请看下面的图: ​ 请注意Synchronized上锁的对象,请记住,Synchronized是靠对象的对象头和此对象对应的monitor来保证上锁的,也就是对象头里的重量级锁标志指向了monitor,而monitor呢,内部则保存了一个当前线程,也就是抢到了锁的线程. ​ 那么这里的这个f是什么呢?它是Node链表里的每一个Node,也就是说,Synchronized是将每一个Node对象作为了一个锁,这样做的好处是什么呢?将锁细化了,也就是说,除非两个线程同时操作一个Node,注意,是一个Node而不是一个Node链表哦,那么才会争抢同一把锁. ​ 如果使用ReentrantLock其实也可以将锁细化成这样的,只要让Node类继承ReentrantLock就行了,这样的话调用f.lock()就能做到和Synchronized(f)同样的效果,但为什么不这样做呢? ​ 请大家试想一下,锁已经被细化到这种程度了,那么出现并发争抢的可能性还高吗?还有就是,哪怕出现争抢了,只要线程可以在30到50次自旋里拿到锁,那么Synchronized就不会升级为重量级锁,而等待的线程也就不用被挂起,我们也就少了挂起和唤醒这个上下文切换的过程开销. ​ 但如果是ReentrantLock呢?它则只有在线程没有抢到锁,然后新建Node节点后再尝试一次而已,不会自旋,而是直接被挂起,这样一来,我们就很容易会多出线程上下文开销的代价.当然,你也可以使用tryLock(),但是这样又出现了一个问题,你怎么知道tryLock的时间呢?在时间范围里还好,假如超过了呢? ​ 所以,在锁被细化到如此程度上,使用Synchronized是最好的选择了.这里再补充一句,Synchronized和ReentrantLock他们的开销差距是在释放锁时唤醒线程的数量,Synchronized是唤醒锁池里所有的线程+刚好来访问的线程,而ReentrantLock则是当前线程后进来的第一个线程+刚好来访问的线程. ​ 如果是线程并发量不大的情况下,那么Synchronized因为自旋锁,偏向锁,轻量级锁的原因,不用将等待线程挂起,偏向锁甚至不用自旋,所以在这种情况下要比ReentrantLock高效 1.8弃用分段锁的原因由以下几点： 1. 加入多个分段锁浪费内存空间。 2. 生产环境中， map 在放入时竞争同一个锁的概率非常小，分段锁反而会造成更新等操作的长时间等待。 3. 为了提高 GC 的效率 HashSet实现原理要点概括 HashSet由哈希表(实际上是一个HashMap实例)支持，不保证set的迭代顺序，并允许使用null元素。 对于HashSet而言，它是基于HashMap实现的，HashSet底层使用HashMap来保存所有元素，因此HashSet 的实现比较简单，相关HashSet的操作，基本上都是直接调用底层HashMap的相关方法来完成，API也是对HashMap的行为进行了封装，可参考HashMap LinkedHashMap实现原理要点概括 LinkedHashMap继承于HashMap，底层使用哈希表和双向链表来保存所有元素，并且它是非同步，允许使用null值和null键。 基本操作与父类HashMap相似，通过重写HashMap相关方法，重新定义了数组中保存的元素Entry，来实现自己的链接列表特性。该Entry除了保存当前对象的引用外，还保存了其上一个元素before和下一个元素after的引用，从而构成了双向链接列表。 LinkedHashSet实现原理要点概括 对于LinkedHashSet而言，它继承与HashSet、又基于LinkedHashMap来实现的。LinkedHashSet底层使用LinkedHashMap来保存所有元素，它继承与HashSet，其所有的方法操作上又与HashSet相同。 java集合遍历的几种方式总结及比较 集合类的通用遍历方式, 用迭代器迭代: Iterator it = list.iterator(); while(it.hasNext()) { Object obj = it.next(); } Map遍历方式： 1、通过获取所有的key按照key来遍历 //Set&lt;Integer&gt; set = map.keySet(); //得到所有key的集合 for (Integer in : map.keySet()) { String str = map.get(in);//得到每个key多对用value的值 } 2、通过Map.entrySet使用iterator遍历key和value Iterator&lt;Map.Entry&lt;Integer, String&gt;&gt; it = map.entrySet().iterator(); while (it.hasNext()) { Map.Entry&lt;Integer, String&gt; entry = it.next(); System.out.println(&quot;key= &quot; + entry.getKey() + &quot; and value= &quot; + entry.getValue()); } 3、通过Map.entrySet遍历key和value，推荐，尤其是容量大时 for (Map.Entry&lt;Integer, String&gt; entry : map.entrySet()) { //Map.entry&lt;Integer,String&gt; 映射项（键-值对） 有几个方法：用上面的名字entry //entry.getKey() ;entry.getValue(); entry.setValue(); //map.entrySet() 返回此映射中包含的映射关系的 Set视图。 System.out.println(&quot;key= &quot; + entry.getKey() + &quot; and value= &quot; + entry.getValue()); } 4、通过Map.values()遍历所有的value，但不能遍历key for (String v : map.values()) { System.out.println(&quot;value= &quot; + v); } List遍历方式： 第一种： Iterator iterator = list.iterator(); while(iterator.hasNext()){ int i = (Integer) iterator.next(); System.out.println(i); } 第二种： for (Object object : list) { System.out.println(object); } 第三种： for(int i = 0 ;i&lt;list.size();i++) { int j= (Integer) list.get(i); System.out.println(j); } 每个遍历方法的实现原理是什么？ 传统的for循环遍历，基于计数器的： ​ 遍历者自己在集合外部维护一个计数器，然后依次读取每一个位置的元素，当读取到最后一个元素后，停止。主要就是需要按元素的位置来读取元素。 迭代器遍历，Iterator： ​ 每一个具体实现的数据集合，一般都需要提供相应的Iterator。相比于传统for循环，Iterator取缔了显式的遍历计数器。所以基于顺序存储集合的Iterator可以直接按位置访问数据。而基于链式存储集合的Iterator，正常的实现，都是需要保存当前遍历的位置。然后根据当前位置来向前或者向后移动指针。 foreach循环遍历： ​ 根据反编译的字节码可以发现，foreach内部也是采用了Iterator的方式实现，只不过Java编译器帮我们生成了这些代码。 各遍历方式对于不同的存储方式，性能如何？ 1、传统的for循环遍历，基于计数器的： ​ 因为是基于元素的位置，按位置读取。所以我们可以知道，对于顺序存储，因为读取特定位置元素的平均时间复杂度是O(1)，所以遍历整个集合的平均时间复杂度为O(n)。而对于链式存储，因为读取特定位置元素的平均时间复杂度是O(n)，所以遍历整个集合的平均时间复杂度为O(n2)（n的平方）。 ArrayList按位置读取的代码：直接按元素位置读取。 transient Object[] elementData; public E get(int index) { rangeCheck(index); return elementData(index); } E elementData(int index) { return (E) elementData[index]; } LinkedList按位置读取的代码：每次都需要从第0个元素开始向后读取。其实它内部也做了小小的优化。 transient int size = 0; transient Node&lt;E&gt; first; transient Node&lt;E&gt; last; public E get(int index) { checkElementIndex(index); return node(index).item; } Node&lt;E&gt; node(int index) { if (index &lt; (size &gt;&gt; 1)) { //查询位置在链表前半部分，从链表头开始查找 Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; } else { //查询位置在链表后半部分，从链表尾开始查找 Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; } } 2、迭代器遍历，Iterator： ​ 那么对于RandomAccess类型的集合来说，没有太多意义，反而因为一些额外的操作，还会增加额外的运行时间。但是对于Sequential Access的集合来说，就有很重大的意义了，因为Iterator内部维护了当前遍历的位置，所以每次遍历，读取下一个位置并不需要从集合的第一个元素开始查找，只要把指针向后移一位就行了，这样一来，遍历整个集合的时间复杂度就降低为O(n)； （这里只用LinkedList做例子）LinkedList的迭代器，内部实现，就是维护当前遍历的位置，然后操作指针移动就可以了： public E next() { checkForComodification(); if (!hasNext()) throw new NoSuchElementException(); lastReturned = next; next = next.next; nextIndex++; return lastReturned.item; } public E previous() { checkForComodification(); if (!hasPrevious()) throw new NoSuchElementException(); lastReturned = next = (next == null) ? last : next.prev; nextIndex--; return lastReturned.item; } 3、foreach循环遍历： ​ 分析Java字节码可知，foreach内部实现原理，也是通过Iterator实现的，只不过这个Iterator是Java编译器帮我们生成的，所以我们不需要再手动去编写。但是因为每次都要做类型转换检查，所以花费的时间比Iterator略长。时间复杂度和Iterator一样。 Iterator和foreach字节码如下： //使用Iterator的字节码： Code: 0: new #16 // class java/util/ArrayList 3: dup 4: invokespecial #18 // Method java/util/ArrayList.&quot;&lt;init&gt;&quot;:()V 7: astore_1 8: aload_1 9: invokeinterface #19, 1 // InterfaceMethod java/util/List.iterator:()Ljava/util/Iterator; 14: astore_2 15: goto 25 18: aload_2 19: invokeinterface #25, 1 // InterfaceMethod java/util/Iterator.next:()Ljava/lang/Object; 24: pop 25: aload_2 26: invokeinterface #31, 1 // InterfaceMethod java/util/Iterator.hasNext:()Z 31: ifne 18 34: return //使用foreach的字节码： Code: 0: new #16 // class java/util/ArrayList 3: dup 4: invokespecial #18 // Method java/util/ArrayList.&quot;&lt;init&gt;&quot;:()V 7: astore_1 8: aload_1 9: invokeinterface #19, 1 // InterfaceMethod java/util/List.iterator:()Ljava/util/Iterator; 14: astore_3 15: goto 28 18: aload_3 19: invokeinterface #25, 1 // InterfaceMethod java/util/Iterator.next:()Ljava/lang/Object; 24: checkcast #31 // class loop/Model 27: astore_2 28: aload_3 29: invokeinterface #33, 1 // InterfaceMethod java/util/Iterator.hasNext:()Z 34: ifne 18 37: return 各遍历方式的适用于什么场合？ 传统的for循环遍历，基于计数器的： ​ 顺序存储：读取性能比较高。适用于遍历顺序存储集合。 ​ 链式存储：时间复杂度太大，不适用于遍历链式存储的集合。 迭代器遍历，Iterator： ​ 顺序存储：如果不是太在意时间，推荐选择此方式，毕竟代码更加简洁，也防止了Off-By-One的问题。 ​ 链式存储：意义就重大了，平均时间复杂度降为O(n)，还是挺诱人的，所以推荐此种遍历方式。 foreach循环遍历： ​ foreach只是让代码更加简洁了，但是他有一些缺点，就是遍历过程中不能操作数据集合（删除等），所以有些场合不使用。而且它本身就是基于Iterator实现的，但是由于类型转换的问题，所以会比直接使用Iterator慢一点，但是还好，时间复杂度都是一样的。所以怎么选择，参考上面两种方式，做一个折中的选择。 RandomAccess接口标记 Java数据集合框架中，提供了一个RandomAccess接口，该接口没有方法，只是一个标记。通常被List接口的实现使用，用来标记该List的实现是否支持Random Access。 一个数据集合实现了该接口，就意味着它支持Random Access，按位置读取元素的平均时间复杂度为O(1)。比如ArrayList。 而没有实现该接口的，就表示不支持Random Access。比如LinkedList。 所以看来JDK开发者也是注意到这个问题的，那么推荐的做法就是，如果想要遍历一个List，那么先判断是否支持Random Access，也就是 list instanceof RandomAccess。 比如： if (list instanceof RandomAccess) { //使用传统的for循环遍历。 } else { //使用Iterator或者foreach。 } Array、List、Set互转 Array、List互转 Array 转List String[] s = new String[]{&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;,&quot;E&quot;}; List&lt;String&gt; list = Arrays.asList(s); # 注意这里list里面的元素直接是s里面的元素( list backed by the specified array)，换句话就是说：对s的修改，直接影响list。 s[0] =&quot;AA&quot;; System.out.println(&quot;list: &quot; + list); # 输出结果 list: [AA, B, C, D, E] List转Array String[] dest = list.toArray(new String[0]);//new String[0]是指定返回数组的类型 System.out.println(&quot;dest: &quot; + Arrays.toString(dest)); # 输出结果 dest: [AA, B, C, D, E] # 注意这里的dest里面的元素不是list里面的元素，换句话就是说：对list中关于元素的修改，不会影响dest。 list.set(0, &quot;Z&quot;); System.out.println(&quot;modified list: &quot; + list); System.out.println(&quot;dest: &quot; + Arrays.toString(dest)); # 输出结果 modified list: [Z, B, C, D, E] dest: [AA, B, C, D, E] # 可以看到list虽然被修改了，但是dest数组没有没修改。 List、Set互转 因为List和Set都实现了Collection接口，且addAll(Collection&lt;? extends E&gt; c);方法，因此可以采用addAll()方法将List和Set互相转换；另外，List和Set也提供了Collection&lt;? extends E&gt; c作为参数的构造函数，因此通常采用构造函数的形式完成互相转化。 //List转Set Set&lt;String&gt; set = new HashSet&lt;&gt;(list); System.out.println(&quot;set: &quot; + set); //Set转List List&lt;String&gt; list_1 = new ArrayList&lt;&gt;(set); System.out.println(&quot;list_1: &quot; + list_1); # 和toArray()一样，被转换的List(Set)的修改不会对被转化后的Set（List）造成影响。 Array、Set互转 //array转set s = new String[]{&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;,&quot;E&quot;}; set = new HashSet&lt;&gt;(Arrays.asList(s)); System.out.println(&quot;set: &quot; + set); //set转array dest = set.toArray(new String[0]); System.out.println(&quot;dest: &quot; + Arrays.toString(dest)); Java 中初始化 List 集合的 6 种方式! 常规方式 # 后面缺失的泛型类型在 JDK 7 之后就可以不用写具体的类型了，改进后会自动推断类型 List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(&quot;1&quot;); list.add(&quot;2&quot;); list.add(&quot;3&quot;); Arrays 工具类 import static java.util.Arrays.asList; List&lt;String&gt; list = asList(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;); # 注意，上面的 asList 是 Arrays 的静态方法，这里使用了静态导入。这种方式添加的是不可变的 List, 即不能添加、删除等操作，需要警惕。。 # 如果要可变，那就使用 ArrayList 再包装一下，如下面所示。 List&lt;String&gt; numbers = new ArrayList&lt;&gt;(Arrays.asList(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;)); numbers.add(&quot;4&quot;); Collections 工具类 List&lt;String&gt; list = Collections.nCopies(3, &quot;list&quot;); # 这种方式添加的是不可变的、复制某个元素N遍的工具类，以上程序输出： # [list, list, list] # 老规则，如果要可变，使用 ArrayList 包装一遍。 List&lt;String&gt; list = new ArrayList&lt;&gt;(Collections.nCopies(3, &quot;list&quot;)); list.add(&quot;list&quot;); # 还有初始化单个对象的 List 工具类，这种方式也是不可变的，集合内只能有一个元素，这种也用得很少啊。 List&lt;String&gt; list = Collections.singletonList(&quot;list&quot;); # 还有一个创建空 List 的工具类，没有默认容量，节省空间，但不知道实际工作中没有用。 List&lt;String&gt; list = Collections.emptyList(&quot;list&quot;); 匿名内部类 List&lt;String&gt; list = new ArrayList&lt;&gt;() {{ add(&quot;1&quot;); add(&quot;2&quot;); add(&quot;3&quot;); }}; 这种其实有效率和内存泄漏的问题，参考： https://blog.csdn.net/xukun5137/article/details/78275201 https://www.cnblogs.com/wenbronk/p/7000643.html JDK8 Stream import static java.util.stream.Collectors.toList; List&lt;String&gt; list = Stream.of(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;).collect(toList()); JDK 9 List.of List&lt;String&gt; list = List.of(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;); 这是 JDK 9 里面新增的 List 接口里面的静态方法，同样也是不可变的。 ","link":"https://tinaxiawuhao.github.io/post/FRRzAbqMV/"},{"title":"Java线程同步五种方法","content":"线程同步 即当有一个线程在对内存进行操作时，其他线程都不可以对这个内存地址进行操作，直到该线程完成操作， 其他线程才能对该内存地址进行操作，而其他线程又处于等待状态，实现线程同步的方法有很多，下面介绍java线程同步的五种方法。 同步方法 使用synchronized关键字修饰的方法。由于java的每个对象都有一个内置锁，当用关键字修饰此方法时，内置锁会保护整个方法。在调用该方法前，需要获得内置锁，否则该线程就处于阻塞状态。 public synchronized void method(){} # synchronized关键字也可以修饰静态方法，此时如果调用该静态方法，将会锁住整个类。 public static synchronized void method(){} 同步代码块 即有synchronized关键字修饰的语句块。被关键字修饰的的语句块会自动加上内置锁，从而实现同步。 synchronized(object){ # 代码 } 注：同步是一种高开销的操作，因此应该尽量减少同步的内容。通常没有必要去同步整个方法，使用关键字synchronized修饰关键代码即可。 使用特殊域变量修饰符volatile实现线程同步 （1）volatile关键字是非锁，比synchronized稍弱的同步机制 （2）保证此变量对所有的线程的可见性，当一个线程修改了这个变量的值，volatile 保证了新值能立即同步到主内存，以及每次使用前立即从主内存刷新。 （3）volatile不会提供原子操作，并不能保证线程安全，也不能用来修饰final类型的变量。 （4）禁止指令重排序优化。有volatile修饰的变量，赋值后多执行了一个添加内存屏障操作（指令重排序时不能把后面的指令重排序到内存屏障之前的位置） public class Test extends Thread { volatile int x = 0; //此处可以将volatile去除 或者 替换为 static，经过对比可看出volatile的作用 private void write() { x = 5; } private void read() { while (x != 5) {} if(x == 5){ System.out.println(&quot;------stoped&quot;); } } public static void main(String[] args) throws Exception { Test example = new Test(); Thread writeThread = new Thread(new Runnable() { public void run() { example.write(); } }); Thread readThread = new Thread(new Runnable() { public void run() { example.read(); } }); readThread.start(); TimeUnit.SECONDS.sleep(5); //记住此处一定要暂停5秒，以保证writeThread一定会在readThread中执行 System.out.println(&quot;------&quot;); writeThread.start(); } } 注：多线程中的非同步问题主要出现在对域的读写上，如果域自身避免这个问题，那么就不需要修改操作该域的方法。用final域，有锁保护的域可以避免非同步的问题。 使用重入锁ReentrantLock实现线程同步 jdk1.5中java.util.concurrent包下ReentrantLock类是可重入、互斥、实现了Lock接口的锁。它与synchronized修饰的方法具有相同的基本行为与语义，并且扩展了其能力。 ReentrantLock类的常用方法有： （1）ReentrantLock()：创建一个ReentrantLock实例。 （2）lock():获得锁 （3）unlock()：释放锁 上述代码可以修改为： public class Test extends Thread { private int x = 0; /** * 使用可重入锁 */ private Lock lock=new ReentrantLock(); private void write() { lock.lock(); try { x = 5; } catch (Exception e) { // TODO Auto-generated catch block e.printStackTrace(); }finally { lock.unlock();//释放锁 } } private void read() { lock.lock(); try { while (x != 5) {} if(x == 5){ System.out.println(&quot;------stoped&quot;); } } catch (Exception e) { // TODO Auto-generated catch block e.printStackTrace(); }finally { lock.unlock();//释放锁 } } public static void main(String[] args) throws Exception { Test example = new Test(); Thread writeThread = new Thread(new Runnable() { public void run() { example.write(); } }); Thread readThread = new Thread(new Runnable() { public void run() { example.read(); } }); readThread.start(); TimeUnit.SECONDS.sleep(5); //记住此处一定要暂停5秒，以保证writeThread一定会在readThread中执行 System.out.println(&quot;------&quot;); writeThread.start(); } } 注： ReentrantLock()还可以通过public ReentrantLock(boolean fair)构造方法创建公平锁，即，优先运行等待时间最长的线程，这样大幅度降低程序运行效率。 关于Lock对象和synchronized关键字的选择： （1）最好两个都不用，使用一种java.util.concurrent包提供的机制，能够帮助用户处理所有与锁相关的代码。 （2）如果synchronized关键字能够满足用户的需求，就用synchronized，他能简化代码。 （3）如果需要使用更高级的功能，就用ReentrantLock类，此时要注意及时释放锁，否则会出现死锁，通常在finally中释放锁。 使用ThreadLocal管理局部变量实现线程同步 ThreadLocal管理变量，则每一个使用该变量的线程都获得一个该变量的副本，副本之间相互独立，这样每一个线程都可以随意修改自己的副本，而不会对其他线程产生影响。 ThreadLocal类常用的方法： get()：返回该线程局部变量的当前线程副本中的值。 initialValue()：返回此线程局部变量的当前线程的”初始值“。 remove()：移除此线程局部变量当前线程的值。 set(T value)：将此线程局部变量的当前线程副本中的值设置为指定值value。 上述代码修改为： public class Test extends Thread { private static ThreadLocal&lt;Integer&gt; count=new ThreadLocal&lt;Integer&gt;(){ @Override protected Integer initialValue() { // TODO Auto-generated method stub return 0; } }; private void write() { count.set(count.get()+5); } private void read() { while (count.get() != 5) {} if(count.get() == 5){ System.out.println(&quot;------stoped&quot;); } } public static void main(String[] args) throws Exception { Test example = new Test(); Thread writeThread = new Thread(new Runnable() { public void run() { example.write(); } }); Thread readThread = new Thread(new Runnable() { public void run() { example.read(); } }); readThread.start(); TimeUnit.SECONDS.sleep(5); //记住此处一定要暂停5秒，以保证writeThread一定会在readThread中执行 System.out.println(&quot;------&quot;); writeThread.start(); } } ","link":"https://tinaxiawuhao.github.io/post/fE1AHZqDl/"},{"title":"ThreadLocal是什么","content":" ThreadLocal是一个本地线程副本变量工具类。主要用于将私有线程和该线程存放的副本对象做一个映射，各个线程之间的变量互不干扰，在高并发场景下，可以实现无状态的调用，特别适用于各个线程依赖不通的变量值完成操作的场景。 数据结构 下图为ThreadLocal的内部结构图 ThreadLocal结构内部核心机制: 每个Thread线程内部都有一个Map。 Map里面存储线程本地对象（key）和线程的变量副本（value） Thread内部的Map是由ThreadLocal维护的，由ThreadLocal负责向map获取和设置线程的变量值。 所以对于不同的线程，每次获取副本值时，别的线程并不能获取到当前线程的副本值，形成了副本的隔离，互不干扰。 Thread线程内部的Map在类中描述如下： public class Thread implements Runnable { /* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ ThreadLocal.ThreadLocalMap threadLocals = null; } 深入解析ThreadLocal ThreadLocal类提供如下几个核心方法： public T get() public void set(T value) public void remove() # get()方法用于获取当前线程的副本变量值。 # set()方法用于保存当前线程的副本变量值。 # initialValue()为当前线程初始副本变量值。 # remove()方法移除当前前程的副本变量值。 get()方法 /** * Returns the value in the current thread's copy of this * thread-local variable. If the variable has no value for the * current thread, it is first initialized to the value returned * by an invocation of the {@link #initialValue} method. * * @return the current thread's value of this thread-local */ public T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) return (T)e.value; } return setInitialValue(); } ThreadLocalMap getMap(Thread t) { return t.threadLocals; } private T setInitialValue() { T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value; } protected T initialValue() { return null; } 步骤： 获取当前线程的ThreadLocalMap对象threadLocals 从map中获取线程存储的K-V Entry节点。 从Entry节点获取存储的Value副本值返回。 map为空的话返回初始值null，即线程变量副本为null，在使用时需要注意判断NullPointerException。 set()方法 /** * Sets the current thread's copy of this thread-local variable * to the specified value. Most subclasses will have no need to * override this method, relying solely on the {@link #initialValue} * method to set the values of thread-locals. * * @param value the value to be stored in the current thread's copy of * this thread-local. */ public void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); } ThreadLocalMap getMap(Thread t) { return t.threadLocals; } void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue); } 步骤： 获取当前线程的成员变量map map非空，则重新将ThreadLocal和新的value副本放入到map中。 map空，则对线程的成员变量ThreadLocalMap进行初始化创建，并将ThreadLocal和value副本放入map中。 remove()方法 remove方法比较简单，不做赘述。 ThreadLocalMap ThreadLocalMap是ThreadLocal的内部类，没有实现Map接口，用独立的方式实现了Map的功能，其内部的Entry也独立实现。 ThreadLocalMap类图 在ThreadLocalMap中，也是用Entry来保存K-V结构数据的。但是Entry中key只能是ThreadLocal对象，这点被Entry的构造方法已经限定死了。 static class Entry extends WeakReference&lt;ThreadLocal&gt; { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal k, Object v) { super(k); value = v; } } Entry继承自WeakReference（弱引用，生命周期只能存活到下次GC前），但只有Key是弱引用类型的，Value并非弱引用。 ThreadLocalMap的成员变量： static class ThreadLocalMap { /** * The initial capacity -- MUST be a power of two. */ private static final int INITIAL_CAPACITY = 16; /** * The table, resized as necessary. * table.length MUST always be a power of two. */ private Entry[] table; /** * The number of entries in the table. */ private int size = 0; /** * The next size value at which to resize. */ private int threshold; // Default to 0 } Hash冲突怎么解决 和HashMap的最大的不同在于，ThreadLocalMap结构非常简单，没有next引用，也就是说ThreadLocalMap中解决Hash冲突的方式并非链表的方式，而是采用线性探测的方式，所谓线性探测，就是根据初始key的hashcode值确定元素在table数组中的位置，如果发现这个位置上已经有其他key值的元素被占用，则利用固定的算法寻找一定步长的下个位置，依次判断，直至找到能够存放的位置。 ThreadLocalMap解决Hash冲突的方式就是简单的步长加1或减1，寻找下一个相邻的位置。 /** * Increment i modulo len. */ private static int nextIndex(int i, int len) { return ((i + 1 &lt; len) ? i + 1 : 0); } /** * Decrement i modulo len. */ private static int prevIndex(int i, int len) { return ((i - 1 &gt;= 0) ? i - 1 : len - 1); } 显然ThreadLocalMap采用线性探测的方式解决Hash冲突的效率很低，如果有大量不同的ThreadLocal对象放入map中时发送冲突，或者发生二次冲突，则效率很低。 所以这里引出的良好建议是：每个线程只存一个变量，这样的话所有的线程存放到map中的Key都是相同的ThreadLocal，如果一个线程要保存多个变量，就需要创建多个ThreadLocal，多个ThreadLocal放入Map中时会极大的增加Hash冲突的可能。 ThreadLocalMap的问题 由于ThreadLocalMap的key是弱引用，而Value是强引用。这就导致了一个问题，ThreadLocal在没有外部对象强引用时，发生GC时弱引用Key会被回收，而Value不会回收，如果创建ThreadLocal的线程一直持续运行，那么这个Entry对象中的value就有可能一直得不到回收，发生内存泄露。 如何避免泄漏 既然Key是弱引用，那么我们要做的事，就是在调用ThreadLocal的get()、set()方法时完成后再调用remove方法，将Entry节点和Map的引用关系移除，这样整个Entry对象在GC Roots分析后就变成不可达了，下次GC的时候就可以被回收。 如果使用ThreadLocal的set方法之后，没有显示的调用remove方法，就有可能发生内存泄露，所以养成良好的编程习惯十分重要，使用完ThreadLocal之后，记得调用remove方法。 ThreadLocal&lt;Session&gt; threadLocal = new ThreadLocal&lt;Session&gt;(); try { threadLocal.set(new Session(1, &quot;Misout的博客&quot;)); // 其它业务逻辑 } finally { threadLocal.remove(); } 应用场景 还记得Hibernate的session获取场景吗？ private static final ThreadLocal&lt;Session&gt; threadLocal = new ThreadLocal&lt;Session&gt;(); //获取Session public static Session getCurrentSession(){ Session session = threadLocal.get(); //判断Session是否为空，如果为空，将创建一个session，并设置到本地线程变量中 try { if(session ==null&amp;&amp;!session.isOpen()){ if(sessionFactory==null){ rbuildSessionFactory();// 创建Hibernate的SessionFactory }else{ session = sessionFactory.openSession(); } } threadLocal.set(session); } catch (Exception e) { // TODO: handle exception } return session; } 为什么？每个线程访问数据库都应当是一个独立的Session会话，如果多个线程共享同一个Session会话，有可能其他线程关闭连接了，当前线程再执行提交时就会出现会话已关闭的异常，导致系统异常。此方式能避免线程争抢Session，提高并发下的安全性。 使用ThreadLocal的典型场景正如上面的数据库连接管理，线程会话管理等场景，只适用于独立变量副本的情况，如果变量为全局共享的，则不适用在高并发下使用。 总结 每个ThreadLocal只能保存一个变量副本，如果想要上线一个线程能够保存多个副本以上，就需要创建多个ThreadLocal。 ThreadLocal内部的ThreadLocalMap键为弱引用，会有内存泄漏的风险。 适用于无状态，副本变量独立后不影响业务逻辑的高并发场景。如果如果业务逻辑强依赖于副本变量，则不适合用ThreadLocal解决，需要另寻解决方案。 ","link":"https://tinaxiawuhao.github.io/post/cQAJ3QFSZ/"},{"title":"HashMap负载因子为什么是0.75，链表转红黑树阈值为什么是8","content":"1.HashMap负载因子为什么是0.75 这是时间与空间成本上的折中。 1.1时间成本 假设负载因子是1时，虽然空间利用率高了，但是随之提高的是哈希碰撞的概率。 而Hashmap中哈希碰撞的解决方法采用的拉链法，哈希冲突高了会导致链表越来越长（虽然后面会转换成红黑树），我们知道链表的查询效率是比较低的，所以负载因子太高会导致时间成本上升。 1.2空间成本 那么为了减少哈希冲突，提高查询效率，负载因子是不是越低越好呢？答案显然是否定的。 假设负载因子为0.5时，那么空间利用率只有50%。例如大小为64时，至少有32没有被利用，大小为1024时，就有512没有被利用。扩容后的空间越大，空出的空间也就越大。 所以Java开发人员经过权衡，负载因子不能太大也不能太小，折中选择为0.75。 2.链表转红黑树阈值为什么是8 当负载因子是0.75的情况下，哈希碰撞的概率遵循参数约为0.5的泊松分布 这是概率论的范畴，不知道的同学只需要记住当负载因子是0.75的情况下，我们能够计算出哈希碰撞的概率 Because TreeNodes are about twice the size of regular nodes, we use them only when bins contain enough nodes to warrant use (see TREEIFY_THRESHOLD). And when they become too small (due to removal or resizing) they are converted back to plain bins. In usages with well-distributed user hashCodes, tree bins are rarely used. Ideally, under random hashCodes, the frequency of nodes in bins follows a Poisson distribution (http://en.wikipedia.org/wiki/Poisson_distribution) with a parameter of about 0.5 on average for the default resizing threshold of 0.75, although with a large variance because of resizing granularity. Ignoring variance, the expected occurrences of list size k are (exp(-0.5)*pow(0.5, k)/factorial(k)). The first values are: 0: 0.60653066 1: 0.30326533 2: 0.07581633 3: 0.01263606 4: 0.00157952 5: 0.00015795 6: 0.00001316 7: 0.00000094 8: 0.00000006 more: less than 1 in ten million 在HashMap源码中，给出了计算出的哈希碰撞的概率。 我们看到碰撞8次(链表长度达到8)的概率为0.00000006,几乎是一个不可能事件。 所以Java开发人员将链表转红黑树阈值默认设为8，是为了避免链表转红黑树这种耗时操作的事件发生。 ","link":"https://tinaxiawuhao.github.io/post/mdZO-HjSu/"},{"title":"java中静态代码块，非静态代码块，构造函数之间的执行顺序","content":"它们之间的执行顺序为：静态代码块—&gt;非静态代码块—&gt;构造方法。 静态代码块只在第一次加载类的时候执行一次，之后不再执行；而非静态代码块和构造函数都是在每new一次就执行一次，只不过非静态代码块在构造函数之前执行而已。 如果存在子类，则加载顺序为先父类后子类。 看如下的代码： package com.ykp.test; class ClassA { public ClassA() { System.out.println(&quot;父类构造函数&quot;); } { System.out.println(&quot;父类非静态代码块1&quot;); } { System.out.println(&quot;父类非静态代码块2&quot;); } static { System.out.println(&quot;父类静态代码块 1&quot;); } static { System.out.println(&quot;父类静态代码块 2&quot;); } } public class ClassB extends ClassA { public ClassB() { System.out.println(&quot;子类构造函数&quot;); } { System.out.println(&quot;子类非静态代码块2&quot;); } { System.out.println(&quot;子类非静态代码块1&quot;); } static { System.out.println(&quot;子类静态代码块 2&quot;); } static { System.out.println(&quot;子类静态代码块 1&quot;); } public static void main(String[] args) { System.out.println(&quot;....主方法开始....&quot;); new ClassB(); System.out.println(&quot;************&quot;); new ClassB(); System.out.println(&quot;....主方法结束....&quot;); } } 执行结果： 父类静态代码块 1 父类静态代码块 2 子类静态代码块 2 子类静态代码块 1 ....主方法开始.... 父类非静态代码块1 父类非静态代码块2 父类构造函数 子类非静态代码块2 子类非静态代码块1 子类构造函数 ************ 父类非静态代码块1 父类非静态代码块2 父类构造函数 子类非静态代码块2 子类非静态代码块1 子类构造函数 ....主方法结束.... 从结果可以看出，首先加载类，加载的时候先加载父类，然后子类，类加载的时候就执行静态代码快，也就是说静态代码块是在类加载的时候就加载的，而且只加载一次。如果存在多个执行顺序按照代码的先后来。 对于非静态代码块，是在new的时候加载的，只是在构造函数之前加载而已。如果存在多个执行顺序按照代码的先后来。 ","link":"https://tinaxiawuhao.github.io/post/8H8sHTUTf/"},{"title":"指令重排序","content":"数据依赖性 如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。数据依赖分下列三种类型： 名称 代码示例 说明 写后读 a = 1;b = a; 写一个变量之后，再读这个位置。 写后写 a = 1;a = 2; 写一个变量之后，再写这个变量。 读后写 a = b;b = 1; 读一个变量之后，再写这个变量。 上面三种情况，只要重排序两个操作的执行顺序，程序的执行结果将会被改变。 前面提到过，编译器和处理器可能会对操作做重排序。编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序。 注意，这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作，不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑。 as-if-serial 语义 as-if-serial 语义的意思指：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器，runtime 和处理器都必须遵守 as-if-serial 语义。 为了遵守 as-if-serial 语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作可能被编译器和处理器重排序。为了具体说明，请看下面计算圆面积的代码示例： double pi = 3.14; //A double r = 1.0; //B double area = pi * r * r; //C 上面三个操作的数据依赖关系如下图所示： 如上图所示，A 和 C 之间存在数据依赖关系，同时 B 和 C 之间也存在数据依赖关系。因此在最终执行的指令序列中，C 不能被重排序到 A 和 B 的前面（C 排到 A 和 B 的前面，程序的结果将会被改变）。但 A 和 B 之间没有数据依赖关系，编译器和处理器可以重排序 A 和 B 之间的执行顺序。下图是该程序的两种执行顺序： as-if-serial 语义把单线程程序保护了起来，遵守 as-if-serial 语义的编译器，runtime 和处理器共同为编写单线程程序的程序员创建了一个幻觉：单线程程序是按程序的顺序来执行的。as-if-serial 语义使单线程程序员无需担心重排序会干扰他们，也无需担心内存可见性问题。 程序顺序规则 根据 happens- before 的程序顺序规则，上面计算圆的面积的示例代码存在三个 happens- before 关系： A happens- before B； B happens- before C； A happens- before C； 这里的第3个 happens- before 关系，是根据 happens- before 的传递性推导出来的。 这里 A happens- before B，但实际执行时 B 却可以排在 A 之前执行（看上面的重排序后的执行顺序）。在第一章提到过，如果 A happens- before B，JMM 并不要求 A 一定要在 B 之前执行。JMM 仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前。这里操作 A 的执行结果不需要对操作 B 可见；而且重排序操作 A 和操作 B 后的执行结果，与操作 A 和操作 B 按 happens- before 顺序执行的结果一致。在这种情况下，JMM 会认为这种重排序并不非法（not illegal），JMM 允许这种重排序。 在计算机中，软件技术和硬件技术有一个共同的目标：在不改变程序执行结果的前提下，尽可能的开发并行度。编译器和处理器遵从这一目标，从 happens- before 的定义我们可以看出，JMM 同样遵从这一目标。 重排序对多线程的影响 现在让我们来看看，重排序是否会改变多线程程序的执行结果。请看下面的示例代码： class ReorderExample { int a = 0; boolean flag = false; public void writer() { a = 1; //1 flag = true; //2 } Public void reader() { if (flag) { //3 int i = a * a; //4 …… } } } flag 变量是个标记，用来标识变量 a 是否已被写入。这里假设有两个线程 A 和 B，A 首先执行writer() 方法，随后 B 线程接着执行 reader() 方法。线程B在执行操作4时，能否看到线程 A 在操作1对共享变量 a 的写入？ 答案是：不一定能看到。 由于操作1和操作2没有数据依赖关系，编译器和处理器可以对这两个操作重排序；同样，操作3和操作4没有数据依赖关系，编译器和处理器也可以对这两个操作重排序。让我们先来看看，当操作1和操作2重排序时，可能会产生什么效果？请看下面的程序执行时序图： 如上图所示，操作1和操作2做了重排序。程序执行时，线程A首先写标记变量 flag，随后线程 B 读这个变量。由于条件判断为真，线程 B 将读取变量a。此时，变量 a 还根本没有被线程 A 写入，在这里多线程程序的语义被重排序破坏了！ ※注：本文统一用红色的虚箭线表示错误的读操作，用绿色的虚箭线表示正确的读操作。 下面再让我们看看，当操作3和操作4重排序时会产生什么效果（借助这个重排序，可以顺便说明控制依赖性）。下面是操作3和操作4重排序后，程序的执行时序图： 在程序中，操作3和操作4存在控制依赖关系。当代码中存在控制依赖性时，会影响指令序列执行的并行度。为此，编译器和处理器会采用猜测（Speculation）执行来克服控制相关性对并行度的影响。以处理器的猜测执行为例，执行线程 B 的处理器可以提前读取并计算 a*a，然后把计算结果临时保存到一个名为重排序缓冲（reorder buffer ROB）的硬件缓存中。当接下来操作3的条件判断为真时，就把该计算结果写入变量i中。 从图中我们可以看出，猜测执行实质上对操作3和4做了重排序。重排序在这里破坏了多线程程序的语义！ 在单线程程序中，对存在控制依赖的操作重排序，不会改变执行结果（这也是 as-if-serial 语义允许对存在控制依赖的操作做重排序的原因）；但在多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。 编译器指令重排 下面我们简单看一个编译器重排的例子： 线程 1 线程 2 1：x2 = a ; 3: x1 = b ; 2: b = 1; 4: a = 2 ; 两个线程同时执行，分别有1、2、3、4四段执行代码，其中1、2属于线程1 ， 3、4属于线程2 ，从程序的执行顺序上看，似乎不太可能出现x1 = 1 和x2 = 2 的情况，但实际上这种情况是有可能发现的，因为如果编译器对这段程序代码执行重排优化后，可能出现下列情况 线程 1 线程 2 2: b = 1; 4: a = 2 ; 1：x2 = a ; 3: x1 = b ; 这种执行顺序下就有可能出现x1 = 1 和x2 = 2 的情况，这也就说明在多线程环境下，由于编译器优化重排的存在，两个线程中使用的变量能否保证一致性是无法确定的。 源代码和Runtime时执行的代码很可能不一样，这是因为编译器、处理器常常会为了追求性能对改变执行顺序。然而改变顺序执行很危险，很有可能使得运行结果和预想的不一样，特别是当重排序共享变量时。 从源代码到Runtime需要经过三步的重排序： 编译器重排序 为了提高性能，在不改变单线程的执行结果下，可以改变语句执行顺序。 比如尽可能的减少寄存器的读写次数，充分利用局部性。像下面这段代码这样，交替的读x、y，会导致寄存器频繁的交替存储x和y，最糟的情况下寄存器要存储3次x和3次y。如果能让x的一系列操作一块做完，y的一块做完，理想情况下寄存器只需要存储1次x和1次y。 //优化前 int x = 1; int y = 2; int a1 = x * 1; int b1 = y * 1; int a2 = x * 2; int b2 = y * 2; int a3 = x * 3; int b3 = y * 3; //优化后 int x = 1; int y = 2; int a1 = x * 1; int a2 = x * 2; int a3 = x * 3; int b1 = y * 1; int b2 = y * 2; int b3 = y * 3; 指令重排序 指令重排序是处理器层面做的优化。处理器在执行时往往会因为一些限制而等待，如访存的地址不在cache中发生miss，这时就需要到内存甚至外存去取，然而内存和外区的读取速度比CPU执行速度慢得多。 早期处理器是顺序执行(in-order execution)的，在内存、外存读取数据这段时间，处理器就一直处于等待状态。现在处理器一般都是乱序执行(out-of-order execution)，处理器会在等待数据的时候去执行其他已准备好的操作，不会让处理器一直等待。 满足乱序执行的条件： 该缓存的操作数缓存好 有空闲的执行单元 对于下面这段汇编代码，操作1如果发生cache miss，则需要等待读取内存外存。看看有没有能优先执行的指令，操作2依赖于操作1，不能被优先执行，操作3不依赖1和2，所以能优先执行操作3。 所以实际执行顺序是3&gt;1&gt;2 LDR R1, [R0];//操作1 ADD R2, R1, R1;//操作2 ADD R3, R4, R4;//操作3 内存系统重排序 由于处理器有读、写缓存区，写缓存区没有及时刷新到内存，造成其他处理器读到的值不是最新的，使得处理器执行的读写操作与内存上反应出的顺序不一致。 如下面这个例子，可能造成处理器A读到的b=0，处理器B读到的a=0。A1写a=1先写到处理器A的写缓存区中，此时内存中a=0。如果这时处理器B从内存中读a，读到的将是0。 以处理器A来说，处理器A执行的顺序是A1&gt;A2&gt;A3，但是由于写缓存区没有及时刷新到内存，所以实际顺序为A2&gt;A1&gt;A3。 初始化： a = 0; b = 0; 处理器A执行 a = 1; //A1 read(b); //A2 处理器B执行 b = 2; //B1 read(a); //B2 阻止重排序 不论哪种重排序都可能造成共享变量中线程间不可见，这会改变程序运行结果。所以需要禁止对那些要求可见的共享变量重排序。 阻止编译重排序：禁止编译器在某些时候重排序。 阻止指令重排序和内存系统重排序：使用内存屏障或Lock前缀指令 ","link":"https://tinaxiawuhao.github.io/post/J4bdQ7uDb/"},{"title":"happens-before原则和volatile","content":"基本概念 先补充一下概念：Java 内存模型中的可见性、原子性和有序性。 可见性： 可见性是一种复杂的属性，因为可见性中的错误总是会违背我们的直觉。通常，我们无法确保执行读操作的线程能适时地看到其他线程写入的值，有时甚至是根本不可能的事情。为了确保多个线程之间对内存写入操作的可见性，必须使用同步机制。 可见性，是指线程之间的可见性，一个线程修改的状态对另一个线程是可见的也就是一个线程修改的结果。另一个线程马上就能看到。比如：用volatile修饰的变量，就会具有可见性。volatile修饰的变量不允许线程内部缓存和重排序，即直接修改内存。所以对其他线程是可见的。但是这里需要注意一个问题，volatile只能让被他修饰内容具有可见性，但不能保证它具有原子性。比如 volatile int a = 0；之后有一个操作 a++；这个变量a具有可见性，但是a++ 依然是一个非原子操作，也就是这个操作同样存在线程安全问题。 在 Java 中 volatile、synchronized 和final 实现可见性。 原子性： 原子是世界上的最小单位，具有不可分割性，比如 a=0；（a非long和double类型） 这个操作是不可分割的，那么我们说这个操作是原子操作。再比如：a++； 这个操作实际是a = a + 1；是可分割的，所以他不是一个原子操作。非原子操作都会存在线程安全问题，需要我们使用同步技术（sychronized）来让它变成一个原子操作。一个操作是原子操作，那么我们称它具有原子性。java的concurrent包下提供了一些原子类，我们可以通过阅读API来了解这些原子类的用法。比如：AtomicInteger、AtomicLong、AtomicReference等。 在 Java 中 synchronized 和在 lock、unlock 中操作保证原子性。 有序性： Java 语言提供了 volatile 和 synchronized 两个关键字来保证线程之间操作的有序性，volatile 是因为其本身包含“禁止指令重排序”的语义，synchronized 是由“一个变量在同一个时刻只允许一条线程对其进行 lock 操作”这条规则获得的，此规则决定了持有同一个对象锁的两个同步块只能串行执行。 happens-before 在学习Java内存模型(JMM, Java Memory Model)时，关于线程、主存(main memory)、工作内存(working memory)，这些概念都有着实体的相互对应，线程可能对应着一个内核线程，主存对应着内存，而工作内存则涵盖了写缓冲区、缓存(cache)、寄存器等一系列为了提高数据存取效率的暂存区域。但是，一提到happens-before原则，就让人有点“丈二和尚摸不着头脑”。这个涵盖了整个JMM中可见性原则的规则，究竟如何理解？ 两个操作间具有happens-before关系，并不意味着前一个操作必须要在后一个操作之前执行。happens-before仅仅要求前一个操作对后一个操作可见。happens-before原则和一般意义上的时间先后是不同的 有哪些happens-before规则 程序次序规则：在一个线程内一段代码的执行结果是有序的。就是还会指令重排，但是随便它怎么排，结果是按照我们代码的顺序生成的不会变。 管程锁定规则：就是无论是在单线程环境还是多线程环境，对于同一个锁来说，一个线程对这个锁解锁之后，另一个线程获取了这个锁都能看到前一个线程的操作结果！(管程是一种通用的同步原语，synchronized就是管程的实现） volatile变量规则：就是如果一个线程先去写一个volatile变量，然后一个线程去读这个变量，那么这个写操作的结果一定对读的这个线程可见。 线程启动规则：在主线程A执行过程中，启动子线程B，那么线程A在启动子线程B之前对共享变量的修改结果对线程B可见。 线程终止规则：在主线程A执行过程中，子线程B终止，那么线程B在终止之前对共享变量的修改结果在线程A中可见。也称线程join()规则。 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程代码检测到中断事件的发生，可以通过Thread.interrupted()检测到是否发生中断。 传递性规则：这个简单的，就是happens-before原则具有传递性，即hb(A, B) ， hb(B, C)，那么hb(A, C)。 对象终结规则：这个也简单的，就是一个对象的初始化的完成，也就是构造函数执行的结束一定 happens-before它的finalize()方法。 顺序一致性内存模型 顺序一致性内存模型是一个被计算机科学家理想化了的理论参考模型，它为程序员提供了极强的内存可见性保证。顺序一致性内存模型有两大特性： 一个线程中的所有操作必须按照程序的顺序来执行。 （不管程序是否同步）所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。 在概念上，顺序一致性模型有一个单一的全局内存，这个内存通过一个开关可以连接到任意一个线程。同时，每一个线程必须按程序的顺序来执行内存读/写操作。在任意时间点最多只能有一个线程可以连接到内存。当多个线程并发执行时，开关装置能把所有线程的所有内存读/写操作串行化。 为了更好的理解，下面我们来对顺序一致性模型的特性做进一步的说明。 假设有两个线程A和B并发执行。其中A线程有三个操作，它们在程序中的顺序是：A1-&gt;A2-&gt;A3。B线程也有三个操作，它们在程序中的顺序是：B1-&gt;B2-&gt;B3。 假设这两个线程使用监视器来正确同步：A线程的三个操作执行后释放监视器，随后B线程获取同一个监视器。 现在我们再假设这两个线程没有做同步，未同步程序在顺序一致性模型中虽然整体执行顺序是无序的，但所有线程都只能看到一个一致的整体执行顺序。假设，线程A和B看到的执行顺序都是：B1-&gt;A1-&gt;A2-&gt;B2-&gt;A3-&gt;B3。之所以能得到这个保证是因为顺序一致性内存模型中的每个操作必须立即对任意线程可见。 但是，在JMM中就没有这个保证。未同步程序在JMM中不但整体的执行顺序是无序的，而且所有线程看到的操作执行顺序也可能不一致。比如，在当前线程把写过的数据缓存在本地内存中，且还没有刷新到主内存之前，这个写操作仅对当前线程可见；从其他线程的角度来观察，会认为这个写操作根本还没有被当前线程执行。只有当前线程把本地内存中写过的数据刷新到主内存之后，这个写操作才能对其他线程可见。在这种情况下，当前线程和其它线程看到的操作执行顺序将不一致。 Java内存模型 关于Java内存模型的书籍文章，汗牛充栋，想必大家也都有自己的理解。那就仅仅由上面的顺序一致性模型来引出JMM，看看具体区别在哪。 可以看出，本地内存是一个明显区别于顺序一致性内存模型的地方。事实上，造成可见性问题的根源之一，就在于这个本地内存（强调一下，包括缓存、写缓冲和寄存器等等）。本地内存使得每个线程都有了自己的私有存储，大部分时间对数据的存取工作都在这个区域完成。但是我们写一个数据，是直到数据写到主存中才算真正完成。实际上每个线程维护了一个副本，所有线程都在自己的本地内存中不断地读/写一个共享内存中的数据的副本。单线程情况下，这个副本不会造成任何问题；但一旦到多线程，有一个线程将变量写到主存，其他线程却不知道，其他线程的副本就都过期。比如，由于本地内存的存在，程序员写的一段代码，写一个普通的共享变量，其可能先被写到缓冲区，那指令完成的时间就被推迟了，实际表现也就是我们常说的“指令重排序”（这实际上是内存模型层面的重排序，重排序还可能是编译器、机器指令层级上的乱序）。 因此，在Java内存模型中，每个线程不再像顺序一致性模型中那样有确定的指令执行视图，一个指令可能被重排了。从一个线程的角度看，其他线程（甚至是这个线程本身）执行的指令顺序有多种可能性，也就是说，一个线程的执行结果对其他线程的可见性无法保证。 总结一下导致可见性问题的原因： 数据的写无法及时通知到别的线程，如写缓冲区的引入 线程不能及时读到其他线程对共享变量的修改，如缓存的使用 各种层级上对指令的重排序，导致指令执行的顺序无法确定 所以要解决可见性问题，本质是要让线程对共享变量的修改，及时同步到其他线程。我们所使用的硬件架构下，不具备顺序一致性内存模型的全局一致的指令执行顺序，讨论指令执行的时间先后并不存在意义或者说根本没办法确定时间上的先后。可以看看下面程序，每个线程中的flag副本会在多久后被更新呢？答案是：无法确定，看线程何时刷新自己的本地内存。 public class testVisibility { public static boolean flag = false; public static void main(String[] args) { List&lt;Thread&gt; thdList = new ArrayList&lt;Thread&gt;(); for(int i = 0; i &lt; 10; i++) { Thread t = new Thread(new Runnable(){ public void run() { while (true) { if (flag) { // 多运行几次，可能并不会打印出来也可能会打印出来 // 如果不打印，则表示Thread看到的仍然是本地内存中的flag // 可以尝试将flag变成volatile再运行几次看看 System.out.println(Thread.currentThread().getId() + &quot; is true now&quot;); } } } }); t.start(); thdList.add(t); } flag = true; System.out.println(&quot;set flag true&quot;); // 等待线程执行完毕 try { for (Thread t : thdList) { t.join(); } } catch (Exception e) { } } } 那么既然我们无法讨论指令执行的先后，也不需要讨论，我们实际只想知道某线程的操作对另一个线程是否可见，于是就规定了happens-before这个可见性原则，程序员可以基于这个原则进行可见性的判断。 Volatile原理 Java语言提供了一种稍弱的同步机制，即volatile变量，用来确保将变量的更新操作通知到其他线程。当把变量声明为volatile类型后，编译器与运行时都会注意到这个变量是共享的，因此不会将该变量上的操作与其他内存操作一起重排序。volatile变量不会被缓存在寄存器或者对其他处理器不可见的地方，因此在读取volatile类型的变量时总会返回最新写入的值。 在访问volatile变量时不会执行加锁操作，因此也就不会使执行线程阻塞，因此volatile变量是一种比sychronized关键字更轻量级的同步机制。 当对非 volatile 变量进行读写的时候，每个线程先从内存拷贝变量到CPU缓存中。如果计算机有多个CPU，每个线程可能在不同的CPU上被处理，这意味着每个线程可以拷贝到不同的 CPU cache 中。 而声明变量是 volatile 的，JVM 保证了每次读变量都从内存中读，跳过 CPU cache 这一步。 当一个变量定义为 volatile 之后，将具备两种特性： 1. 保证此变量对所有的线程的可见性，这里的“可见性”，如本文开头所述，当一个线程修改了这个变量的值，volatile 保证了新值能立即同步到主内存，以及每次使用前立即从主内存刷新。但普通变量做不到这点，普通变量的值在线程间传递均需要通过主内存来完成。 2. 禁止指令重排序优化。有volatile修饰的变量，赋值后多执行了一个添加内存屏障操作（指令重排序时不能把后面的指令重排序到内存屏障之前的位置），只有一个CPU访问内存时，并不需要内存屏障；（什么是指令重排序：是指CPU采用了允许将多条指令不按程序规定的顺序分开发送给各相应电路单元处理）。 volatile 性能： volatile 的读性能消耗与普通变量几乎相同，但是写操作稍慢，因为它需要在本地代码中插入许多内存屏障指令来保证处理器不发生乱序执行。 volatile变量 volatile就是一个践行happens-before的关键字。看以下对volatile的描述，就不难知道，happens-before指的是线程接收其他线程修改共享变量的消息与该线程读取共享变量的先后关系。大家可以再细想一下，如果没有happens-before原则，岂不是相当于一个线程读取自己的共享变量副本时，其他线程修改这个变量的消息还没有同步过来？这就是可见性问题。 volatile变量规则：对一个volatile的写，happens-before于任意后续对这个volatile变量的读。 线程A写一个volatile变量，实质上是线程A向接下来要获取这个锁的某个线程发出了（线程A对共享变量修改的）消息。 线程B读一个volatile变量，实质上是线程B接收了之前某个线程发出的（对共享变量所做修改的）消息。 线程A写一个volatile变量，随后线程B读这个变量，这个过程实质上是线程A通过主内存向线程B发送消息。 其实仔细看看volatile的实现方式，实际上就是限制了重排序的范围——加入内存屏障(Memory Barrier or Memory Fence)。也即是说，允许指令执行的时间先后顺序在一定范围内发生变化，而这个范围就是根据happens-before原则来规定。内存屏障概括起来有两个功能： 使写缓冲区的内容刷新到内存，保证对其他线程/CPU可见 禁止读写操作的越过内存屏障进行重排序 每个volatile写操作的前面插入一个StoreStore屏障 每个volatile写操作的后面插入一个StoreLoad屏障 每个volatile读操作的后面插入一个LoadLoad屏障 每个volatile读操作的后面插入一个LoadStore屏障 关于内存屏障的种类，这里不是研究的重点。一直困扰我的是，在多处理器系统下，这个屏障如何能跨越处理器来阻止操作执行的顺序呢？比如下面的读写操作： public static volatile int race = 0; // Thread A public static void save(int src) { race = src; } // Thread B public static int load() { return race; } 这就要提到从操作系统到硬件层面的观念转换，可以参看总线事务（Bus transaction）的概念。当CPU要与内存进行数据交换的时候，实际上总线会同步数据交换操作，同一时刻只能有一个CPU进行读/写内存，所以我们所看到的多处理器并行，并行的是CPU的计算资源。在总线看来，对于存储的读写操作就是串行的，是按照一定顺序的。这也就是为什么一个内存屏障能够跨越处理器去限制读写、去完成通信。 ","link":"https://tinaxiawuhao.github.io/post/67CnuNlVA/"},{"title":"Java内存模型JMM","content":"为什么要有内存模型 在介绍Java内存模型之前，先来看一下到底什么是计算机内存模型，然后再来看Java内存模型在计算机内存模型的基础上做了哪些事情。要说计算机的内存模型，就要说一下一段古老的历史，看一下为什么要有内存模型。 内存模型，英文名Memory Model，他是一个很老的老古董了。他是与计算机硬件有关的一个概念。那么我先给你介绍下他和硬件到底有啥关系。 CPU和缓存一致性 我们应该都知道，计算机在执行程序的时候，每条指令都是在CPU中执行的，而执行的时候，又免不了要和数据打交道。而计算机上面的数据，是存放在主存当中的，也就是计算机的物理内存啦。 刚开始，还相安无事的，但是随着CPU技术的发展，CPU的执行速度越来越快。而由于内存的技术并没有太大的变化，所以从内存中读取和写入数据的过程和CPU的执行速度比起来差距就会越来越大,这就导致CPU每次操作内存都要耗费很多等待时间。 这就像一家创业公司，刚开始，创始人和员工之间工作关系其乐融融，但是随着创始人的能力和野心越来越大，逐渐和员工之间出现了差距，普通员工原来越跟不上CEO的脚步。老板的每一个命令，传到到基层员工之后，由于基层员工的理解能力、执行能力的欠缺，就会耗费很多时间。这也就无形中拖慢了整家公司的工作效率。 可是，不能因为内存的读写速度慢，就不发展CPU技术了吧，总不能让内存成为计算机处理的瓶颈吧。 所以，人们想出来了一个好的办法，就是在CPU和内存之间增加高速缓存。缓存的概念大家都知道，就是保存一份数据拷贝。他的特点是速度快，内存小，并且昂贵。 那么，程序的执行过程就变成了： 当程序在运行过程中，会将运算需要的数据从主存复制一份到CPU的高速缓存当中，那么CPU进行计算时就可以直接从它的高速缓存读取数据和向其中写入数据，当运算结束之后，再将高速缓存中的数据刷新到主存当中。 之后，这家公司开始设立中层管理人员，管理人员直接归CEO领导，领导有什么指示，直接告诉管理人员，然后就可以去做自己的事情了。管理人员负责去协调底层员工的工作。因为管理人员是了解手下的人员以及自己负责的事情的。所以，大多数时候，公司的各种决策，通知等，CEO只要和管理人员之间沟通就够了。 而随着CPU能力的不断提升，一层缓存就慢慢的无法满足要求了，就逐渐的衍生出多级缓存。 按照数据读取顺序和与CPU结合的紧密程度，CPU缓存可以分为一级缓存（L1），二级缓存（L2），部分高端CPU还具有三级缓存（L3），每一级缓存中所储存的全部数据都是下一级缓存的一部分。 这三种缓存的技术难度和制造成本是相对递减的，所以其容量也是相对递增的。 那么，在有了多级缓存之后，程序的执行就变成了： 当CPU要读取一个数据时，首先从一级缓存中查找，如果没有找到再从二级缓存中查找，如果还是没有就从三级缓存或内存中查找。 随着公司越来越大，老板要管的事情越来越多，公司的管理部门开始改革，开始出现高层，中层，底层等管理者。一级一级之间逐层管理。 单核CPU只含有一套L1，L2，L3缓存； 如果CPU含有多个核心，即多核CPU，则每个核心都含有一套L1（甚至和L2）缓存，而共享L3（或者和L2）缓存。 公司也分很多种，有些公司只有一个大Boss，他一个人说了算。但是有些公司有比如联席总经理、合伙人等机制。 单核CPU就像一家公司只有一个老板，所有命令都来自于他，那么就只需要一套管理班底就够了。 多核CPU就像一家公司是由多个合伙人共同创办的，那么，就需要给每个合伙人都设立一套供自己直接领导的高层管理人员，多个合伙人共享使用的是公司的底层员工。 还有的公司，不断壮大，开始差分出各个子公司。各个子公司就是多个CPU了，互相之前没有共用的资源。互不影响。 下图为一个单CPU双核的缓存结构。 随着计算机能力不断提升，开始支持多线程。那么问题就来了。我们分别来分析下单线程、多线程在单核CPU、多核CPU中的影响。 单线程。cpu核心的缓存只被一个线程访问。缓存独占，不会出现访问冲突等问题。 单核CPU，多线程进程中的多个线程会同时访问进程中的共享数据，CPU将某块内存加载到缓存后，不同线程在访问相同的物理地址的时候，都会映射到相同的缓存位置，这样即使发生线程的切换，缓存仍然不会失效。但由于任何时刻只能有一个线程在执行，因此不会出现缓存访问冲突。 多核CPU，多线程每个核都至少有一个L1 缓存。多个线程访问进程中的某个共享内存，且这多个线程分别在不同的核心上执行，则每个核心都会在各自的caehe中保留一份共享内存的缓冲。由于多核是可以并行的，可能会出现多个线程同时写各自的缓存的情况，而各自的cache之间的数据就有可能不同。 在CPU和主存之间增加缓存，在多线程场景下就可能存在缓存一致性问题，也就是说，在多核CPU中，每个核的自己的缓存中，关于同一个数据的缓存内容可能不一致。 如果这家公司的命令都是串行下发的话，那么就没有任何问题。 如果这家公司的命令都是并行下发的话，并且这些命令都是由同一个CEO下发的，这种机制是也没有什么问题。因为他的命令执行者只有一套管理体系。 如果这家公司的命令都是并行下发的话，并且这些命令是由多个合伙人下发的，这就有问题了。因为每个合伙人只会把命令下达给自己直属的管理人员，而多个管理人员管理的底层员工可能是公用的。 比如，合伙人1要辞退员工a，合伙人2要给员工a升职，升职后的话他再被辞退需要多个合伙人开会决议。两个合伙人分别把命令下发给了自己的管理人员。合伙人1命令下达后，管理人员a在辞退了员工后，他就知道这个员工被开除了。而合伙人2的管理人员2这时候在没得到消息之前，还认为员工a是在职的，他就欣然的接收了合伙人给他的升职a的命令。 处理器优化和指令重排 上面提到在在CPU和主存之间增加缓存，在多线程场景下会存在缓存一致性问题。除了这种情况，还有一种硬件问题也比较重要。那就是为了使处理器内部的运算单元能够尽量的被充分利用，处理器可能会对输入代码进行乱序执行处理。这就是处理器优化。 除了现在很多流行的处理器会对代码进行优化乱序处理，很多编程语言的编译器也会有类似的优化，比如Java虚拟机的即时编译器（JIT）也会做指令重排。 可想而知，如果任由处理器优化和编译器对指令重排的话，就可能导致各种各样的问题。 关于员工组织调整的情况，如果允许人事部在接到多个命令后进行随意拆分乱序执行或者重排的话，那么对于这个员工以及这家公司的影响是非常大的。 并发编程的问题 前面说的和硬件有关的概念你可能听得有点蒙，还不知道他到底和软件有啥关系。但是关于并发编程的问题你应该有所了解，比如原子性问题，可见性问题和有序性问题。 其实，原子性问题，可见性问题和有序性问题。是人们抽象定义出来的。而这个抽象的底层问题就是前面提到的缓存一致性问题、处理器优化问题和指令重排问题等。 这里简单回顾下这三个问题，并不准备深入展开，感兴趣的读者可以自行学习。我们说，并发编程，为了保证数据的安全，需要满足以下三个特性： 原子性是指在一个操作中就是cpu不可以在中途暂停然后再调度，既不被中断操作，要不执行完成，要不就不执行。 可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。 有序性即程序执行的顺序按照代码的先后顺序执行。 有没有发现，缓存一致性问题其实就是可见性问题。而处理器优化是可以导致原子性问题的。指令重排即会导致有序性问题。所以，后文将不再提起硬件层面的那些概念，而是直接使用大家熟悉的原子性、可见性和有序性。 什么是内存模型 前面提到的，缓存一致性问题、处理器器优化的指令重排问题是硬件的不断升级导致的。那么，有没有什么机制可以很好的解决上面的这些问题呢？ 最简单直接的做法就是废除处理器和处理器的优化技术、废除CPU缓存，让CPU直接和主存交互。但是，这么做虽然可以保证多线程下的并发问题。但是，这就有点因噎废食了。 所以，为了保证并发编程中可以满足原子性、可见性及有序性。有一个重要的概念，那就是——内存模型。 为了保证共享内存的正确性（可见性、有序性、原子性），内存模型定义了共享内存系统中多线程程序读写操作行为的规范通过这些规则来规范对内存的读写操作，从而保证指令执行的正确性。它与处理器有关、与缓存有关、与并发有关、与编译器也有关。他解决了CPU多级缓存、处理器优化、指令重排等导致的内存访问问题，保证了并发场景下的一致性、原子性和有序性。 内存模型解决并发问题主要采用两种方式：限制处理器优化和使用内存屏障。本文就不深入底层原理来展开介绍了，感兴趣的朋友可以自行学习。 什么是Java内存模型 前面介绍过了计算机内存模型，这是解决多线程场景下并发问题的一个重要规范。那么具体的实现是如何的呢，不同的编程语言，在实现上可能有所不同。 我们知道，Java程序是需要运行在Java虚拟机上面的，Java内存模型（Java Memory Model ,JMM）就是一种符合内存模型规范的，屏蔽了各种硬件和操作系统的访问差异的，保证了Java程序在各种平台下对内存的访问都能保证效果一致的机制及规范。 提到Java内存模型，一般指的是JDK 5开始使用的新的内存模型，主要由JSR-133: JavaTM Memory Model and Thread Specification描述。感兴趣的可以参看下这份PDF文档（http://www.cs.umd.edu/~pugh/java/memoryModel/jsr133.pdf） Java内存模型规定了所有的变量都存储在主内存中，每条线程还有自己的工作内存，线程的工作内存中保存了该线程中是用到的变量的主内存副本拷贝，线程对变量的所有操作都必须在工作内存中进行，而不能直接读写主内存。不同的线程之间也无法直接访问对方工作内存中的变量，线程间变量的传递均需要自己的工作内存和主存之间进行数据同步进行。 而JMM就作用于工作内存和主存之间数据同步过程。他规定了如何做数据同步以及什么时候做数据同步。 这里面提到的主内存和工作内存，读者可以简单的类比成计算机内存模型中的主存和缓存的概念。特别需要注意的是，主内存和工作内存与JVM内存结构中的Java堆、栈、方法区等并不是同一个层次的内存划分，无法直接类比。《深入理解Java虚拟机》中认为，如果一定要勉强对应起来的话，从变量、主内存、工作内存的定义来看，主内存主要对应于Java堆中的对象实例数据部分。工作内存则对应于虚拟机栈中的部分区域。 所以，再来总结下，JMM是一种规范，目的是解决由于多线程通过共享内存进行通信时，存在的本地内存数据不一致、编译器会对代码指令重排序、处理器会对代码乱序执行等带来的问题。目的是保证并发编程场景中的原子性、可见性和有序性。 Java内存模型的实现 了解Java多线程的朋友都知道，在Java中提供了一系列和并发处理相关的关键字，比如volatile、synchronized、final、concurrent包等。其实这些就是Java内存模型封装了底层的实现后提供给程序员使用的一些关键字。 在开发多线程的代码的时候，我们可以直接使用synchronized等关键字来控制并发，从来就不需要关心底层的编译器优化、缓存一致性等问题。所以，Java内存模型，除了定义了一套规范，还提供了一系列原语，封装了底层实现后，供开发者直接使用。 本文并不准备把所有的关键字逐一介绍其用法，因为关于各个关键字的用法，网上有很多资料。读者可以自行学习。本文还有一个重点要介绍的就是，我们前面提到，并发编程要解决原子性、有序性和一致性的问题，我们就再来看下，在Java中，分别使用什么方式来保证。 原子性 在Java中，为了保证原子性，提供了两个高级的字节码指令monitorenter和monitorexit。在synchronized的实现原理文章中，介绍过，这两个字节码，在Java中对应的关键字就是synchronized。 因此，在Java中可以使用synchronized来保证方法和代码块内的操作是原子性的。 可见性 Java内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值的这种依赖主内存作为传递媒介的方式来实现的。 Java中的volatile关键字提供了一个功能，那就是被其修饰的变量在被修改后可以立即同步到主内存，被其修饰的变量在每次是用之前都从主内存刷新。因此，可以使用volatile来保证多线程操作时变量的可见性。 除了volatile，Java中的synchronized和final两个关键字也可以实现可见性。只不过实现方式不同，被synchronized修饰的代码，在开始执行时会加锁，执行完成后会进行解锁，但在一个变量解锁之前，必须先把此变量同步回主存中，这样解锁后，后续其它线程就可以访问到被修改后的值，从而保证可见性。 有序性 在Java中，可以使用synchronized和volatile来保证多线程之间操作的有序性。实现方式有所区别： volatile关键字会禁止指令重排。synchronized关键字保证同一时刻只允许一条线程操作。 好了，这里简单的介绍完了Java并发编程中解决原子性、可见性以及有序性可以使用的关键字。读者可能发现了，好像synchronized关键字是万能的，他可以同时满足以上三种特性，这其实也是很多人滥用synchronized的原因。 但是synchronized是比较影响性能的，虽然编译器提供了很多锁优化技术，但是也不建议过度使用。 总结 在读完本文之后，相信你应该了解了什么是Java内存模型、Java内存模型的作用以及Java中内存模型做了什么事情等。 关于Java中这些和内存模型有关的关键字，希望读者还可以继续深入学习，并且自己写几个例子亲自体会一下。可以参考《深入理解Java虚拟机》和《Java并发编程的艺术》两本书。 面试如何回答 前面我介绍完了一些和Java内存模型有关的基础知识，只是基础，并不是全部，因为随便一个知识点还是都可以展开的，如volatile是如何实现可见性的？synchronized是如何实现有序性的？ 但是，当面试官问你：能简单介绍下你理解的内存模型吗？ 首先，先和面试官确认一下：您说的内存模型指的是JMM，也就是和并发编程有关的那一个吧？ 在得到肯定答复后，再开始介绍（如果不是，那可能就要回答堆、栈、方法区哪些了….囧…）： Java内存模型，其实是保证了Java程序在各种平台下对内存的访问都能够得到一致效果的机制及规范。目的是解决由于多线程通过共享内存进行通信时，存在的原子性、可见性（缓存一致性）以及有序性问题。 除此之外，Java内存模型还提供了一系列原语，封装了底层实现后，供开发者直接使用。如我们常用的一些关键字：synchronized、volatile以及并发包等。 回答到这里就可以了，然后面试官可能会继续追问，然后根据他的追问再继续往下回答即可。 所以，当有人再问你Java内存模型的时候，不要一张嘴就直接回答堆栈、方法区甚至GC了，那样显得很不专业！ 作者： Hollis 原文地址：Java内存模型 ","link":"https://tinaxiawuhao.github.io/post/rNVfAh0Yb/"},{"title":" JVM 的核心知识梳理","content":"前言 运行时数据区域 java虚拟机运行时有哪些数据区域，他们都有什么用途？ 有程序计数器、java虚拟机栈、本地方法栈、堆和方法区五大模块。请看下图： 程序计数器 程序计数器是一块较小的内存空间，他可以看做是当期线程所执行的字节码的行号指令器。字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令。在任何一个确定的时刻，一个处理器都只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各个线程之间互不影响，独立存储，所以程序计数器是“线程私有的”。另外，程序计数器是唯一一个在java虚拟机规范中没有规定OOM的区域。 Java虚拟机栈 Java虚拟机栈也是线程私有的，它的生命周期与线程相同，虚拟机栈描述的是Java方法执行的内存模型，每个方法在执行的同时都会创建一个栈帧用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每个方法从调用直至执行完成的过程，对应着一个栈帧在虚拟机栈中入栈到出栈的过程。（程序员经常会把Java内存划分为堆内存和栈内存，这种说法比较粗糙，其中的栈内存就是指虚拟机栈，或者说是虚拟机栈中的局部变量表的部分） 在Java虚拟机规范中，对这个区域规定了两种异常：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverFlowError异常。如果虚拟机栈可以动态扩展，如果扩展时无法申请到足够的内存，就会抛出OOM异常。 本地方法栈 本地方法栈与虚拟机栈作用类似，他们之间的区别不过是虚拟机栈是为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的Native方法服务。与虚拟机栈一样，本地方法栈也会抛出StackOverFlowError异常和OOM异常。 堆 Java堆是Java虚拟机管理的最大的一块内存，是所有线程共享的区域，在虚拟机启动时就创建。堆用来存放对象实例，几乎所有的对象实例都在这里分配内存（注意是几乎所有）。这一点在Java虚拟机规范中描述为：所有的对象实例以及数组都要在堆上分配，但随着JIT编译器的发展和逃逸分析技术的成熟，栈上分配、标量替换技术将会导致一些微妙的变化发生，所有对象都分配在堆上也不是那么绝对了。 如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OOM异常。 方法区 方法区与Java堆一样，是各个线程共享的区域，它用来存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。根据Java虚拟机规范，当方法区无法满足内存分配的 需求时，将抛出OOM异常。 运行时常量池是方法区的一部分，用于存放编译器生成的各种字面量和符号引用。运行时常量池相对于Class文件常量池的另外一个重要特征就是具备动态性。也就是说运行期间也可能将新的常量 放入池中，这种特性被利用的比较多的就是String类的intern()方法。 直接内存 直接内存并不是运行时数据区的一部分，也不是Java虚拟机定义的内存区域。本机直接内存的分配不受Java堆大小的限制，但是受本机总内存大小以及处理器寻址空间的限制。 内存溢出 堆内存溢出 Java堆用于存储对象实例，只要不断创建对象，并且保证GC Roots到对象之间有可达路径来避免垃圾回收机制清除这些对象，那么在对象数量达到最大堆容量限制后就会OOM。（轻易不要运行） public class HeapOOMTest { static class OOMObject { } public static void main(String [] args) { List&lt;OOMObject&gt; list = new ArrayList&lt;&gt;(); while (true) { list.add(new OOMObject()); } } } Java堆内存的OOM异常是实际应用中最常见的内存溢出，当出现了咋办？一般的手段是先通过内存映像分析工具对Dump出来的堆转储快照进行分析，重点是确认内存中的对象是否是必要的，也就是确认是内存泄露还是内存溢出。 如果是内存泄露，可进一步通过工具查看泄露对象到GCRoots的引用链，找到为什么垃圾收集器无法回收它们。如果不存在泄露，就是内存中的对象必须都存活，那就要检查虚拟机的堆内存是否可以调大，从代码上检查是否某些对象生命周期过长，减少内存消耗，优化代码。 虚拟机栈溢出 关于虚拟机栈，在java虚拟机规范中描述了两种异常： （1）如果线程请求的栈深度大于虚拟机所允许的最大深度，将抛出StackOverFlowError异常。 （2）如果虚拟机在扩展栈时无法申请到足够的内存空间，则抛出OutOdMemoryError异常。 这里描述的两种情况实际上有些重叠：当栈空间无法继续分配的时候，到底是内存太小导致的，还是已使用的栈空间太大，本质上是一样的。 public class StackSOFTest { private int stackLength = -1; public void stackLeak() { stackLength++; stackLeak(); } public static void main(String [] args) throws Throwable { StackSOFTest oom = new StackSOFTest(); try { oom.stackLeak(); } catch (Throwable e) { System.out.println(&quot;stack length:&quot;+oom.stackLength); throw e; } } } 运行结果： tack length:13980 Exception in thread &quot;main&quot; java.lang.StackOverflowError at oom.StackSOFTest.stackLeak(StackSOFTest.java:14) at oom.StackSOFTest.stackLeak(StackSOFTest.java:14) at oom.StackSOFTest.stackLeak(StackSOFTest.java:14) ...后续省略 实验结果表明：在单个线程下，无论是由于栈帧太大还是虚拟机栈容量太小，当内存无法分配时，虚拟机抛出的都是StackOverflowError异常。 哪些内存需要回收？ 猿们都知道JVM的内存结构包括五大区域：程序计数器、虚拟机栈、本地方法栈、堆区、方法区。其中程序计数器、虚拟机栈、本地方法栈3个区域随线程而生、随线程而灭，因此这几个区域的内存分配和回收都具备确定性，就不需要过多考虑回收的问题，因为方法结束或者线程结束时，内存自然就跟随着回收了。而Java堆区和方法区则不一样、不一样!(怎么不一样说的朗朗上口)，这部分内存的分配和回收是动态的，正是垃圾收集器所需关注的部分。 垃圾收集器在对堆区和方法区进行回收前，首先要确定这些区域的对象哪些可以被回收，哪些暂时还不能回收，这就要用到判断对象是否存活的算法！（面试官肯定没少问你吧） 2.1 引用计数算法 2.1.1 算法分析 引用计数是垃圾收集器中的早期策略。在这种方法中，堆中每个对象实例都有一个引用计数。当一个对象被创建时，就将该对象实例分配给一个变量，该变量计数设置为1。当任何其它变量被赋值为这个对象的引用时，计数加1（a = b,则b引用的对象实例的计数器+1），但当一个对象实例的某个引用超过了生命周期或者被设置为一个新值时，对象实例的引用计数器减1。任何引用计数器为0的对象实例可以被当作垃圾收集。当一个对象实例被垃圾收集时，它引用的任何对象实例的引用计数器减1。 2.1.2 优缺点 优点：引用计数收集器可以很快的执行，交织在程序运行中。对程序需要不被长时间打断的实时环境比较有利。 缺点：无法检测出循环引用。如父对象有一个对子对象的引用，子对象反过来引用父对象。这样，他们的引用计数永远不可能为0。 2.1.3 是不是很无趣，来段代码压压惊 public class ReferenceFindTest { public static void main(String[] args) { MyObject object1 = new MyObject(); MyObject object2 = new MyObject(); object1.object = object2; object2.object = object1; object1 = null; object2 = null; } } 这段代码是用来验证引用计数算法不能检测出循环引用。最后面两句将object1和object2赋值为null，也就是说object1和object2指向的对象已经不可能再被访问，但是由于它们互相引用对方，导致它们的引用计数器都不为0，那么垃圾收集器就永远不会回收它们。 2.2 可达性分析算法 可达性分析算法是从离散数学中的图论引入的，程序把所有的引用关系看作一张图，从一个节点GC ROOT开始，寻找对应的引用节点，找到这个节点以后，继续寻找这个节点的引用节点，当所有的引用节点寻找完毕之后，剩余的节点则被认为是没有被引用到的节点，即无用的节点，无用的节点将会被判定为是可回收的对象。 在Java语言中，可作为GC Roots的对象包括下面几种： a) 虚拟机栈中引用的对象（栈帧中的本地变量表）； b) 方法区中类静态属性引用的对象； c) 方法区中常量引用的对象； d) 本地方法栈中JNI（Native方法）引用的对象。 对象是“生”是“死 对象的四种引用 引用分为强引用，软引用，弱引用和虚引用四种，这四种引用强度依次逐渐减弱。 强引用 强引用就是指在程序代码中普遍存在的，是指创建一个对象并把这个对象赋给一个引用变量，类似Object obj = new Object()这类的引用，只要强引用还存在，垃圾收集器就永远不会回收被引用的对象。如果想中断强引用和某个对象之间的关联，可以显示的将引用赋值为null，这样jvm在合适的时间就会回收该对象。 软引用 软引用是用来描述一些还有用但并非必需的对象。对于软引用关联着的对象，在系统将会发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收。SoftReference的特点是它的一个实例保存对一个Java对象的软引用，该软引用的存在不妨碍垃圾收集器线程对该Java对象的回收。 弱引用 弱引用也是用来描述非必需对象的。当JVM进行垃圾回收时，无论内存是否充足，都会回收被弱引用关联的对象。在java中，用java.lang.ref.WeakReference类来表示。 虚引用 虚引用和前面的软引用和弱引用不同，它并不影响对象的生命周期。在java中使用PhantomReference类来表示。如果一个对象与虚引用关联，跟没有引用与之关联一样，任何时候都可能被回收。要注意的是，虚引用必须和引用队列关联使用。当垃圾收集器准备回收一个对象时，如果发现它还有虚引用，就会把这个虚引用加入到与之关联的引用队列中。为对象设置虚引用的唯一目的就是能在这个对象被垃圾收集器回收时收到一个系统通知。 引用计数法的缺陷 引用计数法就是给对象添加一个引用计数器，每当有一个地方引用它时，计数器值就加1.当引用失效时，计数器的值就减一。任何时刻计数器值为0的对象就是不可能再被使用的。 优缺点：实现简单，判断效率高，大部分情况下是个很不错的算法。但是致命问题是没办法解决对象之间相互循环引用的问题。 public class ReferenceCountingGC { private Object instance = null; private static final int _1MB = 1024*1024; private byte[] bigSize = new byte[2*_1MB]; public static void main(String [] args) { ReferenceCountingGC objA = new ReferenceCountingGC(); ReferenceCountingGC objB = new ReferenceCountingGC(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; //假设在这行发生GC，ObjA和ObjB是否能被回收 System.gc(); } } 观察GC日志可以看出GC发生了内存回收，意味着虚拟机并没有因为这两个对象相互引用就不回收它们，这也从侧面说明虚拟机并没有采用引用计数法来判断对象是否存活。 可达性分析 这个算法的基本思想是通过一系列被称为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索走过的路径叫做引用链，当一个对象到GC Roots没有任何引用链相连 （用图论的话来说就是从GC Roots到这个对象不可达），则证明此对象是不可用的。 在 Java语言中，可作为GC Roots的对象包括以下几种 （1）虚拟机栈（栈帧中的本地变量表）中引用的对象。 （2）方法区中类静态属性引用的对象。 （3）方法区中常量引用的对象。 （4）本地方法栈中JNI（即一般说的Native方法）引用的对象。 对象是生存还是死亡？ 即使在可达性分析法中不可达的对象，也并非“非死不可”，他们还有拯救自己的机会。要宣告一个对象死亡，至少要经历两次标记过程：如果对象在进行可达性分析后没有与GC Roots的引用链，那么它将会被第一次标记，并且此时需要判断是否有必要执行finalize()方法。没有必要的话，那么这个对象就宣告死亡，可以回收了。 如果有必要执行，那么这个对象会被放置在一个叫做F-Queue的队列中，并在稍后由虚拟机自动建立的低优先级的Finalizer线程去执行它。finalize()是对象拯救自己的最后一次机会-只要重新与引用链上的 任何一个对象建立关联即可（譬如把自己赋值给某个类变量或者对象的成员变量），那么在第二次标记时它将被移除“可回收”的集合，如果对象还没有逃脱，基本上就真的被回收了。 具体的过程见下图： 垃圾收集算法 标记清除算法 标记-清除算法是最基础的算法，分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收。 它的主要不足有两个：一是效率问题，标记和清除两个过程的效率都不高；另一个是空间问题，标记清楚之后会产生大量不连续的内存碎片。 标记-清除算法的执行过程见下图： 复制算法 为了解决效率问题，“复制”算法出现了。它将内存空间划分为大小相等的两块，每次只使用其中的一块，当这一块的内存用完了，就将还存活的对象复制到另外一块上，然后再把已使用的的内存空间一次性清理掉。 这样每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。但是这种算法的代码是将内存空间缩小为原来的一半。 复制算法的执行过程见下图： 标记-整理算法 标记过程仍然与标记-清除算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉边界以外的内存。 标记-整理算法的执行过程见下图： 分代收集算法 分代收集算法是目前大部分JVM的垃圾收集器采用的算法。它的核心思想是根据对象存活的生命周期将内存划分为若干个不同的区域。一般情况下将堆区划分为老年代（Tenured Generation）和新生代（Young Generation），在堆区之外还有一个代就是永久代（Permanet Generation）。老年代的特点是每次垃圾收集时只有少量对象需要被回收，而新生代的特点是每次垃圾回收时都有大量的对象需要被回收，那么就可以根据不同代的特点采取最适合的收集算法。 3.4.1 年轻代（Young Generation）的回收算法 a) 所有新生成的对象首先都是放在年轻代的。年轻代的目标就是尽可能快速的收集掉那些生命周期短的对象。 b) 新生代内存按照8:1:1的比例分为一个eden区和两个survivor(survivor0,survivor1)区。一个Eden区，两个 Survivor区(一般而言)。大部分对象在Eden区中生成。回收时先将eden区存活对象复制到一个survivor0区，然后清空eden区，当这个survivor0区也存放满了时，则将eden区和survivor0区存活对象复制到另一个survivor1区，然后清空eden和这个survivor0区，此时survivor0区是空的，然后将survivor0区和survivor1区交换，即保持survivor1区为空， 如此往复。 c) 当survivor1区不足以存放 eden和survivor0的存活对象时，就将存活对象直接存放到老年代。若是老年代也满了就会触发一次Full GC，也就是新生代、老年代都进行回收。 d) 新生代发生的GC也叫做Minor GC，MinorGC发生频率比较高(不一定等Eden区满了才触发)。 3.4.2 年老代（Old Generation）的回收算法 a) 在年轻代中经历了N次垃圾回收后仍然存活的对象，就会被放到年老代中。因此，可以认为年老代中存放的都是一些生命周期较长的对象。 b) 内存比新生代也大很多(大概比例是1:2)，当老年代内存满时触发Major GC即Full GC，Full GC发生频率比较低，老年代对象存活时间比较长，存活率标记高。 3.4.3 持久代（Permanent Generation）的回收算法 用于存放静态文件，如Java类、方法等。持久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如Hibernate 等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。持久代也称方法区， 垃圾收集器 堆内存是垃圾收集器主要回收垃圾对象的地方，堆内存可以根据对象生命周期的不同分为新生代和老年代，分代收集，新生代使用复制算法，老年代使用标记清除或者标记整理算法。 HotSpot虚拟机提供了7种垃圾收集器，其中新生代三种：Serial/ParNew/Parallel Scavenge收集器，老年代三种：Serial Old/Parallel Old/CMS，都适用的是G1收集器。所有垃圾收集器组合 情况如下图： Serial收集器 最基本也是发展历史最长的垃圾收集器，在进行垃圾收集时，必须Stop The World(暂停其他工作线程)，直到收集结束。只使用一条线程完成垃圾收集，但是效率高，因为没有线程交互的开销，拥有更高的单线程收集效率。发生在新生代区域，使用复制算法。 ParNew收集器 Serial收集器的多线程版本。在进行垃圾收集时同样需要Stop The World（暂停其他工作线程），直到收集结束。使用多条线程进行垃圾收集（由于存在线程交互的开销，所以在单CPU的环境下，性能差于Serial收集器）。目前，只有Parnew收集器能与CMS收集器配合工作。发生在新生代区域，使用复制算法。 Parallel Scavenge收集器 ParNew收集器的升级版，具备ParNew收集器并发多线程收集的特点，以达到可控制吞吐量为目标。（吞吐量：CPU用于运行用户代码的时间与CPU总消耗时间（运行用户代码时间+垃圾收集时间）的比值）。该垃圾收集器能根据当前系统运行情况，动态调整自身参数，从而达到最大吞吐量的目标。（该特性成为GC自适应的调节策略）。发生在新生代，使用复制算法。 Serial Old收集器 Serial 收集器应用在老年代的版本。并发、单线程、效率高。使用标记整理算法。 Parallel Old收集器 是Parallel Scavenge应用在老年代的版本，以达到可控制吞吐量、自适应调节和多线程收集为目标，使用标记整理算法。 CMS收集器 CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。使用这类收集器的应用重视服务的响应速度，希望系统停顿时间最短，以带来更好的用户体验。 使用标记清除算法，一共四个步骤：初始标记、并发标记、重新标记和并发清除。详情见下表： 下面说下CMS的优缺点： 优点： （1）并行：用户线程和垃圾收集线程同时进行。 （2）单线程收集：只使用一条线程完成垃圾收集。 （3）垃圾收集停顿时间短：获取最短的回收停顿时间，即希望系统停顿的时间最短，提高响应速度。 缺点： （1）总吞吐量会降低：因为该收集器对CPU资源非常敏感，在并发阶段不会导致停顿用户线程，但会因为占用部分线程（CPU资源）导致应用程序变慢，总吞吐量会降低。 （2）无法处理浮动垃圾：由于并发清理时用户线程还在运行，所以会有新的垃圾不断产生，只能等到下一次GC时再清理。（因为这一部分垃圾出现在标记过程之后，所以CMS 无法在当次GC中处理他们，因此CMS无法等到老年代填满再进行Full GC，CMS需要预留一部分空间）。 （3）垃圾收集后会产生大量的内存碎片：因为CMS收集器是使用标记-清除算法的。 下面一张图了解下CMS的工作过程： G1收集器 G1收集器是最新最前沿的垃圾收集器。特点如下： （1）并行：用户线程和垃圾收集线程同时进行。 （2）多线程：即使用多条垃圾收集线程进行垃圾回收。（并发和并行充分利用多CPU和多核环境的硬件优势来缩短垃圾收集的停顿时间） （3）垃圾收集效率高：G1收集器是针对性对Java堆内存区域进行垃圾收集，而非每次都对整个区域进行收集。即G1除了将Java堆内存分为新生代和老年代之外，还会细分为许多个 大小相等的独立区域（Region），然后G1收集器会跟踪每个Region里的垃圾代价值大小，并在后台维护一个列表。每次回收时，会根据允许的垃圾收集时间优先回收价值最大的 Region，从而避免了对整个Java堆内存区域的回收，提高了效率。因为上述机制，G1收集器还能建立可预测的时间模型：即让使用者明确执行一个长度为M毫秒的时间片段，消耗在 垃圾收集上的时间不得超出N毫秒。即具备实时性。 （4）不会产生内存碎片。从整理上看，G1收集器是基于标记-整理算法的，从局部看是基于复制算法的。在新生代使用复制算法，在老年代使用标记-整理算法。 下面了解下工作流程，跟CMS有点像。 下面是G1的工作过程： 总结 本文讲解了运行时数据区域，内存溢出，如何判断对象是否存活，垃圾回收算法和垃圾收集器几个方面，涵盖jvm的核心考点，希望你有所收获。 ","link":"https://tinaxiawuhao.github.io/post/8iu5aa5DN/"},{"title":"Java内存结构JVM","content":"JVM内存模型 内存空间(Runtime Data Area)中可以按照是否线程共享分为两块，线程共享的是方法区(Method Area)和堆(Heap)，线程独享的是Java虚拟机栈(Java Stack)，本地方法栈(Native Method Stack)和PC寄存器(Program Counter Register)。具体参见下图： 虚拟机栈： 每个线程有一个私有的栈，随着线程的创建而创建。栈里面存放着一种叫做“栈帧”的东西，每个方法会创建一个栈帧，栈帧中存放了局部变量表(基本数据类型和对象引用)、操作数栈、方法出口等信息。栈的大小可以固定也可以动态扩展。当栈调用深度大于JVM所允许的范围，会抛出StackOverflowError的错误，不过这个深度范围不是一个恒定的值，我们通过下面这段程序可以测试一下这个结果： // 栈溢出测试源码 ackage com.paddx.test.memory; /** * Created by root on 2/28/17. */ public class StackErrorMock { private static int index = 1; public void call() { index++; call(); } public static void main(String[] args) { StackErrorMock mock = new StackErrorMock(); try { mock.call(); } catch(Throwable e) { System.out.println(&quot;Stack deep: &quot; + index); e.printStackTrace(); } } } 运行三次，可以看出每次栈的深度都是不一样的，输出结果如下： 查看三张结果图，可以看出每次的Stack deep值都有所不同。究其原因，就需要深入到JVM的源码中才能探讨，这里不作赘述。 虚拟机栈除了上述错误外，还有另一种错误，那就是当申请不到空间时，会抛出OutOfMemoryError。这里有一个小细节需要注意，catch捕获的是Throwable，而不是Exception，这是因为StackOverflowError和OutOfMemoryError都不属于Exception的子类。 本地方法栈： 这部分主要与虚拟机用到的Native方法相关，一般情况下，Java应用程序员并不需要关系这部分内容。 PC寄存器： PC寄存器，也叫程序计数器。JVM支持多个线程同时运行，每个线程都有自己的程序计数器。倘若当前执行的是JVM方法，则该寄存器中保存当前执行指令的地址；倘若执行的是native方法，则PC寄存器为空。 堆 ： 堆内存是JVM所有线程共享的部分，在虚拟机启动的时候就已经创建。所有的对象和数组都在堆上进行分配。这部分空间可通过GC进行回收。当申请不到空间时，会抛出OutOfMemoryError。下面我们简单的模拟一个堆内存溢出的情况： package com.paddx.test.memory; import java.util.ArrayList; import java.util.List; /** * Created by root on 2/28/17. */ public class HeapOomMock { public static void main(String[] args) { List&lt;byte[]&gt; list = new ArrayList&lt;byte[]&gt;(); int i = 0; boolean flag = true; while(flag) { try { i++; list.add(new byte[1024 * 1024]); // 每次增加1M大小的数组对象 }catch(Throwable e) { e.printStackTrace(); flag = false; System.out.println(&quot;Count = &quot; + i); // 记录运行的次数 } } } } 首先配置运行时虚拟机的启动参数： 然后运行代码，输出结果如下： 注意，这里我们指定了堆内存的大小为16M，所以这个地方显示的Count=13(这个数字不是固定的)，至于为什么会是13或其他数字，需要根据GC日志来判断。 方法区： 方法区也是所有线程共享的。主要用于存储类的信息、常量池、方法数据、方法代码等。方法区逻辑上属于堆的一部分，但是为了与堆进行区分，通常又叫“非堆”。关于方法区的内存溢出问题会在下文中详细讨论。 PermGen(永久代) 绝大部分Java程序员应该都见过“java.lang.OutOfMemoryError: PremGen space”异常。这里的“PermGen space”其实指的就是方法区。不过方法区和“PermGen space”又有着本质的区别。前者是JVM的规范，而后者则是JVM规范的一种实现，并且只有HotSpot才有“PermGen space”，而对于其他类型的虚拟机，如JRockit(Oracle)、J9(IBM)并没有“PermGen space”。由于方法区主要存储类的相关信息，所以对于动态生成类的情况比较容易出现永久代的内存溢出。最典型的场景就是，在JSP页面比较多的情况，容易出现永久代内存溢出。我们现在通过动态生成类来模拟“PermGen space”的内存溢出： package com.paddx.test.memory; import java.io.File; import java.net.URL; import java.net.URLClassLoader; import java.util.ArrayList; import java.util.List; public class PermGenOomMock { public static void main(String[] args) { URL url = null; List&lt;ClassLoader&gt; classLoaderList = new ArrayList&lt;ClassLoader&gt;(); try { url = new File(&quot;/tmp&quot;).toURI().toURL(); URL[] urls = {url}; while(true) { ClassLoader loader = new URLClassLoader(urls); classLoaderList.add(loader); loader.loadClass(&quot;com.paddx.test.memory.Test&quot;); } }catch(Exception e) { e.printStackTrace(); } } } package com.paddx.test.memory; public class Test {} 运行结果如下： 本例中使用的JDK版本是1.7，指定的PermGen区的大小为8M。通过每次生成不同URLClassLoader对象加载Test类，从而生成不同的类对象，这样就能看到我们熟悉的“java.lang.OutOfMemoryError: PermGen space”异常了。这里之所以采用JDK 1.7，是因为在JDK 1.8中，HotSpot已经没有“PermGen space”这个区间了，取而代之是一个叫做Metaspace(元空间)的东西。下面我们就来看看Metaspace与PermGen space的区别。 Metaspace(元空间) 其实，移除永久代的工作从JDK 1.7就开始了。JDK 1.7中，存储在永久代的部分数据就已经转移到Java Heap或者Native Heap。但永久代仍存在于JDK 1.7中，并没有完全移除，譬如符号引用(Symbols)转移到了native heap；字面量(interned strings)转移到了Java heap；类的静态变量(class statics)转移到了Java heap。我们可以通过一段程序来比较JDK 1.6、JDK 1.7与JDK 1.8的区别，以字符串常量为例： package com.paddx.test.memory; import java.util.ArrayList; import java.util.List; public class StringOomMock { static String base = &quot;string&quot;; public static void main(String[] args) { List&lt;String&gt; list = new ArrayList&lt;String&gt;(); for (int i = 0; i &lt; Integer.MAX_VALUE; i++) { String str = base + base; base = str; list.add(str.intern()); } } } 这段程序以2的指数级不断的生成新的字符串，这样可以比较快速的消耗内存。我们通过JDK 1.6、JDK 1.7和JDK 1.8分别运行： JDK 1.6的运行结果： JDK 1.7的运行结果： JDK 1.8的运行结果： 从上述结果可以看出，JDK 1.6下，会出现“PermGen space”的内存溢出，而在JDK 1.7和JDK 1.8中，会出现堆内存溢出，并且JDK 1.8中参数PermSize和MaxPermSize已经失效。因此，可以大致验证JDK 1.7和JDK 1.8中将字符串常量由永久代转移到堆中，并且JDK 1.8中已经不存在永久代的结论。现在我们来看一看元空间到底是一个什么东西？ JDK1.8对JVM架构的改造将类元数据放到本地内存中，另外，将常量池和静态变量放到Java堆里。HotSpot VM将会为类的元数据明确分配和释放本地内存。在这种架构下，类元信息就突破了原来-XX:MaxPermSize的限制，现在可以使用更多的本地内存。这样就从一定程度上解决了原来在运行时生成大量类造成经常Full GC问题，如运行时使用反射、代理等。所以升级以后Java堆空间可能会增加。 元空间的本质和永久代类似，都是对JVM规范中方法区的实现。不过元空间与永久代之间的最大区别在于：元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存限制，但可以通过以下参数指定元空间的大小： -XX:MetaspaceSize，初始空间大小，达到该值就会触发垃圾收集进行类型卸载，同时GC会对改值进行调整：如果释放了大量的空间，就适当降低该值；如果释放了很少的空间，那么在不超过MaxMetaspaceSize时，适当提高该值。 -XX:MaxMetaspaceSize，最大空间，默认是没有限制的。 除了上面的两个指定大小的选项外，还有两个与GC相关的属性： -XX:MinMetaspaceFreeRatio，在GC之后，最小的Metaspace剩余空间容量的百分比，减少为分配空间所导致的垃圾收集。 -XX:MaxMetaspaceFreeRatio，在GC之后，最大的Metaspace剩余空间容量的百分比，减少为释放空间所导致的垃圾收集。 现在我们在JDK 1.8重新运行一下上面第二部分(PermGen(永久代))的代码，不过这次不再指定PermSize和MaxPermSize。而是制定MetaspaceSize和MaxMetaspaceSize的大小。输出结果如下： 从输出结果，我们可以看出，这次不再出现永久代溢出，而是出现元空间的溢出。 四、总结 通过上面的分析，大家应该大致了解了JVM的内存划分，也清楚了JDK 1.8中永久代向元空间的转换。不过大家应该有一个疑问，就是为什么要做这个转换？以下为大家总结几点原因： 字符串在永久代中，容易出现性能问题和内存溢出。 类及方法的信息等比较难确定大小，因此对于永久代的大小指定比较困难，太小容易出现永久代溢出，太大则容易出现老年代溢出。 永久代会为GC带来不必要的复杂度，并且回收效率偏低。 Oracle可能会将HotSpot与JRockit合二为一。 作者：liuxiaopeng 博客地址：https://www.cnblogs.com/paddix/p/5309550.html ","link":"https://tinaxiawuhao.github.io/post/nxmi3zjCk/"},{"title":" 漫画：什么是B+树？","content":"原创 [程序员小灰(公众号，强烈推荐)] 在上一篇漫画中，我们介绍了B-树的原理和应用，没看过的小伙伴们可以点击下面的链接： 漫画：什么是B-树？ 这一次我们来介绍B+树。 ————————————————— 一个m阶的B树具有如下几个特征： 1.根结点至少有两个子女。 2.每个中间节点都包含k-1个元素和k个孩子，其中 m/2 &lt;= k &lt;= m 3.每一个叶子节点都包含k-1个元素，其中 m/2 &lt;= k &lt;= m 4.所有的叶子结点都位于同一层。 5.每个节点中的元素从小到大排列，节点当中k-1个元素正好是k个孩子包含的元素的值域分划。 一个m阶的B+树具有如下几个特征： 1.有k个子树的中间节点包含有k个元素（B树中是k-1个元素），每个元素不保存数据，只用来索引，所有数据都保存在叶子节点。 2.所有的叶子结点中包含了全部元素的信息，及指向含这些元素记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 3.所有的中间节点元素都同时存在于子节点，在子节点元素中是最大（或最小）元素。 B-树中的卫星数据（Satellite Information）： B+树中的卫星数据（Satellite Information）： 需要补充的是，在数据库的聚集索引（Clustered Index）中，叶子节点直接包含卫星数据。在非聚集索引（NonClustered Index）中，叶子节点带有指向卫星数据的指针。 第一次磁盘IO： 第二次磁盘IO： 第三次磁盘IO： B-树的范围查找过程 ** ** 自顶向下，查找到范围的下限（3）： 中序遍历到元素6： 中序遍历到元素8： 中序遍历到元素9： 中序遍历到元素11，遍历结束： B+树的范围查找过程 ** ** 自顶向下，查找到范围的下限（3）： 通过链表指针，遍历到元素6, 8： 通过链表指针，遍历到元素9, 11，遍历结束： B+树的特征： 1.有k个子树的中间节点包含有k个元素（B树中是k-1个元素），每个元素不保存数据，只用来索引，所有数据都保存在叶子节点。 2.所有的叶子结点中包含了全部元素的信息，及指向含这些元素记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 3.所有的中间节点元素都同时存在于子节点，在子节点元素中是最大（或最小）元素。 B+树的优势： 1.单一节点存储更多的元素，使得查询的IO次数更少。 2.所有查询都要查找到叶子节点，查询性能稳定。 3.所有叶子节点形成有序链表，便于范围查询。 —————END————— ","link":"https://tinaxiawuhao.github.io/post/jVTlR3ljT/"},{"title":"漫画：什么是B-树？","content":"原创 [程序员小灰(公众号，强烈推荐)] ———————————— ———————————— 二叉查找树的结构： 第1次磁盘IO： 第2次磁盘IO： 第3次磁盘IO： 第4次磁盘IO： 下面来具体介绍一下B-树（Balance Tree），一个m阶的B树具有如下几个特征： 1.根结点至少有两个子女。 2.每个中间节点都包含k-1个元素和k个孩子，其中 m/2 &lt;= k &lt;= m 3.每一个叶子节点都包含k-1个元素，其中 m/2 &lt;= k &lt;= m 4.所有的叶子结点都位于同一层。 5.每个节点中的元素从小到大排列，节点当中k-1个元素正好是k个孩子包含的元素的值域分划。 第1次磁盘IO： 在内存中定位（和9比较）： 第2次磁盘IO： 在内存中定位（和2，6比较）： 第3次磁盘IO： 在内存中定位（和3，5比较）： 自顶向下查找4的节点位置，发现4应当插入到节点元素3，5之间。 节点3，5已经是两元素节点，无法再增加。父亲节点 2， 6 也是两元素节点，也无法再增加。根节点9是单元素节点，可以升级为两元素节点。于是拆分节点3，5与节点2，6，让根节点9升级为两元素节点4，9。节点6独立为根节点的第二个孩子。 自顶向下查找元素11的节点位置。 删除11后，节点12只有一个孩子，不符合B树规范。因此找出12,13,15三个节点的中位数13，取代节点12，而节点12自身下移成为第一个孩子。（这个过程称为左旋） —————END————— ","link":"https://tinaxiawuhao.github.io/post/LhRxiTagh/"},{"title":"数据结构常见的八大排序算法","content":"八大排序，三大查找是《数据结构》当中非常基础的知识点，在这里为了复习顺带总结了一下常见的八种排序算法。 常见的八大排序算法，他们之间关系如下： 他们的性能比较： 下面，利用java分别将他们进行实现。 直接插入排序 算法思想： 直接插入排序的核心思想就是：将数组中的所有元素依次跟前面已经排好的元素相比较，如果选择的元素比已排序的元素小，则交换，直到全部元素都比较过。 因此，从上面的描述中我们可以发现，直接插入排序可以用两个循环完成： 第一层循环：遍历待比较的所有数组元素 第二层循环：将本轮选择的元素(selected)与已经排好序的元素(ordered)相比较。 如果：selected &lt; ordered，那么将二者交换 代码实现 package sort; import java.util.Arrays; /** * @author wuhao * @desc 直接插入排序 * @date 2020-12-02 15:04:57 * 直接插入排序的核心思想就是：将数组中的所有元素依次跟前面已经排好的元素相比较，如果选择的元素比已排序的元素小，则交换，直到全部元素都比较过。 * 因此，从上面的描述中我们可以发现，直接插入排序可以用两个循环完成： * 第一层循环：遍历待比较的所有数组元素 * 第二层循环：将本轮选择的元素(selected)与已经排好序的元素(ordered)相比较。 * 如果：selected &lt; ordered，那么将二者交换 */ public class DirectInsertionSort { public static void main(String[] args) { int[] arr = {12, 32, 22, 7, 48}; insertSort(arr); } private static void insertSort(int[] arr) { for (int i = 1; i &lt; arr.length; i++) { int temp = arr[i]; int j; for (j = i - 1; j &gt;= 0; j--) { if (temp &lt; arr[j]) { arr[j + 1] = arr[j]; } else { break; } } arr[j + 1] = temp; } System.out.println(Arrays.toString(arr)); } } 希尔排序 算法思想： 希尔排序的算法思想：将待排序数组按照步长gap进行分组，然后将每组的元素利用直接插入排序的方法进行排序；每次将gap折半减小，循环上述操作；当gap=1时，利用直接插入，完成排序。 同样的：从上面的描述中我们可以发现：希尔排序的总体实现应该由三个循环完成： 第一层循环：将gap依次折半，对序列进行分组，直到gap=1 第二、三层循环：也即直接插入排序所需要的两次循环。具体描述见上。 代码实现 package sort; import java.util.Arrays; /** * @author wuhao * @desc 希尔排序 * @date 2020-12-02 15:20:43 * 希尔排序的算法思想：将待排序数组按照步长gap进行分组，然后将每组的元素利用直接插入排序的方法进行排序；每次将gap折半减小，循环上述操作；当gap=1时，利用直接插入，完成排序。 * 同样的：从上面的描述中我们可以发现：希尔排序的总体实现应该由三个循环完成： * 第一层循环：将gap依次折半，对序列进行分组，直到gap=1 * 第二、三层循环：也即直接插入排序所需要的两次循环。具体描述见上。 */ public class HillSort { public static void main(String[] args) { int[] arr = {12, 32, 22, 7, 48, 3, 5, 6, 8, 24}; shellSort(arr); } private static void shellSort(int[] arr) { for (int step = arr.length / 2; step &gt; 0; step /= 2) { // for (int i = step; i &lt; arr.length; i++) { // int temp = arr[i]; // int j; // for (j = i - step; j &gt;= 0 &amp;&amp; arr[j] &gt; temp; j -= step) { // arr[j + step] = arr[j]; // } // arr[j + step] = temp; // } for (int i = step; i &lt; arr.length; i++) { int k=i; for (int j = i-step; j &gt;=0 &amp;&amp; arr[j] &gt; arr[k]; j -= step,k -=step) { int temp = arr[j]; arr[j]= arr[k]; arr[k]=temp; } } } System.out.println(Arrays.toString(arr)); } } 简单选择排序 算法思想 简单选择排序的基本思想：比较+交换。 从待排序序列中，找到关键字最小的元素； 如果最小元素不是待排序序列的第一个元素，将其和第一个元素互换； 从余下的 N - 1 个元素中，找出关键字最小的元素，重复(1)、(2)步，直到排序结束。 因此我们可以发现，简单选择排序也是通过两层循环实现。 第一层循环：依次遍历序列当中的每一个元素 第二层循环：将遍历得到的当前元素依次与余下的元素进行比较，符合最小元素的条件，则交换。 代码实现 package sort; import java.util.Arrays; /** * @author wuhao * @desc 简单选择排序 * @date 2020-12-02 16:21:47 * 简单选择排序的基本思想：比较+交换。 * 从待排序序列中，找到关键字最小的元素； * 如果最小元素不是待排序序列的第一个元素，将其和第一个元素互换； * 从余下的 N - 1 个元素中，找出关键字最小的元素，重复(1)、(2)步，直到排序结束。 * 因此我们可以发现，简单选择排序也是通过两层循环实现。 * 第一层循环：依次遍历序列当中的每一个元素 * 第二层循环：将遍历得到的当前元素依次与余下的元素进行比较，符合最小元素的条件，则交换。 */ public class SimpleSelectionSort { public static void main(String[] args) { int[] arr = {12, 32, 22, 7, 48, 3, 5, 6, 8, 24}; selectionSort(arr); } private static void selectionSort(int[] arr) { for (int i = 0; i &lt; arr.length; i++) { int k = i; for (int j = i + 1; j &lt; arr.length; j++) { if (arr[j] &lt; arr[k]) { k = j; } } if (k != i) { int temp = arr[i]; arr[i] = arr[k]; arr[k] = temp; } } System.out.println(Arrays.toString(arr)); } } 堆排序 堆的概念 堆：本质是一种数组对象。特别重要的一点性质：任意的叶子节点小于（或大于）它所有的父节点。对此，又分为大顶堆和小顶堆，大顶堆要求节点的元素都要大于其孩子，小顶堆要求节点元素都小于其左右孩子，两者对左右孩子的大小关系不做任何要求。 利用堆排序，就是基于大顶堆或者小顶堆的一种排序方法。下面，我们通过大顶堆来实现。 基本思想： 堆排序可以按照以下步骤来完成： 构建大顶堆.png Paste_Image.png 构建初始堆，将待排序列构成一个大顶堆(或者小顶堆)，升序大顶堆，降序小顶堆； 将堆顶元素与堆尾元素交换，并断开(从待排序列中移除)堆尾元素。 重新构建堆。 重复2~3，直到待排序列中只剩下一个元素(堆顶元素)。 代码实现： package sort; import java.util.Arrays; /** * @author wuhao * @desc 堆排序 * @date 2020-12-04 09:13:58 * 堆排序可以按照以下步骤来完成： * 首先将序列构建称为大顶堆； * （这样满足了大顶堆那条性质：位于根节点的元素一定是当前序列的最大值） * 取出当前大顶堆的根节点，将其与序列末尾元素进行交换； * （此时：序列末尾的元素为已排序的最大值；由于交换了元素，当前位于根节点的堆并不一定满足大顶堆的性质） * 对交换后的n-1个序列元素进行调整，使其满足大顶堆的性质； * 重复2.3步骤，直至堆中只有1个元素为止 */ public class HeapSort { public static void main(String[] args) { int[] arr = {11, 44, 23, 67, 88, 65, 34, 48, 9, 12}; heapSort(arr); System.out.println(Arrays.toString(arr)); } private static void heapSort(int[] a) { // 首先需要创建根堆 for (int i = a.length / 2; i &gt;= 0; i--) { // 从最后一个非终端结点开始，然后一次-- HeapAdjust(a, i, a.length); } for (int i = a.length - 1; i &gt; 0; --i) {// 这个循环是把最大值a[0]放到末尾 ， int temp = a[0]; a[0] = a[i]; // 此时i代表最后一个元素 a[i] = temp; HeapAdjust(a, 0, i ); } } // 调整堆 private static void HeapAdjust(int[] a, int parent, int m) {// parent代表当前 m代表最后 int temp = a[parent]; // 先把a[parent]的值赋给temp保存起来 for (int j = 2 * parent; j &lt; m; j *= 2) { if (j+1 &lt; m &amp;&amp; a[j] &lt; a[j + 1]) { // 判断是a[parent]大还是a[j + 1]大，如果a[j + 1]大 就++j，把j换成当前最大 j++; } if (temp &gt;= a[j]) { // 如果temp中比最大值还大，代表本身就是一个根堆，break break;// 如果大于，就代表当前为大跟对，退出 } a[parent] = a[j];// 否则就把最大给[parent] parent = j;// 然后把最大下标给parent，继续循环,检查是否因为调整根堆而破坏了子树 } a[parent] = temp; } /** * 创建堆， * @param arr 待排序列 */ // private static void heapSort(int[] arr) { // //创建堆 // for (int i = (arr.length - 1) / 2; i &gt;= 0; i--) { // //从第一个非叶子结点从下至上，从右至左调整结构 // adjustHeap(arr, i, arr.length); // } // // //调整堆结构+交换堆顶元素与末尾元素 // for (int i = arr.length - 1; i &gt; 0; i--) { // //将堆顶元素与末尾元素进行交换 // int temp = arr[i]; // arr[i] = arr[0]; // arr[0] = temp; // // //重新对堆进行调整 // adjustHeap(arr, 0, i); // } // } /** * 调整堆 * @param arr 待排序列 * @param parent 父节点 * @param length 待排序列尾元素索引 */ // private static void adjustHeap(int[] arr, int parent, int length) { // //将temp作为父节点 // int temp = arr[parent]; // //左孩子 // int lChild = 2 * parent + 1; // // while (lChild &lt; length) { // //右孩子 // int rChild = lChild + 1; // // 如果有右孩子结点，并且右孩子结点的值大于左孩子结点，则选取右孩子结点 // if (rChild &lt; length &amp;&amp; arr[lChild] &lt; arr[rChild]) { // lChild++; // } // // // 如果父结点的值已经大于孩子结点的值，则直接结束 // if (temp &gt;= arr[lChild]) { // break; // } // // // 把孩子结点的值赋给父结点 // arr[parent] = arr[lChild]; // // //选取孩子结点的左孩子结点,继续向下筛选 // parent = lChild; // lChild = 2 * lChild + 1; // } // arr[parent] = temp; // } } 冒泡排序 基本思想 冒泡排序思路比较简单： 将序列当中的左右元素，依次比较，保证右边的元素始终大于左边的元素； （ 第一轮结束后，序列最后一个元素一定是当前序列的最大值；） 对序列当中剩下的n-1个元素再次执行步骤1。 对于长度为n的序列，一共需要执行n-1轮比较 （利用while循环可以减少执行次数） 代码实现 package sort; import java.util.Arrays; /** * @author wuhao * @desc 冒泡排序 * @date 2020-12-02 16:34:57 * 冒泡排序思路比较简单： * 将序列当中的左右元素，依次比较，保证右边的元素始终大于左边的元素； * （ 第一轮结束后，序列最后一个元素一定是当前序列的最大值；） * 对序列当中剩下的n-1个元素再次执行步骤1。 * 对于长度为n的序列，一共需要执行n-1轮比较 * （利用while循环可以减少执行次数） */ public class BubbleSort { public static void main(String[] args) { int[] arr = {12, 32, 22, 7, 48}; bubbleSort(arr); } private static void bubbleSort(int[] arr) { for (int i = 0; i &lt; arr.length; i++) { for (int j = 0; j &lt; i; j++) { if (arr[i] &lt; arr[j]) { int temp=arr[j]; arr[j]=arr[i]; arr[i]=temp; } } } System.out.println(Arrays.toString(arr)); } } 快速排序 算法思想： 快速排序的基本思想：挖坑填数+分治法 从序列当中选择一个基准数(pivot) 在这里我们选择序列当中第一个数为基准数 将序列当中的所有数依次遍历，比基准数大的位于其右侧，比基准数小的位于其左侧 重复步骤a.b，直到所有子集当中只有一个元素为止。 用伪代码描述如下： 1．i =L; j = R; 将基准数挖出形成第一个坑a[i]。 2．j--由后向前找比它小的数，找到后挖出此数填前一个坑a[i]中。 3．i++由前向后找比它大的数，找到后也挖出此数填到前一个坑a[j]中。 4．再重复执行2，3二步，直到i==j，将基准数填入a[i]中 代码实现： package sort; import java.util.Arrays; import java.util.Stack; /** * @author wuhao * @desc 快速排序 * @date 2020-12-02 17:08:57 * 快速排序的基本思想：挖坑填数+分治法 * 从序列当中选择一个基准数(pivot) * 在这里我们选择序列当中第一个数为基准数 * 将序列当中的所有数依次遍历，比基准数大的位于其右侧，比基准数小的位于其左侧 * 重复步骤a.b，直到所有子集当中只有一个元素为止。 * 用伪代码描述如下： * 1．i =L; j = R; 将基准数挖出形成第一个坑a[i]。 * 2．j--由后向前找比它小的数，找到后挖出此数填前一个坑a[i]中。 * 3．i++由前向后找比它大的数，找到后也挖出此数填到前一个坑a[j]中。 * 4．再重复执行2，3二步，直到i==j，将基准数填入a[i]中 */ public class QuickSort { public static void main(String[] args) { int[] arr = {12, 32, 22, 7, 48, 3, 35, 6, 8, 42}; // quickSort(arr, 0, arr.length - 1); System.out.println(Arrays.toString(arr)); /*-----------非递归实现----------*/ sort(arr, 0, arr.length - 1); System.out.println(Arrays.toString(arr)); } private static void quickSort(int[] arr, int left, int right) { if (left &gt; right) { return; } // base中存放基准数 int base = arr[left]; int i = left, j = right; while (i != j) { // 顺序很重要，先从右边开始往左找，直到找到比base值小的数 while (arr[j] &gt;= base &amp;&amp; i &lt; j) { j--; } // 再从左往右边找，直到找到比base值大的数 while (arr[i] &lt;= base &amp;&amp; i &lt; j) { i++; } // 上面的循环结束表示找到了位置或者(i&gt;=j)了，交换两个数在数组中的位置 if (i &lt; j) { int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp; } } // 将基准数放到中间的位置（基准数归位） arr[left] = arr[i]; arr[i] = base; // 递归，继续向基准的左右两边执行和上面同样的操作 // i的索引处为上面已确定好的基准值的位置，无需再处理 quickSort(arr, left, i - 1); quickSort(arr, i + 1, right); } /*-----------------------------非递归实现------------------------------*/ public static void sort(int[] arr, int left, int right) { int privot, top, last; Stack&lt;Integer&gt; s = new Stack&lt;&gt;(); privot = QuickSort(arr, left, right); if (privot &gt; left + 1) { s.push(left); s.push(privot - 1); } if (privot &lt; right - 1) { s.push(privot + 1); s.push(right); } while (!s.empty()) { top = s.pop(); last = s.pop(); privot = QuickSort(arr, last, top); if (privot &gt; last + 1) { s.push(last); s.push(privot - 1); } if (privot &lt; top - 1) { //System.out.println(top); s.push(privot + 1); s.push(top); } } } public static int QuickSort(int[] arr, int left, int right) { int privot = left; while (left &lt; right) { while ((arr[privot] &lt; arr[right]) &amp; left &lt; right) { right--; } int temp = arr[privot]; arr[privot] = arr[right]; arr[right] = temp; privot = right; while ((arr[privot] &gt; arr[left]) &amp; left &lt; right) { left++; } temp = arr[privot]; arr[privot] = arr[left]; arr[left] = temp; privot = left; } return privot; } } 归并排序 算法思想： 归并排序是建立在归并操作上的一种有效的排序算法，该算法是采用0的一个典型的应用。它的基本操作是：将已有的子序列合并，达到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。 归并排序其实要做两件事： 分解----将序列每次折半拆分 合并----将划分后的序列段两两排序合并 因此，归并排序实际上就是两个操作，拆分+合并 如何合并？ L[first...mid]为第一段，L[mid+1...last]为第二段，并且两端已经有序，现在我们要将两端合成达到L[first...last]并且也有序。 首先依次从第一段与第二段中取出元素比较，将较小的元素赋值给temp[] 重复执行上一步，当某一段赋值结束，则将另一段剩下的元素赋值给temp[] 此时将temp[]中的元素复制给L[]，则得到的L[first...last]有序 如何分解？ 在这里，我们采用递归的方法，首先将待排序列分成A,B两组；然后重复对A、B序列 分组；直到分组后组内只有一个元素，此时我们认为组内所有元素有序，则分组结束。 代码实现 package sort; import java.util.Arrays; /** * @author wuhao * @desc 归并排序 * @date 2020-12-03 09:35:00 * 归并排序是建立在归并操作上的一种有效的排序算法，该算法是采用0的一个典型的应用。它的基本操作是：将已有的子序列合并，达到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。 * 归并排序其实要做两件事： * 分解----将序列每次折半拆分 * 合并----将划分后的序列段两两排序合并 * 因此，归并排序实际上就是两个操作，拆分+合并 * 如何合并？ * L[first...mid]为第一段，L[mid+1...last]为第二段，并且两端已经有序，现在我们要将两端合成达到L[first...last]并且也有序。 * 首先依次从第一段与第二段中取出元素比较，将较小的元素赋值给temp[] * 重复执行上一步，当某一段赋值结束，则将另一段剩下的元素赋值给temp[] * 此时将temp[]中的元素复制给L[]，则得到的L[first...last]有序 * 如何分解？ * 在这里，我们采用递归的方法，首先将待排序列分成A,B两组；然后重复对A、B序列 * 分组；直到分组后组内只有一个元素，此时我们认为组内所有元素有序，则分组结束。 */ public class MergeSort { public static void main(String[] args) { int[] arr = {11, 44, 23, 67, 88, 65, 34, 48, 9, 12}; int[] tmp = new int[arr.length]; //新建一个临时数组存放 mergeSort(arr, 0, arr.length - 1, tmp); System.out.println(Arrays.toString(arr)); } private static void mergeSort(int[] arr, int low, int high, int[] tmp) { if (low &lt; high) { int mid = (low + high) / 2; mergeSort(arr, low, mid, tmp); mergeSort(arr, mid + 1, high, tmp); merge(arr, low, mid, high, tmp); } } private static void merge(int[] arr, int low, int mid, int high, int[] tmp) { int i = 0; int j = low, k = mid + 1; while (j &lt;= mid &amp;&amp; k &lt;= high) { if (arr[j] &lt; arr[k]) { tmp[i++] = arr[j++]; } else { tmp[i++] = arr[k++]; } } while (j &lt;= mid) { tmp[i++] = arr[j++]; } while (k &lt;= high) { tmp[i++] = arr[k++]; } for (int l = 0; l &lt; i; l++) { arr[low+l] = tmp[l]; } } } 基数排序 算法思想 基数排序：通过序列中各个元素的值，对排序的N个元素进行若干趟的“分配”与“收集”来实现排序。 分配：我们将L[i]中的元素取出，首先确定其个位上的数字，根据该数字分配到与之序号相同的桶中 收集：当序列中所有的元素都分配到对应的桶中，再按照顺序依次将桶中的元素收集形成新的一个待排序列L[ ] 对新形成的序列L[]重复执行分配和收集元素中的十位、百位...直到分配完该序列中的最高位，则排序结束 代码实现 package sort; import java.util.Arrays; /** * @author wuhao * @desc 基数排序 * @date 2020-12-03 09:58:32 * 基数排序：通过序列中各个元素的值，对排序的N个元素进行若干趟的“分配”与“收集”来实现排序。 * 分配：我们将L[i]中的元素取出，首先确定其个位上的数字，根据该数字分配到与之序号相同的桶中 * 收集：当序列中所有的元素都分配到对应的桶中，再按照顺序依次将桶中的元素收集形成新的一个待排序列L[ ] * 对新形成的序列L[]重复执行分配和收集元素中的十位、百位...直到分配完该序列中的最高位，则排序结束 * 根据上述“基数排序”的展示，我们可以清楚的看到整个实现的过程 */ public class BaseSort { public static void main(String[] args) { int[] arr = {63, 157, 189, 51, 101, 47, 141, 121, 157, 156, 194, 117, 98, 139, 67, 133, 181, 12, 28, 0, 109}; radixSort(arr); System.out.println(Arrays.toString(arr)); } private static void radixSort(int[] arr) { //待排序列最大值 int max = arr[0]; int exp;//指数 //计算最大值 for (int anArr : arr) { if (anArr &gt; max) { max = anArr; } } //从个位开始，对数组进行排序 for (exp = 1; max / exp &gt; 0; exp *= 10) { //存储待排元素的临时数组 int[] temp = new int[arr.length]; //分桶个数 int[] buckets = new int[10]; //将数据出现的次数存储在buckets中 for (int value : arr) { //(value / exp) % 10 :value的最底位(个位) buckets[(value / exp) % 10]++; } //更改buckets[i]， for (int i = 1; i &lt; 10; i++) { buckets[i] += buckets[i - 1]; } //将数据存储到临时数组temp中 for (int i = arr.length - 1; i &gt;= 0; i--) { temp[buckets[(arr[i] / exp) % 10] - 1] = arr[i]; buckets[(arr[i] / exp) % 10]--; } //将有序元素temp赋给arr System.arraycopy(temp, 0, arr, 0, arr.length); } } } 后记 写完之后运行了一下时间比较： 从运行结果上来看，堆排序、归并排序、基数排序是真的快。 对于快速排序迭代深度超过的问题，可以将考虑将快排通过非递归的方式进行实现。 ","link":"https://tinaxiawuhao.github.io/post/0BBhLzjWl/"},{"title":"Linux文件操作高频使用命令","content":"新建操作： mkdir abc #新建一个文件夹 touch abc.sh #新建一个文件 查看操作 查看目录： ll #显示目录文件详细信息 du -h 文件/目录 #查看大小 pwd #显示路径 查看文件内容： cat|head|tail命令 #查看abc的内容 cat abc.txt #查看abc前5行内容。默认是10行 head -5 abc.txt tail [选项] 文件名 各选项的含义如下： +num：从第num行以后开始显示 -num：从距文件尾num行处开始显示。如果省略num参数，系统默认值为10. -f: 循环读取,例如查看服务器日志时，可以实时观察 #filename 文件里的最尾部的内容显示在屏幕上，并且不断刷新。 tail -f filename #查看最后20行 tail -f filename more命令： more命令一次显示一屏信息，若信息未显示完屏幕底部将出现“-More-（xx%）”。 此时按Space键，可显示下一屏内容； 按“回车”键，显示下一行内容； 按B键，显示上一屏； 按Q键，可退出more命令。 less命令： 和more命令类似，但是比more命令更强大。在很多时候，必须使用less,比如管道。例如： ll /etc | less stat 命令： 查看文件的详细信息，比如创建修改时间，大小等 [root@localhost zx]# stat index.html 文件：&quot;index.html&quot; 大小：29006 块：64 IO 块：4096 普通文件 设备：fd00h/64768d Inode：17589607 硬链接：1 权限：(0644/-rw-r--r--) Uid：( 0/ root) Gid：( 0/ root) 环境：unconfined_u:object_r:home_root_t:s0 最近访问：2021-04-25 21:47:41.824053666 +0800 最近更改：2021-04-25 21:44:33.588587500 +0800 最近改动：2021-04-25 21:44:33.588587500 +0800 创建时间：- du 命令： 选项：-h 以合适的单位显示（会根据文件的大小自动选择kb或M等单位） [root@localhost zx]# du -h index.html 32K index.html 复制操作 同一机器的复制： cp:复制文件或目录 语法： cp [options] source dest -a：此选项通常在复制目录时使用，它保留链接、文件属性，并复制目录下的所有内容。其作用等于dpR参数组合。 -d：复制时保留链接。这里所说的链接相当于Windows系统中的快捷方式。 -f：覆盖已经存在的目标文件而不给出提示。 -i：与-f选项相反，在覆盖目标文件之前给出提示，要求用户确认是否覆盖，回答&quot;y&quot;时目标文件将被覆盖。 -p：除复制文件的内容外，还把修改时间和访问权限也复制到新文件中。 -r：若给出的源文件是一个目录文件，此时将复制该目录下所有的子目录和文件。 -l：不复制文件，只是生成链接文件。 举例： #将../html/index.html 复制到当前目录 cp ../html/index.html . #将../html/ 目录下的文件及子目录复制到当前的tt目录下，如果tt不存在，会自动创建 cp -r ../html/ tt/ #将文件file复制到目录/usr/men/tmp下，并改名为file1 cp file /usr/men/tmp/file1 #如果dir2目录已存在，则需要使用 cp -r dir1/. dir2 #如果这时使用cp -r dir1 dir2,则也会将dir1目录复制到dir2中，明显不符合要求。 ps:dir1、dir2改成对应的目录路径即可 远程复制 #将当前目录下的test.txt复制到远程111.12机器的/zx目录下 scp test.txt root@192.168.111.12:/zx #将test.txt复制到远程用户的根目录，并命名为textA.txt scp test.txt root@192.168.111.12:testA.txt #也可以不指定用户，在后续提示中再输入，如下： scp test.txt 192.168.111.12:/zx #从远程复制到本地： -r用于递归整个目录 scp -r remote_user@remote_ip:remote_folder local_path 移动操作: 移动操作可以理解成复制文件后，删除原文件。 #复制/zx/soft目录中的所有文件到当前目录 mv /zx/soft/* . #复制当前目录a.txt到当前的test目录下。 mv a.txt ./test/a.txt #复制文件夹到/tmp/下，必须保证tmp是存在的文件夹 mv /zx/soft/ /tmp/soft 重命名操作 重命名还是用的移动操作命令，比如： #将目录(文件)A重命名为B mv A B #将/a目录(文件)移动到/b下，并重命名为c。要保证b目录存在。 mv /a /b/c #将当前test1目录移动到当前的test目录并命名为b mv ./test1 ./test/b 解压压缩操作 tar -c: 建立压缩档案 -x：解压 -t：查看内容 -r：向压缩归档文件末尾追加文件 -u：更新原压缩包中的文件 这五个是独立的命令，压缩解压都要用到其中一个，可以和别的命令连用但只能用其中一个。下面的参数是根据需要在压缩或解压档案时可选的。 -z：有gzip属性的 -j：有bz2属性的 -Z：有compress属性的 -v：显示所有过程 -O：将文件解开到标准输出 下面的参数-f是必须的 -f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。 举例说明 tar -cf all.tar *.jpg 这条命令是将所有.jpg的文件打成一个名为all.tar的包。-c是表示产生新的包，-f指定包的文件名。 tar -tf all.tar 这条命令是列出all.tar包中所有文件，-t是列出文件的意思 tar -xf all.tar 这条命令是解出all.tar包中所有文件，-x是解开的意思 压缩 tar –cvf jpg.tar *.jpg //将目录里所有jpg文件打包成jpg.tar eg2: tar -xzf nginx-1.14.0.tar.gz //解压到当前目录 tar -zxf nginx-1.14.0.tar.gz -C /usr/local/nginx #解压到对应目录 eg3: tar -zxvf nginx...tar.gz #解压并显示过程 注意：有些压缩程序提示命令找不到，需要进行安装，例如： yum install unzip 或在ubuntu上： apt-get install unzip 总结 1、*.tar 用 tar –xvf 解压 2、*.gz 用 gzip -d或者gunzip 解压 3、*.tar.gz和*.tgz 用 tar –xzf 解压 4、*.bz2 用 bzip2 -d或者用bunzip2 解压 5、*.tar.bz2用tar –xjf 解压 6、*.Z 用 uncompress 解压 7、*.tar.Z 用tar –xZf 解压 8、*.rar 用 unrar e解压 9、*.zip 用 unzip 解压 解压的时候，有时候不想覆盖已经存在的文件，那么可以加上-n参数 unzip -n test.zip unzip -n -d /temp test.zip 只看一下zip压缩包中包含哪些文件，不进行解压缩 unzip -l test.zip 查看显示的文件列表还包含压缩比率 unzip -v test.zip 检查zip文件是否损坏 unzip -t test.zip 如果已有相同的文件存在，要求unzip命令覆盖原先的文件 unzip -o test.zip -d /tmp/ 示例： eg1: unzip mydata.zip -d mydatabak #解压到mydatabak目录 1. xz 这是两层压缩，外面是xz压缩方式，里层是tar压缩,所以可以分两步实现解压 $ xz -d node-v6.10.1-linux-x64.tar.xz $ tar -xvf node-v6.10.1-linux-x64.tar 上传文件工具 从本地windows上传一些文件到远程Linux服务器可以通过xshell的xftp也可以通过下面这个小工具lrzsz，使用更加方便。 #安装工具 yum install lrzsz 常用命令： #下载文件dist.zip到本地 sz dist.zip #会打开窗口，上传文件到远程服务器 rz ln、file和touch命令 ln命令 用于创建链接文件，包括硬链接(Hard Link)和符号链接（Symbolic Link) 。我们常用的是符号链接，也称软连接。软连接就类似windows里的快捷方式。 示例： #在当前目录创建一个软连接，指向/etc/fastab，名称也是fastab ln -s /etc/fastab #在当前目录创建一个指向/boot/grub的软连接，命名为gb ln -s /boot/grub gb 注意：删除软连接 正确方式是： rm -rf ./gb 错误方式: rm -rf ./gb/ 这样会删除了原有grub下的内容。特别是针对系统文件的软连接，删除一定要慎重。 file命令 用于识别文件的类型 Linux中文件后缀只是方便使用者识别，没有实质的约束作用。file命令可以查看文件的实质类型： file [-bcLz] 文件|目录 选项说明： 文件|目录：需要识别的文件或目录 -b: 显示识别结果时，不显示文件名 -c: 显示执行过程 -L: 直接显示符号链接文件指向的文件类型 -z: 尝试去解读压缩文件的内容 示例： 可以看出，index.mp4本质是一个HTML而非一个mp4文件 [root@VM_0_13_centos soft]# file index.mp4 index.mp4: HTML document, UTF-8 Unicode text, with very long lines**touch命令：** 用于改变文件或目录的访问时间和修改时间。 touch命令： 用于改变文件或目录的访问时间和修改时间。 touch [-am] [-t&lt;日期时间&gt;] [目录|文件] 如果指定目录文件不存在，则会直接创建一个空文件，所以touch也常用来创建一个空白文件 #创建一个新文件aa.txt touch aa.txt 选项说明： -a: 只修改访问时间 -m : 只修改 修改时间 -t : 使用指定日期时间，而非系统时间 。例如要修改为2019年10月20日16：38分13秒。参数就是：‘20191020163813’ 示例： 修改之前可以先查看文件的时间戳: 用stat 命令查看 [root@VM_0_13_centos soft]# stat index.html File: ‘index.html’ Size: 17215 Blocks: 40 IO Block: 4096 regular file Device: fd01h/64769d Inode: 529352 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2021-04-25 15:15:37.280616254 +0800 Modify: 2021-04-25 15:15:37.280616254 +0800 Change: 2021-04-25 15:15:37.290616257 +0800 Birth: - 开始修改：将index.html文件的访问和修改时间修改成当前系统的时间。 touch index.html 查找操作命令： 对于要用到的文件，目录等，经常有忘记的时候，所以查找命令就显得极为必要： find: 查找文件或目录 (常用) 语法如下： find [目录…] [-amin &lt;分钟&gt;] [-atime &lt;24小时数&gt;] [-cmin &lt;分钟&gt;] [-ctime&lt;24小时数&gt;][-empty][-exec&lt;执行命令&gt;][-fls&lt;列表文件&gt;][-follow] [-fstype &lt;系统文件类型&gt;] [-gid &lt;组编号&gt;] [-group &lt;组名称&gt;] [-nogroup] [-mmin &lt;分钟&gt;] [-mtime &lt;24小时数&gt;] [-name &lt;查找内容&gt;] [-nogroup] [-nouser] [-perm &lt;权限数值&gt;] [-size &lt;文件大小&gt;] [-uid &lt;用户编号&gt;] [-user &lt;用户名称&gt;] [-nouser] 几个常用选项说明： -size &lt;文件大小&gt;：查找符合指定大小的文件。文件大小单位可以是“c”表示Byte；“k”表示KB。如配置为“100k”，find命令会查找文件大小正好100KB的文件；配置为“+100k”，find命令会查找文件大小大于100KB的文件；配置为“-100k”，find命令会查找文件大小小于100KB的文件。 -user&lt;用户名称&gt;：查找所有者是指定用户的文件或目录，也能以用户编号指定 -name &lt;查找内容&gt;：查找指定的内容，在查找内容中使用“*” 表示任意个字符；使用“?”表示任何一个字符 -mtime &lt;24小时数&gt;：查找在指定时间**曾更改过内容**的文件或目录，单位以24小时计算。如配置为2，find命令会查找刚好在48小时之前更改过内容的文件；配置为+2，find命令会查找超过在48小时之前更改过内容的文件；配置为-2，find命令会查找在48小时之内更改过内容的文件。 -mmin &lt;分钟&gt;：查找在指定时间曾被**更改过内容**的文件或目录，单位以分钟计算。 -cmin &lt;分钟&gt;：查找在指定时间曾被**更改过权限**属性的文件或目录，单位以分钟计算。-ctime对应小时。 -amin &lt;分钟&gt;：查找的是指定时间**访问过**的文件或目录。-atim对应小时。 -perm &lt;权限数值&gt;：查找符合指定权限数值（有关权限数值见第6章）的文件或目录。如配置为“0700”，find命令会查找权限数值正好是“0700”的文件或目录；配置为“+0700”，find命令会查找权限数值大于 “0700”的文件或目录；配置为“-0700”，find 选项大概有以下几类： 1.按时间范围查找 2.按文件大小查找 3.按文件名称查找 4.按其他：比如权限、用户组、类型等 示例： #从根目开始，查找名称以nginx开头的目录和文件 find / -name nginx* #查找文件大小超过100M的文件 find / -size +100M #查找/home/zx目录下，10分钟内被修改过的文件和目录 find /home/zx/ -mmin -10 locate： 查找文件或目录(不常用) locate 查找内容 例如：locate nginx 会将所有包含nginx的目录和文件都列出来。可以用* 或？等匹配符。 locate的查找速度非常快，因为该命令查找的是数据库，所以有些刚修改的文件和目录，可能无法找到。可以采用：updatedb 命令更新数据库。 which: 查找文件(不常用) which [文件] which命令只会在PATH环境变量定义的路径及命令别名中查找，所以范围有限。 whereis : 查找文件(不常用) whichis [-bu] [-B&lt;目录&gt;] [-M&lt;目录&gt;] [-S&lt;目录&gt;] [文件] 常用选项： 文件：要查找的命令 -b: 只查找二进制文件 -u: 查找不包含指定类型的文件 -B&lt;目录&gt;： 只在指定目录下查找二进制文件 -M&lt;目录&gt;：只在指定目录查找帮助文件 -S&lt;目录&gt;：只在指定目录查找源码目录 例如： 默认只会在指定目录查找（/bin ,/etc ,/usr) [root@VM_0_13_centos soft]# whereis nginx nginx: /usr/local/nginx /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx.bak ","link":"https://tinaxiawuhao.github.io/post/yzriNkrRn/"},{"title":"JVM调优","content":"堆大小设置 ​ JVM 中最大堆大小有三方面限制：相关操作系统的数据模型（32-bt还是64-bit）限制；系统的可用虚拟内存限制；系统的可用物理内存限制。32位系统下，一般限制在1.5G~2G；64为操作系统对内存无限制。我在Windows Server 2003 系统，3.5G物理内存，JDK5.0下测试，最大可设置为1478m。 典型设置： java -Xmx3550m -Xms3550m -Xmn2g -Xss128k // -Xmx3550m：设置JVM最大堆可用内存为3550M。 // -Xms3550m：设置JVM初始堆内存为3550m。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。 // -Xmn2g：设置年轻代大小为2G。整个JVM内存大小=年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。 // -Xss128k：设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。根据应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。 java -Xmx3550m -Xms3550m -Xss128k -XX:NewRatio=4 -XX:SurvivorRatio=4 -XX:MaxPermSize=16m -XX:MaxTenuringThreshold=0 // -XX:NewRatio=4:设置年轻代（包括Eden和两个Survivor区）与年老代的比值（除去持久代）。设置为4，则年轻代与年老代所占比值为1：4，年轻代占整个堆栈的1/5 // -XX:SurvivorRatio=4：设置年轻代中Eden区与Survivor区的大小比值。设置为4，则两个Survivor区与一个Eden区的比值为2:4，一个Survivor区占整个年轻代的1/6 // -XX:MaxPermSize=16m:设置持久代大小为16m。 // -XX:MaxTenuringThreshold=0：设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概论。 回收器选择 ​ JVM给了三种选择：串行收集器、并行收集器、并发收集器，但是串行收集器只适用于小数据量的情况，所以这里的选择主要针对并行收集器和并发收集器。默认情况下，JDK5.0以前都是使用串行收集器，如果想使用其他收集器需要在启动时加入相应参数。JDK5.0以后，JVM会根据当前系统配置进行判断。 吞吐量优先的并行收集器 如上文所述，并行收集器主要以到达一定的吞吐量为目标，适用于科学技术和后台处理等。 典型配置： java -Xmx3800m -Xms3800m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:ParallelGCThreads=20 // -XX:+UseParallelGC：选择垃圾收集器为并行收集器。此配置仅对年轻代有效。即上述配置下，年轻代使用并发收集，而年老代仍旧使用串行收集。 // -XX:ParallelGCThreads=20：配置并行收集器的线程数，即：同时多少个线程一起进行垃圾回收。此值最好配置与处理器数目相等。 java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:ParallelGCThreads=20 -XX:+UseParallelOldGC // -XX:+UseParallelOldGC：配置年老代垃圾收集方式为并行收集。JDK6.0支持对年老代并行收集。 java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:MaxGCPauseMillis=100 // -XX:MaxGCPauseMillis=100:设置每次年轻代垃圾回收的最长时间，如果无法满足此时间，JVM会自动调整年轻代大小，以满足此值。 java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:MaxGCPauseMillis=100 -XX:+UseAdaptiveSizePolicy // -XX:+UseAdaptiveSizePolicy：设置此选项后，并行收集器会自动选择年轻代区大小和相应的Survivor区比例，以达到目标系统规定的最低相应时间或者收集频率等，此值建议使用并行收集器时，一直打开。 响应时间优先的并发收集器 如上文所述，并发收集器主要是保证系统的响应时间，减少垃圾收集时的停顿时间。适用于应用服务器、电信领域等。 典型配置： java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:ParallelGCThreads=20 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC // -XX:+UseConcMarkSweepGC：设置年老代为并发收集。测试中配置这个以后，-XX:NewRatio=4的配置失效了，原因不明。所以，此时年轻代大小最好用-Xmn设置。 // -XX:+UseParNewGC:设置年轻代为并行收集。可与CMS收集同时使用。JDK5.0以上，JVM会根据系统配置自行设置，所以无需再设置此值。 java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseConcMarkSweepGC -XX:CMSFullGCsBeforeCompaction=5 -XX:+UseCMSCompactAtFullCollection // -XX:CMSFullGCsBeforeCompaction：由于并发收集器不对内存空间进行压缩、整理，所以运行一段时间以后会产生“碎片”，使得运行效率降低。此值设置运行多少次GC以后对内存空间进行压缩、整理。 // -XX:+UseCMSCompactAtFullCollection：打开对年老代的压缩。可能会影响性能，但是可以消除碎片 辅助信息 JVM提供了大量命令行参数，打印信息，供调试使用。主要有以下一些： -XX:+PrintGC 输出形式：[GC 118250K-&gt;113543K(130112K), 0.0094143 secs] [Full GC 121376K-&gt;10414K(130112K), 0.0650971 secs] -XX:+PrintGCDetails 输出形式：[GC [DefNew: 8614K-&gt;781K(9088K), 0.0123035 secs] 118250K-&gt;113543K(130112K), 0.0124633 secs] [GC [DefNew: 8614K-&gt;8614K(9088K), 0.0000665 secs][Tenured: 112761K-&gt;10414K(121024K), 0.0433488 secs] 121376K-&gt;10414K(130112K), 0.0436268 secs] -XX:+PrintGCTimeStamps -XX:+PrintGC：PrintGCTimeStamps可与上面两个混合使用 输出形式：11.851: [GC 98328K-&gt;93620K(130112K), 0.0082960 secs] -XX:+PrintGCApplicationConcurrentTime:打印每次垃圾回收前，程序未中断的执行时间。可与上面混合使用 输出形式：Application time: 0.5291524 seconds -XX:+PrintGCApplicationStoppedTime：打印垃圾回收期间程序暂停的时间。可与上面混合使用 输出形式：Total time for which application threads were stopped: 0.0468229 seconds -XX:PrintHeapAtGC:打印GC前后的详细堆栈信息 -Xloggc:filename:与上面几个配合使用，把相关日志信息记录到文件以便分析。 常见配置汇总 堆设置 -Xms:等价于(-XX:InitialHeapSize)初始堆大小 -Xmx:等价于(-XX:MaxHeapSize)最大堆大小 -Xmn:堆中新生代初始及最大大小，如果需要进一步细化，初始化大小用-XX:NewSize，最大大小用-XX:MaxNewSize -Xss:等价于(-XX:ThreadStackSize)每个线程堆栈的大小 -XX:NewSize=n:设置年轻代大小 -XX:NewRatio=n:设置年轻代和年老代的比值。如:为3，表示年轻代与年老代比值为1：3，年轻代占整个年轻代年老代和的1/4 -XX:SurvivorRatio=n:年轻代中Eden区与两个Survivor区的比值。注意Survivor区有两个。如：3，表示Eden：Survivor=3：2，一个Survivor区占整个年轻代的1/5 -XX:MaxPermSize=n:设置持久代大小 -XX:MaxTenuringThreshold=n:设置垃圾最大年龄,年轻代复制次数 收集器设置 -XX:+UseSerialGC:设置串行收集器 -XX:+UseParallelGC:设置并行收集器 -XX:+UseParalledlOldGC:设置并行年老代收集器 -XX:+UseConcMarkSweepGC:设置并发收集器 垃圾回收统计信息 -XX:+PrintGC :打印GC信息 -XX:+PrintGCDetails:打印GC时的内存 -XX:+PrintGCTimeStamps：选择打印GC的方式后，再添加此参数。比如：-XX:+PrintGC -XX:+PrintGCTimeStamps每次GC时会打印程序启动后至GC发生的时间戳。 -Xloggc:filename：将GC日志输出到指定位置 并行收集器设置 -XX:ParallelGCThreads=n:设置并行收集器收集时使用的CPU数。并行收集线程数。 -XX:MaxGCPauseMillis=n:设置并行收集最大暂停时间 -XX:GCTimeRatio=n:设置垃圾回收时间占程序运行时间的百分比。公式为1/(1+n) 并发收集器设置 -XX:+CMSIncrementalMode:设置为增量模式。适用于单CPU情况。 -XX:ParallelGCThreads=n:设置并发收集器年轻代收集方式为并行收集时，使用的CPU数。并行收集线程数。 调优总结 年轻代大小选择 响应时间优先的应用：尽可能设大，直到接近系统的最低响应时间限制（根据实际情况选择）。在此种情况下，年轻代收集发生的频率也是最小的。同时，减少到达年老代的对象。 吞吐量优先的应用：尽可能的设置大，可能到达Gbit的程度。因为对响应时间没有要求，垃圾收集可以并行进行，一般适合8CPU以上的应用。 年老代大小选择 响应时间优先的应用：年老代使用并发收集器，所以其大小需要小心设置，一般要考虑并发会话率和会话持续时间等一些参数。如果堆设置小了，可以会造成内存碎片、高回收频率以及应用暂停而使用传统的标记清除方式；如果堆大了，则需要较长的收集时间。最优化的方案，一般需要参考以下数据获得： 并发垃圾收集信息 持久代并发收集次数 传统GC信息 花在年轻代和年老代回收上的时间比例 减少年轻代和年老代花费的时间，一般会提高应用的效率 吞吐量优先的应用：一般吞吐量优先的应用都有一个很大的年轻代和一个较小的年老代。原因是，这样可以尽可能回收掉大部分短期对象，减少中期的对象，而年老代尽存放长期存活对象。 较小堆引起的碎片问题 因为年老代的并发收集器使用标记、清除算法，所以不会对堆进行压缩。当收集器回收时，他会把相邻的空间进行合并，这样可以分配给较大的对象。但是，当堆空间较小时，运行一段时间以后，就会出现“碎片”，如果并发收集器找不到足够的空间，那么并发收集器将会停止，然后使用传统的标记、清除方式进行回收。如果出现“碎片”，可能需要进行如下配置： -XX:+UseCMSCompactAtFullCollection：使用并发收集器时，开启对年老代的压缩。 -XX:CMSFullGCsBeforeCompaction=0：上面配置开启的情况下，这里设置多少次Full GC后，对年老代进行压缩 实际应用命令 jps 查看所有的jvm进程，包括进程ID，进程启动的路径等等。 我自己也用PS，即：ps -ef | grep java jstack 观察jvm中当前所有线程的运行情况和线程当前状态。 系统崩溃了？如果java程序崩溃生成core文件，jstack工具可以用来获得core文件的java stack和native stack的信息，从而可以轻松地知道java程序是如何崩溃和在程序何处发生问题。 系统hung住了？jstack工具还可以附属到正在运行的java程序中，看到当时运行的java程序的java stack和native stack的信息, 如果现在运行的java程序呈现hung的状态，jstack是非常有用的。 jstat jstat利用JVM内建的指令对Java应用程序的资源和性能进行实时的命令行的监控，包括了对进程的classloader，compiler，gc情况； 特别的，一个极强的监视内存的工具，可以用来监视VM内存内的各种堆和非堆的大小及其内存使用量，以及加载类的数量。 jmap 监视进程运行中的jvm物理内存的占用情况，该进程内存内，所有对象的情况，例如产生了哪些对象，对象数量； 系统崩溃了？jmap 可以从core文件或进程中获得内存的具体匹配情况，包括Heap size, Perm size等等 jinfo 观察进程运行环境参数，包括Java System属性和JVM命令行参数 系统崩溃了？jinfo可以从core文件里面知道崩溃的Java应用程序的配置信息。 java -XX:+PrintFlagsInitial -version:查看所有初始化默认配置 java -XX:+PrintFlagsFinal -version:查看所有及修改更新配置 java -XX:+printCommandLineFlags -version:打印命令参数配置 :=:被jvm或人为修改过的参数 -XX:+UseParallelGC:采用并行垃圾回收器 -XX:+UseSerialGC:采用串行垃圾回收器 ","link":"https://tinaxiawuhao.github.io/post/HEMJYLqMW/"},{"title":"Java的值传递","content":"形参与实参 我们先来重温一组语法： 形参：方法被调用时需要传递进来的参数，如：func(int a)中的a，它只有在func被调用期间a才有意义，也就是会被分配内存空间，在方法func执行完成后，a就会被销毁释放空间，也就是不存在了 实参：方法被调用时传入的实际值，它在方法被调用前就已经被初始化并且在方法被调用时传入。 举个栗子： public static void func(int a){ a=20; System.out.println(a); } public static void main(String[] args) { int a=10; func(a); } 例子中 int a=10;中的a在被调用之前就已经创建并初始化，在调用func方法时，他被当做参数传入，所以这个a是实参。 而func(int a)中的a只有在func被调用时它的生命周期才开始，而在func调用结束之后，它也随之被JVM释放掉，所以这个a是形参。 Java的数据类型 所谓数据类型，是编程语言中对内存的一种抽象表达方式，我们知道程序是由代码文件和静态资源组成，在程序被运行前，这些代码存在在硬盘里，程序开始运行，这些代码会被转成计算机能识别的内容放到内存中被执行。 因此 数据类型实质上是用来定义编程语言中相同类型的数据的存储形式，也就是决定了如何将代表这些值的位存储到计算机的内存中。 所以，数据在内存中的存储，是根据数据类型来划定存储形式和存储位置的。 那么 Java的数据类型有哪些？ 基本类型：编程语言中内置的最小粒度的数据类型。它包括四大类八种类型： 4种整数类型：byte、short、int、long 2种浮点数类型：float、double 1种字符类型：char 1种布尔类型：boolean 引用类型：引用也叫句柄，引用类型，是编程语言中定义的在句柄中存放着实际内容所在地址的地址值的一种数据形式。它主要包括： 类 接口 数组 有了数据类型，JVM对程序数据的管理就规范化了，不同的数据类型，它的存储形式和位置是不一样的，要想知道JVM是怎么存储各种类型的数据，就得先了解JVM的内存划分以及每部分的职能。 JVM内存的划分及职能 ​ Java语言本身是不能操作内存的，它的一切都是交给JVM来管理和控制的，因此Java内存区域的划分也就是JVM的区域划分，在说JVM的内存划分之前，我们先来看一下Java程序的执行过程，如下图： 有图可以看出：Java代码被编译器编译成字节码之后，JVM开辟一片内存空间（也叫运行时数据区），通过类加载器加到到运行时数据区来存储程序执行期间需要用到的数据和相关信息，在这个数据区中，它由以下几部分组成： 虚拟机栈 堆 程序计数器 方法区 本地方法栈 我们接着来了解一下每部分的原理以及具体用来存储程序执行过程中的哪些数据。 虚拟机栈 虚拟机栈是Java方法执行的内存模型，栈中存放着栈帧，每个栈帧分别对应一个被调用的方法，方法的调用过程对应栈帧在虚拟机中入栈到出栈的过程。 栈是线程私有的，也就是线程之间的栈是隔离的；当程序中某个线程开始执行一个方法时就会相应的创建一个栈帧并且入栈（位于栈顶），在方法结束后，栈帧出栈。 下图表示了一个Java栈的模型以及栈帧的组成： 栈帧:是用于支持虚拟机进行方法调用和方法执行的数据结构，它是虚拟机运行时数据区中的虚拟机栈的栈元素。 每个栈帧中包括： 局部变量表:用来存储方法中的局部变量（非静态变量、函数形参）。当变量为基本数据类型时，直接存储值，当变量为引用类型时，存储的是指向具体对象的引用。 操作数栈:Java虚拟机的解释执行引擎被称为&quot;基于栈的执行引擎&quot;，其中所指的栈就是指操作数栈。 指向运行时常量池的引用:存储程序执行时可能用到常量的引用。 方法返回地址:存储方法执行完成后的返回地址。 堆 堆是用来存储对象本身和数组的，在JVM中只有一个堆，因此，堆是被所有线程共享的。 方法区： ​ 方法区是一块所有线程共享的内存逻辑区域，在JVM中只有一个方法区，用来存储一些线程可共享的内容，它是线程安全的，多个线程同时访问方法区中同一个内容时，只能有一个线程装载该数据，其它线程只能等待。 ​ 方法区可存储的内容有：类的全路径名、类的直接超类的权全限定名、类的访问修饰符、类的类型（类或接口）、类的直接接口全限定名的有序列表、常量池（字段，方法信息，静态变量，类型引用（class））等。 本地方法栈： ​ 本地方法栈的功能和虚拟机栈是基本一致的，并且也是线程私有的，它们的区别在于虚拟机栈是为执行Java方法服务的，而本地方法栈是为执行本地方法服务的。 有人会疑惑：什么是本地方法？为什么Java还要调用本地方法？ 程序计数器： ​ 线程私有的。记录着当前线程所执行的字节码的行号指示器，在程序运行过程中，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、异常处理、线程恢复等基础功能都需要依赖计数器完成。 数据如何在内存中存储？ 从上面程序运行图我们可以看到，JVM在程序运行时的内存分配有三个地方： 堆 栈 静态方法区 常量区 相应地，每个存储区域都有自己的内存分配策略： 堆式： 栈式 静态 我们已经知道：Java中的数据类型有基本数据类型和引用数据类型，那么这些数据的存储都使用哪一种策略呢？ 这里要分以下的情况进行探究： 基本数据类型的存储： A. 基本数据类型的局部变量 B. 基本数据类型的成员变量 C. 基本数据类型的静态变量 引用数据类型的存储 基本数据类型的存储 我们分别来研究一下： A.基本数据类型的局部变量 定义基本数据类型的局部变量以及数据都是直接存储在内存中的栈上，也就是前面说到的“虚拟机栈”，数据本身的值就是存储在栈空间里面。 如上图，在方法内定义的变量直接存储在栈中，如 int age=50; int weight=50; int grade=6; 当我们写“int age=50；”，其实是分为两步的： int age;//定义变量age=50;//赋值 ​ 首先JVM创建一个名为age的变量，存于局部变量表中，然后去栈中查找是否存在有字面量值为50的内容，如果有就直接把age指向这个地址，如果没有，JVM会在栈中开辟一块空间来存储“50”这个内容，并且把age指向这个地址。因此我们可以知道： ​ 我们声明并初始化基本数据类型的局部变量时，变量名以及字面量值都是存储在栈中，而且是真实的内容。 ​ 我们再来看“int weight=50；”，按照刚才的思路：字面量为50的内容在栈中已经存在，因此weight是直接指向这个地址的。由此可见：栈中的数据在当前线程下是共享的。 ​ 那么如果再执行下面的代码呢？ ​ weight=40； ​ 当代码中重新给weight变量进行赋值时，JVM会去栈中寻找字面量为40的内容，发现没有，就会开辟一块内存空间存储40这个内容，并且把weight指向这个地址。由此可知： ​ 基本数据类型的数据本身是不会改变的，当局部变量重新赋值时，并不是在内存中改变字面量内容，而是重新在栈中寻找已存在的相同的数据，若栈中不存在，则重新开辟内存存新数据，并且把要重新赋值的局部变量的引用指向新数据所在地址。 B. 基本数据类型的成员变量 成员变量：顾名思义，就是在类体中定义的变量。 看下图： 我们看per的地址指向的是堆内存中的一块区域，我们来还原一下代码： public class Person{ private int age; private String name; private int grade; //省略setter getter方法 static void run(){ System.out.println(&quot;run....&quot;); }; } //调用 Person per=new Person(); ​ 同样是局部变量的age、name、grade却被存储到了堆中为per对象开辟的一块空间中。因此可知：基本数据类型的成员变量名和值都存储于堆中，其生命周期和对象的是一致的。 C. 基本数据类型的静态变量 ​ 前面提到方法区用来存储一些共享数据，因此基本数据类型的静态变量名以及值存储于方法区的运行时常量池中，静态变量随类加载而加载，随类消失而消失 引用数据类型的存储: 上面提到：堆是用来存储对象本身和数组，而引用（句柄）存放的是实际内容的地址值，因此通过上面的程序运行图，也可以看出，当我们定义一个对象时 Person per=new Person(); 实际上，它也是有两个过程： Person per;//定义变量 per=new Person();//赋值 ​ 在执行Person per;时，JVM先在虚拟机栈中的变量表中开辟一块内存存放per变量，在执行per=new Person()时，JVM会创建一个Person类的实例对象并在堆中开辟一块内存存储这个实例，同时把实例的地址值赋值给per变量。因此可见： 对于引用数据类型的对象/数组，变量名存在栈中，变量值存储的是对象的地址，并不是对象的实际内容。 值传递和引用传递 ​ 前面已经介绍过形参和实参，也介绍了数据类型以及数据在内存中的存储形式，接下来，就是文章的主题：值传递和引用的传递。 值传递： 在方法被调用时，实参通过形参把它的内容副本传入方法内部，此时形参接收到的内容是实参值的一个拷贝，因此在方法内对形参的任何操作，都仅仅是对这个副本的操作，不影响原始值的内容。 来看个例子： public static void valueCrossTest(int age,float weight){ System.out.println(&quot;传入的age：&quot;+age); System.out.println(&quot;传入的weight：&quot;+weight); age=33; weight=89.5f; System.out.println(&quot;方法内重新赋值后的age：&quot;+age); System.out.println(&quot;方法内重新赋值后的weight：&quot;+weight); } //测试 public static void main(String[] args) { int a=25; float w=77.5f; valueCrossTest(a,w); System.out.println(&quot;方法执行后的age：&quot;+a); System.out.println(&quot;方法执行后的weight：&quot;+w); } 输出结果： 传入的age：25 传入的weight：77.5 方法内重新赋值后的age：33 方法内重新赋值后的weight：89.5 方法执行后的age：25 方法执行后的weight：77.5 从上面的打印结果可以看到： a和w作为实参传入valueCrossTest之后，无论在方法内做了什么操作，最终a和w都没变化。 这是什么造型呢？！！ 下面我们根据上面学到的知识点，进行详细的分析： 首先程序运行时，调用mian()方法，此时JVM为main()方法往虚拟机栈中压入一个栈帧，即为当前栈帧，用来存放main()中的局部变量表(包括参数)、操作栈、方法出口等信息，如a和w都是mian()方法中的局部变量，因此可以断定，a和w是躺着mian方法所在的栈帧中 如图： 而当执行到valueCrossTest()方法时，JVM也为其往虚拟机栈中压入一个栈，即为当前栈帧，用来存放valueCrossTest()中的局部变量等信息，因此age和weight是躺着valueCrossTest方法所在的栈帧中，而他们的值是从a和w的值copy了一份副本而得，如图： 因而可以a和age、w和weight对应的内容是不一致的，所以当在方法内重新赋值时，实际流程如图： 也就是说，age和weight的改动，只是改变了当前栈帧（valueCrossTest方法所在栈帧）里的内容，当方法执行结束之后，这些局部变量都会被销毁，mian方法所在栈帧重新回到栈顶，成为当前栈帧，再次输出a和w时，依然是初始化时的内容。 因此： 值传递传递的是真实内容的一个副本，对副本的操作不影响原内容，也就是形参怎么变化，不会影响实参对应的内容。 引用传递： ​ ”引用”也就是指向真实内容的地址值，在方法调用时，实参的地址通过方法调用被传递给相应的形参，在方法体内，形参和实参指向同一块内存地址，对形参的操作会影响的真实内容。 举个栗子： 先定义一个对象： public class Person { private String name; private int age; public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } } 我们写个函数测试一下： public static void PersonCrossTest(Person person){ System.out.println(&quot;传入的person的name：&quot;+person.getName()); person.setName(&quot;我是张三&quot;); System.out.println(&quot;方法内重新赋值后的name：&quot;+person.getName()); } //测试 public static void main(String[] args) { Person p=new Person(); p.setName(&quot;我是李四&quot;); p.setAge(45); PersonCrossTest(p); System.out.println(&quot;方法执行后的name：&quot;+p.getName()); } 输出结果： 传入的person的name：我是李四 方法内重新赋值后的name：我是张三 方法执行后的name：我是张三 可以看出，person经过personCrossTest()方法的执行之后，内容发生了改变，这印证了上面所说的“引用传递”，对形参的操作，改变了实际对象的内容。 那么，到这里就结题了吗？ 不是的，没那么简单， 能看得到想要的效果 是因为刚好选对了例子而已！！！ 下面我们对上面的例子稍作修改，加上一行代码， public static void PersonCrossTest(Person person){ System.out.println(&quot;传入的person的name：&quot;+person.getName()); person=new Person();//加多此行代码 person.setName(&quot;我是张三&quot;); System.out.println(&quot;方法内重新赋值后的name：&quot;+person.getName()); } 输出结果： 传入的person的name：我是李四 方法内重新赋值后的name：我是张三 方法执行后的name：我是李四 为什么这次的输出和上次的不一样了呢？ 看出什么问题了吗？ 按照上面讲到JVM内存模型可以知道，对象和数组是存储在Java堆区的，而且堆区是共享的，因此程序执行到main（）方法中的下列代码时 Person p=new Person(); p.setName(&quot;我是李四&quot;); p.setAge(45); PersonCrossTest(p); JVM会在堆内开辟一块内存，用来存储p对象的所有内容，同时在main（）方法所在线程的栈区中创建一个引用p存储堆区中p对象的真实地址，如图： 当执行到PersonCrossTest()方法时，因为方法内有这么一行代码： person=new Person(); JVM需要在堆内另外开辟一块内存来存储new Person()，假如地址为“xo3333”，那此时形参person指向了这个地址，假如真的是引用传递，那么由上面讲到：引用传递中形参实参指向同一个对象，形参的操作会改变实参对象的改变。 可以推出：实参也应该指向了新创建的person对象的地址，所以在执行PersonCrossTest()结束之后，最终输出的应该是后面创建的对象内容。 然而实际上，最终的输出结果却跟我们推测的不一样，最终输出的仍然是一开始创建的对象的内容。 由此可见：引用传递，在Java中并不存在。 但是有人会疑问：为什么第一个例子中，在方法内修改了形参的内容，会导致原始对象的内容发生改变呢？ 这是因为：无论是基本类型和是引用类型，在实参传入形参时，都是值传递，也就是说传递的都是一个副本，而不是内容本身。 有图可以看出，方法内的形参person和实参p并无实质关联，它只是由p处copy了一份指向对象的地址，此时： p和person都是指向同一个对象。 因此在第一个例子中，对形参p的操作，会影响到实参对应的对象内容。而在第二个例子中，当执行到new Person()之后，JVM在堆内开辟一块空间存储新对象，并且把person改成指向新对象的地址，此时： p依旧是指向旧的对象，person指向新对象的地址。 所以此时对person的操作，实际上是对新对象的操作，于实参p中对应的对象毫无关系。 结语 因此可见：在Java中所有的参数传递，不管基本类型还是引用类型，都是值传递，或者说是副本传递。 只是在传递过程中： 如果是对基本数据类型的数据进行操作，由于原始内容和副本都是存储实际值，并且是在不同的栈区，因此形参的操作，不影响原始内容。 如果是对引用类型的数据进行操作，分两种情况，一种是形参和实参保持指向同一个对象地址，则形参的操作，会影响实参指向的对象的内容。一种是形参被改动指向新的对象地址（如重新赋值引用），则形参的操作，不会影响实参指向的对象的内容。 ","link":"https://tinaxiawuhao.github.io/post/nRZ1sLPT7/"},{"title":"附录 kafka常见面试问题汇总","content":"基础题目 1、Apache Kafka 是什么? Apach Kafka 是一款分布式流处理框架，用于实时构建流处理应用。它有一个核心 的功能广为人知，即作为企业级的消息引擎被广泛使用。 你一定要先明确它的流处理框架地位，这样能给面试官留 下一个很专业的印象。 2、什么是消费者组? 消费者组是 Kafka 独有的概念，如果面试官问这 个，就说明他对此是有一定了解的。我先给出标准答案： 1、定义：即消费者组是 Kafka 提供的可扩展且具有容错性的消费者机制。 2、原理：在 Kafka 中，消费者组是一个由多个消费者实例 构成的组。多个实例共同订阅若干个主题，实现共同消费。同一个组下的每个实例都配置有 相同的组 ID，被分配不同的订阅分区。当某个实例挂掉的时候，其他实例会自动地承担起 它负责消费的分区。 此时，又有一个小技巧给到你:消费者组的题目，能够帮你在某种程度上掌控下面的面试方向。 2.1消费者组的位移提交机制 broker维护消费者的消费位移信息，老的版本存储在zk上，新版本存储在内部的topic里。本质上，位移信息消费者自己维护也可以，但是如果消费者挂了或者重启，对于某一个分区的消费位移不就丢失了吗？所以，还是需要提交到broker端做持久化的。 2.2消费者组与 Broker 之间的交互 Kafka中消费者的消费方式 consumer采用pull(拉)模式从broker中读取数据。 拉取模式也有不足，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，kafka消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间后再返回，这段时长即为timeout Kafka的分区分配策略 一个消费者组中有多个消费者，一个broker有多个分区，所有必然会涉及到分区分配问题，即确定哪一个分区由哪一个consumer来消费。kafka有两种分区分配策略：RoundRobin和Range 1） RoundRobin 按照消费者组划分，将消费者组作为一个整体，要求整个消费者组内的消费者订阅相同的主题，否则会导致错误的分配的问题。 2） Range （默认） 按照单个主题划分，可能导致消费者消费分区个数不对等的问题； offset的维护 由于kafka可能出现故障，故障之后要恢复到上一次消费的位置，往下继续进行消费。因此consumer需要实时记录自己消费到了哪一个offset，从0.9版本开始，consumer默认将offset保存在kafka的内置topic中，该topic为_consumer_offsets（0.9版本前放在zookeeper中） 3、在 Kafka 中，ZooKeeper 的作用是什么? 目前，Kafka 使用 ZooKeeper 存放集群元数据、成员管理、Controller 选举，以及其他一些管理类任务。之后，等 KIP-500 提案完成后，Kafka 将完全不再依赖 于 ZooKeeper。 记住，一定要突出“目前”，以彰显你非常了解社区的演进计划。“存放元数据”是指主题 分区的所有数据都保存在 ZooKeeper 中，且以它保存的数据为权威，其他“人”都要与它 保持对齐。“成员管理”是指 Broker 节点的注册、注销以及属性变更，等 等。“Controller 选举”是指选举集群 Controller，而其他管理类任务包括但不限于主题 删除、参数配置等。 不过，抛出 KIP-500 也可能是个双刃剑。碰到非常资深的面试官，他可能会进一步追问你 KIP-500 是怎么做的。一言以蔽之:KIP-500 思想，是使用社区自研的基于 Raft 的共识算法， 替代 ZooKeeper，实现 Controller 自选举。 4、解释下 Kafka 中位移(offset)的作用 在 Kafka 中，每个 主题分区下的每条消息都被赋予了一个唯一的 ID 数值，用于标识它在分区中的位置。这个 ID 数值，就被称为位移，或者叫偏移量。一旦消息被写入到分区日志，它的位移值将不能 被修改。 答完这些之后，你还可以把整个面试方向转移到你希望的地方。常见方法有以下 3 种: 如果你深谙 Broker 底层日志写入的逻辑，可以强调下消息在日志中的存放格式; 如果你明白位移值一旦被确定不能修改，可以强调下“Log Cleaner 组件都不能影响位 移值”这件事情; 如果你对消费者的概念还算熟悉，可以再详细说说位移值和消费者位移值之间的区别。 5、Partition的Leader Replica和Follower Replica的区别 这道题表面上是考核你对 Leader 和 Follower 区别的理解，但很容易引申到 Kafka 的同步 机制上。因此，我建议你主动出击，一次性地把隐含的考点也答出来，也许能够暂时把面试 官“唬住”，并体现你的专业性。 你可以这么回答:Kafka 副本当前分为领导者副本和追随者副本。只有 Leader 副本才能 对外提供读写服务，响应 Clients 端的请求。Follower 副本只是采用拉(PULL)的方 式，被动地同步 Leader 副本中的数据，并且在 Leader 副本所在的 Broker 宕机后，随时 准备应聘 Leader 副本。 通常来说，回答到这个程度，其实才只说了 60%，因此，我建议你再回答两个额外的加分 项。 强调 Follower 副本也能对外提供读服务。自 Kafka 2.4 版本开始，社区通过引入新的 Broker 端参数，允许 Follower 副本有限度地提供读服务。 强调 Leader 和 Follower 的消息序列在实际场景中不一致。很多原因都可能造成 Leader 和 Follower 保存的消息序列不一致，比如程序 Bug、网络问题等。这是很严重 的错误，必须要完全规避。你可以补充下，之前确保一致性的主要手段是高水位机制， 但高水位值无法保证 Leader 连续变更场景下的数据一致性，因此，社区引入了 Leader Epoch 机制，来修复高水位值的弊端。关于“Leader Epoch 机制”，国内的资料不是 很多，它的普及度远不如高水位，不妨大胆地把这个概念秀出来，力求惊艳一把。 炫技式题目 6、LEO、LSO、AR、ISR、HW 都表示什么含义? LEO:Log End Offset。日志末端位移值或末端偏移量，表示日志下一条待插入消息的 位移值。举个例子，如果日志有 10 条消息，位移值从 0 开始，那么，第 10 条消息的位 移值就是 9。此时，LEO = 10。 LSO:Log Stable Offset。这是 Kafka 事务的概念。如果你没有使用到事务，那么这个 值不存在(其实也不是不存在，只是设置成一个无意义的值)。该值控制了事务型消费 者能够看到的消息范围。它经常与 Log Start Offset，即日志起始位移值相混淆，因为 有些人将后者缩写成 LSO，这是不对的。在 Kafka 中，LSO 就是指代 Log Stable Offset。 AR:Assigned Replicas。AR 是主题被创建后，分区创建时被分配的副本集合，副本个 数由副本因子决定。 ISR:In-Sync Replicas。Kafka 中特别重要的概念，指代的是 AR 中那些与 Leader 保 持同步的副本集合。在 AR 中的副本可能不在 ISR 中，但 Leader 副本天然就包含在 ISR 中。关于 ISR，还有一个常见的面试题目是如何判断副本是否应该属于 ISR。目前的判断 依据是:Follower 副本的 LEO 落后 Leader LEO 的时间，是否超过了 Broker 端参数 replica.lag.time.max.ms 值。如果超过了，副本就会被从 ISR 中移除。 HW:高水位值(High watermark)。这是控制消费者可读取消息范围的重要字段。一 个普通消费者只能“看到”Leader 副本上介于 Log Start Offset 和 HW(不含)之间的 所有消息。水位以上的消息是对消费者不可见的。关于 HW，问法有很多，我能想到的 最高级的问法，就是让你完整地梳理下 Follower 副本拉取 Leader 副本、执行同步机制 的详细步骤。这就是我们的第 20 道题的题目，一会儿我会给出答案和解析。 7，Kafka 的 ISR 机制是什么? 这个机制简单来说，就是会自动给每个 Partition 维护一个 ISR 列表，这个列表里一定会有 Leader，然后还会包含跟 Leader 保持同步的 Follower。 也就是说，只要 Leader 的某个 Follower 一直跟他保持数据同步，那么就会存在于 ISR 列表里。 但是如果 Follower 因为自身发生一些问题，导致不能及时的从 Leader 同步数据过去，那么这个 Follower 就会被认为是“out-of-sync”，被从 ISR 列表里踢出去。 所以大家先得明白这个 ISR 是什么，说白了，就是 Kafka 自动维护和监控哪些 Follower 及时的跟上了 Leader 的数据同步。 8，Kafka 写入的数据如何保证不丢失? 所以如果要让写入 Kafka 的数据不丢失，你需要保证如下几点： 每个 Partition 都至少得有 1 个 Follower 在 ISR 列表里，跟上了 Leader 的数据同步。 每次写入数据的时候，都要求至少写入 Partition Leader 成功，同时还有至少一个 ISR 里的 Follower 也写入成功，才算这个写入是成功了。 如果不满足上述两个条件，那就一直写入失败，让生产系统不停的尝试重试，直到满足上述两个条件，然后才能认为写入成功。 按照上述思路去配置相应的参数，才能保证写入 Kafka 的数据不会丢失。 如上图所示，假如现在 Leader 没有 Follower 了，或者是刚写入 Leader，Leader 立马就宕机，还没来得及同步给 Follower。 在这种情况下，写入就会失败，然后你就让生产者不停的重试，直到 Kafka 恢复正常满足上述条件，才能继续写入。这样就可以让写入 Kafka 的数据不丢失。 9. 高水位和Epoch 在Kafka中，高水位的作用主要有两个 定义消息可见性，即用来标识分区下的哪些消息是可以被消费者消费的。 帮助Kafka完成副本同步 下面这张图展示了多个与高水位相关的 Kafka 术语。 假设这是某个分区 Leader 副本的高水位图。首先，请注意图中的“已提交消息”和“未提交消息”。之前在讲到 Kafka 持久性保障的时候，特意对两者进行了区分。现在，再次强调一下。在分区高水位以下的消息被认为是已提交消息，反之就是未提交消息。 消费者只能消费已提交消息，即图中位移小于 8 的所有消息。注意，这里我们不讨论 Kafka 事务，因为事务机制会影响消费者所能看到的消息的范围，它不只是简单依赖高水位来判断。它依靠一个名为 LSO（Log Stable Offset）的位移值来判断事务型消费者的可见性。 另外，位移值等于高水位的消息也属于未提交消息。也就是说，高水位上的消息是不能被消费者消费的。 Log End Offset（LEO） Log End Offset（LEO）表示副本写入下一条消息的位移值。注意，数字 15 所在的方框是虚线，这就说明，这个副本当前只有 15 条消息，位移值是从 0 到 14，下一条新消息的位移是 15。显然，介于高水位和 LEO 之间的消息就属于未提交消息。这也从侧面告诉了我们一个重要的事实，那就是：同一个副本对象，其高水位值不会大于 LEO 值。 高水位和 LEO 是副本对象的两个重要属性。Kafka 所有副本都有对应的高水位和 LEO 值，而不仅仅是 Leader 副本。只不过 Leader 副本比较特殊，Kafka 使用 Leader 副本的高水位来定义所在分区的高水位。换句话说，分区的高水位就是其 Leader 副本的高水位。 高水位和LEO更新机制 现在，我们知道了每个副本对象都保存了一组高水位值和 LEO 值，但实际上，在 Leader 副本所在的 Broker 上，还保存了其他 Follower 副本的HW和LEO 值。 如上图所示，Broker 0 上保存了某分区的 Leader 副本和所有 Follower 副本的 LEO 值，而 Broker 1 上仅仅保存了该分区的某个 Follower 副本。Kafka 把 Broker 0 上保存的这些 Follower 副本又称为远程副本（Remote Replica）。Kafka 副本机制在运行过程中，会更新 Broker 1 上 Follower 副本的高水位和 LEO 值，同时也会更新 Broker 0 上 Leader 副本的高水位和 LEO 以及所有远程副本的 LEO，但它不会更新远程副本的高水位值，也就是我在图中标记为灰色的部分。 保存远程副本的作用主要是帮助 Leader 副本确定其高水位，也就是分区高水位。下图是副本同步机制： 10,Leader副本保持同步 与Leader副本保持同步的判断条件有两个： 该远程Follower副本在ISR中。 该远程Follower副本LEO值落后于Leader副本LEO值的时间，不超过Broker端参数replica.lag.time.max.ms的值（默认值10秒） 副本同步全流程 当生产者发送一条消息时，Leader 和 Follower 副本对应的高水位是怎么被更新的呢？ 首先是初始状态。下面这张图中的 remote LEO 就是刚才的远程副本的 LEO 值。在初始状态时，所有值都是 0。 当生产者给主题分区发送一条消息后，状态变更为： 此时，Leader 副本成功将消息写入了本地磁盘，故 LEO 值被更新为 1。 Follower 再次尝试从 Leader 拉取消息。和之前不同的是，这次有消息可以拉取了，因此状态进一步变更为： 这时，Follower 副本也成功地更新 LEO 为 1。此时，Leader 和 Follower 副本的 LEO 都是 1，但各自的高水位依然是 0，还没有被更新。它们需要在下一轮的拉取中被更新，如下图所示： 在新一轮的拉取请求中，由于位移值是 0 的消息已经拉取成功，因此 Follower 副本这次请求拉取的是位移值 =1 的消息。Leader 副本接收到此请求后，更新远程副本 LEO 为 1，然后更新 Leader 高水位为 1。做完这些之后，它会将当前已更新过的高水位值 1 发送给 Follower 副本。Follower 副本接收到以后，也将自己的高水位值更新成 1。至此，一次完整的消息同步周期就结束了。事实上，Kafka 就是利用这样的机制，实现了 Leader 和 Follower 副本之间的同步。 Leader Epoch 从刚才的分析中，我们知道，Follower 副本的高水位更新需要一轮额外的拉取请求才能实现。如果把上面那个例子扩展到多个 Follower 副本，情况可能更糟，也许需要多轮拉取请求。也就是说，Leader 副本高水位更新和 Follower 副本高水位更新在时间上是存在错配的。这种错配是很多“数据丢失”或“数据不一致”问题的根源。基于此，社区在 0.11 版本正式引入了 Leader Epoch 概念，来规避因高水位更新错配导致的各种不一致问题。 所谓 Leader Epoch，我们大致可以认为是 Leader 版本。它由两部分数据组成。 Epoch。一个单调增加的版本号。每当副本领导权发生变更时，都会增加该版本号。小版本号的 Leader 被认为是过期 Leader，不能再行使 Leader 权力。 起始位移（Start Offset）。Leader 副本在该 Epoch 值上写入的首条消息的位移。 举个例子来说明一下 Leader Epoch。假设现在有两个 Leader Epoch&lt;0, 0&gt; 和 &lt;1, 120&gt;，那么，第一个 Leader Epoch 表示版本号是 0，这个版本的 Leader 从位移 0 开始保存消息，一共保存了 120 条消息。之后，Leader 发生了变更，版本号增加到 1，新版本的起始位移是 120。 Kafka Broker 会在内存中为每个分区都缓存 Leader Epoch 数据，同时它还会定期地将这些信息持久化到一个 checkpoint 文件中。当 Leader 副本写入消息到磁盘时，Broker 会尝试更新这部分缓存。如果该 Leader 是首次写入消息，那么 Broker 会向缓存中增加一个 Leader Epoch 条目，否则就不做更新。这样，每次有 Leader 变更时，新的 Leader 副本会查询这部分缓存，取出对应的 Leader Epoch 的起始位移，以避免数据丢失和不一致的情况。 接下来看一个实际的例子，它展示的是 Leader Epoch 是如何防止数据丢失的。 引用 Leader Epoch 机制后，Follower 副本 B 重启回来后，需要向 A 发送一个特殊的请求去获取 Leader 的 LEO 值。在这个例子中，该值为 2。当获知到 Leader LEO=2 后，B 发现该 LEO 值不比它自己的 LEO 值小，而且缓存中也没有保存任何起始位移值 &gt; 2 的 Epoch 条目，因此 B 无需执行任何日志截断操作。这是对高水位机制的一个明显改进，即副本是否执行日志截断不再依赖于高水位进行判断。 现在，副本 A 宕机了，B 成为 Leader。同样地，当 A 重启回来后，执行与 B 相同的逻辑判断，发现也不用执行日志截断，至此位移值为 1 的那条消息在两个副本中均得到保留。后面当生产者程序向 B 写入新消息时，副本 B 所在的 Broker 缓存中，会生成新的 Leader Epoch 条目：[Epoch=1, Offset=2]。之后，副本 B 会使用这个条目帮助判断后续是否执行日志截断操作。这样，通过 Leader Epoch 机制，Kafka 完美地规避了这种数据丢失场景。 深度思考题 11、Kafka 为什么不支持读写分离? 这道题目考察的是你对 Leader/Follower 模型的思考。 Leader/Follower 模型并没有规定 Follower 副本不可以对外提供读服务。很多框架都是允 许这么做的，只是 Kafka 最初为了避免不一致性的问题，而采用了让 Leader 统一提供服 务的方式。 不过，在开始回答这道题时，你可以率先亮出观点:自 Kafka 2.4 之后，Kafka 提供了有限度的读写分离，也就是说，Follower 副本能够对外提供读服务。 说完这些之后，你可以再给出之前的版本不支持读写分离的理由。 场景不适用。读写分离适用于那种读负载很大，而写操作相对不频繁的场景，可 Kafka 不属于这样的场景。 同步机制。Kafka 采用 PULL 方式实现 Follower 的同步，因此，Follower 与 Leader 存 在不一致性窗口。如果允许读 Follower 副本，就势必要处理消息滞后(Lagging)的问题。 12、如何调优 Kafka? 回答任何调优问题的第一步，就是 确定优化目标，并且定量给出目标! 这点特别重要。对于 Kafka 而言，常见的优化目标是吞吐量、延时、持久性和可用性。每一个方向的优化思路都 是不同的，甚至是相反的。 确定了目标之后，还要明确优化的维度。有些调优属于通用的优化思路，比如对操作系统、 JVM 等的优化;有些则是有针对性的，比如要优化 Kafka 的 TPS。我们需要从 3 个方向去考虑 Producer 端:增加 batch.size、linger.ms，启用压缩，关闭重试等。 Broker 端:增加 num.replica.fetchers，提升 Follower 同步 TPS，避免 Broker Full GC 等。 Consumer:增加 fetch.min.bytes 等 13、Controller 发生网络分区(Network Partitioning)时，Kafka 会怎 么样? 这道题目能够诱发我们对分布式系统设计、CAP 理论、一致性等多方面的思考。不过，针 对故障定位和分析的这类问题，我建议你首先言明“实用至上”的观点，即不论怎么进行理论分析，永远都要以实际结果为准。一旦发生 Controller 网络分区，那么，第一要务就是 查看集群是否出现“脑裂”，即同时出现两个甚至是多个 Controller 组件。这可以根据 Broker 端监控指标 ActiveControllerCount 来判断。 现在，我们分析下，一旦出现这种情况，Kafka 会怎么样。 由于 Controller 会给 Broker 发送 3 类请求，即LeaderAndIsrRequest、 StopReplicaRequest 和 UpdateMetadataRequest，因此，一旦出现网络分区，这些请求将不能顺利到达 Broker 端。这将影响主题的创建、修改、删除操作的信息同步，表现为 集群仿佛僵住了一样，无法感知到后面的所有操作。因此，网络分区通常都是非常严重的问 题，要赶快修复。 14、Java Consumer 为什么采用单线程来获取消息? 在回答之前，如果先把这句话说出来，一定会加分:Java Consumer 是双线程的设计。一 个线程是用户主线程，负责获取消息;另一个线程是心跳线程，负责向 Kafka 汇报消费者 存活情况。将心跳单独放入专属的线程，能够有效地规避因消息处理速度慢而被视为下线 的“假死”情况。 单线程获取消息的设计能够避免阻塞式的消息获取方式。单线程轮询方式容易实现异步非阻塞式，这样便于将消费者扩展成支持实时流处理的操作算子。因为很多实时流处理操作算子都不能是阻塞式的。另外一个可能的好处是，可以简化代码的开发。多线程交互的代码是非常容易出错的。 15、简述 Follower 副本消息同步的完整流程 首先，Follower 发送 FETCH 请求给 Leader。接着，Leader 会读取底层日志文件中的消 息数据，再更新它内存中的 Follower 副本的 LEO 值，更新为 FETCH 请求中的 fetchOffset 值。最后，尝试更新分区高水位值。Follower 接收到 FETCH 响应之后，会把 消息写入到底层日志，接着更新 LEO 和 HW 值。 Leader 和 Follower 的 HW 值更新时机是不同的，Follower 的 HW 更新永远落后于 Leader 的 HW。这种时间上的错配是造成各种不一致的原因。 ","link":"https://tinaxiawuhao.github.io/post/aFuh1RRNc/"},{"title":"第三章 Apache Kafka 与Spark的集成","content":"在本章中，我们将讨论如何将Apache Kafka与Spark Streaming API集成。 关于Spark Spark Streaming API支持实时数据流的可扩展，高吞吐量，容错流处理。 数据可以从诸如Kafka，Flume，Twitter等许多源中提取，并且可以使用复杂的算法来处理，例如地图，缩小，连接和窗口等高级功能。 最后，处理的数据可以推送到文件系统，数据库和活动仪表板。 弹性分布式数据集(RDD)是Spark的基本数据结构。 它是一个不可变的分布式对象集合。 RDD中的每个数据集划分为逻辑分区，可以在集群的不同节点上计算。 与Spark集成 Kafka是Spark流式传输的潜在消息传递和集成平台。 Kafka充当实时数据流的中心枢纽，并使用Spark Streaming中的复杂算法进行处理。 一旦数据被处理，Spark Streaming可以将结果发布到另一个Kafka主题或存储在HDFS，数据库或仪表板中。 下图描述了概念流程。 现在，让我们详细了解Kafka-Spark API。 SparkConf API 它表示Spark应用程序的配置。 用于将各种Spark参数设置为键值对。 SparkConf 类有以下方法 - set(string key，string value) - 设置配置变量。 remove(string key) - 从配置中移除密钥。 setAppName(string name) - 设置应用程序的应用程序名称。 get(string key) - get key StreamingContext API 这是Spark功能的主要入口点。 SparkContext表示到Spark集群的连接，可用于在集群上创建RDD，累加器和广播变量。 签名的定义如下所示。 public StreamingContext(String master, String appName, Duration batchDuration, String sparkHome, scala.collection.Seq&lt;String&gt; jars, scala.collection.Map&lt;String,String&gt; environment) 主 - 要连接的群集网址(例如mesos:// host:port，spark:// host:port，local [4])。 appName - 作业的名称，以显示在集群Web UI上 batchDuration - 流式数据将被分成批次的时间间隔 public StreamingContext(SparkConf conf, Duration batchDuration) 通过提供新的SparkContext所需的配置创建StreamingContext。 conf - Spark参数 batchDuration - 流式数据将被分成批次的时间间隔 KafkaUtils API KafkaUtils API用于将Kafka集群连接到Spark流。 此API具有如下定义的显着方法 createStream 。 public static ReceiverInputDStream&lt;scala.Tuple2&lt;String,String&gt;&gt; createStream( StreamingContext ssc, String zkQuorum, String groupId, scala.collection.immutable.Map&lt;String,Object&gt; topics, StorageLevel storageLevel) 上面显示的方法用于创建从Kafka Brokers提取消息的输入流。 ssc - StreamingContext对象。 zkQuorum - Zookeeper quorum。 groupId - 此消费者的组ID。 主题 - 返回要消费的主题的地图。 storageLevel - 用于存储接收的对象的存储级别。 KafkaUtils API有另一个方法createDirectStream，用于创建一个输入流，直接从Kafka Brokers拉取消息，而不使用任何接收器。 这个流可以保证来自Kafka的每个消息都包含在转换中一次。 示例应用程序在Scala中完成。 要编译应用程序，请下载并安装 sbt ，scala构建工具(类似于maven)。 主要应用程序代码如下所示。 import java.util.HashMap import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, Produc-erRecord} import org.apache.spark.SparkConf import org.apache.spark.streaming._ import org.apache.spark.streaming.kafka._ object KafkaWordCount { def main(args: Array[String]) { if (args.length &lt; 4) { System.err.println(&quot;Usage: KafkaWordCount &lt;zkQuorum&gt;&lt;group&gt; &lt;topics&gt; &lt;numThreads&gt;&quot;) System.exit(1) } val Array(zkQuorum, group, topics, numThreads) = args val sparkConf = new SparkConf().setAppName(&quot;KafkaWordCount&quot;) val ssc = new StreamingContext(sparkConf, Seconds(2)) ssc.checkpoint(&quot;checkpoint&quot;) val topicMap = topics.split(&quot;,&quot;).map((_, numThreads.toInt)).toMap val lines = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap).map(_._2) val words = lines.flatMap(_.split(&quot; &quot;)) val wordCounts = words.map(x =&gt; (x, 1L)) .reduceByKeyAndWindow(_ &amp;plus; _, _ - _, Minutes(10), Seconds(2), 2) wordCounts.print() ssc.start() ssc.awaitTermination() } } 构建脚本 spark-kafka集成取决于Spark，Spark流和Spark与Kafka的集成jar。 创建一个新文件 build.sbt ，并指定应用程序详细信息及其依赖关系。 在编译和打包应用程序时， sbt 将下载所需的jar。 name := &quot;Spark Kafka Project&quot; version := &quot;1.0&quot; scalaVersion := &quot;2.10.5&quot; libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % &quot;1.6.0&quot; libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-streaming&quot; % &quot;1.6.0&quot; libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-streaming-kafka&quot; % &quot;1.6.0&quot; 编译/包装 运行以下命令以编译和打包应用程序的jar文件。 我们需要将jar文件提交到spark控制台以运行应用程序。 sbt package 提交到Spark 启动Kafka Producer CLI(在上一章中解释)，创建一个名为 my-first-topic 的新主题，并提供一些样本消息，如下所示。 Another spark test message 运行以下命令将应用程序提交到spark控制台。 /usr/local/spark/bin/spark-submit --packages org.apache.spark:spark-streaming -kafka_2.10:1.6.0 --class &quot;KafkaWordCount&quot; --master local[4] target/scala-2.10/spark -kafka-project_2.10-1.0.jar localhost:2181 &lt;group name&gt; &lt;topic name&gt; &lt;number of threads&gt; 此应用程序的示例输出如下所示。 spark console messages .. (Test,1) (spark,1) (another,1) (message,1) spark console message .. 原文：https://www.w3cschool.cn/apache_kafka/apache_kafka_integration_spark.html ","link":"https://tinaxiawuhao.github.io/post/2csXIfKdw/"},{"title":"第二章 Apache Kafka 整合 Storm","content":"在本章中，我们将学习如何将Kafka与Apache Storm集成。 关于Storm Storm最初由Nathan Marz和BackType的团队创建。 在短时间内，Apache Storm成为分布式实时处理系统的标准，允许您处理大量数据。 Storm是非常快的，并且一个基准时钟为每个节点每秒处理超过一百万个元组。 Apache Storm持续运行，从配置的源(Spouts)消耗数据，并将数据传递到处理管道(Bolts)。 联合，Spouts和Bolt构成一个拓扑。 与Storm集成 Kafka和Storm自然互补，它们强大的合作能够实现快速移动的大数据的实时流分析。 Kafka和Storm集成是为了使开发人员更容易地从Storm拓扑获取和发布数据流。 概念流 Spouts是流的源。 例如，一个喷头可以从Kafka Topic读取元组并将它们作为流发送。 Bolt消耗输入流，处理并可能发射新的流。 Bolt可以从运行函数，过滤元组，执行流聚合，流连接，与数据库交谈等等做任何事情。 Storm拓扑中的每个节点并行执行。 拓扑无限运行，直到终止它。 Storm将自动重新分配任何失败的任务。 此外，Storm保证没有数据丢失，即使机器停机和消息被丢弃。 让我们详细了解Kafka-Storm集成API。 有三个主要类集成Kafka与Storm。 他们如下 - BrokerHosts - ZkHosts &amp; StaticHosts BrokerHosts是一个接口，ZkHosts和StaticHosts是它的两个主要实现。 ZkHosts用于通过在ZooKeeper中维护细节来动态跟踪Kafka代理，而StaticHosts用于手动/静态设置Kafka代理及其详细信息。 ZkHosts是访问Kafka代理的简单快捷的方式。 ZkHosts的签名如下 - public ZkHosts(String brokerZkStr, String brokerZkPath) public ZkHosts(String brokerZkStr) 其中brokerZkStr是ZooKeeper主机，brokerZkPath是ZooKeeper路径以维护Kafka代理详细信息。 KafkaConfig API 此API用于定义Kafka集群的配置设置。 Kafka Con-fig的签名定义如下 public KafkaConfig(BrokerHosts hosts, string topic) 主机 - BrokerHosts可以是ZkHosts / StaticHosts。 主题 - 主题名称。 SpoutConfig API Spoutconfig是KafkaConfig的扩展，支持额外的ZooKeeper信息。 public SpoutConfig(BrokerHosts hosts, string topic, string zkRoot, string id) 主机 - BrokerHosts可以是BrokerHosts接口的任何实现 主题 - 主题名称。 zkRoot - ZooKeeper根路径。 id - spouts存储在Zookeeper中消耗的偏移量的状态。 ID应该唯一标识您的喷嘴。 SchemeAsMultiScheme SchemeAsMultiScheme是一个接口，用于指示如何将从Kafka中消耗的ByteBuffer转换为风暴元组。 它源自MultiScheme并接受Scheme类的实现。 有很多Scheme类的实现，一个这样的实现是StringScheme，它将字节解析为一个简单的字符串。 它还控制输出字段的命名。 签名定义如下。 public SchemeAsMultiScheme(Scheme scheme) 方案 - 从kafka消耗的字节缓冲区。 KafkaSpout API KafkaSpout是我们的spout实现，它将与Storm集成。 它从kafka主题获取消息，并将其作为元组发送到Storm生态系统。 KafkaSpout从SpoutConfig获取其配置详细信息。 下面是一个创建一个简单的Kafka喷水嘴的示例代码。 // ZooKeeper connection string BrokerHosts hosts = new ZkHosts(zkConnString); //Creating SpoutConfig Object SpoutConfig spoutConfig = new SpoutConfig(hosts, topicName, &quot;/&quot; + topicName UUID.randomUUID().toString()); //convert the ByteBuffer to String. spoutConfig.scheme = new SchemeAsMultiScheme(new StringScheme()); //Assign SpoutConfig to KafkaSpout. KafkaSpout kafkaSpout = new KafkaSpout(spoutConfig); 创建Bolt Bolt是一个使用元组作为输入，处理元组，并产生新的元组作为输出的组件。 Bolt将实现IRichBolt接口。 在此程序中，使用两个Bolt类WordSplitter-Bolt和WordCounterBolt来执行操作。 IRichBolt接口有以下方法 - 准备 - 为Bolt提供要执行的环境。 执行器将运行此方法来初始化喷头。 执行 - 处理单个元组的输入。 清理 - 当Bolt要关闭时调用。 declareOutputFields - 声明元组的输出模式。 让我们创建SplitBolt.java，它实现逻辑分割一个句子到词和CountBolt.java，它实现逻辑分离独特的单词和计数其出现。 SplitBolt.java import java.util.Map; import backtype.storm.tuple.Tuple; import backtype.storm.tuple.Fields; import backtype.storm.tuple.Values; import backtype.storm.task.OutputCollector; import backtype.storm.topology.OutputFieldsDeclarer; import backtype.storm.topology.IRichBolt; import backtype.storm.task.TopologyContext; public class SplitBolt implements IRichBolt { private OutputCollector collector; @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) { this.collector = collector; } @Override public void execute(Tuple input) { String sentence = input.getString(0); String[] words = sentence.split(&quot; &quot;); for(String word: words) { word = word.trim(); if(!word.isEmpty()) { word = word.toLowerCase(); collector.emit(new Values(word)); } } collector.ack(input); } @Override public void declareOutputFields(OutputFieldsDeclarer declarer) { declarer.declare(new Fields(&quot;word&quot;)); } @Override public void cleanup() {} @Override public Map&lt;String, Object&gt; getComponentConfiguration() { return null; } } CountBolt.java import java.util.Map; import java.util.HashMap; import backtype.storm.tuple.Tuple; import backtype.storm.task.OutputCollector; import backtype.storm.topology.OutputFieldsDeclarer; import backtype.storm.topology.IRichBolt; import backtype.storm.task.TopologyContext; public class CountBolt implements IRichBolt{ Map&lt;String, Integer&gt; counters; private OutputCollector collector; @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) { this.counters = new HashMap&lt;String, Integer&gt;(); this.collector = collector; } @Override public void execute(Tuple input) { String str = input.getString(0); if(!counters.containsKey(str)){ counters.put(str, 1); }else { Integer c = counters.get(str) +1; counters.put(str, c); } collector.ack(input); } @Override public void cleanup() { for(Map.Entry&lt;String, Integer&gt; entry:counters.entrySet()){ System.out.println(entry.getKey()&amp;plus;&quot; : &quot; &amp;plus; entry.getValue()); } } @Override public void declareOutputFields(OutputFieldsDeclarer declarer) { } @Override public Map&lt;String, Object&gt; getComponentConfiguration() { return null; } } 提交拓扑 Storm拓扑基本上是一个Thrift结构。 TopologyBuilder类提供了简单而容易的方法来创建复杂的拓扑。 TopologyBuilder类具有设置spout(setSpout)和设置bolt(setBolt)的方法。 最后，TopologyBuilder有createTopology来创建to-pology。 shuffleGrouping和fieldsGrouping方法有助于为喷头和Bolt设置流分组。 本地集群 - 为了开发目的，我们可以使用 LocalCluster 对象创建本地集群，然后使用 LocalCluster的 submitTopology 类。 KafkaStormSample.java import backtype.storm.Config; import backtype.storm.LocalCluster; import backtype.storm.topology.TopologyBuilder; import java.util.ArrayList; import java.util.List; import java.util.UUID; import backtype.storm.spout.SchemeAsMultiScheme; import storm.kafka.trident.GlobalPartitionInformation; import storm.kafka.ZkHosts; import storm.kafka.Broker; import storm.kafka.StaticHosts; import storm.kafka.BrokerHosts; import storm.kafka.SpoutConfig; import storm.kafka.KafkaConfig; import storm.kafka.KafkaSpout; import storm.kafka.StringScheme; public class KafkaStormSample { public static void main(String[] args) throws Exception{ Config config = new Config(); config.setDebug(true); config.put(Config.TOPOLOGY_MAX_SPOUT_PENDING, 1); String zkConnString = &quot;localhost:2181&quot;; String topic = &quot;my-first-topic&quot;; BrokerHosts hosts = new ZkHosts(zkConnString); SpoutConfig kafkaSpoutConfig = new SpoutConfig (hosts, topic, &quot;/&quot; + topic, UUID.randomUUID().toString()); kafkaSpoutConfig.bufferSizeBytes = 1024 * 1024 * 4; kafkaSpoutConfig.fetchSizeBytes = 1024 * 1024 * 4; kafkaSpoutConfig.forceFromStart = true; kafkaSpoutConfig.scheme = new SchemeAsMultiScheme(new StringScheme()); TopologyBuilder builder = new TopologyBuilder(); builder.setSpout(&quot;kafka-spout&quot;, new KafkaSpout(kafkaSpoutCon-fig)); builder.setBolt(&quot;word-spitter&quot;, new SplitBolt()).shuffleGroup-ing(&quot;kafka-spout&quot;); builder.setBolt(&quot;word-counter&quot;, new CountBolt()).shuffleGroup-ing(&quot;word-spitter&quot;); LocalCluster cluster = new LocalCluster(); cluster.submitTopology(&quot;KafkaStormSample&quot;, config, builder.create-Topology()); Thread.sleep(10000); cluster.shutdown(); } } 在移动编译之前，Kakfa-Storm集成需要策展人ZooKeeper客户端java库。 策展人版本2.9.1支持Apache Storm 0.9.5版(我们在本教程中使用)。 下载下面指定的jar文件并将其放在java类路径中。 curator-client-2.9.1.jar curator-framework-2.9.1.jar 在包括依赖文件之后，使用以下命令编译程序， javac -cp &quot;/path/to/Kafka/apache-storm-0.9.5/lib/*&quot; *.java 执行 启动Kafka Producer CLI(在上一章节中解释)，创建一个名为 my-first-topic 的新主题，并提供一些样本消息，如下所示 - hello kafka storm spark test message another test message 现在使用以下命令执行应用程序 - java -cp “/path/to/Kafka/apache-storm-0.9.5/lib/*&quot;:. KafkaStormSample 此应用程序的示例输出如下所示 - storm : 1 test : 2 spark : 1 another : 1 kafka : 1 hello : 1 message : 2 原文：https://www.w3cschool.cn/apache_kafka/apache_kafka_integration_storm.html ","link":"https://tinaxiawuhao.github.io/post/wAqEEPZon/"},{"title":"第一章 Kafka的安装与使用","content":"Kafka 基础知识 对于大数据，我们要考虑的问题有很多，首先海量数据如何收集（如 Flume），然后对于收集到的数据如何存储（典型的分布式文件系统 HDFS、分布式数据库 HBase、NoSQL 数据库 Redis），其次存储的数据不是存起来就没事了，要通过计算从中获取有用的信息，这就涉及到计算模型（典型的离线计算 MapReduce、流式实时计算Storm、Spark），或者要从数据中挖掘信息，还需要相应的机器学习算法。在这些之上，还有一些各种各样的查询分析数据的工具（如 Hive、Pig 等）。除此之外，要构建分布式应用还需要一些工具，比如分布式协调服务 Zookeeper 等等。 这里，我们讲到的是消息系统，Kafka 专为分布式高吞吐量系统而设计，其他消息传递系统相比，Kafka 具有更好的吞吐量，内置分区，复制和固有的容错能力，这使得它非常适合大规模消息处理应用程序。 消息系统 ​ 点对点消息系统：生产者发送一条消息到queue，一个queue可以有很多消费者，但是一个消息只能被一个消费者接受，当没有消费者可用时，这个消息会被保存直到有 一个可用的消费者，所以Queue实现了一个可靠的负载均衡。 ​ 发布订阅消息系统：发布者发送到topic的消息，只有订阅了topic的订阅者才会收到消息。topic实现了发布和订阅，当你发布一个消息，所有订阅这个topic的服务都能得到这个消息，所以从1到N个订阅者都能得到这个消息的拷贝。 kafka术语 Apache Kafka 是一个分布式发布 - 订阅消息系统和一个强大的队列，可以处理大量的数据，并使你能够将消息从一个端点传递到另一个端点。 Kafka 适合离线和在线消息消费。 Kafka 消息保留在磁盘上，并在群集内复制以防止数据丢失。 Kafka 构建在 ZooKeeper 同步服务之上。 它与 Apache Storm 和 Spark 非常好地集成，用于实时流式数据分析。 Kafka 是一个分布式消息队列，具有高性能、持久化、多副本备份、横向扩展能力。生产者往队列里写消息，消费者从队列里取消息进行业务逻辑。一般在架构设计中起到解耦、削峰、异步处理的作用。 ​ 消息由producer产生，消息按照topic归类，并发送到broker中，broker中保存了一个或多个topic的消息，consumer通过订阅一组topic的消息，通过持续的poll操作从broker获取消息，并进行后续的消息处理。 Producer ：消息生产者，就是向broker发指定topic消息的客户端。 Consumer ：消息消费者，通过订阅一组topic的消息，从broker读取消息的客户端。 Broker ：一个kafka集群包含一个或多个服务器，一台kafka服务器就是一个broker，用于保存producer发送的消息。一个broker可以容纳多个topic。 Topic ：每条发送到broker的消息都有一个类别，可以理解为一个队列或者数据库的一张表。 Partition：一个topic的消息由多个partition队列存储的，一个partition队列在kafka上称为一个分区。每个partition是一个有序的队列，多个partition间则是无序的。partition中的每条消息都会被分配一个有序的id（offset）。 Offset：偏移量。kafka为每条在分区的消息保存一个偏移量offset，这也是消费者在分区的位置。kafka的存储文件都是按照offset.kafka来命名，位于2049位置的即为2048.kafka的文件。比如一个偏移量是5的消费者，表示已经消费了从0-4偏移量的消息，下一个要消费的消息的偏移量是5。 Consumer Group （CG）：若干个Consumer组成的集合。这是kafka用来实现一个topic消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个topic可以有多个CG。topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个CG只会把消息发给该CG中的一个consumer。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。 ​ 假如一个消费者组有两个消费者，订阅了一个具有4个分区的topic的消息，那么这个消费者组的每一个消费者都会消费两个分区的消息。消费者组的成员是动态维护的，如果新增或者减少了消费者组中的消费者，那么每个消费者消费的分区的消息也会动态变化。比如原来一个消费者组有两个消费者，其中一个消费者因为故障而不能继续消费消息了，那么剩下一个消费者将会消费全部4个分区的消息。 Apache Kafka基本原理 1、分布式和分区（distributed、partitioned） 我们说 kafka 是一个分布式消息系统，所谓的分布式，实际上我们已经大致了解。消息保存在 Topic 中，而为了能够实现大数据的存储，一个 topic 划分为多个分区，每个分区对应一个文件，可以分别存储到不同的机器上，以实现分布式的集群存储。另外，每个 partition 可以有一定的副本，备份到多台机器上，以提高可用性。 总结起来就是：一个 topic 对应的多个 partition 分散存储到集群中的多个 broker 上，存储方式是一个 partition 对应一个文件，每个 broker 负责存储在自己机器上的 partition 中的消息读写。 2、副本（replicated ） kafka 还可以配置 partitions 需要备份的个数(replicas),每个 partition 将会被备份到多台机器上,以提高可用性，备份的数量可以通过配置文件指定。 这种冗余备份的方式在分布式系统中是很常见的，那么既然有副本，就涉及到对同一个文件的多个备份如何进行管理和调度。kafka 采取的方案是：每个 partition 选举一个 server 作为“leader”，由 leader 负责所有对该分区的读写，其他 server 作为 follower 只需要简单的与 leader 同步，保持跟进即可。如果原来的 leader 失效，会重新选举由其他的 follower 来成为新的 leader。 至于如何选取 leader，实际上如果我们了解 ZooKeeper，就会发现其实这正是 Zookeeper 所擅长的，Kafka 使用 ZK 在 Broker 中选出一个 Controller，用于 Partition 分配和 Leader 选举。 另外，这里我们可以看到，实际上作为 leader 的 server 承担了该分区所有的读写请求，因此其压力是比较大的，从整体考虑，从多少个 partition 就意味着会有多少个leader，kafka 会将 leader 分散到不同的 broker 上，确保整体的负载均衡。 3、整体数据流程 Kafka 的总体数据流满足下图，该图可以说是概括了整个 kafka 的基本原理。 （1）数据生产过程（Produce） 对于生产者要写入的一条记录，可以指定四个参数：分别是 topic、partition、key 和 value，其中 topic 和 value（要写入的数据）是必须要指定的，而 key 和 partition 是可选的。 对于一条记录，先对其进行序列化，然后按照 Topic 和 Partition，放进对应的发送队列中。如果 Partition 没填，那么情况会是这样的：a、Key 有填。按照 Key 进行哈希，相同 Key 去一个 Partition。b、Key 没填。Round-Robin 来选 Partition。 producer 将会和Topic下所有 partition leader 保持 socket 连接，消息由 producer 直接通过 socket 发送到 broker。其中 partition leader 的位置( host : port )注册在 zookeeper 中，producer 作为 zookeeper client，已经注册了 watch 用来监听 partition leader 的变更事件，因此，可以准确的知道谁是当前的 leader。 producer 端采用异步发送：将多条消息暂且在客户端 buffer 起来，并将他们批量的发送到 broker，小数据 IO 太多，会拖慢整体的网络延迟，批量延迟发送事实上提升了网络效率。 （2）数据消费过程（Consume） 对于消费者，不是以单独的形式存在的，每一个消费者属于一个 consumer group，一个 group 包含多个 consumer。特别需要注意的是：订阅 Topic 是以一个消费组来订阅的，发送到 Topic 的消息，只会被订阅此 Topic 的每个 group 中的一个 consumer 消费。 如果所有的 Consumer 都具有相同的 group，那么就像是一个点对点的消息系统；如果每个 consumer 都具有不同的 group，那么消息会广播给所有的消费者。 具体说来，这实际上是根据 partition 来分的，一个 Partition，只能被消费组里的一个消费者消费，但是可以同时被多个消费组消费，消费组里的每个消费者是关联到一个 partition 的，因此有这样的说法：对于一个 topic,同一个 group 中不能有多于 partitions 个数的 consumer 同时消费,否则将意味着某些 consumer 将无法得到消息。 同一个消费组的两个消费者不会同时消费一个 partition。 在 kafka 中，采用了 pull 方式，即 consumer 在和 broker 建立连接之后，主动去 pull(或者说 fetch )消息，首先 consumer 端可以根据自己的消费能力适时的去 fetch 消息并处理，且可以控制消息消费的进度(offset)。 partition 中的消息只有一个 consumer 在消费，且不存在消息状态的控制，也没有复杂的消息确认机制，可见 kafka broker 端是相当轻量级的。当消息被 consumer 接收之后，需要保存 Offset 记录消费到哪，以前保存在 ZK 中，由于 ZK 的写性能不好，以前的解决方法都是 Consumer 每隔一分钟上报一次，在 0.10 版本后，Kafka 把这个 Offset 的保存，从 ZK 中剥离，保存在一个名叫 consumeroffsets topic 的 Topic 中，由此可见，consumer 客户端也很轻量级。 4、消息传送机制 Kafka 支持 3 种消息投递语义,在业务中，常常都是使用 At least once 的模型。 At most once：最多一次，消息可能会丢失，但不会重复。 At least once：最少一次，消息不会丢失，可能会重复。 Exactly once：只且一次，消息不丢失不重复，只且消费一次。 kafka安装和使用 在Windows安装运行Kafka：https://blog.csdn.net/weixin_38004638/article/details/91893910 kafka运行 一次写入，支持多个应用读取，读取信息是相同的 kafka-study.pom &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka_2.12&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-nop&lt;/artifactId&gt; &lt;version&gt;1.7.24&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; Producer生产者 ​ 发送消息的方式，只管发送，不管结果：只调用接口发送消息到 Kafka 服务器，但不管成功写入与否。由于 Kafka 是高可用的，因此大部分情况下消息都会写入，但在异常情况下会丢消息 同步发送：调用 send() 方法返回一个 Future 对象，我们可以使用它的 get() 方法来判断消息发送成功与否 异步发送：调用 send() 时提供一个回调方法，当接收到 broker 结果后回调此方法 public class MyProducer { private static KafkaProducer&lt;String, String&gt; producer; //初始化 static { Properties properties = new Properties(); //kafka启动，生产者建立连接broker的地址 properties.put(&quot;bootstrap.servers&quot;, &quot;127.0.0.1:9092&quot;); //kafka序列化方式 properties.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); properties.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); //自定义分区分配器 properties.put(&quot;partitioner.class&quot;, &quot;com.imooc.kafka.CustomPartitioner&quot;); producer = new KafkaProducer&lt;&gt;(properties); } /** * 创建topic：.\\bin\\windows\\kafka-topics.bat --create --zookeeper localhost:2181 * --replication-factor 1 --partitions 1 --topic kafka-study * 创建消费者：.\\bin\\windows\\kafka-console-consumer.bat --bootstrap-server localhost:9092 * --topic kafka-study --from-beginning */ //发送消息，发送完后不做处理 private static void sendMessageForgetResult() { ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(&quot;kafka-study&quot;, &quot;name&quot;, &quot;ForgetResult&quot;); producer.send(record); producer.close(); } //发送同步消息，获取发送的消息 private static void sendMessageSync() throws Exception { ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(&quot;kafka-study&quot;, &quot;name&quot;, &quot;sync&quot;); RecordMetadata result = producer.send(record).get(); System.out.println(result.topic());//kafka-study System.out.println(result.partition());//分区为name的hash对应分区 System.out.println(result.offset());//已发送一条消息，此时偏移量+1 producer.close(); } /** * 创建topic：.\\bin\\windows\\kafka-topics.bat --create --zookeeper localhost:2181 * --replication-factor 1 --partitions 3 --topic kafka-study-x * 创建消费者：.\\bin\\windows\\kafka-console-consumer.bat --bootstrap-server localhost:9092 * --topic kafka-study-x --from-beginning */ //发送异步消息，获取回调的消息 private static void sendMessageCallback() { ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(&quot;kafka-study-x&quot;, &quot;name&quot;, &quot;callback&quot;); producer.send(record, new MyProducerCallback()); //发送多条消息 record = new ProducerRecord&lt;&gt;(&quot;kafka-study-x&quot;, &quot;name-x&quot;, &quot;callback&quot;); producer.send(record, new MyProducerCallback()); producer.close(); } } //发送异步消息 //场景：每条消息发送有延迟，多条消息发送，无需同步等待，可以执行其他操作，程序会自动异步调用 private static class MyProducerCallback implements Callback { @Override public void onCompletion(RecordMetadata recordMetadata, Exception e) { if (e != null) { e.printStackTrace(); return; } System.out.println(&quot;*** MyProducerCallback ***&quot;); System.out.println(recordMetadata.topic()); System.out.println(recordMetadata.partition()); System.out.println(recordMetadata.offset()); } } public static void main(String[] args) throws Exception { //sendMessageForgetResult(); //sendMessageSync(); sendMessageCallback(); } } 自定义分区分配器：决定消息存放在哪个分区.。默认分配器使用轮询存放，轮到已满分区将会写入失败。 public class CustomPartitioner implements Partitioner { @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) { //获取topic所有分区 List&lt;PartitionInfo&gt; partitionInfos = cluster.partitionsForTopic(topic); int numPartitions = partitionInfos.size(); //消息必须有key if (null == keyBytes || !(key instanceof String)) { throw new InvalidRecordException(&quot;kafka message must have key&quot;); } //如果只有一个分区，即0号分区 if (numPartitions == 1) {return 0;} //如果key为name，发送至最后一个分区 if (key.equals(&quot;name&quot;)) {return numPartitions - 1;} return Math.abs(Utils.murmur2(keyBytes)) % (numPartitions - 1); } @Override public void close() {} @Override public void configure(Map&lt;String, ?&gt; map) {} } 启动生产者发送消息，通过自定义分区分配器分配，查询到topic信息的offset、partitioner topic partition offset kafka-study 0 1 kafka-study 1 2 kafka-study-x 0 1 Kafka消费者（组） public class MyConsumer { private static KafkaConsumer&lt;String, String&gt; consumer; private static Properties properties; //初始化 static { properties = new Properties(); //建立连接broker的地址 properties.put(&quot;bootstrap.servers&quot;, &quot;127.0.0.1:9092&quot;); //kafka反序列化 properties.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); properties.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); //指定消费者组 properties.put(&quot;group.id&quot;, &quot;KafkaStudy&quot;); } //自动提交位移：由consume自动管理提交 private static void generalConsumeMessageAutoCommit() { //配置 properties.put(&quot;enable.auto.commit&quot;, true); consumer = new KafkaConsumer&lt;&gt;(properties); //指定topic consumer.subscribe(Collections.singleton(&quot;kafka-study-x&quot;)); try { while (true) { boolean flag = true; //拉取信息，超时时间100ms ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); //遍历打印消息 for (ConsumerRecord&lt;String, String&gt; record : records) { System.out.println(String.format( &quot;topic = %s, partition = %s, key = %s, value = %s&quot;, record.topic(), record.partition(), record.key(), record.value() )); //消息发送完成 if (record.value().equals(&quot;done&quot;)) { flag = false; } } if (!flag) { break; } } } finally { consumer.close(); } } //手动同步提交当前位移，根据需求提交，但容易发送阻塞，提交失败会进行重试直到抛出异常 private static void generalConsumeMessageSyncCommit() { properties.put(&quot;auto.commit.offset&quot;, false); consumer = new KafkaConsumer&lt;&gt;(properties); consumer.subscribe(Collections.singletonList(&quot;kafka-study-x&quot;)); while (true) { boolean flag = true; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) { System.out.println(String.format( &quot;topic = %s, partition = %s, key = %s, value = %s&quot;, record.topic(), record.partition(), record.key(), record.value() )); if (record.value().equals(&quot;done&quot;)) { flag = false; } } try { //手动同步提交 consumer.commitSync(); } catch (CommitFailedException ex) { System.out.println(&quot;commit failed error: &quot; + ex.getMessage()); } if (!flag) { break; } } } //手动异步提交当前位移，提交速度快，但失败不会记录 private static void generalConsumeMessageAsyncCommit() { properties.put(&quot;auto.commit.offset&quot;, false); consumer = new KafkaConsumer&lt;&gt;(properties); consumer.subscribe(Collections.singletonList(&quot;kafka-study-x&quot;)); while (true) { boolean flag = true; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) { System.out.println(String.format( &quot;topic = %s, partition = %s, key = %s, value = %s&quot;, record.topic(), record.partition(), record.key(), record.value() )); if (record.value().equals(&quot;done&quot;)) { flag = false; } } //手动异步提交 consumer.commitAsync(); if (!flag) { break; } } } //手动异步提交当前位移带回调 private static void generalConsumeMessageAsyncCommitWithCallback() { properties.put(&quot;auto.commit.offset&quot;, false); consumer = new KafkaConsumer&lt;&gt;(properties); consumer.subscribe(Collections.singletonList(&quot;kafka-study-x&quot;)); while (true) { boolean flag = true; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) { System.out.println(String.format( &quot;topic = %s, partition = %s, key = %s, value = %s&quot;, record.topic(), record.partition(), record.key(), record.value() )); if (record.value().equals(&quot;done&quot;)) { flag = false; } } //使用java8函数式编程 consumer.commitAsync((map, e) -&gt; { if (e != null) { System.out.println(&quot;commit failed for offsets: &quot; + e.getMessage()); } }); if (!flag) { break; } } } //混合同步与异步提交位移 @SuppressWarnings(&quot;all&quot;) private static void mixSyncAndAsyncCommit() { properties.put(&quot;auto.commit.offset&quot;, false); consumer = new KafkaConsumer&lt;&gt;(properties); consumer.subscribe(Collections.singletonList(&quot;kafka-study-x&quot;)); try { while (true) { //boolean flag = true; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) { System.out.println(String.format( &quot;topic = %s, partition = %s, key = %s, &quot; + &quot;value = %s&quot;, record.topic(), record.partition(), record.key(), record.value() )); //if (record.value().equals(&quot;done&quot;)) { flag = false; } } //手动异步提交，保证性能 consumer.commitAsync(); //if (!flag) { break; } } } catch (Exception ex) { System.out.println(&quot;commit async error: &quot; + ex.getMessage()); } finally { try { //异步提交失败，再尝试手动同步提交 consumer.commitSync(); } finally { consumer.close(); } } } public static void main(String[] args) { //自动提交位移 generalConsumeMessageAutoCommit(); //手动同步提交当前位移 //generalConsumeMessageSyncCommit(); //手动异步提交当前位移 //generalConsumeMessageAsyncCommit(); //手动异步提交当前位移带回调 //generalConsumeMessageAsyncCommitWithCallback() //混合同步与异步提交位移 //mixSyncAndAsyncCommit(); } } 先启动消费者等待接收消息，再启动生产者发送消息，进行消费消息 topic partition key value kafka-study-x 1 name-x callback kafka-study-x 2 name-x callback kafka-study-x 1 name-x callback kafka-study-x 1 name-x callback kafka-study-x 2 name-x callback ","link":"https://tinaxiawuhao.github.io/post/w7MMk_8-a/"},{"title":"附录 Flink常见面试问题汇总","content":"面试题一：应用架构 问题： 公司怎么提交的实时任务， 有多少 Job Manager、 Task Manager？ 解答： 我们使用 yarn session 模式提交任务； 另一种方式是每次提交都会创建一个新的 Flink 集群， 为每一个 job 提供资源， 任务之间互相独立， 互不影响， 方便管理。 任务执行完成之后创建的集群也会消失。 线上命令脚本如下：bin/yarn-session.sh -n 7 -s 8 -jm 3072 -tm 32768 -qu root.. -nm - -d其中申请 7 个 taskManager， 每个 8 核， 每个 taskmanager 有 32768M 内存。 集群默认只有一个 Job Manager。 但为了防止单点故障， 我们配置了高可用。 对于 standlone 模式， 我们公司一般配置一个主 Job Manager， 两个备用 Job Manager， 然后结合 ZooKeeper 的使用， 来达到高可用； 对于 yarn 模式， yarn 在 Job Mananger 故障会自动进行重启， 所以只需要一个， 我们配置的最大重启次数是 10 次。 面试题二：压测和监控 问题： 怎么做压力测试和监控？ 解答： 我们一般碰到的压力来自以下几个方面： 产生数据流的速度如果过快， 而下游的算子消费不过来的话， 会产生背压。 背压的监控可以使用 Flink Web UI(localhost:8081) 来可视化监控 Metrics， 一旦报警 就能知道。 一般情况下背压问题的产生可能是由于 sink 这个 操作符没有优化好， 做一下优化就可以了。 比如如果是写入 ElasticSearch， 那么可以改成批量写入， 可以调大 ElasticSearch 队列的大小等等策略。 设置 watermark 的最大延迟时间这个参数， 如果设置的过大， 可能会造成 内存的压力。 可以设置最大延迟时间小一些， 然后把迟到元素发送到侧输出流中去。 晚一点更新结果。 或者使用类似于 RocksDB 这样的状态后端， RocksDB 会开辟 堆外存储空间， 但 IO 速度会变慢， 需要权衡。 还有就是滑动窗口的长度如果过长， 而滑动距离很短的话， Flink 的性能 会下降的很厉害。 我们主要通过时间分片的方法， 将每个元素只存入一个“ 重叠窗 口” ， 这样就可以减少窗口处理中状态的写入。 参见链接： https://www.infoq.cn/article/sIhs_qY6HCpMQNblTI9M 状态后端使用 RocksDB， 还没有碰到被撑爆的问题。 面试题三：为什么用 Flink 问题： 为什么使用 Flink 替代 Spark？ 解答： 主要考虑的是 flink 的低延迟、 高吞吐量和对流式数据应用场景更好的支 持； 另外， flink 可以很好地处理乱序数据， 而且可以保证 exactly-once 的状态一致 性。 详见文档第一章， 有 Flink 和 Spark 的详细对比。 面试题四：checkpoint 的存储 问题： Flink 的 checkpoint 存在哪里？ 解答： 可以是内存， 文件系统， 或者 RocksDB。 详见文档 第九章 状态编程和容错机制。 面试题五：exactly-once 的保证 问题： 如果下级存储不支持事务， Flink 怎么保证 exactly-once？ 解答： 端到端的 exactly-once 对 sink 要求比较高， 具体实现主要有幂等写入和 事务性写入两种方式。 幂等写入的场景依赖于业务逻辑， 更常见的是用事务性写入。 而事务性写入又有预写日志（ WAL） 和两阶段提交（ 2PC） 两种方式。 如果外部系统不支持事务， 那么可以用预写日志的方式， 把结果数据先当成状 态保存， 然后在收到 checkpoint 完成的通知时， 一次性写入 sink 系统。 参见文档第九章 状态编程和容错机制 面试题六：状态机制 问题： 说一下 Flink 状态机制？ 解答： Flink 内置的很多算子， 包括源 source， 数据存储 sink 都是有状态的。 在 Flink 中， 状态始终与特定算子相关联。 Flink 会以 checkpoint 的形式对各个任务的 状态进行快照， 用于保证故障恢复时的状态一致性。 Flink 通过状态后端来管理状态 和 checkpoint 的存储， 状态后端可以有不同的配置选择。 详见文档第九章。 面试题七：海量 key 去重 问题： 怎么去重？ 考虑一个实时场景： 双十一场景， 滑动窗口长度为 1 小时， 滑动距离为 10 秒钟， 亿级用户， 怎样计算 UV？ 解答： 使用类似于 scala 的 set 数据结构或者 redis 的 set 显然是不行的， 因为可能有上亿个 Key， 内存放不下。 所以可以考虑使用布隆过滤器（ Bloom Filter） 来去重。 面试题八：checkpoint 与 spark 比较 问题： Flink 的 checkpoint 机制对比 spark 有什么不同和优势？ 解答： spark streaming 的 checkpoint 仅仅是针对 driver 的故障恢复做了数据 和元数据的 checkpoint。 而 flink 的 checkpoint 机制 要复杂了很多， 它采用的是 轻量级的分布式快照， 实现了每个算子的快照， 及流动中的数据的快照。 参见文档 第九章 状态编程和容错机制及文章链接： https://cloud.tencent.com/developer/article/1189624 面试题九：watermark 机制 问题： 请详细解释一下 Flink 的 Watermark 机制。 解答： Watermark 本质是 Flink 中衡量 EventTime 进展的一个机制， 主要用来处 理乱序数据。 详见文档第七章 时间语义与 Wartermark。 面试题十：exactly-once 如何实现 问题： Flink 中 exactly-once 语义是如何实现的， 状态是如何存储的？ 解答： Flink 依靠 checkpoint 机制来实现 exactly-once 语义， 如果要实现端到端 的 exactly-once， 还需要外部 source 和 sink 满足一定的条件。 状态的存储通过状态 后端来管理， Flink 中可以配置不同的状态后端。 详见文档 第九章 状态编程和容错机制。 面试题十一：CEP 问题： Flink CEP 编程中当状态没有到达的时候会将数据保存在哪里？ 解答： 在流式处理中， CEP 当然是要支持 EventTime 的， 那么相对应的也要 支持数据的迟到现象， 也就是 watermark 的处理逻辑。 CEP 对未匹配成功的事件序 列的处理， 和迟到数据是类似的。 在 Flink CEP 的处理逻辑中， 状态没有满足的和 迟到的数据， 都会存储在一个 Map 数据结构中， 也就是说， 如果我们限定判断事件 序列的时长为 5 分钟， 那么内存中就会存储 5 分钟的数据， 这在我看来， 也是对内 存的极大损伤之一。 面试题十二：三种时间语义 问题： Flink 三种时间语义是什么， 分别说出应用场景？ 解答： Event Time： 这是实际应用最常见的时间语义， 具体见文档第七章。 Processing Time： 没有事件时间的情况下， 或者对实时性要求超高的情况下。 Ingestion Time： 存在多个 Source Operator 的情况下， 每个 Source Operator 可以使用自己本地系统时钟指派 Ingestion Time。 后续基于时间相关的各种操作， 都会使用数据记录中的 Ingestion Time。 面试题十三：数据高峰的处理 问题： Flink 程序在面对数据高峰期时如何处理？ 解答： 使用大容量的 Kafka 把数据先放到消息队列里面作为数据源， 再使用 Flink 进行消费， 不过这样会影响到一点实时性 ","link":"https://tinaxiawuhao.github.io/post/EEiV7EREL/"},{"title":"第十章 Table API 与 SQL","content":"Table API 是流处理和批处理通用的关系型 API， Table API 可以基于流输入或者批输入来运行而不需要进行任何修改。 Table API 是 SQL 语言的超集并专门为 Apache Flink 设计的， Table API 是 Scala 和 Java 语言集成式的 API。 与常规 SQL 语言中将查询指定为字符串不同， Table API 查询是以 Java 或 Scala 中的语言嵌入样式来定义 的， 具有 IDE 支持如:自动完成和语法检测。 需要引入的 pom 依赖 如果你想在 IDE 本地运行你的程序，你需要添加下面的模块，具体用哪个取决于你使用哪个 Planner： &lt;!-- Either... (for the old planner that was available before Flink 1.9) --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner_2.12&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- or.. (for the new Blink planner) --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner-blink_2.12&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; 此外，根据目标编程语言，您需要添加Java或Scala API。 &lt;!-- Either... --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-api-java-bridge_2.11&lt;/artifactId&gt; &lt;version&gt;1.8.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- or... --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-api-scala-bridge_2.11&lt;/artifactId&gt; &lt;version&gt;1.8.0&lt;/version&gt; &lt;/dependency&gt; 在内部，表生态系统的一部分是在Scala中实现的。 因此，请确保为批处理和流应用程序添加以下依赖项： &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-scala_2.11&lt;/artifactId&gt; &lt;version&gt;1.8.0&lt;/version&gt; &lt;/dependency&gt; 如果要实现与Kafka或一组用户定义函数交互的自定义格式，以下依赖关系就足够了，可用于SQL客户端的JAR文件： &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-common&lt;/artifactId&gt; &lt;version&gt;1.8.0&lt;/version&gt; &lt;/dependency&gt; 目前，该模块包括以下扩展点： SerializationSchemaFactory DeserializationSchemaFactory ScalarFunction TableFunction AggregateFunction 两种计划器（Planner）的主要区别 Blink 将批处理作业视作流处理的一种特例。严格来说，Table 和 DataSet 之间不支持相互转换，并且批处理作业也不会转换成 DataSet 程序而是转换成 DataStream 程序，流处理作业也一样。 Blink 计划器不支持 BatchTableSource，而是使用有界的 StreamTableSource 来替代。 旧计划器和 Blink 计划器中 FilterableTableSource 的实现是不兼容的。旧计划器会将 PlannerExpression 下推至 FilterableTableSource，而 Blink 计划器则是将 Expression 下推。 基于字符串的键值配置选项仅在 Blink 计划器中使用。（详情参见 配置 ） PlannerConfig 在两种计划器中的实现（CalciteConfig）是不同的。 Blink 计划器会将多sink（multiple-sinks）优化成一张有向无环图（DAG），TableEnvironment 和 StreamTableEnvironment 都支持该特性。旧计划器总是将每个sink都优化成一个新的有向无环图，且所有图相互独立。 旧计划器目前不支持 catalog 统计数据，而 Blink 支持。 简单了解 TableAPI 所有用于批处理和流处理的 Table API 和 SQL 程序都遵循相同的模式。下面的代码示例展示了 Table API 和 SQL 程序的通用结构。 public static void main(String[] args) throws Exception { // 对于批处理程序来说使用 ExecutionEnvironment 来替换 StreamExecutionEnvironment StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 创建一个TableEnvironment // 对于批处理程序来说使用 BatchTableEnvironment 替换 StreamTableEnvironment StreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env); // 注册一个 Table tableEnv.registerTable(&quot;table1&quot;, ...) // 或者 tableEnv.registerTableSource(&quot;table2&quot;, ...); // 或者 tableEnv.registerExternalCatalog(&quot;extCat&quot;, ...); // 从Table API的查询中创建一个Table Table tapiResult = tableEnv.scan(&quot;table1&quot;).select(...); // 从SQL查询中创建一个Table Table sqlResult = tableEnv.sql(&quot;SELECT ... FROM table2 ... &quot;); // 将Table API 种的结果 Table 发射到TableSink中 , SQL查询也是一样的 tapiResult.writeToSink(...); // 执行 env.execute(); } 创建 TableEnvironment TableEnvironment 是 Table API 和 SQL 的核心概念。它负责: 在内部的 catalog 中注册 Table 注册外部的 catalog 加载可插拔模块 执行 SQL 查询 注册自定义函数 （scalar、table 或 aggregation） 将 DataStream 或 DataSet 转换成 Table 持有对 ExecutionEnvironment 或 StreamExecutionEnvironment 的引用 Table 总是与特定的 TableEnvironment 绑定。不能在同一条查询中使用不同 TableEnvironment 中的表，例如，对它们进行 join 或 union 操作。 TableEnvironment 可以通过静态方法 BatchTableEnvironment.create() 或者 StreamTableEnvironment.create() 在 StreamExecutionEnvironment 或者 ExecutionEnvironment 中创建，TableConfig 是可选项。TableConfig可用于配置TableEnvironment或定制的查询优化和转换过程(参见 查询优化)。 请确保选择与你的编程语言匹配的特定的计划器BatchTableEnvironment/StreamTableEnvironment。 如果两种计划器的 jar 包都在 classpath 中（默认行为），你应该明确地设置要在当前程序中使用的计划器。 // ********************** // FLINK STREAMING QUERY // ********************** import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.EnvironmentSettings; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; EnvironmentSettings fsSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build(); StreamExecutionEnvironment fsEnv = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment fsTableEnv = StreamTableEnvironment.create(fsEnv, fsSettings); // or TableEnvironment fsTableEnv = TableEnvironment.create(fsSettings); // ****************** // FLINK BATCH QUERY // ****************** import org.apache.flink.api.java.ExecutionEnvironment; import org.apache.flink.table.api.bridge.java.BatchTableEnvironment; ExecutionEnvironment fbEnv = ExecutionEnvironment.getExecutionEnvironment(); BatchTableEnvironment fbTableEnv = BatchTableEnvironment.create(fbEnv); // ********************** // BLINK STREAMING QUERY // ********************** import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.EnvironmentSettings; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment(); EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build(); StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings); // or TableEnvironment bsTableEnv = TableEnvironment.create(bsSettings); // ****************** // BLINK BATCH QUERY // ****************** import org.apache.flink.table.api.EnvironmentSettings; import org.apache.flink.table.api.TableEnvironment; EnvironmentSettings bbSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build(); TableEnvironment bbTableEnv = TableEnvironment.create(bbSettings); 在 Catalog 中创建表 TableEnvironment 维护着一个由标识符（identifier）创建的表 catalog 的映射。标识符由三个部分组成：catalog 名称、数据库名称以及对象名称。如果 catalog 或者数据库没有指明，就会使用当前默认值（参见表标识符扩展章节中的例子）。 Table 可以是虚拟的（视图 VIEWS）也可以是常规的（表 TABLES）。视图 VIEWS可以从已经存在的Table中创建，一般是 Table API 或者 SQL 的查询结果。 表TABLES描述的是外部数据，例如文件、数据库表或者消息队列。 临时表（Temporary Table）和永久表（Permanent Table） 表可以是临时的，并与单个 Flink 会话（session）的生命周期相关，也可以是永久的，并且在多个 Flink 会话和群集（cluster）中可见。 永久表需要 catalog（例如 Hive Metastore）以维护表的元数据。一旦永久表被创建，它将对任何连接到 catalog 的 Flink 会话可见且持续存在，直至被明确删除。 另一方面，临时表通常保存于内存中并且仅在创建它们的 Flink 会话持续期间存在。这些表对于其它会话是不可见的。它们不与任何 catalog 或者数据库绑定但可以在一个命名空间（namespace）中创建。即使它们对应的数据库被删除，临时表也不会被删除。 屏蔽（Shadowing） 可以使用与已存在的永久表相同的标识符去注册临时表。临时表会屏蔽永久表，并且只要临时表存在，永久表就无法访问。所有使用该标识符的查询都将作用于临时表。 这可能对实验（experimentation）有用。它允许先对一个临时表进行完全相同的查询，例如只有一个子集的数据，或者数据是不确定的。一旦验证了查询的正确性，就可以对实际的生产表进行查询。 创建表 虚拟表 在 SQL 的术语中，Table API 的对象对应于视图（虚拟表）。它封装了一个逻辑查询计划。它可以通过以下方法在 catalog 中创建： // get a TableEnvironment TableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section // table is the result of a simple projection query Table projTable = tableEnv.from(&quot;X&quot;).select(...); // register the Table projTable as table &quot;projectedTable&quot; tableEnv.createTemporaryView(&quot;projectedTable&quot;, projTable); 注意： 从传统数据库系统的角度来看，Table 对象与 VIEW 视图非常像。也就是，定义了 Table 的查询是没有被优化的， 而且会被内嵌到另一个引用了这个注册了的 Table的查询中。如果多个查询都引用了同一个注册了的Table，那么它会被内嵌每个查询中并被执行多次， 也就是说注册了的Table的结果不会被共享（注：Blink 计划器的TableEnvironment会优化成只执行一次）。 Connector Tables 另外一个方式去创建 TABLE 是通过 connector 声明。Connector 描述了存储表数据的外部系统。存储系统例如 Apache Kafka 或者常规的文件系统都可以通过这种方式来声明。 tableEnvironment .connect(...) .withFormat(...) .withSchema(...) .inAppendMode() .createTemporaryTable(&quot;MyTable&quot;) 扩展表标识符 表总是通过三元标识符注册，包括 catalog 名、数据库名和表名。 用户可以指定一个 catalog 和数据库作为 “当前catalog” 和”当前数据库”。有了这些，那么刚刚提到的三元标识符的前两个部分就可以被省略了。如果前两部分的标识符没有指定， 那么会使用当前的 catalog 和当前数据库。用户也可以通过 Table API 或 SQL 切换当前的 catalog 和当前的数据库。 标识符遵循 SQL 标准，因此使用时需要用反引号（```）进行转义。 TableEnvironment tEnv = ...; tEnv.useCatalog(&quot;custom_catalog&quot;); tEnv.useDatabase(&quot;custom_database&quot;); Table table = ...; // register the view named 'exampleView' in the catalog named 'custom_catalog' // in the database named 'custom_database' tableEnv.createTemporaryView(&quot;exampleView&quot;, table); // register the view named 'exampleView' in the catalog named 'custom_catalog' // in the database named 'other_database' tableEnv.createTemporaryView(&quot;other_database.exampleView&quot;, table); // register the view named 'example.View' in the catalog named 'custom_catalog' // in the database named 'custom_database' tableEnv.createTemporaryView(&quot;`example.View`&quot;, table); // register the view named 'exampleView' in the catalog named 'other_catalog' // in the database named 'other_database' tableEnv.createTemporaryView(&quot;other_catalog.other_database.exampleView&quot;, table); 查询表 Table API Table API 是关于 Scala 和 Java 的集成语言式查询 API。与 SQL 相反，Table API 的查询不是由字符串指定，而是在宿主语言中逐步构建。 Table API 是基于 Table 类的，该类表示一个表（流或批处理），并提供使用关系操作的方法。这些方法返回一个新的 Table 对象，该对象表示对输入 Table 进行关系操作的结果。 一些关系操作由多个方法调用组成，例如 table.groupBy(...).select()，其中 groupBy(...) 指定 table 的分组，而 select(...) 在 table 分组上的投影。 文档 Table API 说明了所有流处理和批处理表支持的 Table API 算子。 以下示例展示了一个简单的 Table API 聚合查询： // get a TableEnvironment TableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section // register Orders table // scan registered Orders table Table orders = tableEnv.from(&quot;Orders&quot;); // compute revenue for all customers from France Table revenue = orders .filter($(&quot;cCountry&quot;).isEqual(&quot;FRANCE&quot;)) .groupBy($(&quot;cID&quot;), $(&quot;cName&quot;) .select($(&quot;cID&quot;), $(&quot;cName&quot;), $(&quot;revenue&quot;).sum().as(&quot;revSum&quot;)); // emit or convert Table // execute query SQL Flink SQL 是基于实现了SQL标准的 Apache Calcite 的。SQL 查询由常规字符串指定。 文档 SQL 描述了Flink对流处理和批处理表的SQL支持。 下面的示例演示了如何指定查询并将结果作为 Table 对象返回。 // get a TableEnvironment TableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section // register Orders table // compute revenue for all customers from France Table revenue = tableEnv.sqlQuery( &quot;SELECT cID, cName, SUM(revenue) AS revSum &quot; + &quot;FROM Orders &quot; + &quot;WHERE cCountry = 'FRANCE' &quot; + &quot;GROUP BY cID, cName&quot; ); // emit or convert Table // execute query 如下的示例展示了如何指定一个更新查询，将查询的结果插入到已注册的表中。 // get a TableEnvironment TableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section // register &quot;Orders&quot; table // register &quot;RevenueFrance&quot; output table // compute revenue for all customers from France and emit to &quot;RevenueFrance&quot; tableEnv.executeSql( &quot;INSERT INTO RevenueFrance &quot; + &quot;SELECT cID, cName, SUM(revenue) AS revSum &quot; + &quot;FROM Orders &quot; + &quot;WHERE cCountry = 'FRANCE' &quot; + &quot;GROUP BY cID, cName&quot; ); 混用 Table API 和 SQL Table API 和 SQL 查询的混用非常简单因为它们都返回 Table 对象： 可以在 SQL 查询返回的 Table 对象上定义 Table API 查询。 在 TableEnvironment 中注册的结果表可以在 SQL 查询的 FROM 子句中引用，通过这种方法就可以在 Table API 查询的结果上定义 SQL 查询。 输出表 Table 通过写入 TableSink 输出。TableSink 是一个通用接口，用于支持多种文件格式（如 CSV、Apache Parquet、Apache Avro）、存储系统（如 JDBC、Apache HBase、Apache Cassandra、Elasticsearch）或消息队列系统（如 Apache Kafka、RabbitMQ）。 批处理 Table 只能写入 BatchTableSink，而流处理 Table 需要指定写入 AppendStreamTableSink，RetractStreamTableSink 或者 UpsertStreamTableSink。 请参考文档 Table Sources &amp; Sinks 以获取更多关于可用 Sink 的信息以及如何自定义 TableSink。 方法 Table.executeInsert(String tableName) 将 Table 发送至已注册的 TableSink。该方法通过名称在 catalog 中查找 TableSink 并确认Table schema 和 TableSink schema 一致。 下面的示例演示如何输出 Table： // get a TableEnvironment TableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section // create an output Table final Schema schema = new Schema() .field(&quot;a&quot;, DataTypes.INT()) .field(&quot;b&quot;, DataTypes.STRING()) .field(&quot;c&quot;, DataTypes.BIGINT()); tableEnv.connect(new FileSystem().path(&quot;/path/to/file&quot;)) .withFormat(new Csv().fieldDelimiter('|').deriveSchema()) .withSchema(schema) .createTemporaryTable(&quot;CsvSinkTable&quot;); // compute a result Table using Table API operators and/or SQL queries Table result = ... // emit the result Table to the registered TableSink result.executeInsert(&quot;CsvSinkTable&quot;); 翻译与执行查询 两种计划器翻译和执行查询的方式是不同的。 Blink planner Old planner 不论输入数据源是流式的还是批式的，Table API 和 SQL 查询都会被转换成 DataStream 程序。查询在内部表示为逻辑查询计划，并被翻译成两个阶段： 优化逻辑执行计划 翻译成 DataStream 程序 Table API 或者 SQL 查询在下列情况下会被翻译： 当 TableEnvironment.executeSql() 被调用时。该方法是用来执行一个 SQL 语句，一旦该方法被调用， SQL 语句立即被翻译。 当 Table.executeInsert() 被调用时。该方法是用来将一个表的内容插入到目标表中，一旦该方法被调用， TABLE API 程序立即被翻译。 当 Table.execute() 被调用时。该方法是用来将一个表的内容收集到本地，一旦该方法被调用， TABLE API 程序立即被翻译。 当 StatementSet.execute() 被调用时。Table （通过 StatementSet.addInsert() 输出给某个 Sink）和 INSERT 语句 （通过调用 StatementSet.addInsertSql()）会先被缓存到 StatementSet 中，StatementSet.execute() 方法被调用时，所有的 sink 会被优化成一张有向无环图。 当 Table 被转换成 DataStream 时（参阅与 DataStream 和 DataSet API 结合）。转换完成后，它就成为一个普通的 DataStream 程序，并会在调用 StreamExecutionEnvironment.execute() 时被执行。 注意 从 1.11 版本开始，sqlUpdate 方法 和 insertInto 方法被废弃，从这两个方法构建的 Table 程序必须通过 StreamTableEnvironment.execute() 方法执行，而不能通过 StreamExecutionEnvironment.execute() 方法来执行。 与 DataStream 和 DataSet API 结合 在流处理方面两种计划器都可以与 DataStream API 结合。只有旧计划器可以与 DataSet API 结合。在批处理方面，Blink 计划器不能同两种计划器中的任何一个结合。 注意： 下文讨论的 DataSet API 只与旧计划起有关。 Table API 和 SQL 可以被很容易地集成并嵌入到 DataStream 和 DataSet 程序中。例如，可以查询外部表（例如从 RDBMS），进行一些预处理，例如过滤，投影，聚合或与元数据 join，然后使用 DataStream 或 DataSet API（以及在这些 API 之上构建的任何库，例如 CEP 或 Gelly）。相反，也可以将 Table API 或 SQL 查询应用于 DataStream 或 DataSet 程序的结果。 这种交互可以通过 DataStream 或 DataSet 与 Table 的相互转化实现。本节我们会介绍这些转化是如何实现的。 Scala 隐式转换 Scala Table API 含有对 DataSet、DataStream 和 Table 类的隐式转换。 通过为 Scala DataStream API 导入 org.apache.flink.table.api.bridge.scala._ 包以及 org.apache.flink.api.scala._ 包，可以启用这些转换。 通过 DataSet 或 DataStream 创建视图 在 TableEnvironment 中可以将 DataStream 或 DataSet 注册成视图。结果视图的 schema 取决于注册的 DataStream 或 DataSet 的数据类型。请参阅文档 数据类型到 table schema 的映射获取详细信息。 注意： 通过 DataStream 或 DataSet 创建的视图只能注册成临时视图。 // get StreamTableEnvironment // registration of a DataSet in a BatchTableEnvironment is equivalent StreamTableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ... // register the DataStream as View &quot;myTable&quot; with fields &quot;f0&quot;, &quot;f1&quot; tableEnv.createTemporaryView(&quot;myTable&quot;, stream); // register the DataStream as View &quot;myTable2&quot; with fields &quot;myLong&quot;, &quot;myString&quot; tableEnv.createTemporaryView(&quot;myTable2&quot;, stream, $(&quot;myLong&quot;), $(&quot;myString&quot;)); 将 DataStream 或 DataSet 转换成表 与在 TableEnvironment 注册 DataStream 或 DataSet 不同，DataStream 和 DataSet 还可以直接转换成 Table。如果你想在 Table API 的查询中使用表，这将非常便捷。 // get StreamTableEnvironment // registration of a DataSet in a BatchTableEnvironment is equivalent StreamTableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ... // Convert the DataStream into a Table with default fields &quot;f0&quot;, &quot;f1&quot; Table table1 = tableEnv.fromDataStream(stream); // Convert the DataStream into a Table with fields &quot;myLong&quot;, &quot;myString&quot; Table table2 = tableEnv.fromDataStream(stream, $(&quot;myLong&quot;), $(&quot;myString&quot;)); 将表转换成 DataStream 或 DataSet Table 可以被转换成 DataStream 或 DataSet。通过这种方式，定制的 DataSet 或 DataStream 程序就可以在 Table API 或者 SQL 的查询结果上运行了。 将 Table 转换为 DataStream 或者 DataSet 时，你需要指定生成的 DataStream 或者 DataSet 的数据类型，即，Table 的每行数据要转换成的数据类型。通常最方便的选择是转换成 Row 。以下列表概述了不同选项的功能： Row: 字段按位置映射，字段数量任意，支持 null 值，无类型安全（type-safe）检查。 POJO: 字段按名称映射（POJO 必须按Table 中字段名称命名），字段数量任意，支持 null 值，无类型安全检查。 Case Class: 字段按位置映射，不支持 null 值，有类型安全检查。 Tuple: 字段按位置映射，字段数量少于 22（Scala）或者 25（Java），不支持 null 值，无类型安全检查。 Atomic Type: Table 必须有一个字段，不支持 null 值，有类型安全检查。 将表转换成 DataStream 流式查询（streaming query）的结果表会动态更新，即，当新纪录到达查询的输入流时，查询结果会改变。因此，像这样将动态查询结果转换成 DataStream 需要对表的更新方式进行编码。 将 Table 转换为 DataStream 有两种模式： Append Mode: 仅当动态 Table 仅通过INSERT更改进行修改时，才可以使用此模式，即，它仅是追加操作，并且之前输出的结果永远不会更新。 Retract Mode: 任何情形都可以使用此模式。它使用 boolean 值对 INSERT 和 DELETE 操作的数据进行标记。 // get StreamTableEnvironment. StreamTableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section // Table with two fields (String name, Integer age) Table table = ... // convert the Table into an append DataStream of Row by specifying the class DataStream&lt;Row&gt; dsRow = tableEnv.toAppendStream(table, Row.class); // convert the Table into an append DataStream of Tuple2&lt;String, Integer&gt; // via a TypeInformation TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = new TupleTypeInfo&lt;&gt;( Types.STRING(), Types.INT()); DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = tableEnv.toAppendStream(table, tupleType); // convert the Table into a retract DataStream of Row. // A retract stream of type X is a DataStream&lt;Tuple2&lt;Boolean, X&gt;&gt;. // The boolean field indicates the type of the change. // True is INSERT, false is DELETE. DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; retractStream = tableEnv.toRetractStream(table, Row.class); 注意： 文档动态表给出了有关动态表及其属性的详细讨论。 注意 一旦 Table 被转化为 DataStream，必须使用 StreamExecutionEnvironment 的 execute 方法执行该 DataStream 作业。 将表转换成 DataSet 将 Table 转换成 DataSet 的过程如下： // get BatchTableEnvironment BatchTableEnvironment tableEnv = BatchTableEnvironment.create(env); // Table with two fields (String name, Integer age) Table table = ... // convert the Table into a DataSet of Row by specifying a class DataSet&lt;Row&gt; dsRow = tableEnv.toDataSet(table, Row.class); // convert the Table into a DataSet of Tuple2&lt;String, Integer&gt; via a TypeInformation TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = new TupleTypeInfo&lt;&gt;( Types.STRING(), Types.INT()); DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = tableEnv.toDataSet(table, tupleType); 注意 一旦 Table 被转化为 DataSet，必须使用 ExecutionEnvironment 的 execute 方法执行该 DataSet 作业。 数据类型到 Table Schema 的映射 Flink 的 DataStream 和 DataSet APIs 支持多样的数据类型。例如 Tuple（Scala 内置以及Flink Java tuple）、POJO 类型、Scala case class 类型以及 Flink 的 Row 类型等允许嵌套且有多个可在表的表达式中访问的字段的复合数据类型。其他类型被视为原子类型。下面，我们讨论 Table API 如何将这些数据类型类型转换为内部 row 表示形式，并提供将 DataStream 转换成 Table 的样例。 数据类型到 table schema 的映射有两种方式：基于字段位置或基于字段名称。 基于位置映射 基于位置的映射可在保持字段顺序的同时为字段提供更有意义的名称。这种映射方式可用于具有特定的字段顺序的复合数据类型以及原子类型。如 tuple、row 以及 case class 这些复合数据类型都有这样的字段顺序。然而，POJO 类型的字段则必须通过名称映射（参见下一章）。可以将字段投影出来，但不能使用as重命名。 定义基于位置的映射时，输入数据类型中一定不能存在指定的名称，否则 API 会假定应该基于字段名称进行映射。如果未指定任何字段名称，则使用默认的字段名称和复合数据类型的字段顺序，或者使用 f0 表示原子类型。 // get a StreamTableEnvironment, works for BatchTableEnvironment equivalently StreamTableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section; DataStream&lt;Tuple2&lt;Long, Integer&gt;&gt; stream = ... // convert DataStream into Table with default field names &quot;f0&quot; and &quot;f1&quot; Table table = tableEnv.fromDataStream(stream); // convert DataStream into Table with field &quot;myLong&quot; only Table table = tableEnv.fromDataStream(stream, $(&quot;myLong&quot;)); // convert DataStream into Table with field names &quot;myLong&quot; and &quot;myInt&quot; Table table = tableEnv.fromDataStream(stream, $(&quot;myLong&quot;), $(&quot;myInt&quot;)); 基于名称的映射 基于名称的映射适用于任何数据类型包括 POJO 类型。这是定义 table schema 映射最灵活的方式。映射中的所有字段均按名称引用，并且可以通过 as 重命名。字段可以被重新排序和映射。 若果没有指定任何字段名称，则使用默认的字段名称和复合数据类型的字段顺序，或者使用 f0 表示原子类型。 // get a StreamTableEnvironment, works for BatchTableEnvironment equivalently StreamTableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section DataStream&lt;Tuple2&lt;Long, Integer&gt;&gt; stream = ... // convert DataStream into Table with default field names &quot;f0&quot; and &quot;f1&quot; Table table = tableEnv.fromDataStream(stream); // convert DataStream into Table with field &quot;f1&quot; only Table table = tableEnv.fromDataStream(stream, $(&quot;f1&quot;)); // convert DataStream into Table with swapped fields Table table = tableEnv.fromDataStream(stream, $(&quot;f1&quot;), $(&quot;f0&quot;)); // convert DataStream into Table with swapped fields and field names &quot;myInt&quot; and &quot;myLong&quot; Table table = tableEnv.fromDataStream(stream, $(&quot;f1&quot;).as(&quot;myInt&quot;), $(&quot;f0&quot;).as(&quot;myLong&quot;)); 原子类型 Flink 将基础数据类型（Integer、Double、String）或者通用数据类型（不可再拆分的数据类型）视为原子类型。原子类型的 DataStream 或者 DataSet 会被转换成只有一条属性的 Table。属性的数据类型可以由原子类型推断出，还可以重新命名属性。 // get a StreamTableEnvironment, works for BatchTableEnvironment equivalently StreamTableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section DataStream&lt;Long&gt; stream = ... // convert DataStream into Table with default field name &quot;f0&quot; Table table = tableEnv.fromDataStream(stream); // convert DataStream into Table with field name &quot;myLong&quot; Table table = tableEnv.fromDataStream(stream, $(&quot;myLong&quot;)); Tuple类型（Scala 和 Java）和 Case Class类型（仅 Scala） Flink 支持 Scala 的内置 tuple 类型并给 Java 提供自己的 tuple 类型。两种 tuple 的 DataStream 和 DataSet 都能被转换成表。可以通过提供所有字段名称来重命名字段（基于位置映射）。如果没有指明任何字段名称，则会使用默认的字段名称。如果引用了原始字段名称（对于 Flink tuple 为f0、f1 … …，对于 Scala tuple 为_1、_2 … …），则 API 会假定映射是基于名称的而不是基于位置的。基于名称的映射可以通过 as 对字段和投影进行重新排序。 // get a StreamTableEnvironment, works for BatchTableEnvironment equivalently StreamTableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ... // convert DataStream into Table with default field names &quot;f0&quot;, &quot;f1&quot; Table table = tableEnv.fromDataStream(stream); // convert DataStream into Table with renamed field names &quot;myLong&quot;, &quot;myString&quot; (position-based) Table table = tableEnv.fromDataStream(stream, $(&quot;myLong&quot;), $(&quot;myString&quot;)); // convert DataStream into Table with reordered fields &quot;f1&quot;, &quot;f0&quot; (name-based) Table table = tableEnv.fromDataStream(stream, $(&quot;f1&quot;), $(&quot;f0&quot;)); // convert DataStream into Table with projected field &quot;f1&quot; (name-based) Table table = tableEnv.fromDataStream(stream, $(&quot;f1&quot;)); // convert DataStream into Table with reordered and aliased fields &quot;myString&quot;, &quot;myLong&quot; (name-based) Table table = tableEnv.fromDataStream(stream, $(&quot;f1&quot;).as(&quot;myString&quot;), $(&quot;f0&quot;).as(&quot;myLong&quot;)); POJO 类型 （Java 和 Scala） Flink 支持 POJO 类型作为复合类型。确定 POJO 类型的规则记录在这里. 在不指定字段名称的情况下将 POJO 类型的 DataStream 或 DataSet 转换成 Table 时，将使用原始 POJO 类型字段的名称。名称映射需要原始名称，并且不能按位置进行。字段可以使用别名（带有 as 关键字）来重命名，重新排序和投影。 // get a StreamTableEnvironment, works for BatchTableEnvironment equivalently StreamTableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section // Person is a POJO with fields &quot;name&quot; and &quot;age&quot; DataStream&lt;Person&gt; stream = ... // convert DataStream into Table with default field names &quot;age&quot;, &quot;name&quot; (fields are ordered by name!) Table table = tableEnv.fromDataStream(stream); // convert DataStream into Table with renamed fields &quot;myAge&quot;, &quot;myName&quot; (name-based) Table table = tableEnv.fromDataStream(stream, $(&quot;age&quot;).as(&quot;myAge&quot;), $(&quot;name&quot;).as(&quot;myName&quot;)); // convert DataStream into Table with projected field &quot;name&quot; (name-based) Table table = tableEnv.fromDataStream(stream, $(&quot;name&quot;)); // convert DataStream into Table with projected and renamed field &quot;myName&quot; (name-based) Table table = tableEnv.fromDataStream(stream, $(&quot;name&quot;).as(&quot;myName&quot;)); Row类型 Row 类型支持任意数量的字段以及具有 null 值的字段。字段名称可以通过 RowTypeInfo 指定，也可以在将 Row 的 DataStream 或 DataSet 转换为 Table 时指定。Row 类型的字段映射支持基于名称和基于位置两种方式。字段可以通过提供所有字段的名称的方式重命名（基于位置映射）或者分别选择进行投影/排序/重命名（基于名称映射）。 // get a StreamTableEnvironment, works for BatchTableEnvironment equivalently StreamTableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section // DataStream of Row with two fields &quot;name&quot; and &quot;age&quot; specified in `RowTypeInfo` DataStream&lt;Row&gt; stream = ... // convert DataStream into Table with default field names &quot;name&quot;, &quot;age&quot; Table table = tableEnv.fromDataStream(stream); // convert DataStream into Table with renamed field names &quot;myName&quot;, &quot;myAge&quot; (position-based) Table table = tableEnv.fromDataStream(stream, $(&quot;myName&quot;), $(&quot;myAge&quot;)); // convert DataStream into Table with renamed fields &quot;myName&quot;, &quot;myAge&quot; (name-based) Table table = tableEnv.fromDataStream(stream, $(&quot;name&quot;).as(&quot;myName&quot;), $(&quot;age&quot;).as(&quot;myAge&quot;)); // convert DataStream into Table with projected field &quot;name&quot; (name-based) Table table = tableEnv.fromDataStream(stream, $(&quot;name&quot;)); // convert DataStream into Table with projected and renamed field &quot;myName&quot; (name-based) Table table = tableEnv.fromDataStream(stream, $(&quot;name&quot;).as(&quot;myName&quot;)); 查询优化 Blink planner Old planner Apache Flink 使用并扩展了 Apache Calcite 来执行复杂的查询优化。 这包括一系列基于规则和成本的优化，例如： 基于 Apache Calcite 的子查询解相关 投影剪裁 分区剪裁 过滤器下推 子计划消除重复数据以避免重复计算 特殊子查询重写，包括两部分： 将 IN 和 EXISTS 转换为 left semi-joins 将 NOT IN 和 NOT EXISTS 转换为 left anti-join 可选 join 重新排序 通过 table.optimizer.join-reorder-enabled 启用 注意： 当前仅在子查询重写的结合条件下支持 IN / EXISTS / NOT IN / NOT EXISTS。 优化器不仅基于计划，而且还基于可从数据源获得的丰富统计信息以及每个算子（例如 io，cpu，网络和内存）的细粒度成本来做出明智的决策。 高级用户可以通过 CalciteConfig 对象提供自定义优化，可以通过调用 TableEnvironment＃getConfig＃setPlannerConfig 将其提供给 TableEnvironment。 解释表 Table API 提供了一种机制来解释计算 Table 的逻辑和优化查询计划。 这是通过 Table.explain() 方法或者 StatementSet.explain() 方法来完成的。Table.explain() 返回一个 Table 的计划。StatementSet.explain() 返回多 sink 计划的结果。它返回一个描述三种计划的字符串： 关系查询的抽象语法树（the Abstract Syntax Tree），即未优化的逻辑查询计划， 优化的逻辑查询计划，以及 物理执行计划。 可以用 TableEnvironment.explainSql() 方法和 TableEnvironment.executeSql() 方法支持执行一个 EXPLAIN 语句获取逻辑和优化查询计划，请参阅 EXPLAIN 页面. 以下代码展示了一个示例以及对给定 Table 使用 Table.explain() 方法的相应输出： StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream1 = env.fromElements(new Tuple2&lt;&gt;(1, &quot;hello&quot;)); DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream2 = env.fromElements(new Tuple2&lt;&gt;(1, &quot;hello&quot;)); // explain Table API Table table1 = tEnv.fromDataStream(stream1, $(&quot;count&quot;), $(&quot;word&quot;)); Table table2 = tEnv.fromDataStream(stream2, $(&quot;count&quot;), $(&quot;word&quot;)); Table table = table1 .where($(&quot;word&quot;).like(&quot;F%&quot;)) .unionAll(table2); System.out.println(table.explain()); 上述例子的结果是： == Abstract Syntax Tree == LogicalUnion(all=[true]) LogicalFilter(condition=[LIKE($1, _UTF-16LE'F%')]) FlinkLogicalDataStreamScan(id=[1], fields=[count, word]) FlinkLogicalDataStreamScan(id=[2], fields=[count, word]) == Optimized Logical Plan == DataStreamUnion(all=[true], union all=[count, word]) DataStreamCalc(select=[count, word], where=[LIKE(word, _UTF-16LE'F%')]) DataStreamScan(id=[1], fields=[count, word]) DataStreamScan(id=[2], fields=[count, word]) == Physical Execution Plan == Stage 1 : Data Source content : collect elements with CollectionInputFormat Stage 2 : Data Source content : collect elements with CollectionInputFormat Stage 3 : Operator content : from: (count, word) ship_strategy : REBALANCE Stage 4 : Operator content : where: (LIKE(word, _UTF-16LE'F%')), select: (count, word) ship_strategy : FORWARD Stage 5 : Operator content : from: (count, word) ship_strategy : REBALANCE 以下代码展示了一个示例以及使用 StatementSet.explain() 的多 sink 计划的相应输出： EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build(); TableEnvironment tEnv = TableEnvironment.create(settings); final Schema schema = new Schema() .field(&quot;count&quot;, DataTypes.INT()) .field(&quot;word&quot;, DataTypes.STRING()); tEnv.connect(new FileSystem().path(&quot;/source/path1&quot;)) .withFormat(new Csv().deriveSchema()) .withSchema(schema) .createTemporaryTable(&quot;MySource1&quot;); tEnv.connect(new FileSystem().path(&quot;/source/path2&quot;)) .withFormat(new Csv().deriveSchema()) .withSchema(schema) .createTemporaryTable(&quot;MySource2&quot;); tEnv.connect(new FileSystem().path(&quot;/sink/path1&quot;)) .withFormat(new Csv().deriveSchema()) .withSchema(schema) .createTemporaryTable(&quot;MySink1&quot;); tEnv.connect(new FileSystem().path(&quot;/sink/path2&quot;)) .withFormat(new Csv().deriveSchema()) .withSchema(schema) .createTemporaryTable(&quot;MySink2&quot;); StatementSet stmtSet = tEnv.createStatementSet(); Table table1 = tEnv.from(&quot;MySource1&quot;).where($(&quot;word&quot;).like(&quot;F%&quot;)); stmtSet.addInsert(&quot;MySink1&quot;, table1); Table table2 = table1.unionAll(tEnv.from(&quot;MySource2&quot;)); stmtSet.addInsert(&quot;MySink2&quot;, table2); String explanation = stmtSet.explain(); System.out.println(explanation); 多 sink 计划的结果是： == Abstract Syntax Tree == LogicalLegacySink(name=[MySink1], fields=[count, word]) +- LogicalFilter(condition=[LIKE($1, _UTF-16LE'F%')]) +- LogicalTableScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]]) LogicalLegacySink(name=[MySink2], fields=[count, word]) +- LogicalUnion(all=[true]) :- LogicalFilter(condition=[LIKE($1, _UTF-16LE'F%')]) : +- LogicalTableScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]]) +- LogicalTableScan(table=[[default_catalog, default_database, MySource2, source: [CsvTableSource(read fields: count, word)]]]) == Optimized Logical Plan == Calc(select=[count, word], where=[LIKE(word, _UTF-16LE'F%')], reuse_id=[1]) +- TableSourceScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word]) LegacySink(name=[MySink1], fields=[count, word]) +- Reused(reference_id=[1]) LegacySink(name=[MySink2], fields=[count, word]) +- Union(all=[true], union=[count, word]) :- Reused(reference_id=[1]) +- TableSourceScan(table=[[default_catalog, default_database, MySource2, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word]) == Physical Execution Plan == Stage 1 : Data Source content : collect elements with CollectionInputFormat Stage 2 : Operator content : CsvTableSource(read fields: count, word) ship_strategy : REBALANCE Stage 3 : Operator content : SourceConversion(table:Buffer(default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]), fields:(count, word)) ship_strategy : FORWARD Stage 4 : Operator content : Calc(where: (word LIKE _UTF-16LE'F%'), select: (count, word)) ship_strategy : FORWARD Stage 5 : Operator content : SinkConversionToRow ship_strategy : FORWARD Stage 6 : Operator content : Map ship_strategy : FORWARD Stage 8 : Data Source content : collect elements with CollectionInputFormat Stage 9 : Operator content : CsvTableSource(read fields: count, word) ship_strategy : REBALANCE Stage 10 : Operator content : SourceConversion(table:Buffer(default_catalog, default_database, MySource2, source: [CsvTableSource(read fields: count, word)]), fields:(count, word)) ship_strategy : FORWARD Stage 12 : Operator content : SinkConversionToRow ship_strategy : FORWARD Stage 13 : Operator content : Map ship_strategy : FORWARD Stage 7 : Data Sink content : Sink: CsvTableSink(count, word) ship_strategy : FORWARD Stage 14 : Data Sink content : Sink: CsvTableSink(count, word) ship_strategy : FORWARD 动态表 如果流中的数据类型是 case class 可以直接根据 case class 的结构生成 table tableEnv.fromDataStream(dataStream) 或者根据字段顺序单独命名 tableEnv.fromDataStream(dataStream,’id,’timestamp .......) 最后的动态表可以转换为流进行输出 table.toAppendStream[(String,String)] 字段 用一个单引放到字段前面来标识字段名, 如 ‘name , ‘id ,’amount 等 TableAPI 的窗口聚合操作 通过一个例子了解 TableAPI package myflink.sql; import org.apache.flink.api.java.DataSet; import org.apache.flink.api.java.ExecutionEnvironment; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.java.BatchTableEnvironment; public class WordCountSQL { public static void main(String[] args) throws Exception { ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); BatchTableEnvironment tEnv = BatchTableEnvironment.getTableEnvironment(env); DataSet&lt;WC&gt; input = env.fromElements( WC.of(&quot;hello&quot;, 1), WC.of(&quot;hqs&quot;, 1), WC.of(&quot;world&quot;, 1), WC.of(&quot;hello&quot;, 1) ); //注册数据集 tEnv.registerDataSet(&quot;WordCount&quot;, input, &quot;word, frequency&quot;); //执行SQL，并结果集做为一个新表 Table table = tEnv.sqlQuery(&quot;SELECT word, SUM(frequency) as frequency FROM WordCount GROUP BY word&quot;); DataSet&lt;WC&gt; result = tEnv.toDataSet(table, WC.class); result.print(); } public static class WC { public String word; //hello public long frequency; //创建构造方法，让flink进行实例化 public WC() {} public static WC of(String word, long frequency) { WC wc = new WC(); wc.word = word; wc.frequency = frequency; return wc; } @Override public String toString() { return &quot;WC &quot; + word + &quot; &quot; + frequency; } } } 关于 group by 如果了使用 groupby， table 转换为流的时候只能用 toRetractDstream DataStream&lt;Tuple2&lt;Boolean, Tuple2&lt;Integer, Integer&gt;&gt;&gt; joinStream = bsTableEnv.toRetractStream(queryTable, Types.TUPLE(Types.INT, Types.INT)); toRetractDstream 得到的第一个 boolean 型字段标识 true 就是最新的数据 (Insert)， false 表示过期老数据(Delete) 如果使用的 api 包括时间窗口， 那么窗口的字段必须出现在 groupBy 中。 Table resultTable= dataTable .window( Tumble over 10.seconds on 'ts as 'tw ) .groupBy('id, 'tw) .select('id, 'id.count) 关于时间窗口 用到时间窗口， 必须提前声明时间字段， 如果是 processTime 直接在创建动 态表时进行追加就可以。 Table dataTable= tableEnv.fromDataStream(dataStream, 'id,'temperature, 'ps.proctime) 如果是 EventTime 要在创建动态表时声明 Table dataTable= tableEnv.fromDataStream(dataStream, 'id,'temperature, 'ts.rowtime) 滚动窗口可以使用 Tumble over 10000.millis on 来表示 Table resultTable = dataTable .window( Tumble over 10.seconds on 'ts as 'tw ) .groupBy('id, 'tw) .select('id, 'id.count) SQL 如何编写 从一个txt文件中读取数据，txt文件中包含id, 人字, 书名，价格信息。然后将数据注册成一个表，然后将这个表的结果进行统计，按人名统计出来这个人买书所花费的钱，将结果sink到一个文件中。上代码。 package myflink.sql; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.common.typeinfo.TypeInformation; import org.apache.flink.api.java.DataSet; import org.apache.flink.api.java.ExecutionEnvironment; import org.apache.flink.api.java.operators.DataSource; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.Types; import org.apache.flink.table.api.java.BatchTableEnvironment; import org.apache.flink.table.sinks.CsvTableSink; import org.apache.flink.table.sinks.TableSink; public class SQLFromFile { public static void main(String[] args) throws Exception { ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); BatchTableEnvironment tableEnv = BatchTableEnvironment.getTableEnvironment(env); env.setParallelism(1); //读取文件 DataSource&lt;String&gt; input = env.readTextFile(&quot;test.txt&quot;); //将读取到的文件进行输出 input.print(); //转换为DataSet DataSet&lt;Orders&gt; inputDataSet = input.map(new MapFunction&lt;String, Orders&gt;() { @Override public Orders map(String s) throws Exception { String[] splits = s.split(&quot; &quot;); return Orders.of(Integer.valueOf(splits[0]), String.valueOf(splits[1]), String.valueOf(splits[2]), Double.valueOf(splits[3])); } }); //转换为table Table order = tableEnv.fromDataSet(inputDataSet); //注册Orders表名 tableEnv.registerTable(&quot;Orders&quot;, order); Table nameResult = tableEnv.scan(&quot;Orders&quot;).select(&quot;name&quot;); //输出一下表 nameResult.printSchema(); //执行一下查询 Table sqlQueryResult = tableEnv.sqlQuery(&quot;select name, sum(price) as total from Orders group by name order by total desc&quot;); //查询结果转换为DataSet DataSet&lt;Result&gt; result = tableEnv.toDataSet(sqlQueryResult, Result.class); result.print(); //以tuple的方式进行输出 result.map(new MapFunction&lt;Result, Tuple2&lt;String, Double&gt;&gt;() { @Override public Tuple2&lt;String, Double&gt; map(Result result) throws Exception { String name = result.name; Double total = result.total; return Tuple2.of(name, total); } }).print(); TableSink sink = new CsvTableSink(&quot;SQLText.txt&quot;, &quot; | &quot;); //设置字段名 String[] filedNames = {&quot;name&quot;, &quot;total&quot;}; //设置字段类型 TypeInformation[] filedTypes = {Types.STRING(), Types.DOUBLE()}; tableEnv.registerTableSink(&quot;SQLTEXT&quot;, filedNames, filedTypes, sink); sqlQueryResult.insertInto(&quot;SQLTEXT&quot;); env.execute(); } public static class Orders { public Integer id; public String name; public String book; public Double price; public Orders() { super(); } public static Orders of(Integer id, String name, String book, Double price) { Orders orders = new Orders(); orders.id = id; orders.name = name; orders.book = book; orders.price = price; return orders; } } public static class Result { public String name; public Double total; public Result() { super(); } public static Result of(String name, Double total) { Result result = new Result(); result.name = name; result.total = total; return result; } } } 操作 Table API支持下面的操作，请注意并不是所有的操作都同时支持批程序和流程序，不支持的会被响应的标记出来。 Scan, Projection, and Filter Operators Description From Batch Streaming 与SQL查询中的FROM子句类似。 执行已注册表的扫描。 Table orders = tableEnv.from(&quot;Orders&quot;); Select Batch Streaming 与SQL SELECT语句类似。 执行选择操作。 Table orders= tableEnv.from(&quot;Orders&quot;) Table result = orders.select($(&quot;a&quot;), $(&quot;c&quot;).as(&quot;d&quot;)) 可以使用星号（***）作为通配符，选择表格中的所有列。 Table orders = tableEnv.from(&quot;Orders&quot;) Table result = orders.select($(&quot;*&quot;)) As Batch Streaming 重命名字段。 Table orders = tableEnv.from(&quot;Orders&quot;).as('x, 'y, 'z, 't) Where / Filter Batch Streaming 与SQL WHERE子句类似。 过滤掉未通过过滤谓词的行。 Table orders = tableEnv.from(&quot;Orders&quot;) Table result = orders.filter($(&quot;a&quot;).mod(2).isEqual(0)) orTable orders= tableEnv.from(&quot;Orders&quot;) Table result = orders.where($(&quot;b&quot;).isEqual(&quot;red&quot;)) Column Operations Operators Description AddColumns Batch Streaming 执行字段添加操作。如果添加的字段已经存在，它将引发异常。 Table orders = tableEnv.from(&quot;Orders&quot;); Table result = orders.addColumns(concat($(&quot;c&quot;), &quot;sunny&quot;)) AddOrReplaceColumns Batch Streaming 执行字段添加操作。如果添加列名称与现有列名称相同，则现有字段将被替换。此外，如果添加的字段具有重复的字段名称，则使用最后一个。 Table orders = tableEnv.from(&quot;Orders&quot;); Table result = orders.addOrReplaceColumns(concat($(&quot;c&quot;), &quot;sunny&quot;).as(&quot;desc&quot;)) DropColumns Batch Streaming 执行字段删除操作。字段表达式应该是字段引用表达式，并且只能删除现有字段。 Table orders = tableEnv.from(&quot;Orders&quot;); Table result = orders.dropColumns($(&quot;b&quot;), $(&quot;c&quot;)) RenameColumns Batch Streaming 执行字段重命名操作。字段表达式应该是别名表达式，并且只有现有字段可以重命名。 Table orders = tableEnv.from(&quot;Orders&quot;); Table result = orders.renameColumns($(&quot;b&quot;).as(&quot;b2&quot;), $(&quot;c&quot;).as(&quot;c2&quot;)) Aggregations Operators Description GroupBy聚合 Batch Streaming Result Updating 类似于SQL GROUP BY子句。使用以下正在运行的聚合运算符将分组键上的行分组，以逐行聚合行。 Table orders: Table = tableEnv.scan(&quot;Orders&quot;) Table result = orders.groupBy($(&quot;a&quot;)).select($(&quot;a&quot;), $(&quot;b&quot;).sum().as(&quot;d&quot;)) 注意：对于流式查询，计算查询结果所需的状态可能会无限增长，具体取决于聚合类型和不同分组键的数量。 请提供具有有效保留间隔的查询配置，以防止状态过大。 请参阅查询配置 GroupBy窗口聚合 Batch Streaming 在组窗口可能的一个或多个分组key上对表进行分组和聚集。 val orders: Table = tableEnv.scan(&quot;Orders&quot;) Table result = orders .window(Tumble.over(lit(5).minutes()).on($(&quot;rowtime&quot;)).as(&quot;w&quot;)) // define window .groupBy($(&quot;a&quot;), $(&quot;w&quot;)) // group by key and window // access window properties and aggregate .select( $(&quot;a&quot;), $(&quot;w&quot;).start(), $(&quot;w&quot;).end(), $(&quot;w&quot;).rowtime(), $(&quot;b&quot;).sum().as(&quot;d&quot;) ); Over 窗口聚合 Batch Streaming 类似于SQL OVER子句。基于前一行和后一行的窗口（范围），为每一行计算窗口聚合。 Table orders = tableEnv.from(&quot;Orders&quot;); Table result = orders // define window .window( Over .partitionBy($(&quot;a&quot;)) .orderBy($(&quot;rowtime&quot;)) .preceding(UNBOUNDED_RANGE) .following(CURRENT_RANGE) .as(&quot;w&quot;)) // sliding aggregate .select( $(&quot;a&quot;), $(&quot;b&quot;).avg().over($(&quot;w&quot;)), $(&quot;b&quot;).max().over($(&quot;w&quot;)), $(&quot;b&quot;).min().over($(&quot;w&quot;)) );**Note:**必须在同一窗口（即相同的分区，排序和范围）上定义所有聚合。当前，仅支持PRECEDING（无边界和有界）到CURRENT ROW范围的窗口。目前尚不支持带有FOLLOWING的范围。必须在单个时间属性上指定ORDER BY。 Distinct聚合 Batch Streaming Result Updating 类似于SQL DISTINCT AGGREGATION子句，例如COUNT（DISTINCT a）。不同的聚合声明聚合函数（内置或用户定义的）仅应用于不同的输入值。Distinct聚合可以用于GroupBy聚合，GroupBy窗口聚合和Over窗口聚合。Table orders = tableEnv.from(&quot;Orders&quot;); // Distinct aggregation on group by Table groupByDistinctResult = orders .groupBy($(&quot;a&quot;)) .select($(&quot;a&quot;), $(&quot;b&quot;).sum().distinct().as(&quot;d&quot;));// Distinct aggregation on time window group by Table groupByWindowDistinctResult = orders .window(Tumble .over(lit(5).minutes()) .on($(&quot;rowtime&quot;)) .as(&quot;w&quot;) ) .groupBy($(&quot;a&quot;), $(&quot;w&quot;)) .select($(&quot;a&quot;), $(&quot;b&quot;).sum().distinct().as(&quot;d&quot;));// Distinct aggregation on over window Table result = orders .window(Over .partitionBy($(&quot;a&quot;)) .orderBy($(&quot;rowtime&quot;)) .preceding(UNBOUNDED_RANGE) .as(&quot;w&quot;)) .select( $(&quot;a&quot;), $(&quot;b&quot;).avg().distinct().over($(&quot;w&quot;)), $(&quot;b&quot;).max().over($(&quot;w&quot;)), $(&quot;b&quot;).min().over($(&quot;w&quot;)) );用户定义的聚合函数也可以与DISTINCT修饰符一起使用。要仅针对不同值计算聚合结果，只需向聚合函数添加distinct修饰符即可。 Table orders = tEnv.from(&quot;Orders&quot;); // Use distinct aggregation for user-defined aggregate functionstEnv.registerFunction(&quot;myUdagg&quot;, new MyUdagg()); orders.groupBy(&quot;users&quot;) .select( $(&quot;users&quot;), call(&quot;myUdagg&quot;, $(&quot;points&quot;)).distinct().as(&quot;myDistinctResult&quot;) ); **Note:**对于流式查询，计算查询结果所需的状态可能会无限增长，具体取决于聚合类型和不同分组键的数量。 请提供具有有效保留间隔的查询配置，以防止状态过大。 请参阅查询配置 Distinct Batch Streaming Result Updating 类似于SQL DISTINCT子句。返回具有不同值组合的记录。 Table orders= tableEnv.from(&quot;Orders&quot;) Table result = orders.distinct() **Note:**对于流查询，根据查询字段的数量，计算查询结果所需的状态可能会无限增长。请提供具有有效保留间隔的查询配置，以防止出现过多的状态。如果启用了状态清除功能，那么distinct必须发出消息，以防止下游运算符过早地退出状态，这会导致distinct包含结果更新。有关详细信息，请参阅查询配置 Joins Operators Description Inner Join Batch Streaming 类似于SQL JOIN子句。连接两个表。两个表必须具有不同的字段名称，并且至少一个相等的联接谓词必须通过联接运算符或使用where或filter运算符进行定义。Table left = tableEnv.fromDataSet(ds1, &quot;a, b, c&quot;);Table right = tableEnv.fromDataSet(ds2, &quot;d, e, f&quot;);Table result = left.join(right) .where($(&quot;a&quot;).isEqual($(&quot;d&quot;))) .select($(&quot;a&quot;), $(&quot;b&quot;), $(&quot;e&quot;));Note:** 对于流查询，根据不同输入行的数量，计算查询结果所需的状态可能会无限增长。请提供具有有效保留间隔的查询配置，以防止出现过多的状态。有关详细信息，请参阅查询配置 Outer Join Batch Streaming Result Updating 类似于SQL LEFT / RIGHT / FULL OUTER JOIN子句。连接两个表。两个表必须具有不同的字段名称，并且必须至少定义一个相等联接谓词。 Table left = tableEnv.fromDataSet(ds1, &quot;a, b, c&quot;);Table right = tableEnv.fromDataSet(ds2, &quot;d, e, f&quot;);Table leftOuterResult = left.leftOuterJoin(right, $(&quot;a&quot;).isEqual($(&quot;d&quot;))) .select($(&quot;a&quot;), $(&quot;b&quot;), $(&quot;e&quot;));Table rightOuterResult = left.rightOuterJoin(right, $(&quot;a&quot;).isEqual($(&quot;d&quot;))) .select($(&quot;a&quot;), $(&quot;b&quot;), $(&quot;e&quot;));Table fullOuterResult = left.fullOuterJoin(right, $(&quot;a&quot;).isEqual($(&quot;d&quot;))) .select($(&quot;a&quot;), $(&quot;b&quot;), $(&quot;e&quot;));Note: 对于流查询，根据不同输入行的数量，计算查询结果所需的状态可能会无限增长。请提供具有有效保留间隔的查询配置，以防止出现过多的状态。有关详细信息，请参阅查询配置 Inner/Outer Interval JoinBatch Streaming Note: 时间窗口连接是可以以流方式处理的常规子集连接。 时间窗口连接需要至少一个等连接和一个限制双方时间的连接条件。可以通过两个适当的范围谓词（&lt;，&lt;=，&gt; =，&gt;）或比较两个输入表的相同类型的时间属性（即处理时间或事件时间）的单个相等谓词来定义这种条件。 例如，以下是有效的窗口连接条件： 'ltime === 'rtime 'ltime &gt;= 'rtime &amp;&amp; 'ltime &lt; 'rtime + 10.minutes Table left = tableEnv.fromDataSet(ds1, $(&quot;a&quot;), $(&quot;b&quot;), $(&quot;c&quot;), $(&quot;ltime&quot;).rowtime());Table right = tableEnv.fromDataSet(ds2, $(&quot;d&quot;), $(&quot;e&quot;), $(&quot;f&quot;), $(&quot;rtime&quot;).rowtime()));Table result = left.join(right) .where( and( $(&quot;a&quot;).isEqual($(&quot;d&quot;)), $(&quot;ltime&quot;).isGreaterOrEqual($(&quot;rtime&quot;).minus(lit(5).minutes())), $(&quot;ltime&quot;).isLess($(&quot;rtime&quot;).plus(lit(10).minutes())) )) .select($(&quot;a&quot;), $(&quot;b&quot;), $(&quot;e&quot;), $(&quot;ltime&quot;)); Inner Join with Table Function (UDTF) Batch Streaming 使用表函数的结果与表连接。左(外)表的每一行都与表函数的相应调用产生的所有行连接在一起。如果其表函数调用返回空结果，则删除左(外)表的一行。 // register User-Defined Table FunctionTableFunction&lt;String&gt; split = new MySplitUDTF();tableEnv.registerFunction(&quot;split&quot;, split); // joinTable orders = tableEnv.from(&quot;Orders&quot;);Table result = orders .joinLateral(call(&quot;split&quot;, $(&quot;c&quot;)).as(&quot;s&quot;, &quot;t&quot;, &quot;v&quot;)) .select($(&quot;a&quot;), $(&quot;b&quot;), $(&quot;s&quot;), $(&quot;t&quot;), $(&quot;v&quot;)); Left Outer Join with Table Function (UDTF) Batch Streaming 使用表函数的结果连接表。 左(外)表的每一行与表函数的相应调用产生的所有行连接。 如果表函数调用返回空结果，则保留相应的外部行，并使用空值填充结果。 **Note:**目前，表函数的左外连接只能为空或为true。// register User-Defined Table FunctionTableFunction&lt;String&gt; split = new MySplitUDTF();tableEnv.registerFunction(&quot;split&quot;, split);// joinTable orders = tableEnv.from(&quot;Orders&quot;);Table result = orders .leftOuterJoinLateral(call(&quot;split&quot;, $(&quot;c&quot;)).as(&quot;s&quot;, &quot;t&quot;, &quot;v&quot;)) .select($(&quot;a&quot;), $(&quot;b&quot;), $(&quot;s&quot;), $(&quot;t&quot;), $(&quot;v&quot;)); Join with Temporal Table Streaming 时态表是跟踪其随时间变化的表。 时态表功能提供对特定时间点时态表状态的访问。使用时态表函数联接表的语法与使用表函数进行内部联接的语法相同。 **Note:**当前仅支持使用临时表的内部联接。 Table ratesHistory = tableEnv.from(&quot;RatesHistory&quot;);// register temporal table function with a time attribute and primary keyTemporalTableFunction rates = ratesHistory.createTemporalTableFunction( &quot;r_proctime&quot;, &quot;r_currency&quot;);tableEnv.registerFunction(&quot;rates&quot;, rates);// join with &quot;Orders&quot; based on the time attribute and keyTable orders = tableEnv.from(&quot;Orders&quot;);Table result = orders .joinLateral(call(&quot;rates&quot;, $(&quot;o_proctime&quot;)), $(&quot;o_currency&quot;).isEqual($(&quot;r_currency&quot;))) 集合算子 Operators Description Union Batch 类似于SQL UNION子句。合并两个已删除重复记录的表，两个表必须具有相同的字段类型。 Table left = ds1.toTable(tableEnv, 'a, 'b, 'c) Table right = ds2.toTable(tableEnv, 'a, 'b, 'c) Table result = left.union(right) UnionAll Batch Streaming 类似于SQL UNION ALL子句。合并两个表，两个表必须具有相同的字段类型。 Table left = ds1.toTable(tableEnv, 'a, 'b, 'c) Table right = ds2.toTable(tableEnv, 'a, 'b, 'c) Table result = left.unionAll(right) Intersect Batch 类似于SQL INTERSECT子句。相交返回两个表中都存在的记录。如果一个记录在一个或两个表中多次出现，则仅返回一次，即结果表中没有重复的记录。两个表必须具有相同的字段类型。 Table left = ds1.toTable(tableEnv, 'a, 'b, 'c) Table right = ds2.toTable(tableEnv, 'a, 'b, 'c)Table result = left.intersect(right) IntersectAll Batch 类似于SQL INTERSECT ALL子句。IntersectAll返回两个表中都存在的记录。如果一个记录在两个表中都存在一次以上，则返回的次数与两个表中存在的次数相同，即，结果表可能具有重复的记录。两个表必须具有相同的字段类型。 Table left = ds1.toTable(tableEnv, 'a, 'b, 'c) Table right = ds2.toTable(tableEnv, 'a, 'b, 'c) Table result = left.intersectAll(right) Minus Batch 类似于SQL EXCEPT子句。Minus返回左表中右表中不存在的记录。左表中的重复记录只返回一次，即删除重复项。 两个表必须具有相同的字段类型。 Table left = ds1.toTable(tableEnv, 'a, 'b, 'c) Table right = ds2.toTable(tableEnv, 'a, 'b, 'c) Table result = left.minus(right) MinusAll Batch 类似于SQL EXCEPT ALL子句。 MinusAll返回右表中不存在的记录。 在左表中出现n次并在右表中出现m次的记录返回（n-m）次，即，删除右表中出现的重复数。 两个表必须具有相同的字段类型。 Table left = ds1.toTable(tableEnv, 'a, 'b, 'c) Table right = ds2.toTable(tableEnv, 'a, 'b, 'c) Table result = left.minusAll(right) In Batch Streaming 与SQL IN子句类似。 如果表达式存在于给定的表子查询中，则返回true。 子查询表必须包含一列。 此列必须与表达式具有相同的数据类型。 Table left = ds1.toTable(tableEnv, &quot;a, b, c&quot;); Table right = ds2.toTable(tableEnv, &quot;a&quot;); Table result = left.select($(&quot;a&quot;), $(&quot;b&quot;), $(&quot;c&quot;)).where($(&quot;a&quot;).in(right));Note: 对于流查询，该操作将被重写为join and group操作。根据不同的输入行的数量，计算查询结果所需的状态可能会无限增长。请提供具有有效保留间隔的查询配置，以防止出现过多的状态。有关详细信息，请参阅查询配置 OrderBy, Offset &amp; Fetch Operators Description Order By Batch 类似于SQL ORDER BY子句。返回在所有并行分区上全局排序的记录。Table in = tableEnv.fromDataSet(ds, &quot;a, b, c&quot;); Table result = in.orderBy($(&quot;a&quot;).asc()&quot;); Offset &amp; Fetch Batch 类似于SQL OFFSET和FETCH子句。偏移和提取限制了从排序结果返回的记录数。偏移和提取在技术上是Order By运算符的一部分，因此必须在其之前。Table in = tableEnv.fromDataSet(ds, &quot;a, b, c&quot;); // returns the first 5 records from the sorted resultTable result1 = in.orderBy($(&quot;a&quot;).asc()).fetch(5); // skips the first 3 records and returns all following records from the sorted resultTable result2 = in.orderBy($(&quot;a&quot;).asc()).offset(3);// skips the first 10 records and returns the next 5 records from the sorted resultTable result3 = in.orderBy($(&quot;a&quot;).asc()).offset(10).fetch(5); Insert Operators Description Insert Into Batch Streaming 与SQL查询中的INSERT INTO子句相似。在已插入的输出表中执行插入。 输出表必须在TableEnvironment中注册。此外，已注册表的模式必须与查询的模式匹配。 Table orders = tableEnv.from(&quot;Orders&quot;);orders.executeInsert(&quot;OutOrders&quot;); Group Windows 组窗口根据时间或行计数（row-count ）间隔将行组聚合为有限组，并按组聚合函数。 对于批处理表，窗口是按时间间隔对记录进行分组的便捷快捷方式。 Windows是使用window（w：GroupWindow）子句定义的，并且需要使用as子句指定的别名。为了按窗口对表进行分组，必须像常规分组属性一样在groupBy（…）子句中引用窗口别名。 以下示例显示如何在表上定义窗口聚合。 Table table = input .window([GroupWindow w].as(&quot;w&quot;)) // define window with alias w .groupBy($(&quot;w&quot;)) // group the table by window w .select($(&quot;b&quot;).sum()); // aggregate 在流式环境中，如果窗口聚合除了窗口之外还在一个或多个属性上进行分组，则它们只能并行计算。即groupBy（…）子句引用了窗口别名和至少一个其他属性。仅引用窗口别名的groupBy（…）子句（例如上例中的子句）只能由单个非并行任务求值。以下示例显示如何使用其他分组属性定义窗口聚合。 Table table = input .window([GroupWindow w].as(&quot;w&quot;)) // define window with alias w .groupBy($(&quot;w&quot;), $(&quot;a&quot;)) // group the table by attribute a and window w .select($(&quot;a&quot;), $(&quot;b&quot;).sum()); // aggregate 可以在select语句中将窗口属性（例如时间窗口的开始，结束或行时间戳）添加为窗口别名的属性，分别为w.start，w.end和w.rowtime。窗口开始和行时间时间戳是包含窗口的上下边界。相反，窗口结束时间戳是唯一的窗口上边界。例如，从下午2点开始的30分钟滚动窗口将以14：00：00.000作为开始时间戳，以14：29：59.999作为行时间时间戳，以14：30：00.000作为结束时间戳。 Table table = input .window([GroupWindow w].as(&quot;w&quot;)) // define window with alias w .groupBy($(&quot;w&quot;), $(&quot;a&quot;)) // group the table by attribute a and window w .select($(&quot;a&quot;), $(&quot;w&quot;).start(), $(&quot;w&quot;).end(), $(&quot;w&quot;).rowtime(), $(&quot;b&quot;).count()); // aggregate and add window start, end, and rowtime timestamps Window参数定义如何将行映射到窗口。窗口不是用户可以实现的接口。相反，Table API提供了一组具有特定语义的预定义Window类，这些类被转换为基础的DataStream或DataSet操作。支持的窗口定义在下面列出。 Tumble (滚动窗口) 滚动窗口将行分配给固定长度的不重叠的连续窗口。 例如，5分钟的翻滚窗口以5分钟为间隔对行进行分组。 可以在事件时间，处理时间或行数上定义翻滚窗口。使用Tumble类定义翻滚窗口如下： 滚动窗口是使用Tumble类定义的，如下所示： Method Description over 定义窗口的长度，可以是时间间隔也可以是行数间隔。 on 时间属性为组（时间间隔）或排序（行计数）。 对于批处理查询，这可能是任何Long或Timestamp属性。 对于流式查询，这必须是声明的事件时间或处理时间属性。 as 为窗口指定别名。 别名用于引用以下groupBy（）子句中的窗口，并可选择在select（）子句中选择窗口属性，如window start，end或rowtime timestamp。 // Tumbling Event-time Window .window(Tumble.over(lit(10).minutes()).on($(&quot;rowtime&quot;)).as(&quot;w&quot;)); // Tumbling Processing-time Window (assuming a processing-time attribute &quot;proctime&quot;) .window(Tumble.over(lit(10).minutes()).on($(&quot;proctime&quot;)).as(&quot;w&quot;)); // Tumbling Row-count Window (assuming a processing-time attribute &quot;proctime&quot;) .window(Tumble.over(rowInterval(10)).on($(&quot;proctime&quot;)).as(&quot;w&quot;)); Slide (滑动窗口) 滑动窗口的大小固定，并以指定的滑动间隔滑动。如果滑动间隔小于窗口大小，则滑动窗口重叠。因此，可以将行分配给多个窗口。 例如，15分钟大小和5分钟滑动间隔的滑动窗口将每行分配给3个不同的15分钟大小的窗口，这些窗口以5分钟的间隔进行调用。 可以在事件时间，处理时间或行数上定义滑动窗口。 滑动窗口是通过使用Slide类定义的，如下所示： Method Description over 定义窗口的长度，可以是时间或行计数间隔。 every 定义滑动间隔，可以是时间间隔也可以是行数。 滑动间隔必须与大小间隔的类型相同。 on 时间属性为组（时间间隔）或排序（行计数）。 对于批处理查询，这可能是任何Long或Timestamp属性。 对于流式查询，这必须是声明的事件时间或处理时间属性。 as 为窗口指定别名。 别名用于引用以下groupBy（）子句中的窗口，并可选择在select（）子句中选择窗口属性，如window start，end或rowtime timestamp。 // Sliding Event-time Window .window(Slide.over(lit(10).minutes()) .every(lit(5).minutes()) .on($(&quot;rowtime&quot;)) .as(&quot;w&quot;)); // Sliding Processing-time window (assuming a processing-time attribute &quot;proctime&quot;) .window(Slide.over(lit(10).minutes()) .every(lit(5).minutes()) .on($(&quot;proctime&quot;)) .as(&quot;w&quot;)); // Sliding Row-count window (assuming a processing-time attribute &quot;proctime&quot;) .window(Slide.over(rowInterval(10)).every(rowInterval(5)).on($(&quot;proctime&quot;)).as(&quot;w&quot;)); Session (会话窗口) 会话窗口没有固定的大小，但是其边界由不活动的时间间隔定义，即如果在定义的间隔时间内没有事件出现，则会话窗口关闭。 例如，间隔30分钟的会话窗口会在30分钟不活动后观察到一行（否则该行将被添加到现有窗口）后开始，如果30分钟内未添加任何行，则关闭该窗口。会话窗口可以在事件时间或处理时间工作。 会话窗口是通过使用Session类定义的，如下所示： Method Description withGap 将两个窗口之间的间隔定义为时间间隔。 on 时间属性为组（时间间隔）或排序（行计数）。 对于批处理查询，这可能是任何Long或Timestamp属性。 对于流式查询，这必须是声明的事件时间或处理时间属性。 as 为窗口指定别名。 别名用于引用以下groupBy（）子句中的窗口，并可选择在select（）子句中选择窗口属性，如window start，end或rowtime timestamp。 // Session Event-time Window .window(Session.withGap(lit(10).minutes()).on($(&quot;rowtime&quot;)).as(&quot;w&quot;)); // Session Processing-time Window (assuming a processing-time attribute &quot;proctime&quot;) .window(Session.withGap(lit(10).minutes()).on($(&quot;proctime&quot;)).as(&quot;w&quot;)); Over Windows 窗口聚合是标准SQL（OVER子句）已知的，并在查询的SELECT子句中定义。与在GROUP BY子句中指定的组窗口不同，在窗口上方不会折叠行。取而代之的是在窗口聚合中，为每个输入行在其相邻行的范围内计算聚合。 使用window（w：OverWindow ）子句（在Python API中使用over_window（ OverWindow））定义窗口，并在select() 方法中通过别名引用。以下示例显示如何在表上定义窗口聚合。 Table table = input .window([OverWindow w].as(&quot;w&quot;)) // define over window with alias w .select($(&quot;a&quot;), $(&quot;b&quot;).sum().over($(&quot;w&quot;)), $(&quot;c&quot;).min().over($(&quot;w&quot;))); // aggregate over the over window w OverWindow定义了计算聚合的行范围。OverWindow不是用户可以实现的接口。相反，Table API提供了Over类来配置over窗口的属性。可以在事件时间或处理时间以及指定为时间间隔或行计数的范围上定义窗口上方。受支持的over窗口定义作为Over（和其他类）上的方法公开，并在下面列出： Method Required Description partitionBy Optional 在一个或多个属性上定义输入的分区。每个分区都经过单独排序，并且聚合函数分别应用于每个分区。 Note: 在流环境中，如果窗口包含partition by子句，则只能并行计算整个窗口聚合。没有partitionBy（…），流将由单个非并行任务处理。 orderBy Required 定义每个分区内的行顺序，从而定义将聚合函数应用于行的顺序。 Note: 对于流查询，它必须是声明的事件时间或处理时间时间属性。当前，仅支持单个sort属性。 preceding Optional 定义窗口中包含的并在当前行之前的行的间隔。该间隔可以指定为时间间隔或行计数间隔。 用时间间隔的大小指定窗口上的边界，例如，时间间隔为10分钟，行计数间隔为10行。 使用常数指定窗口上的无边界，即对于时间间隔为UNBOUNDED_RANGE或对于行计数间隔为UNBOUNDED_ROW。Windows上的无边界从分区的第一行开始。 如果省略了前面的子句，则将UNBOUNDED_RANGE和CURRENT_RANGE用作窗口的默认前后。 following Optional 定义窗口中包含并紧随当前行的行的窗口间隔。该间隔必须与前面的间隔（时间或行计数）以相同的单位指定。目前，不支持具有当前行之后的行的窗口。而是可以指定两个常量之一： 1. CURRENT_ROW将窗口的上限设置为当前行。 2. CURRENT_RANGE将窗口的上限设置为当前行的排序键，即，与当前行具有相同排序键的所有行都包含在窗口中。 如果省略以下子句，则将时间间隔窗口的上限定义为CURRENT_RANGE，将行计数间隔窗口的上限定义为CURRENT_ROW。 as Required 为上方窗口分配别名。别名用于引用以下select（）子句中的over窗口。 注意：当前，同一select（）调用中的所有聚合函数必须在相同的窗口范围内计算。 Unbounded Over Windows // Unbounded Event-time over window (assuming an event-time attribute &quot;rowtime&quot;) .window(Over.partitionBy($(&quot;a&quot;)).orderBy($(&quot;rowtime&quot;)).preceding(UNBOUNDED_RANGE).as(&quot;w&quot;)); // Unbounded Processing-time over window (assuming a processing-time attribute &quot;proctime&quot;) .window(Over.partitionBy($(&quot;a&quot;)).orderBy(&quot;proctime&quot;).preceding(UNBOUNDED_RANGE).as(&quot;w&quot;)); // Unbounded Event-time Row-count over window (assuming an event-time attribute &quot;rowtime&quot;) .window(Over.partitionBy($(&quot;a&quot;)).orderBy($(&quot;rowtime&quot;)).preceding(UNBOUNDED_ROW).as(&quot;w&quot;)); // Unbounded Processing-time Row-count over window (assuming a processing-time attribute &quot;proctime&quot;) .window(Over.partitionBy($(&quot;a&quot;)).orderBy($(&quot;proctime&quot;)).preceding(UNBOUNDED_ROW).as(&quot;w&quot;)); Bounded Over Windows // Bounded Event-time over window (assuming an event-time attribute &quot;rowtime&quot;) .window(Over.partitionBy($(&quot;a&quot;)).orderBy($(&quot;rowtime&quot;)).preceding(lit(1).minutes()).as(&quot;w&quot;)) // Bounded Processing-time over window (assuming a processing-time attribute &quot;proctime&quot;) .window(Over.partitionBy($(&quot;a&quot;)).orderBy($(&quot;proctime&quot;)).preceding(lit(1).minutes()).as(&quot;w&quot;)) // Bounded Event-time Row-count over window (assuming an event-time attribute &quot;rowtime&quot;) .window(Over.partitionBy($(&quot;a&quot;)).orderBy($(&quot;rowtime&quot;)).preceding(rowInterval(10)).as(&quot;w&quot;)) // Bounded Processing-time Row-count over window (assuming a processing-time attribute &quot;proctime&quot;) .window(Over.partitionBy($(&quot;a&quot;)).orderBy($(&quot;proctime&quot;)).preceding(rowInterval(10)).as(&quot;w&quot;)) 基于行的操作 基于行的操作生成具有多列的输出。 Map Batch Streaming 使用用户定义的标量函数或内置标量函数执行映射操作。如果输出类型是复合类型，则输出将被展平。 public class MyMapFunction extends ScalarFunction { public Row eval(String a) { return Row.of(a, &quot;pre-&quot; + a); } @Override public TypeInformation&lt;?&gt; getResultType(Class&lt;?&gt;[] signature) { return Types.ROW(Types.STRING(), Types.STRING()); } } ScalarFunction func = new MyMapFunction(); tableEnv.registerFunction(&quot;func&quot;, func); Table table = input .map(call(&quot;func&quot;, $(&quot;c&quot;)).as(&quot;a&quot;, &quot;b&quot;)) FlatMap Batch Streaming 使用表函数执行flatMap操作。 public class MyFlatMapFunction extends TableFunction&lt;Row&gt; { public void eval(String str) { if (str.contains(&quot;#&quot;)) { String[] array = str.split(&quot;#&quot;); for (int i = 0; i &lt; array.length; ++i) { collect(Row.of(array[i], array[i].length())); } } } @Override public TypeInformation&lt;Row&gt; getResultType() { return Types.ROW(Types.STRING(), Types.INT()); } } TableFunction func = new MyFlatMapFunction(); tableEnv.registerFunction(&quot;func&quot;, func); Table table = input .flatMap(call(&quot;func&quot;, $(&quot;c&quot;)).as(&quot;a&quot;, &quot;b&quot;)) Aggregate Batch Streaming Result Updating 使用聚合函数执行聚合操作。您必须使用select语句关闭“聚合”，并且select语句不支持聚合函数。如果输出类型是复合类型，则聚合的输出将被展平。 public class MyMinMaxAcc { public int min = 0; public int max = 0; } public class MyMinMax extends AggregateFunction&lt;Row, MyMinMaxAcc&gt; { public void accumulate(MyMinMaxAcc acc, int value) { if (value &lt; acc.min) { acc.min = value; } if (value &gt; acc.max) { acc.max = value; } } @Override public MyMinMaxAcc createAccumulator() { return new MyMinMaxAcc(); } public void resetAccumulator(MyMinMaxAcc acc) { acc.min = 0; acc.max = 0; } @Override public Row getValue(MyMinMaxAcc acc) { return Row.of(acc.min, acc.max); } @Override public TypeInformation&lt;Row&gt; getResultType() { return new RowTypeInfo(Types.INT, Types.INT); } } AggregateFunction myAggFunc = new MyMinMax(); tableEnv.registerFunction(&quot;myAggFunc&quot;, myAggFunc); Table table = input .groupBy($(&quot;key&quot;)) .aggregate(call(&quot;myAggFunc&quot;, $(&quot;a&quot;)).as(&quot;x&quot;, &quot;y&quot;)) .select($(&quot;key&quot;), $(&quot;x&quot;), $(&quot;y&quot;)) Group Window Aggregate Batch Streaming 在组窗口和可能的一个或多个分组键上对表进行分组和聚集。您必须使用select语句关闭“聚合”。并且select语句不支持“ *”或聚合函数。 AggregateFunction myAggFunc = new MyMinMax(); tableEnv.registerFunction(&quot;myAggFunc&quot;, myAggFunc); Table table = input .window(Tumble.over(lit(5).minutes()) .on($(&quot;rowtime&quot;)) .as(&quot;w&quot;)) // define window .groupBy($(&quot;key&quot;), $(&quot;w&quot;)) // group by key and window .aggregate(call(&quot;myAggFunc&quot;, $(&quot;a&quot;)).as(&quot;x&quot;, &quot;y&quot;)) .select($(&quot;key&quot;), $(&quot;x&quot;), $(&quot;y&quot;), $(&quot;w&quot;).start(), $(&quot;w&quot;).end()); // access window properties and aggregate results FlatAggregate Batch Streaming 类似于GroupBy聚合。使用以下运行表聚合运算符将分组键上的行分组，以逐行聚合行。与AggregateFunction的区别在于TableAggregateFunction可以为一个组返回0个或更多记录。您必须使用select语句关闭“ flatAggregate”。并且select语句不支持聚合函数。除了使用emitValue来输出结果之外，还可以使用emitUpdateWithRetract方法。与emitValue不同，emitUpdateWithRetract用于发出已更新的值。此方法在撤消模式下增量输出数据，即，一旦有更新，我们就必须撤消旧记录，然后再发送新的更新记录。如果在表聚合函数中定义了这两种方法，则将优先使用emitUpdateWithRetract方法，因为这两种方法比emitValue更有效，因为它可以增量输出值。 /** * Accumulator for Top2. */ public class Top2Accum { public Integer first; public Integer second; } /** * The top2 user-defined table aggregate function. */ public class Top2 extends TableAggregateFunction&lt;Tuple2&lt;Integer, Integer&gt;, Top2Accum&gt; { @Override public Top2Accum createAccumulator() { Top2Accum acc = new Top2Accum(); acc.first = Integer.MIN_VALUE; acc.second = Integer.MIN_VALUE; return acc; } public void accumulate(Top2Accum acc, Integer v) { if (v &gt; acc.first) { acc.second = acc.first; acc.first = v; } else if (v &gt; acc.second) { acc.second = v; } } public void merge(Top2Accum acc, java.lang.Iterable&lt;Top2Accum&gt; iterable) { for (Top2Accum otherAcc : iterable) { accumulate(acc, otherAcc.first); accumulate(acc, otherAcc.second); } } public void emitValue(Top2Accum acc, Collector&lt;Tuple2&lt;Integer, Integer&gt;&gt; out) { // emit the value and rank if (acc.first != Integer.MIN_VALUE) { out.collect(Tuple2.of(acc.first, 1)); } if (acc.second != Integer.MIN_VALUE) { out.collect(Tuple2.of(acc.second, 2)); } } } tEnv.registerFunction(&quot;top2&quot;, new Top2()); Table orders = tableEnv.from(&quot;Orders&quot;); Table result = orders .groupBy($(&quot;key&quot;)) .flatAggregate(call(&quot;top2&quot;, $(&quot;a&quot;)).as(&quot;v&quot;, &quot;rank&quot;)) .select($(&quot;key&quot;), $(&quot;v&quot;), $(&quot;rank&quot;); **Note:**对于流查询，根据聚合的类型和不同的分组键的数量，计算查询结果所需的状态可能会无限增长。请提供具有有效保留间隔的查询配置，以防止出现过多的状态。有关详细信息，请参见查询配置。 Group Window FlatAggregate Batch Streaming 在组窗口和可能的一个或多个分组键上对表进行分组和聚集。您必须使用select语句关闭“ flatAggregate”。并且select语句不支持聚合函数。 tableEnv.registerFunction(&quot;top2&quot;, new Top2()); Table orders = tableEnv.from(&quot;Orders&quot;); Table result = orders .window(Tumble.over(lit(5).minutes()) .on($(&quot;rowtime&quot;)) .as(&quot;w&quot;)) // define window .groupBy($(&quot;a&quot;), $(&quot;w&quot;)) // group by key and window .flatAggregate(call(&quot;top2&quot;, $(&quot;b&quot;).as(&quot;v&quot;, &quot;rank&quot;)) .select($(&quot;a&quot;), $(&quot;w&quot;).start(), $(&quot;w&quot;).end(), $(&quot;w&quot;).rowtime(), $(&quot;v&quot;), $(&quot;rank&quot;)); // access window properties and aggregate results Data Types 请参阅有关数据类型的专用页面。通用类型和（嵌套的）复合类型（例如POJO，元组，行，Scala案例类）也可以是一行的字段。可以使用值访问功能访问具有任意嵌套的复合类型的字段。泛型类型被视为黑盒，可以通过用户定义的函数传递或处理。 表达式语法 前面几节中的某些运算符期望一个或多个表达式。可以使用嵌入式Scala DSL或字符串指定表达式。请参考上面的示例以了解如何指定表达式。 这是用于表达式的EBNF语法： expressionList = expression , { &quot;,&quot; , expression } ; expression = overConstant | alias ; alias = logic | ( logic , &quot;as&quot; , fieldReference ) | ( logic , &quot;as&quot; , &quot;(&quot; , fieldReference , { &quot;,&quot; , fieldReference } , &quot;)&quot; ) ; logic = comparison , [ ( &quot;&amp;&amp;&quot; | &quot;||&quot; ) , comparison ] ; comparison = term , [ ( &quot;=&quot; | &quot;==&quot; | &quot;===&quot; | &quot;!=&quot; | &quot;!==&quot; | &quot;&gt;&quot; | &quot;&gt;=&quot; | &quot;&lt;&quot; | &quot;&lt;=&quot; ) , term ] ; term = product , [ ( &quot;+&quot; | &quot;-&quot; ) , product ] ; product = unary , [ ( &quot;*&quot; | &quot;/&quot; | &quot;%&quot;) , unary ] ; unary = [ &quot;!&quot; | &quot;-&quot; | &quot;+&quot; ] , composite ; composite = over | suffixed | nullLiteral | prefixed | atom ; suffixed = interval | suffixAs | suffixCast | suffixIf | suffixDistinct | suffixFunctionCall | timeIndicator ; prefixed = prefixAs | prefixCast | prefixIf | prefixDistinct | prefixFunctionCall ; interval = timeInterval | rowInterval ; timeInterval = composite , &quot;.&quot; , (&quot;year&quot; | &quot;years&quot; | &quot;quarter&quot; | &quot;quarters&quot; | &quot;month&quot; | &quot;months&quot; | &quot;week&quot; | &quot;weeks&quot; | &quot;day&quot; | &quot;days&quot; | &quot;hour&quot; | &quot;hours&quot; | &quot;minute&quot; | &quot;minutes&quot; | &quot;second&quot; | &quot;seconds&quot; | &quot;milli&quot; | &quot;millis&quot;) ; rowInterval = composite , &quot;.&quot; , &quot;rows&quot; ; suffixCast = composite , &quot;.cast(&quot; , dataType , &quot;)&quot; ; prefixCast = &quot;cast(&quot; , expression , dataType , &quot;)&quot; ; dataType = &quot;BYTE&quot; | &quot;SHORT&quot; | &quot;INT&quot; | &quot;LONG&quot; | &quot;FLOAT&quot; | &quot;DOUBLE&quot; | &quot;BOOLEAN&quot; | &quot;STRING&quot; | &quot;DECIMAL&quot; | &quot;SQL_DATE&quot; | &quot;SQL_TIME&quot; | &quot;SQL_TIMESTAMP&quot; | &quot;INTERVAL_MONTHS&quot; | &quot;INTERVAL_MILLIS&quot; | ( &quot;MAP&quot; , &quot;(&quot; , dataType , &quot;,&quot; , dataType , &quot;)&quot; ) | ( &quot;PRIMITIVE_ARRAY&quot; , &quot;(&quot; , dataType , &quot;)&quot; ) | ( &quot;OBJECT_ARRAY&quot; , &quot;(&quot; , dataType , &quot;)&quot; ) ; suffixAs = composite , &quot;.as(&quot; , fieldReference , &quot;)&quot; ; prefixAs = &quot;as(&quot; , expression, fieldReference , &quot;)&quot; ; suffixIf = composite , &quot;.?(&quot; , expression , &quot;,&quot; , expression , &quot;)&quot; ; prefixIf = &quot;?(&quot; , expression , &quot;,&quot; , expression , &quot;,&quot; , expression , &quot;)&quot; ; suffixDistinct = composite , &quot;distinct.()&quot; ; prefixDistinct = functionIdentifier , &quot;.distinct&quot; , [ &quot;(&quot; , [ expression , { &quot;,&quot; , expression } ] , &quot;)&quot; ] ; suffixFunctionCall = composite , &quot;.&quot; , functionIdentifier , [ &quot;(&quot; , [ expression , { &quot;,&quot; , expression } ] , &quot;)&quot; ] ; prefixFunctionCall = functionIdentifier , [ &quot;(&quot; , [ expression , { &quot;,&quot; , expression } ] , &quot;)&quot; ] ; atom = ( &quot;(&quot; , expression , &quot;)&quot; ) | literal | fieldReference ; fieldReference = &quot;*&quot; | identifier ; nullLiteral = &quot;nullOf(&quot; , dataType , &quot;)&quot; ; timeIntervalUnit = &quot;YEAR&quot; | &quot;YEAR_TO_MONTH&quot; | &quot;MONTH&quot; | &quot;QUARTER&quot; | &quot;WEEK&quot; | &quot;DAY&quot; | &quot;DAY_TO_HOUR&quot; | &quot;DAY_TO_MINUTE&quot; | &quot;DAY_TO_SECOND&quot; | &quot;HOUR&quot; | &quot;HOUR_TO_MINUTE&quot; | &quot;HOUR_TO_SECOND&quot; | &quot;MINUTE&quot; | &quot;MINUTE_TO_SECOND&quot; | &quot;SECOND&quot; ; timePointUnit = &quot;YEAR&quot; | &quot;MONTH&quot; | &quot;DAY&quot; | &quot;HOUR&quot; | &quot;MINUTE&quot; | &quot;SECOND&quot; | &quot;QUARTER&quot; | &quot;WEEK&quot; | &quot;MILLISECOND&quot; | &quot;MICROSECOND&quot; ; over = composite , &quot;over&quot; , fieldReference ; overConstant = &quot;current_row&quot; | &quot;current_range&quot; | &quot;unbounded_row&quot; | &quot;unbounded_row&quot; ; timeIndicator = fieldReference , &quot;.&quot; , ( &quot;proctime&quot; | &quot;rowtime&quot; ) ; 文字：这里的文字是有效的Java文字。字符串文字可以使用单引号或双引号指定。复制引号以进行转义（例如“是我。”或“我”喜欢”狗。”）。 空文字：空文字必须附加一个类型。使用nullOf（type）（例如nullOf（INT））创建空值。 字段引用：fieldReference指定数据中的一列（如果使用*，则指定所有列），而functionIdentifier指定受支持的标量函数。列名和函数名遵循Java标识符语法。 函数调用：指定为字符串的表达式也可以使用前缀表示法而不是后缀表示法来调用运算符和函数。 小数：如果需要使用精确的数值或大的小数，则Table API还支持Java的BigDecimal类型。在Scala Table API中，小数可以由BigDecimal（“ 123456”）定义，而在Java中，可以通过附加“ p”来精确定义例如123456页 时间表示：为了使用时间值，Table API支持Java SQL的日期，时间和时间戳类型。在Scala Table API中，可以使用java.sql.Date.valueOf（“ 2016-06-27”），java.sql.Time.valueOf（“ 10:10:42”）或java.sql定义文字。Timestamp.valueOf（“ 2016-06-27 10：10：42.123”）。Java和Scala表API还支持调用“ 2016-06-27” .toDate（），“ 10:10:42” .toTime（）和“ 2016-06-27 10：10：42.123” .toTimestamp（）用于将字符串转换为时间类型。注意：由于Java的时态SQL类型取决于时区，因此请确保Flink Client和所有TaskManager使用相同的时区。 时间间隔：时间间隔可以表示为月数（Types.INTERVAL_MONTHS）或毫秒数（Types.INTERVAL_MILLIS）。可以添加或减去相同类型的间隔（例如1.小时+ 10分钟）。可以将毫秒间隔添加到时间点（例如“ 2016-08-10” .toDate + 5.days）。 Scala表达式:：Scala表达式使用隐式转换。因此，请确保将通配符导入org.apache.flink.table.api.scala._添加到程序中。如果文字不被视为表达式，请使用.toExpr（如3.toExpr）强制转换文字。 ","link":"https://tinaxiawuhao.github.io/post/tloS_nSAM/"},{"title":"第九章 状态编程和容错机制","content":"流式计算分为无状态和有状态两种情况。 无状态的计算观察每个独立事件， 并根据最后一个事件输出结果。 例如， 流处理应用程序从传感器接收温度读数， 并在 温度超过 90 度时发出警告。 有状态的计算则会基于多个事件输出结果。 以下是一些 例子。 所有类型的窗口。 例如， 计算过去一小时的平均温度， 就是有状态的计算。 所有用于复杂事件处理的状态机。 例如， 若在一分钟内收到两个相差 20 度 以上的温度读数， 则发出警告， 这是有状态的计算。 流与流之间的所有关联操作， 以及流与静态表或动态表之间的关联操作， 都是有状态的计算。 下图展示了无状态流处理和有状态流处理的主要区别。 无状态流处理分别接收每条数据记录(图中的黑条)， 然后根据最新输入的数据生成输出数据(白条)。 有状态 流处理会维护状态(根据每条输入记录进行更新)， 并基于最新输入的记录和当前的 状态值生成输出记录(灰条)。 图 无状态和有状态的流处理 上图中输入数据由黑条表示。 无状态流处理每次只转换一条输入记录， 并且仅根据最新的输入记录输出结果(白条)。 有状态 流处理维护所有已处理记录的状态 值， 并根据每条新输入的记录更新状态， 因此输出记录(灰条)反映的是综合考虑多 个事件之后的结果。 尽管无状态的计算很重要， 但是流处理对有状态的计算更感兴趣。 事实上， 正确地实现有状态的计算比实现无状态的计算难得多。 旧的流处理系统并不支持有状 态的计算， 而新一代的流处理系统则将状态及其正确性视为重中之重。 有状态的算子和应用程序 Flink 内置的很多算子， 数据源 source， 数据存储 sink 都是有状态的， 流中的数据都是 buffer records， 会保存一定的元素或者元数据。 例如: ProcessWindowFunction会缓存输入流的数据， ProcessFunction 会保存设置的定时器信息等等。 在 Flink 中， 状态始终与特定算子相关联。 总的来说， 有两种类型的状态： 算子状态（ operator state） 键控状态（ keyed state） 算子状态（operator state） 算子状态的作用范围限定为算子任务。 这意味着由同一并行任务所处理的所有数据都可以访问到相同的状态， 状态对于同一任务而言是共享的。 算子状态不能由 相同或不同算子的另一个任务访问。 图 具有算子状态的任务 Flink 为算子状态提供三种基本数据结构： 列表状态（List state） 将状态表示为一组数据的列表。 联合列表状态（Union list state） 也将状态表示为数据的列表。 它与常规列表状态的区别在于， 在发生故障时， 或者从保 存点（savepoint） 启动应用程序时如何恢复。 广播状态（Broadcast state） 如果一个算子有多项任务， 而它的每项任务状态又都相同， 那么这种特殊情况最适合应 用广播状态。 键控状态（keyed state） 键控状态是根据输入数据流中定义的键（key） 来维护和访问的。 Flink 为每个键值维护一个状态实例， 并将具有相同键的所有数据， 都分区到同一个算子任务中， 这个任务会维护和处理这个 key 对应的状态。 当任务处理一条数据时， 它会自动将状态的访问范围限定为当 前数据的 key。 因此， 具有相同 key 的所有数据都会访问相同的状态。 Keyed State 很类似于 一个分布式的 key-value map 数据结构， 只能用于 KeyedStream（ keyBy 算子处理之后） 。 图 具有键控状态的任务 Flink 的 Keyed State 支持以下数据类型： ValueState[T]保存单个的值， 值的类型为 T。 1.1 get 操作: ValueState.value() 1.2 set 操作: ValueState.update(value: T) ListState[T]保存一个列表， 列表里的元素的数据类型为 T。 基本操作如下： 2.1 ListState.add(value: T) 2.2 ListState.addAll(values: java.util.List[T]) 2.3 ListState.get()返回 Iterable[T] 2.4 ListState.update(values: java.util.List[T]) MapState[K, V]保存 Key-Value 对。 3.1 MapState.get(key: K) 3.2 MapState.put(key: K, value: V) 3.3 MapState.contains(key: K) 3.4 MapState.remove(key: K) ReducingState[T] AggregatingState[I, O] State.clear()是清空操作。 val sensorData: DataStream[SensorReading] = ... val keyedData: KeyedStream[SensorReading, String] = sensorData.keyBy(_.id) val alerts: DataStream[(String, Double, Double)] = keyedData .flatMap(new TemperatureAlertFunction(1.7)) class TemperatureAlertFunction(val threshold: Double) extends RichFlatMapFunction[SensorReading, (String, Double, Double)] { private var lastTempState: ValueState[Double] = _ override def open(parameters: Configuration): Unit = { val lastTempDescriptor = new ValueStateDescriptor[Double](&quot;lastTemp&quot;, classOf[Double]) lastTempState = getRuntimeContext.getState[Double](lastTempDescriptor) } override def flatMap(reading: SensorReading, out: Collector[(String, Double, Double)]): Unit = { val lastTemp = lastTempState.value() val tempDiff = (reading.temperature - lastTemp).abs if (tempDiff &gt; threshold) { out.collect((reading.id, reading.temperature, tempDiff)) } this.lastTempState.update(reading.temperature) } } 通过 RuntimeContext 注册 StateDescriptor。 StateDescriptor 以状态 state 的名字和存储的数据类型为参数。 在 open()方法中创建 state 变量。 注意复习之前的 RichFunction 相关知识。接下来我们使用了 FlatMap with keyed ValueState 的快捷方式 flatMapWithState 实现以上需求。 val alerts: DataStream[(String, Double, Double)] = keyedSensorData .flatMapWithState[(String, Double, Double), Double] { case (in: SensorReading, None) =&gt; (List.empty, Some(in.temperature)) case (r: SensorReading, lastTemp: Some[Double]) =&gt; val tempDiff = (r.temperature - lastTemp.get).abs if (tempDiff &gt; 1.7) { (List((r.id, r.temperature, tempDiff)), Some(r.temperature)) } else { (List.empty, Some(r.temperature)) } } 状态一致性 当在分布式系统中引入状态时， 自然也引入了一致性问题。 一致性实际上是\"正确性级别\"的另一种说法， 也就是说在成功处理故障并恢复之后得到的结果， 与没 有发生任何故障时得到的结果相比， 前者到底有多正确？ 举例来说， 假设要对最近 一小时登录的用户计数。 在系统经历故障之后， 计数结果是多少？ 如果有偏差， 是 有漏掉的计数还是重复计数？ 一致性级别 在流处理中， 一致性可以分为 3 个级别： at-most-once: 这其实是没有正确性保障的委婉说法——故障发生之后， 计 数结果可能丢失。 同样的还有 udp。 at-least-once: 这表示计数结果可能大于正确值， 但绝不会小于正确值。 也 就是说， 计数程序在发生故障后可能多算， 但是绝不会少算。 exactly-once: 这指的是系统保证在发生故障后得到的计数结果与正确值一 致。 曾经， at-least-once 非常流行。 第一代流处理器(如 Storm 和 Samza)刚问世时只保证 at-least-once， 原因有二。 保证 exactly-once 的系统实现起来更复杂。 这在基础架构层(决定什么代表 正确， 以及 exactly-once 的范围是什么)和实现层都很有挑战性。 流处理系统的早期用户愿意接受框架的局限性， 并在应用层想办法弥补(例 如使应用程序具有幂等性， 或者用批量计算层再做一遍计算)。 最先保证 exactly-once 的系统(Storm Trident 和 Spark Streaming)在性能和表现力 这两个方面付出了很大的代价。 为了保证 exactly-once， 这些系统无法单独地对每条 记录运用应用逻辑， 而是同时处理多条(一批)记录， 保证对每一批的处理要么全部 成功， 要么全部失败。 这就导致在得到结果前， 必须等待一批记录处理结束。 因此， 用户经常不得不使用两个流处理框架(一个用来保证 exactly-once， 另一个用来对每 个元素做低延迟处理)， 结果使基础设施更加复杂。 曾经， 用户不得不在保证 exactly-once 与获得低延迟和效率之间权衡利弊。 Flink 避免了这种权衡。 Flink 的一个重大价值在于， 它既保证了 exactly-once， 也具有低延迟和高吞吐 的处理能力。 从根本上说， Flink 通过使自身满足所有需求来避免权衡， 它是业界的一次意义 重大的技术飞跃。 尽管这在外行看来很神奇， 但是一旦了解， 就会恍然大悟。 端到端（end-to-end） 状态一致性 目前我们看到的一致性保证都是由流处理器实现的， 也就是说都是在 Flink 流处理器内部保证的； 而在真实应用中， 流处理应用除了流处理器以外还包含了数据 源（ 例如 Kafka） 和输出到持久化系统。 端到端的一致性保证， 意味着结果的正确性贯穿了整个流处理应用的始终； 每一个组件都保证了它自己的一致性， 整个端到端的一致性级别取决于所有组件中一 致性最弱的组件。 具体可以划分如下： 内部保证 —— 依赖 checkpoint source 端 —— 需要外部源可重设数据的读取位置 sink 端 —— 需要保证从故障恢复时， 数据不会重复写入外部系统 而对于 sink 端， 又有两种具体的实现方式： 幂等（ Idempotent） 写入和事务性 （ Transactional） 写入。 幂等写入 所谓幂等操作， 是说一个操作， 可以重复执行很多次， 但只导致一次结果更改， 也就是说， 后面再重复执行就不起作用了。 事务写入 需要构建事务来写入外部系统， 构建的事务对应着 checkpoint， 等到 checkpoint 真正完成的时候， 才把所有对应的结果写入 sink 系统中。 对于事务性写入， 具体又有两种实现方式： 预写日志（ WAL） 和两阶段提交（ 2PC） 。 DataStream API 提供了 GenericWriteAheadSink 模板类和 TwoPhaseCommitSinkFunction 接口， 可以方便地实现这两种方式的事务性写入。 不同 Source 和 Sink 的一致性保证可以用下表说明： 检查点（checkpoint） Flink 具体如何保证 exactly-once 呢? 它使用一种被称为\"检查点\"（ checkpoint）的特性， 在出现故障时将系统重置回正确状态。 下面通过简单的类比来解释检查点 的作用。 假设你和两位朋友正在数项链上有多少颗珠子， 如下图所示。 你捏住珠子， 边数边拨， 每拨过一颗珠子就给总数加一。 你的朋友也这样数他们手中的珠子。 当你 分神忘记数到哪里时， 怎么办呢? 如果项链上有很多珠子， 你显然不想从头再数一 遍， 尤其是当三人的速度不一样却又试图合作的时候， 更是如此(比如想记录前一分 钟三人一共数了多少颗珠子， 回想一下一分钟滚动窗口)。 于是， 你想了一个更好的办法: 在项链上每隔一段就松松地系上一根有色皮筋， 将珠子分隔开; 当珠子被拨动的时候， 皮筋也可以被拨动; 然后， 你安排一个助手， 让他在你和朋友拨到皮筋时记录总数。 用这种方法， 当有人数错时， 就不必从头开 始数。 相反， 你向其他人发出错误警示， 然后你们都从上一根皮筋处开始重数， 助 手则会告诉每个人重数时的起始数值， 例如在粉色皮筋处的数值是多少。 Flink 检查点的作用就类似于皮筋标记。 数珠子这个类比的关键点是: 对于指定的皮筋而言， 珠子的相对位置是确定的; 这让皮筋成为重新计数的参考点。 总状态 (珠子的总数)在每颗珠子被拨动之后更新一次， 助手则会保存与每根皮筋对应的检 查点状态， 如当遇到粉色皮筋时一共数了多少珠子， 当遇到橙色皮筋时又是多少。 当问题出现时， 这种方法使得重新计数变得简单。 Flink 的检查点算法 Flink 检查点的核心作用是确保状态正确， 即使遇到程序中断， 也要正确。 记住这一基本点之后， 我们用一个例子来看检查点是如何运行的。 Flink 为用户提供了用 来定义状态的工具。 例如， 以下这个 Scala 程序按照输入记录的第一个字段(一个字 符串)进行分组并维护第二个字段的计数状态。 val stream: DataStream[(String, Int)] = ... val counts: DataStream[(String, Int)] = stream .keyBy(record =&gt; record._1) .mapWithState( (in: (String, Int), state: Option[Int]) =&gt; state match { case Some(c) =&gt; ( (in._1, c + in._2), Some(c + in._2) ) case None =&gt; ( (in._1, in._2), Some(in._2) ) }) 该程序有两个算子: keyBy 算子用来将记录按照第一个元素(一个字符串)进行分组， 根据该 key 将数据进行重新分区， 然后将记录再发送给下一个算子: 有状态的 map 算子(mapWithState)。 map 算子在接收到每个元素后， 将输入记录的第二个字段 的数据加到现有总数中， 再将更新过的元素发射出去。 下图表示程序的初始状态: 输 入流中的 6 条记录被检查点分割线(checkpoint barrier)隔开， 所有的 map 算子状态均 为 0(计数还未开始)。 所有 key 为 a 的记录将被顶层的 map 算子处理， 所有 key 为 b 的记录将被中间层的 map 算子处理， 所有 key 为 c 的记录则将被底层的 map 算子处 理。 图 按 key 累加计数程序初始状态 上图是程序的初始状态。 注意， a、 b、 c 三组的初始计数状态都是 0， 即三个圆柱上的值。 ckpt 表示检查点分割线（ checkpoint barriers） 。 每条记录在处理顺序上严格地遵守在检查点之前或之后的规定， 例如[\"b\",2]在检查点之前被处理， [\"a\",2] 则在检查点之后被处理。 当该程序处理输入流中的 6 条记录时， 涉及的操作遍布 3 个并行实例(节点、 CPU内核等)。 那么， 检查点该如何保证 exactly-once 呢? 检查点分割线和普通数据记录类似。 它们由算子处理， 但并不参与计算， 而是会触发与检查点相关的行为。 当读取输入流的数据源(在本例中与 keyBy 算子内联) 遇到检查点屏障时， 它将其在输入流中的位置保存到持久化存储中。 如果输入流来 自消息传输系统(Kafka)， 这个位置就是偏移量。 Flink 的存储机制是插件化的， 持久 化存储可以是分布式文件系统， 如 HDFS。 下图展示了这个过程。 图 遇到 checkpoint barrier 时， 保存其在输入流中的位置 当 Flink 数据源(在本例中与 keyBy 算子内联)遇到检查点分界线（ barrier） 时，它会将其在输入流中的位置保存到持久化存储中。 这让 Flink 可以根据该位置重启。检查点像普通数据记录一样在算子之间流动。 当 map 算子处理完前 3 条数据并 收到检查点分界线时， 它们会将状态以异步的方式写入持久化存储， 如下图所示。 图 保存 map 算子状态， 也就是当前各个 key 的计数值 位于检查点之前的所有记录([\"b\",2]、 [\"b\",3]和[\"c\",1])被 map 算子处理之后的情况。 此时， 持久化存储已经备份了检查点分界线在输入流中的位置(备份操作发生在barrier 被输入算子处理的时候)。 map 算子接着开始处理检查点分界线， 并触发将状 态异步备份到稳定存储中这个动作。 当 map 算子的状态备份和检查点分界线的位置备份被确认之后， 该检查点操作就可以被标记为完成， 如下图所示。 我们在无须停止或者阻断计算的条件下， 在一 个逻辑时间点(对应检查点屏障在输入流中的位置)为计算状态拍了快照。 通过确保 备份的状态和位置指向同一个逻辑时间点， 后文将解释如何基于备份恢复计算， 从 而保证 exactly-once。 值得注意的是， 当没有出现故障时， Flink 检查点的开销极小， 检查点操作的速度由持久化存储的可用带宽决定。 回顾数珠子的例子: 除了因为数 错而需要用到皮筋之外， 皮筋会被很快地拨过。 图 检查点操作完成， 继续处理数据 检查点操作完成， 状态和位置均已备份到稳定存储中。 输入流中的所有数据记录都已处理完成。 值得注意的是， 备份的状态值与实际的状态值是不同的。 备份反 映的是检查点的状态。 如果检查点操作失败， Flink 可以丢弃该检查点并继续正常执行， 因为之后的某一个检查点可能会成功。 虽然恢复时间可能更长， 但是对于状态的保证依旧很有力。 只有在一系列连续的检查点操作失败之后， Flink 才会抛出错误， 因为这通常预示着 发生了严重且持久的错误。 现在来看看下图所示的情况: 检查点操作已经完成， 但故障紧随其后。 图 故障紧跟检查点， 导致最底部的实例丢失 在这种情况下， Flink 会重新拓扑(可能会获取新的执行资源)， 将输入流倒回到上一个检查点， 然后恢复状态值并从该处开始继续计算。 在本例中， [\"a\",2]、 [\"a\",2]和[\"c\",2]这几条记录将被重播。 下图展示了这一重新处理过程。 从上一个检查点开始重新计算， 可以保证在剩下的记录被处理之后， 得到的 map 算子的状态值与没有发生故障时的状态值一致。 图 故障时的状态恢复 Flink 将输入流倒回到上一个检查点屏障的位置， 同时恢复 map 算子的状态值。然后， Flink 从此处开始重新处理。 这样做保证了在记录被处理之后， map 算子的状 态值与没有发生故障时的一致。 Flink 检查点算法的正式名称是异步分界线快照(asynchronous barrier snapshotting)。 该算法大致基于 Chandy-Lamport 分布式快照算法。 检查点是 Flink 最有价值的创新之一， 因为它使 Flink 可以保证 exactly-once， 并且不需要牺牲性能。 Flink+Kafka 如何实现端到端的 exactly-once 语义 我们知道， 端到端的状态一致性的实现， 需要每一个组件都实现， 对于 Flink +Kafka 的数据管道系统（ Kafka 进、 Kafka 出） 而言， 各组件怎样保证 exactly-once 语义呢？ 内部 —— 利用 checkpoint 机制， 把状态存盘， 发生故障的时候可以恢复， 保证内部的状态一致性 source —— kafka consumer 作为 source， 可以将偏移量保存下来， 如果后 续任务出现了故障， 恢复的时候可以由连接器重置偏移量， 重新消费数据， 保证一致性 sink —— kafka producer 作为 sink， 采用两阶段提交 sink， 需要实现一个 TwoPhaseCommitSinkFunction 内部的 checkpoint 机制我们已经有了了解， 那 source 和 sink 具体又是怎样运行的呢？ 接下来我们逐步做一个分析。 我们知道 Flink 由 JobManager 协调各个 TaskManager 进行 checkpoint 存储，checkpoint 保存在 StateBackend 中， 默认 StateBackend 是内存级的， 也可以改为文件级的进行持久化保存。 当 checkpoint 启动时， JobManager 会将检查点分界线（ barrier） 注入数据流；barrier 会在算子间传递下去。 每个算子会对当前的状态做个快照， 保存到状态后端。 对于 source 任务而言，就会把当前的 offset 作为状态保存起来。 下次从 checkpoint 恢复时， source 任务可以重新提交偏移量， 从上次保存的位置开始重新消费数据。 每个内部的 transform 任务遇到 barrier 时， 都会把状态存到 checkpoint 里。sink 任务首先把数据写入外部 kafka， 这些数据都属于预提交的事务（ 还不能 被消费） ； 当遇到 barrier 时， 把状态保存到状态后端， 并开启新的预提交事务。 当所有算子任务的快照完成， 也就是这次的 checkpoint 完成时， JobManager 会 向所有任务发通知， 确认这次 checkpoint 完成。 当 sink 任务收到确认通知， 就会正式提交之前的事务， kafka 中未确认的数据就改为“ 已确认” ， 数据就真正可以被消费了。 所以我们看到， 执行过程实际上是一个两段式提交， 每个算子执行完成， 会进行“ 预提交” ， 直到执行完 sink 操作， 会发起“ 确认提交” ， 如果执行失败， 预提 交会放弃掉。 具体的两阶段提交步骤总结如下： 第一条数据来了之后， 开启一个 kafka 的事务（ transaction） ， 正常写入 kafka 分区日志但标记为未提交， 这就是“ 预提交” jobmanager 触发 checkpoint 操作， barrier 从 source 开始向下传递， 遇到 barrier 的算子将状态存入状态后端， 并通知 jobmanager sink 连接器收到 barrier， 保存当前状态， 存入 checkpoint， 通知 jobmanager， 并开启下一阶段的事务， 用于提交下个检查点的数据 jobmanager 收到所有任务的通知， 发出确认信息， 表示 checkpoint 完成 sink 任务收到 jobmanager 的确认信息， 正式提交这段时间的数据 外部 kafka 关闭事务， 提交的数据可以正常消费了。 所以我们也可以看到， 如果宕机需要通过 StateBackend 进行恢复， 只能恢复所有确认提交的操作。 选择一个状态后端(state backend) MemoryStateBackend 内存级的状态后端， 会将键控状态作为内存中的对象进行管理， 将它们存储 在 TaskManager 的 JVM 堆上； 而将 checkpoint 存储在 JobManager 的内存中。 FsStateBackend 将 checkpoint 存到远程的持久化文件系统（ FileSystem） 上。 而对于本地状态， 跟 MemoryStateBackend 一样， 也会存在 TaskManager 的 JVM 堆上。 RocksDBStateBackend 将所有状态序列化后， 存入本地的 RocksDB 中存储。 注意： RocksDB 的支持并不直接包含在 flink 中， 需要引入依赖： &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-statebackend-rocksdb_2.12&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;/dependency&gt; 设置状态后端为 FsStateBackend： val env = StreamExecutionEnvironment.getExecutionEnvironment val checkpointPath: String = ??? val backend = new RocksDBStateBackend(checkpointPath) env.setStateBackend(backend) env.setStateBackend(new FsStateBackend(&quot;file:///tmp/checkpoints&quot;)) env.enableCheckpointing(1000) // 配置重启策略 env.setRestartStrategy(RestartStrategies.fixedDelayRestart(60, Time.of(10, TimeUnit.SECONDS))) ","link":"https://tinaxiawuhao.github.io/post/rxadb61B8/"},{"title":"第八章 ProcessFunction API（底层 API）","content":"我们之前学习的转换算子是无法访问事件的时间戳信息和水位线信息的。 而这在一些应用场景下， 极为重要。 例如 MapFunction 这样的 map 转换算子就无法访问 时间戳或者当前事件的事件时间。 基于此， DataStream API 提供了一系列的 Low-Level 转换算子。 可以访问时间戳、 watermark 以及注册定时事件。 还可以输出特定的一些事件， 例如超时事件等。Process Function 用来构建事件驱动的应用以及实现自定义的业务逻辑(使用之前的 window 函数和转换算子无法实现)。 例如， Flink SQL 就是使用 Process Function 实 现的。 Flink 提供了 8 个 Process Function： ProcessFunction KeyedProcessFunction CoProcessFunction ProcessJoinFunction BroadcastProcessFunction KeyedBroadcastProcessFunction ProcessWindowFunction ProcessAllWindowFunction KeyedProcessFunction 这里我们重点介绍 KeyedProcessFunction。 KeyedProcessFunction 用来操作 KeyedStream。 KeyedProcessFunction 会处理流的每一个元素， 输出为 0 个、 1 个或者多个元素。 所有的 Process Function 都继承自RichFunction 接口， 所以都有 open()、 close()和 getRuntimeContext()等方法。 而KeyedProcessFunction[KEY, IN, OUT]还额外提供了两个方法: processElement(v: IN, ctx: Context, out: Collector[OUT]), 流中的每一个元素 都会调用这个方法， 调用结果将会放在 Collector 数据类型中输出。 Context 可以访问元素的时间戳， 元素的 key， 以及 TimerService 时间服务。 Context 还可以将结果输出到别的流(side outputs)。 onTimer(timestamp: Long, ctx: OnTimerContext, out: Collector[OUT])是一个回 调函数。 当之前注册的定时器触发时调用。 参数 timestamp 为定时器所设定 的触发的时间戳。 Collector 为输出结果的集合。 OnTimerContext 和 processElement 的 Context 参数一样， 提供了上下文的一些信息， 例如定时器 触发的时间信息(事件时间或者处理时间)。 TimerService 和 定时器（Timers） Context 和 OnTimerContext 所持有的 TimerService 对象拥有以下方法: currentProcessingTime(): Long 返回当前处理时间 currentWatermark(): Long 返回当前 watermark 的时间戳 registerProcessingTimeTimer(timestamp: Long): Unit 会注册当前 key 的processing time 的定时器。 当 processing time 到达定时时间时， 触发 timer。 registerEventTimeTimer(timestamp: Long): Unit 会注册当前 key 的 event time 定时器。 当水位线大于等于定时器注册的时间时， 触发定时器执行回调函数。 deleteProcessingTimeTimer(timestamp: Long): Unit 删除之前注册处理时间定 时器。 如果没有这个时间戳的定时器， 则不执行。 deleteEventTimeTimer(timestamp: Long): Unit 删除之前注册的事件时间定时 器， 如果没有此时间戳的定时器， 则不执行。 当定时器 timer 触发时， 会执行回调函数 onTimer()。 注意定时器 timer 只能在 keyed streams 上面使用。 下面举个例子说明 KeyedProcessFunction 如何操作 KeyedStream。 需求： 监控温度传感器的温度值， 如果温度值在一秒钟之内(processing time)连 续上升， 则报警。 val warnings = readings .keyBy(_.id) .process(new TempIncreaseAlertFunction) 看一下 TempIncreaseAlertFunction 如何实现, 程序中使用了 ValueState 这样一个 状态变量。 class TempIncreaseAlertFunction extends KeyedProcessFunction[String, SensorReading, String] { // 保存上一个传感器温度值 lazy val lastTemp: ValueState[Double] = getRuntimeContext.getState( new ValueStateDescriptor[Double](&quot;lastTemp&quot;, Types.of[Double]) ) //保存注册的定时器的时间戳 lazy val currentTimer: ValueState[Long] = getRuntimeContext.getState( new ValueStateDescriptor[Long](&quot;timer&quot;, Types.of[Long]) ) override def processElement(r: SensorReading, ctx: KeyedProcessFunction[String, SensorReading, String]#Context, out: Collector[String]): Unit = { // 取出上一次的温度 val prevTemp = lastTemp.value() // 将当前温度更新到上一次的温度这个变量中 lastTemp.update(r.temperature) val curTimerTimestamp = currentTimer.value() if (prevTemp == 0.0 || r.temperature &lt; prevTemp) { // 温度下降或者是第一个温度值， 删除定时器 ctx.timerService().deleteProcessingTimeTimer(curTimerTimestamp) // 清空状态变量 currentTimer.clear() } else if (r.temperature &gt; prevTemp &amp;&amp; curTimerTimestamp == 0) { // 温度上升且我们并没有设置定时器 val timerTs = ctx.timerService().currentProcessingTime() + 1000 ctx.timerService().registerProcessingTimeTimer(timerTs) currentTimer.update(timerTs) } } override def onTimer(ts: Long, ctx: KeyedProcessFunction[String, SensorReading, String]#OnTimerContext, out: Collector[String]): Unit = { out.collect(&quot;传感器 id 为: &quot; + ctx.getCurrentKey + &quot;的传感器温度值已经连续 1s 上升了。 &quot;) currentTimer.clear() } } 侧输出流（SideOutput） 大部分的 DataStream API 的算子的输出是单一输出， 也就是某种数据类型的流。 除了 split 算子， 可以将一条流分成多条流， 这些流的数据类型也都相同。 process function 的 side outputs 功能可以产生多条流， 并且这些流的数据类型可以不一样。 一个 side output 可以定义为 OutputTag[X]对象， X 是输出流的数据类型。 process function 可以通过 Context 对象发射一个事件到一个或者多个 side outputs。 下面是一个示例程序： val monitoredReadings: DataStream[SensorReading] = readings .process(new FreezingMonitor) monitoredReadings .getSideOutput(new OutputTag[String](&quot;freezing-alarms&quot;)) .print() readings.print() 接下来我们实现 FreezingMonitor 函数， 用来监控传感器温度值， 将温度值低于32F 的温度输出到 side output。 class FreezingMonitor extends ProcessFunction[SensorReading, SensorReading] { // 定义一个侧输出标签 lazy val freezingAlarmOutput: OutputTag[String] = new OutputTag[String](&quot;freezing-alarms&quot;) override def processElement(r: SensorReading, ctx: ProcessFunction[SensorReading, SensorReading]#Context, out: Collector[SensorReading]): Unit = { // 温度在 32F 以下时， 输出警告信息 if (r.temperature &lt; 32.0) { output(freezingAlarmOutput, s&quot;Freezing Alarm for ${r.id}&quot;) } //所有数据直接常规输出到主流 out.collect(r) } } CoProcessFunction 对于两条输入流， DataStream API 提供了 CoProcessFunction 这样的 low-level操作。 CoProcessFunction 提供了操作每一个输入流的方法: processElement1()和processElement2()。 类似于 ProcessFunction， 这两种方法都通过 Context 对象来调用。 这个 Context对象可以访问事件数据， 定时器时间戳， TimerService， 以及 side outputs。CoProcessFunction 也提供了 onTimer()回调函数","link":"https://tinaxiawuhao.github.io/post/cctea1w9G/"},{"title":"第七章 时间语义与 Wartermark","content":"Flink 中的时间语义 在 Flink 的流式处理中， 会涉及到时间的不同概念， 如下图所示： 图 Flink 时间概念 Event Time： 是事件创建的时间。 它通常由事件中的时间戳描述， 例如采集的 日志数据中， 每一条日志都会记录自己的生成时间， Flink 通过时间戳分配器访问事 件时间戳。 Ingestion Time： 是数据进入 Flink 的时间。 Processing Time： 是每一个执行基于时间操作的算子的本地系统时间， 与机器 相关， 默认的时间属性就是 Processing Time。 一个例子——电影《 星球大战》 ： 例如， 一条日志进入 Flink 的时间为 2020-11-12 10:00:00.123， 到达 Window 的 系统时间为 2020-11-12 10:00:01.234， 日志的内容如下： 2020-11-02 18:37:15.624 INFO Fail over to rm2 对于业务来说， 要统计 1min 内的故障日志个数， 哪个时间是最有意义的？ —— eventTime， 因为我们要根据日志的生成时间进行统计。 EventTime 的引入 在 Flink 的流式处理中， 绝大部分的业务都会使用 eventTime， 一般只在eventTime 无法使用时， 才会被迫使用 ProcessingTime 或者 IngestionTime。 如果要使用 EventTime， 那么需要引入 EventTime 的时间属性， 引入方式如下所 示： // 创建 execution environment StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 告诉系统按照 EventTime 处理 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); Watermark 基本概念 我们知道， 流处理从事件产生， 到流经 source， 再到 operator， 中间是有一个过程和时间的， 虽然大部分情况下， 流到 operator 的数据都是按照事件产生的时间顺序来的， 但是也不排除由于网络、 分布式等原因， 导致乱序的产生， 所谓乱序， 就 是指 Flink 接收到的事件的先后顺序不是严格按照事件的 Event Time 顺序排列的。 图 数据的乱序 那么此时出现一个问题， 一旦出现乱序， 如果只根据 eventTime 决定 window 的运行， 我们不能明确数据是否全部到位， 但又不能无限期的等下去， 此时必须要有 个机制来保证一个特定的时间后， 必须触发 window 去进行计算了， 这个特别的机 制， 就是 Watermark。 Watermark 是一种衡量 Event Time 进展的机制。 Watermark 是用于处理乱序事件的， 而正确的处理乱序事件， 通常用 Watermark 机制结合 window 来实现。 数据流中的 Watermark 用于表示 timestamp 小于 Watermark 的数据， 都已经 到达了， 因此， window 的执行也是由 Watermark 触发的。 Watermark 可以理解成一个延迟触发机制， 我们可以设置 Watermark 的延时 时长 t， 每次系统会校验已经到达的数据中最大的 maxEventTime， 然后认定 eventTime 小于 maxEventTime - t 的所有数据都已经到达， 如果有窗口的停止时间等于 maxEventTime – t， 那么这个窗口被触发执行。 有序流的 Watermarker 如下图所示： （ Watermark 设置为 0） 图 有序数据的 Watermark 乱序流的 Watermarker 如下图所示： （ Watermark 设置为 2） 图 无序数据的 Watermark 当 Flink 接收到数据时， 会按照一定的规则去生成 Watermark， 这条 Watermark 就等于当前所有到达数据中的 maxEventTime - 延迟时长， 也就是说， Watermark 是 基于数据携带的时间戳生成的， 一旦 Watermark 比当前未触发的窗口的停止时间要 晚， 那么就会触发相应窗口的执行。 由于 event time 是由数据携带的， 因此， 如果 运行过程中无法获取新的数据， 那么没有被触发的窗口将永远都不被触发。 上图中， 我们设置的允许最大延迟到达时间为 2s， 所以时间戳为 7s 的事件对应 的 Watermark 是 5s， 时间戳为 12s 的事件的 Watermark 是 10s， 如果我们的窗口 1 是 1s~5s， 窗口 2 是 6s~10s， 那么时间戳为 7s 的事件到达时的 Watermarker 恰好触 发窗口 1， 时间戳为 12s 的事件到达时的 Watermark 恰好触发窗口 2。Watermark 就是触发前一窗口的“ 关窗时间” ， 一旦触发关门那么以当前时刻为准在窗口范围内的所有所有数据都会收入窗中。 只要没有达到水位那么不管现实中的时间推进了多久都不会触发关窗。 Watermark 的引入 watermark 的引入很简单， 对于乱序数据， 最常见的引用方式如下： // 抽取出时间和生成 watermark dataStream.assignTimestampsAndWatermarks(new AscendingTimestampExtractor&lt;UserBehavior&gt;() { @Override public long extractAscendingTimestamp(UserBehavior userBehavior) { // 原始数据单位秒，将其转成毫秒 return userBehavior.timestamp * 1000; } }) Event Time 的使用一定要指定数据源中的时间戳。 否则程序无法知道事件的事 件时间是什么(数据源里的数据没有时间戳的话， 就只能使用 Processing Time 了)。 我们看到上面的例子中创建了一个看起来有点复杂的类， 这个类实现的其实就 是分配时间戳的接口。 Flink 暴露了 TimestampAssigner 接口供我们实现， 使我们可 以自定义如何从事件数据中抽取时间戳。 // 创建 execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 通过连接 socket 获取输入数据，这里连接到本地9000端口，如果9000端口已被占用，请换一个端口 DataStream&lt;String&gt; text = env.socketTextStream(&quot;localhost&quot;, 9000, &quot;\\n&quot;); text.assignTimestampsAndWatermarks(new MyAssigner()) MyAssigner 有两种类型 AssignerWithPeriodicWatermarks AssignerWithPunctuatedWatermarks 以上两个接口都继承自 TimestampAssigner。 Assigner with periodic watermarks 周期性的生成 watermark： 系统会周期性的将 watermark 插入到流中(水位线也 是一种特殊的事件!)。 默认周期是 200 毫秒。 可以使用 ExecutionConfig.setAutoWatermarkInterval()方法进行设置。 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) // 每隔 5 秒产生一个 watermark env.getConfig.setAutoWatermarkInterval(5000) 产生 watermark 的逻辑： 每隔 5 秒钟， Flink 会调用 AssignerWithPeriodicWatermarks 的 getCurrentWatermark()方法。 如果方法返回一个 时间戳大于之前水位的时间戳， 新的 watermark 会被插入到流中。 这个检查保证了 水位线是单调递增的。 如果方法返回的时间戳小于等于之前水位的时间戳， 则不会 产生新的 watermark。 例子， 自定义一个周期性的时间戳抽取： public class PeriodicAssigner extends AssignerWithPeriodicWatermarks&lt;SensorReading&gt; { Long bound= 60 * 1000 // 延时为 1 分钟 Long maxTs = Long.MinValue // 观察到的最大时间戳 public Watermark getCurrentWatermark() { new Watermark(maxTs - bound) } public void extractTimestamp(SensorReading r,Long previousTS) = { maxTs = maxTs.max(r.timestamp) r.timestamp } } 一种简单的特殊情况是， 如果我们事先得知数据流的时间戳是单调递增的， 也就是说没有乱序， 那我们可以使用 assignAscendingTimestamps， 这个方法会直接使 用数据的时间戳生成 watermark。 DataStream&lt;SensorReading&gt; stream: = ... DataStream&lt;SensorReading&gt; withTimestampsAndWatermarks = stream .assignAscendingTimestamps(e =&gt; e.timestamp) &gt;&gt; result: E(1), W(1), E(2), W(2), ... 而对于乱序数据流， 如果我们能大致估算出数据流中的事件的最大延迟时间， 就可以使用如下代码： DataStream&lt;SensorReading&gt; stream: = ... DataStream&lt;SensorReading&gt; withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks( new SensorTimeAssigner() ) public class SensorTimeAssigner extends BoundedOutOfOrdernessTimestampExtractor&lt;SensorReading&gt;(Time.seconds(5)) { // 抽取时间戳 public Long extractTimestamp(SensorReading r){ return r.timestamp } } &gt;&gt; relust: E(10), W(0), E(8), E(7), E(11), W(1), ... Assigner with punctuated watermarks 间断式地生成 watermark。 和周期性生成的方式不同， 这种方式不是固定时间的，而是可以根据需要对每条数据进行筛选和处理。 直接上代码来举个例子， 我们只给 sensor_1 的传感器的数据流插入 watermark： public class PunctuatedAssigner extends AssignerWithPunctuatedWatermarks&lt;SensorReading&gt; { Long bound=60 * 1000 public Watermark checkAndGetNextWatermark(SensorReading r , Long extractedTS) { if (r.id == &quot;sensor_1&quot;) { new Watermark(extractedTS - bound) } else { null } } public Long extractTimestamp(SensorReading r, Long previousTS){ return r.timestamp } } EvnetTime 在 window 中的使用 滚动窗口（TumblingEventTimeWindows） public class EventTimeTumblingWindowAllDemo { public static void main(String[] args) throws Exception { // 2021-03-06 21:00:00,1 // 2021-03-06 21:00:05,2 // 结果 ： 2&gt; 1 // 3&gt; 2 StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(new Configuration()); // 老版本必须要设置时间标准 （1.20 之前的） env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); DataStreamSource&lt;String&gt; lines = env.socketTextStream(&quot;linux01&quot;, 8888); // flink里面的时间都精确到毫秒 // 时间提取器 BoundedOutOfOrdernessTimestampExtractor : 允许时间乱序，并且可以指定窗口延时 // WaterMark （水位线，可以让窗口延迟触发的一种机制） // 一个窗口中的一个分区的水位线 = 当前窗口当前分区最大的 EventTime - 延迟时间 Time.seconds(0) SingleOutputStreamOperator&lt;String&gt; linesWithWaterMark = lines.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;String&gt;(Time.seconds(0)) { private SimpleDateFormat dateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;) ; @Override public long extractTimestamp(String s) { String strings = s.split(&quot;,&quot;)[0]; long timestamp = 0; try { Date date = dateFormat.parse(strings); timestamp = date.getTime(); } catch (ParseException e) { e.printStackTrace(); timestamp = System.currentTimeMillis(); } return timestamp; } }); SingleOutputStreamOperator&lt;Integer&gt; nums = linesWithWaterMark.map(new MapFunction&lt;String, Integer&gt;() { @Override public Integer map(String s) throws Exception { int i = Integer.parseInt(s.split(&quot;,&quot;)[1]); return i; } }); // 划分窗口 AllWindowedStream&lt;Integer, TimeWindow&gt; window = nums.windowAll(TumblingProcessingTimeWindows.of(Time.seconds(5))); // 对 window 中的数据 进行聚合 SingleOutputStreamOperator&lt;Integer&gt; sum = window.sum(0); sum.print(); env.execute() ; } } 结果是按照 Event Time 的时间窗口计算得出的， 而无关系统的时间（ 包括输入的快慢） 。 滑动窗口（SlidingEventTimeWindows） public class EventTimeSlidingWindowDemo { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(new Configuration()); // 两秒 调一次方法 env.getConfig().setAutoWatermarkInterval(1000); // 老版本必须要设置时间标准 （1.20 之前的） env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); DataStreamSource&lt;String&gt; lines = env.socketTextStream(&quot;linux01&quot;, 8888); // flink里面的时间都精确到毫秒 // 时间提取器 BoundedOutOfOrdernessTimestampExtractor : 允许时间乱序，并且可以指定窗口延时 // WaterMark （水位线，可以让窗口延迟触发的一种机制） // 一个窗口中的一个分区的水位线 = 当前窗口当前分区最大的 EventTime - 延迟时间 Time.seconds(0) SingleOutputStreamOperator&lt;String&gt; linesWithWaterMark = lines.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;String&gt;(Time.seconds(0)) { @Override public long extractTimestamp(String s) { return Long.parseLong(s.split(&quot;,&quot;)[0]); // EventTime } }); // 提取完 EventTime 后生成 WaterMark ，但数据还是原来的老样子 // 1000,spark,1 --&gt; spark,1 SingleOutputStreamOperator &lt;Tuple2&lt;String,Integer&gt;&gt; WordAndCount = linesWithWaterMark.map(new MapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() { @Override public Tuple2&lt;String,Integer&gt; map(String s) throws Exception { String[] split = s.split(&quot;,&quot;); return Tuple2.of(split[1],Integer.parseInt(split[2])); } }); // 先 keyBy，再划分窗口 KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; keyed = WordAndCount.keyBy(t -&gt; t.f0); // 划分窗口 WindowedStream&lt;Tuple2&lt;String, Integer&gt;, String, TimeWindow&gt; window = keyed.window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))); // 对窗口里面的数据进行 sum SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = window.sum(1); sum.print(); env.execute() ; } } 会话窗口（EventTimeSessionWindows） 相邻两次数据的 EventTime 的时间差超过指定的时间间隔就会触发执行。 如果加入 Watermark， 会在符合窗口触发的情况下进行延迟。 到达延迟水位再进行窗口 触发。 public class EventTimeSessionWindowDemo { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(new Configuration()); // 两秒 调一次方法 env.getConfig().setAutoWatermarkInterval(1000); // 老版本必须要设置时间标准 （1.20 之前的） env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); DataStreamSource&lt;String&gt; lines = env.socketTextStream(&quot;linux01&quot;, 8888); // flink里面的时间都精确到毫秒 // 时间提取器 BoundedOutOfOrdernessTimestampExtractor : 允许时间乱序，并且可以指定窗口延时 // WaterMark （水位线，可以让窗口延迟触发的一种机制） // 一个窗口中的一个分区的水位线 = 当前窗口当前分区最大的 EventTime - 延迟时间 Time.seconds(0) SingleOutputStreamOperator&lt;String&gt; linesWithWaterMark = lines.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;String&gt;(Time.seconds(0)) { @Override public long extractTimestamp(String s) { return Long.parseLong(s.split(&quot;,&quot;)[0]); // EventTime } }); // 提取完 EventTime 后生成 WaterMark ，但数据还是原来的老样子 // 1000,spark,1 --&gt; spark,1 SingleOutputStreamOperator &lt;Tuple2&lt;String,Integer&gt;&gt; WordAndCount = linesWithWaterMark.map(new MapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() { @Override public Tuple2&lt;String,Integer&gt; map(String s) throws Exception { String[] split = s.split(&quot;,&quot;); return Tuple2.of(split[1],Integer.parseInt(split[2])); } }); // 先 keyBy，再划分窗口 KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; keyed = WordAndCount.keyBy(t -&gt; t.f0); // 划分窗口 WindowedStream&lt;Tuple2&lt;String, Integer&gt;, String, TimeWindow&gt; window = keyed.window(EventTimeSessionWindows.withGap(Time.seconds(5))); // 对窗口里面的数据进行 sum SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = window.sum(1); sum.print(); env.execute() ; } } ","link":"https://tinaxiawuhao.github.io/post/GzZxLdaVh/"},{"title":"第六章 Flink 的Window 操作","content":"Window是无限数据流处理的核心，Window将一个无限的stream拆分成有限大小的”buckets”桶，我们可以在这些桶上做计算操作。本文主要聚焦于在Flink中如何进行窗口操作，以及程序员如何从window提供的功能中获得最大的收益。 窗口化的Flink程序的一般结构如下，第一个代码段中是分组的流，而第二段是非分组的流。正如我们所见，唯一的区别是分组的stream调用keyBy(…)和window(…)，而非分组的stream中window()换成了windowAll(…)，这些也将贯穿都这一页的其他部分中。 Keyed Windows stream.keyBy(...) &lt;- keyed versus non-keyed windows .window(...) &lt;- required: &quot;assigner&quot; [.trigger(...)] &lt;- optional: &quot;trigger&quot; (else default trigger) [.evictor(...)] &lt;- optional: &quot;evictor&quot; (else no evictor) [.allowedLateness()] &lt;- optional, else zero .reduce/fold/apply() &lt;- required: &quot;function&quot; Non-Keyed Windows stream.windowAll(...) &lt;- required: &quot;assigner&quot; [.trigger(...)] &lt;- optional: &quot;trigger&quot; (else default trigger) [.evictor(...)] &lt;- optional: &quot;evictor&quot; (else no evictor) [.allowedLateness()] &lt;- optional, else zero .reduce/fold/apply() &lt;- required: &quot;function&quot; 在上面的例子中，方括号[]内的命令是可选的，这表明Flink允许你根据最符合你的要求来定义自己的window逻辑。 Window 的生命周期 简单地说,当一个属于window的元素到达之后这个window就创建了,而当当前时间(事件或者处理时间)为window的创建时间跟用户指定的延迟时间相加时,窗口将被彻底清除。Flink 确保了只清除基于时间的window,其他类型的window不清除,例如:全局window。例如:对于一个每5分钟创建无覆盖的(即 翻滚窗口)窗口,允许一个1分钟的时延的窗口策略，Flink将会在12:00到12:05这段时间内第一个元素到达时创建窗口,当水印通过12:06时,移除这个窗口。 此外,每个 Window都有一个Trigger和一个附属于 Window 的函数(例如: WindowFunction, ReduceFunction 及 FoldFunction)，函数里包含了应用于窗口(Window)内容的计算，而Trigger(触发器)则指定了函数在什么条件下可被应用(函数何时被触发),一个触发策略可以是 &quot;当窗口中的元素个数超过4个时&quot; 或者 &quot;当水印达到窗口的边界时&quot;。触发器还可以决定在窗口创建和删除之间的任意时刻清除窗口的内容,本例中的清除仅指清除窗口的内容而不是窗口的元数据,也就是说新的数据还是可以被添加到当前的window中。 除了上面的提到之外，你还可以指定一个驱逐者, Evictor将在触发器触发之后或者在函数被应用之前或者之后，清楚窗口中的元素。 接下来我们将更深入的去了解上述的部件，我们从上述片段的主要部分开始(如:Keyed vs Non-Keyed Windows, Window Assigner, 及 Window Function),然后是可选部分。 分组和非分组Windows (Keyed vs Non-Keyed Windows) 首先，第一件事是指定你的数据流是分组的还是未分组的，这个必须在定义window 之前指定好。使用 keyBy(...)会将你的无限数据流拆分成逻辑分组的数据流，如果 keyBy(...) 函数不被调用的话，你的数据流将不是分组的。 在分组数据流中,任何正在传入的事件的属性都可以被当做key,分组数据流将你的window计算通过多任务并发执行，以为每一个逻辑分组流在执行中与其他的逻辑分组流是独立地进行的。 在非分组数据流中，你的原始数据流并不会拆分成多个逻辑流并且所有的window逻辑将在一个任务中执行，并发度为1。 窗口分配器(Window Assingers) 指定完你的数据流是分组的还是非分组的之后，接下来你需要定义一个窗口分配器(window assigner)，窗口分配器定义了元素如何分配到窗口中，这是通过在分组数据流中调用window(...)或者非分组数据流中调用windowAll(...)时你选择的窗口分配器(WindowAssigner)来指定的。WindowAssigner是负责将每一个到来的元素分配给一个或者多个窗口(window),Flink 提供了一些常用的预定义窗口分配器，即:滚动窗口、滑动窗口、会话窗口和全局窗口。你也可以通过继承WindowAssigner类来自定义自己的窗口。所有的内置窗口分配器(除了全局窗口 global window)都是通过时间来分配元素到窗口中的，这个时间要么是处理的时间，要么是事件发生的时间。请看一下我们的 event time (https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/event_time.html )部分来了解更多处理时间和事件时间的区别及时间戳(timestamp)和水印(watermark)是如何产生的。 接下来我们将展示Flink的预定义窗口分配器是如何工作的，以及它们在DataStream程序中是如何使用的。接下来我们将展示Flink的预定义窗口分配器是如何工作的，以及它们在DataStream程序中是如何使用的。下图中展示了每个分配器是如何工作的，紫色圆圈代表着数据流中的一个元素，这些元素是通过一些key进行分区(在本例中是 user1,user2,user3), X轴显示的是时间进度。 滚动窗口 滚动窗口分配器将每个元素分配的一个指定窗口大小的窗口中，滚动窗口有一个固定的大小，并且不会出现重叠。例如:如果你指定了一个5分钟大小的滚动窗口，当前窗口将被评估并将按下图说明每5分钟创建一个新的窗口。 下面的代码片段展示了如何使用滚动窗口。 Java 代码 DataStream&lt;T&gt; input = ...; 滚动事件时间窗口( tumbling event-time windows ) input .keyBy(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;); 滚动处理时间窗口(tumbling processing-time windows) input .keyBy(&lt;key selector&gt;) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;); 每日偏移8小时的滚动事件时间窗口(daily tumbling event-time windows offset by -8 hours. ) input .keyBy(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.days(1), Time.hours(-8))) .&lt;windowed transformation&gt;(&lt;window function&gt;); Scala 代码: val input:DataStream[T] = 滚动事件时间窗口(tumbling event-time windows) input .keyBy(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;) 滚动处理时间窗口(tumbling processing-time windows) input .keyBy(&lt;key selector&gt;) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;) 每日偏移8小时的滚动事件时间窗口(daily tumbling event-time windows offset by -8 hours. ) input .keyBy(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.days(1), Time.hours(-8))) .&lt;windowed transformation&gt;(&lt;window function&gt;) 时间间隔可以通过Time.milliseconds(x)，Time.seconds(x)，Time.minutes(x)等其中的一个来指定。 在上面最后的例子中，滚动窗口分配器还接受了一个可选的偏移参数，可以用来改变窗口的排列。例如，没有偏移的话按小时的滚动窗口将按时间纪元来对齐，也就是说你将一个如: 1:00:00.000-1:59:59.999,2:00:00.000-2:59:59.999等，如果你想改变一下，你可以指定一个偏移，如果你指定了一个15分钟的偏移，你将得到1:15:00.000-2:14:59.999,2:15:00.000-3:14:59.999等。时间偏移一个很大的用处是用来调准非0时区的窗口，例如:在中国你需要指定一个8小时的时间偏移。 滑动窗口(Sliding Windows) 滑动窗口分配器将元素分配到固定长度的窗口中，与滚动窗口类似，窗口的大小由窗口大小参数来配置，另一个窗口滑动参数控制滑动窗口开始的频率。因此，滑动窗口如果滑动参数小于滚动参数的话，窗口是可以重叠的，在这种情况下元素会被分配到多个窗口中。 例如，你有10分钟的窗口和5分钟的滑动，那么每个窗口中5分钟的窗口里包含着上个10分钟产生的数据，如下图所示: 下面的代码片段中展示了如何使用滑动窗口: Java 代码: DataStream&lt;T&gt; input = ...; 滑动事件时间窗口 input .keyBy(&lt;key selector&gt;) .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;); 滑动处理时间窗口 input .keyBy(&lt;key selector&gt;) .window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;); 偏移8小时的滑动处理时间窗口(sliding processing-time windows offset by -8 hours) input .keyBy(&lt;key selector&gt;) .window(SlidingProcessingTimeWindows.of(Time.hours(12), Time.hours(1), Time.hours(-8))) .&lt;windowed transformation&gt;(&lt;window function&gt;); Scala 代码: val input: DataStream[T] = ... 滑动事件时间窗口(sliding event-time windows) input .keyBy(&lt;key selector&gt;) .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;) 滑动处理时间窗口(sliding processing-time windows) input .keyBy(&lt;key selector&gt;) .window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;) 偏移8小时的滑动处理时间窗口(sliding processing-time windows offset by -8 hours) input .keyBy(&lt;key selector&gt;) .window(SlidingProcessingTimeWindows.of(Time.hours(12), Time.hours(1), Time.hours(-8))) .&lt;windowed transformation&gt;(&lt;window function&gt;) 时间间隔可以通过Time.milliseconds(x),Time.seconds(x),Time.minutes(x)等来指定。 正如上述例子所示，滑动窗口分配器也有一个可选的偏移参数来改变窗口的对齐。例如，没有偏移参数，按小时的窗口，有30分钟的滑动，将根据时间纪元来对齐，也就是说你将得到如下的窗口1:00:00.000-1:59:59.999,1:30:00.000-2:29:59.999等。而如果你想改变窗口的对齐，你可以给定一个偏移，如果给定一个15分钟的偏移，你将得到如下的窗口:1:15:00.000-2:14.59.999, 1:45:00.000-2:44:59.999等。时间偏移一个很大的用处是用来调准非0时区的窗口，例如:在中国你需要指定一个8小时的时间偏移。 会话窗口(Session Windows) session窗口分配器通过session活动来对元素进行分组，session窗口跟滚动窗口和滑动窗口相比，不会有重叠和固定的开始时间和结束时间的情况。相反，当它在一个固定的时间周期内不再收到元素，即非活动间隔产生，那个这个窗口就会关闭。一个session窗口通过一个session间隔来配置，这个session间隔定义了非活跃周期的长度。当这个非活跃周期产生，那么当前的session将关闭并且后续的元素将被分配到新的session窗口中去。 下面的代码片段中展示了如何使用session窗口 Java代码: DataStream&lt;T&gt; input = ...; 事件时间会话窗口(event-time session windows) input .keyBy(&lt;key selector&gt;) .window(EventTimeSessionWindows.withGap(Time.minutes(10))) .&lt;windowed transformation&gt;(&lt;window function&gt;); 处理时间会话窗口(processing-time session windows) input .keyBy(&lt;key selector&gt;) .window(ProcessingTimeSessionWindows.withGap(Time.minutes(10))) .&lt;windowed transformation&gt;(&lt;window function&gt;); Scala代码: val input: DataStream[T] = ... 事件时间会话窗口(event-time session windows) input .keyBy(&lt;key selector&gt;) .window(EventTimeSessionWindows.withGap(Time.minutes(10))) .&lt;windowed transformation&gt;(&lt;window function&gt;) 处理时间会话窗口(processing-time session windows) input .keyBy(&lt;key selector&gt;) .window(ProcessingTimeSessionWindows.withGap(Time.minutes(10))) .&lt;windowed transformation&gt;(&lt;window function&gt;) 时间间隔可以通过Time.milliseconds(x),Time.seconds(x),Time.minutes(x)等来指定。 注意: 因为session看窗口没有一个固定的开始和结束，他们的评估与滑动窗口和滚动窗口不同。在内部，session操作为每一个到达的元素创建一个新的窗口，并合并间隔时间小于指定非活动间隔的窗口。为了进行合并，session窗口的操作需要指定一个合并触发器(Trigger)和一个合并窗口函数(Window Function),如:ReduceFunction或者WindowFunction(FoldFunction不能合并)。 ###全局窗口(Global Windows) 全局窗口分配器将所有具有相同key的元素分配到同一个全局窗口中，这个窗口模式仅适用于用户还需自定义触发器的情况。否则，由于全局窗口没有一个自然的结尾，无法执行元素的聚合，将不会有计算被执行。 下面的代码片段展示了如何使用全局窗口: Java 代码: DataStream&lt;T&gt; input = ...; input .keyBy(&lt;key selector&gt;) .window(GlobalWindows.create()) .&lt;windowed transformation&gt;(&lt;window function&gt;); Scala代码: val input: DataStream[T] = ... input .keyBy(&lt;key selector&gt;) .window(GlobalWindows.create()) .&lt;windowed transformation&gt;(&lt;window function&gt;) 窗口函数(Window Functions) 定义完窗口分配器后，我们还需要为每一个窗口指定我们需要执行的计算，这是窗口的责任，当系统决定一个窗口已经准备好执行之后，这个窗口函数将被用来处理窗口中的每一个元素(可能是分组的)。请参考:https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/windows.html#triggers 来了解当一个窗口准备好之后，Flink是如何决定的。 window函数可以是ReduceFunction, FoldFunction 或者 WindowFunction 中的一个。前面两个更高效一些(),因为在每个窗口中增量地对每一个到达的元素执行聚合操作。一个 WindowFunction 可以获取一个窗口中的所有元素的一个迭代以及哪个元素属于哪个窗口的额外元信息。 有WindowFunction的窗口化操作会比其他的操作效率要差一些，因为Flink内部在调用函数之前会将窗口中的所有元素都缓存起来。这个可以通过WindowFunction和ReduceFunction或者FoldFunction结合使用来获取窗口中所有元素的增量聚合和WindowFunction接收的额外的窗口元数据，接下来我们将看一看每一种变体的示例。 ReduceFunction ReduceFunction指定了如何通过两个输入的参数进行合并输出一个同类型的参数的过程，Flink使用ReduceFunction来对窗口中的元素进行增量聚合。 一个ReduceFunction 可以通过如下的方式来定义和使用: Java 代码: DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...; input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .reduce(new ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt; { public Tuple2&lt;String, Long&gt; reduce(Tuple2&lt;String, Long&gt; v1, Tuple2&lt;String, Long&gt; v2) { return new Tuple2&lt;&gt;(v1.f0, v1.f1 + v2.f1); } }); Scala 代码: val input: DataStream[(String, Long)] = ... input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .reduce { (v1, v2) =&gt; (v1._1, v1._2 + v2._2) } 上面的例子是将窗口所有元素中元组的第二个属性进行累加操作。 FoldFunction FoldFunction 指定了一个输入元素如何与一个输出类型的元素合并的过程，这个FoldFunction 会被每一个加入到窗口中的元素和当前的输出值增量地调用，第一个元素是与一个预定义的类型为输出类型的初始值合并。 一个FoldFunction可以通过如下的方式定义和调用: Java 代码: DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...; input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .fold(&quot;&quot;, new FoldFunction&lt;Tuple2&lt;String, Long&gt;, String&gt;&gt; { public String fold(String acc, Tuple2&lt;String, Long&gt; value) { return acc + value.f1; } }); Scala 代码: val input: DataStream[(String, Long)] = ... input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .fold(&quot;&quot;) { (acc, v) =&gt; acc + v._2 } 上面例子追加所有输入的长整型到一个空的字符串中。 注意 fold()不能应用于回话窗口或者其他可合并的窗口中。 窗口函数 —— 一般用法(WindowFunction - The Generic Case) 一个WindowFunction将获得一个包含了window中的所有元素迭代(Iterable)，并且提供所有窗口函数的最大灵活性。这些带来了性能的成本和资源的消耗，因为window中的元素无法进行增量迭代，而是缓存起来直到window被认为是可以处理时为止。 WindowFunction的使用说明如下: Java 代码: public interface WindowFunction&lt;IN, OUT, KEY, W extends Window&gt; extends Function, Serializable { /** // Evaluates the window and outputs none or several elements. // @param key The key for which this window is evaluated. // @param window The window that is being evaluated. // @param input The elements in the window being evaluated. // @param out A collector for emitting elements. // @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ void apply(KEY key, W window, Iterable&lt;IN&gt; input, Collector&lt;OUT&gt; out) throws Exception; } Scala 代码: trait WindowFunction[IN, OUT, KEY, W &lt;: Window] extends Function with Serializable { /** // Evaluates the window and outputs none or several elements. // // @param key The key for which this window is evaluated. // @param window The window that is being evaluated. // @param input The elements in the window being evaluated. // @param out A collector for emitting elements. // @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ def apply(key: KEY, window: W, input: Iterable[IN], out: Collector[OUT]) } 一个WindowFunction可以按如下方式来定义和使用: Java 代码: DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...; input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .apply(new MyWindowFunction()); /* ... */ public class MyWindowFunction implements WindowFunction&lt;Tuple&lt;String, Long&gt;, String, String, TimeWindow&gt; { void apply(String key, TimeWindow window, Iterable&lt;Tuple&lt;String, Long&gt;&gt; input, Collector&lt;String&gt; out) { long count = 0; for (Tuple&lt;String, Long&gt; in: input) { count++; } out.collect(&quot;Window: &quot; + window + &quot;count: &quot; + count); } } Scala 代码: val input: DataStream[(String, Long)] = ... input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .apply(new MyWindowFunction()) /* ... */ class MyWindowFunction extends WindowFunction[(String, Long), String, String, TimeWindow] { def apply(key: String, window: TimeWindow, input: Iterable[(String, Long)], out: Collector[String]): () = { var count = 0L for (in &lt;- input) { count = count + 1 } out.collect(s&quot;Window $window count: $count&quot;) } } 上面的例子展示了统计一个window中元素个数的WindowFunction，此外，还将window的信息添加到输出中。 注意:使用WindowFunction来做简单的聚合操作如计数操作，性能是相当差的。下一章节我们将展示如何将ReduceFunction跟WindowFunction结合起来，来获取增量聚合和添加到WindowFunction中的信息。 ProcessWindowFunction 在使用WindowFunction的地方你也可以用ProcessWindowFunction，这跟WindowFunction很类似，除了接口允许查询跟多关于context的信息，context是window评估发生的地方。 下面是ProcessWindowFunction的接口: Java 代码: public abstract class ProcessWindowFunction&lt;IN, OUT, KEY, W extends Window&gt; implements Function { /** // Evaluates the window and outputs none or several elements. // // @param key The key for which this window is evaluated. // @param context The context in which the window is being evaluated. // @param elements The elements in the window being evaluated. // @param out A collector for emitting elements. // // @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ public abstract void process( KEY key, Context context, Iterable&lt;IN&gt; elements, Collector&lt;OUT&gt; out) throws Exception; /** // The context holding window metadata */ public abstract class Context { /** // @return The window that is being evaluated. */ public abstract W window(); } } Scala 代码: abstract class ProcessWindowFunction[IN, OUT, KEY, W &lt;: Window] extends Function { /** // Evaluates the window and outputs none or several elements. // // @param key The key for which this window is evaluated. // @param context The context in which the window is being evaluated. // @param elements The elements in the window being evaluated. // @param out A collector for emitting elements. // @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ @throws[Exception] def process( key: KEY, context: Context, elements: Iterable[IN], out: Collector[OUT]) /** // The context holding window metadata */ abstract class Context { /** // @return The window that is being evaluated. */ def window: W } } ProcessWindowFunction可以通过如下方式调用: Java 代码: DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...; input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .process(new MyProcessWindowFunction());` Scala 代码: `val input: DataStream[(String, Long)] = ... input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .process(new MyProcessWindowFunction()) 有增量聚合功能的WindowFunction (WindowFunction with Incremental Aggregation) WindowFunction可以跟ReduceFunction或者FoldFunction结合来增量地对到达window中的元素进行聚合，当window关闭之后，WindowFunction就能提供聚合结果。当获取到WindowFunction额外的window元信息后就可以进行增量计算窗口了。 标注:你也可以使用ProcessWindowFunction替换WindowFunction来进行增量窗口聚合。 使用FoldFunction 进行增量窗口聚合(Incremental Window Aggregation with FoldFunction) 下面的例子展示了一个增量的FoldFunction如何跟一个WindowFunction结合，来获取窗口的事件数，并同时返回窗口的key和窗口的最后时间。 Java 代码: DataStream&lt;SensorReading&gt; input = ...; input .keyBy(&lt;key selector&gt;) .timeWindow(&lt;window assigner&gt;) .fold(new Tuple3&lt;String, Long, Integer&gt;(&quot;&quot;,0L, 0), new MyFoldFunction(), new MyWindowFunction()) // Function definitions private static class MyFoldFunction implements FoldFunction&lt;SensorReading, Tuple3&lt;String, Long, Integer&gt; &gt; { public Tuple3&lt;String, Long, Integer&gt; fold(Tuple3&lt;String, Long, Integer&gt; acc, SensorReading s) { Integer cur = acc.getField(2); acc.setField(2, cur + 1); return acc; } } private static class MyWindowFunction implements WindowFunction&lt;Tuple3&lt;String, Long, Integer&gt;, Tuple3&lt;String, Long, Integer&gt;, String, TimeWindow&gt; { public void apply(String key, TimeWindow window, Iterable&lt;Tuple3&lt;String, Long, Integer&gt;&gt; counts, Collector&lt;Tuple3&lt;String, Long, Integer&gt;&gt; out) { Integer count = counts.iterator().next().getField(2); out.collect(new Tuple3&lt;String, Long, Integer&gt;(key, window.getEnd(),count)); } } Scala 代码: val input: DataStream[SensorReading] = ... input .keyBy(&lt;key selector&gt;) .timeWindow(&lt;window assigner&gt;) .fold ( (&quot;&quot;, 0L, 0), (acc: (String, Long, Int), r: SensorReading) =&gt; { (&quot;&quot;, 0L, acc._3 + 1) }, ( key: String, window: TimeWindow, counts: Iterable[(String, Long, Int)], out: Collector[(String, Long, Int)] ) =&gt; { val count = counts.iterator.next() out.collect((key, window.getEnd, count._3)) } ) 使用ReduceFunction进行增量窗口聚合(Incremental Window Aggregation with ReduceFunction) 下面例子展示了一个增量额ReduceFunction如何跟一个WindowFunction结合，来获取窗口中最小的事件和窗口的开始时间。 Java 代码: DataStream&lt;SensorReading&gt; input = ...; input .keyBy(&lt;key selector&gt;) .timeWindow(&lt;window assigner&gt;) .reduce(new MyReduceFunction(), new MyWindowFunction()); // Function definitions private static class MyReduceFunction implements ReduceFunction&lt;SensorReading&gt; { public SensorReading reduce(SensorReading r1, SensorReading r2) { return r1.value() &gt; r2.value() ? r2 : r1; } } private static class MyWindowFunction implements WindowFunction&lt;SensorReading, Tuple2&lt;Long, SensorReading&gt;, String, TimeWindow&gt; { public void apply(String key, TimeWindow window, Iterable&lt;SensorReading&gt; minReadings, Collector&lt;Tuple2&lt;Long, SensorReading&gt;&gt; out) { SensorReading min = minReadings.iterator().next(); out.collect(new Tuple2&lt;Long, SensorReading&gt;(window.getStart(), min)); } } Scala 代码: val input: DataStream[SensorReading] = ... input .keyBy(&lt;key selector&gt;) .timeWindow(&lt;window assigner&gt;) .reduce( (r1: SensorReading, r2: SensorReading) =&gt; { if (r1.value &gt; r2.value) r2 else r1 }, ( key: String, window: TimeWindow, minReadings: Iterable[SensorReading], out: Collector[(Long, SensorReading)] ) =&gt; { val min = minReadings.iterator.next() out.collect((window.getStart, min)) } ) 触发器(Triggers) 触发器决定了一个窗口何时可以被窗口函数处理，每一个窗口分配器都有一个默认的触发器，如果默认的触发器不能满足你的需要，你可以通过调用trigger(...)来指定一个自定义的触发器。触发器的接口有5个方法来允许触发器处理不同的事件: * onElement()方法,每个元素被添加到窗口时调用 * onEventTime()方法,当一个已注册的事件时间计时器启动时调用 * onProcessingTime()方法,当一个已注册的处理时间计时器启动时调用 * onMerge()方法，与状态性触发器相关，当使用会话窗口时，两个触发器对应的窗口合并时，合并两个触发器的状态。 * 最后一个clear()方法执行任何需要清除的相应窗口 上面的方法中有两个需要注意的地方: 1)第一、三通过返回一个TriggerResult来决定如何操作调用他们的事件，这些操作可以是下面操作中的一个； CONTINUE:什么也不做 FIRE:触发计算 PURGE:清除窗口中的数据 FIRE_AND_PURGE:触发计算并清除窗口中的数据 2)这些函数可以被用来为后续的操作注册处理时间定时器或者事件时间计时器 触发和清除(Fire and Purge) 一旦一个触发器决定一个窗口已经准备好进行处理，它将触发并返回FIRE或者FIRE_AND_PURGE。这是窗口操作发送当前窗口结果的信号，给定一个拥有一个WindowFunction的窗口那么所有的元素都将发送到WindowFunction中(可能之后还会发送到驱逐器(Evitor)中)。有ReduceFunction或者FoldFunction的Window仅仅发送他们的急切聚合结果。 当一个触发器触发时，它可以是FIRE或者FIRE_AND_PURGE，如果是FIRE的话，将保持window中的内容，FIRE_AND_PURGE的话，会清除window的内容。默认情况下，预实现的触发器仅仅是FIRE，不会清除window的状态。 注意:清除操作仅清除window的内容，并留下潜在的窗口元信息和完整的触发器状态。 窗口分配器默认的触发器(Default Triggers of WindowAssigners) 默认的触发器适用于许多种情况，例如:所有的事件时间分配器都有一个EventTimeTrigger作为默认的触发器，这个触发器仅在当水印通过窗口的最后时间时触发。 注意:GlobalWindow默认的触发器是NeverTrigger，是永远不会触发的，因此，如果你使用的是GlobalWindow的话，你需要定义一个自定义触发器。 注意:通过调用trigger(...)来指定一个触发器你就重写了WindowAssigner的默认触发器。例如:如果你为TumblingEventTimeWindows指定了一个CountTrigger，你就不会再通过时间来获取触发了，而是通过计数。现在，如果你想通过时间和计数来触发的话，你需要写你自己自定义的触发器。 内置的和自定义的触发器(Build-in and Custom Triggers) Flink有一些内置的触发器: *EventTimeTrigger(前面提到过)触发是根据由水印衡量的事件时间的进度来的 *ProcessingTimeTrigger 根据处理时间来触发 *CountTrigger 一旦窗口中的元素个数超出了给定的限制就会触发 *PurgingTrigger 作为另一个触发器的参数并将它转换成一个清除类型 如果你想实现一个自定义的触发器，你需要查看一下这个抽象类Trigger(https://github.com/apache/flink/blob/master//flink-streaming-java/src/main/java/org/apache/flink/streaming/api/windowing/triggers/Trigger.java ),请注意，这个API还在优化中，后续的Flink版本可能会改变。 驱逐器(Evictors) Flink的窗口模型允许指定一个除了WindowAssigner和Trigger之外的可选参数Evitor，这个可以通过调用evitor(...)方法(在这篇文档的开头展示过)来实现。这个驱逐器(evitor)可以在触发器触发之前或者之后，或者窗口函数被应用之前清理窗口中的元素。为了达到这个目的，Evitor接口有两个方法: /** // Optionally evicts elements. Called before windowing function. // // @param elements The elements currently in the pane. // @param size The current number of elements in the pane. // @param window The {@link Window} // @param evictorContext The context for the Evictor /// void evictBefore(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, int size, W window, EvictorContext evictorContext); /** // Optionally evicts elements. Called after windowing function. // // @param elements The elements currently in the pane. // @param size The current number of elements in the pane. // @param window The {@link Window} // @param evictorContext The context for the Evictor */ void evictAfter(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, int size, W window, EvictorContext evictorContext); evitorBefore()方法包含了在window function之前被应用的驱逐逻辑，而evitorAfter()方法包含了在window function之后被应用的驱逐逻辑。在window function应用之前被驱逐的元素将不会再被window function处理。 Flink有三个预实现的驱逐器，他们是: CountEvitor：在窗口中保持一个用户指定数量的元素，并在窗口的开始处丢弃剩余的其他元素 DeltaEvitor: 通过一个DeltaFunction和一个阈值，计算窗口缓存中最近的一个元素和剩余的所有元素的delta值，并清除delta值大于或者等于阈值的元素 TimeEvitor:使用一个interval的毫秒数作为参数，对于一个给定的窗口，它会找出元素中的最大时间戳max_ts，并清除时间戳小于max_tx - interval的元素。 默认情况下:所有预实现的evitor都是在window function前应用它们的逻辑 注意:指定一个Evitor要防止预聚合，因为窗口中的所有元素必须得在计算之前传递到驱逐器中 注意:Flink 并不保证窗口中的元素是有序的，所以驱逐器可能从窗口的开始处清除，元素到达的先后不是那么必要。 允许延迟(Allowed Lateness) 当处理事件时间的window时，可能会出现元素到达晚了，Flink用来与事件时间联系的水印已经过了元素所属的窗口的最后时间。可以查看事件时间(event time https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/event_time.html )尤其是晚到元素(late elements https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/event_time.html#late-elements )来了解Flink如何处理事件时间的讨论。 默认情况下，当水印已经过了窗口的最后时间时晚到的元素会被丢弃。然而，Flink允许为窗口操作指定一个最大允许时延，允许时延指定了元素可以晚到多长时间，默认情况下是0。水印已经过了窗口最后时间后才来的元素，如果还未到窗口最后时间加时延时间，那么元素任然添加到窗口中。如果依赖触发器的使用的话，晚到但是未丢弃的元素可能会导致窗口再次被触发。 为了达到这个目的，Flink将保持窗口的状态直到允许时延的发生，一旦发生，Flink将清除Window，删除window的状态，如Window 生命周期章节中所描述的那样。 默认情况下，允许时延为0，也就是说水印之后到达的元素将被丢弃。 你可以按如下方式来指定一个允许时延： Java 代码: DataStream&lt;T&gt; input = ...; input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .allowedLateness(&lt;time&gt;) .&lt;windowed transformation&gt;(&lt;window function&gt;); Scala 代码: val input: DataStream[T] = ... input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .allowedLateness(&lt;time&gt;) .&lt;windowed transformation&gt;(&lt;window function&gt;) 注意:当使用GlobalWindows分配器时，没有数据会被认为是延迟的，因为Global Window的最后时间是Long.MAX_VALUE。 ###以侧输出来获取延迟数据(Getting Late Data as a Site Output) 使用Flink的侧输出(https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/stream/side_output.html )特性，你可以获得一个已经被丢弃的延迟数据流。 首先你需要在窗口化的数据流中调用sideOutputLateData(OutputTag)指定你需要获取延迟数据，然后，你就可以在window 操作的结果中获取到侧输出流了。 代码如下： Java 代码： final OutputTag&lt;T&gt; lateOutputTag = new OutputTag&lt;T&gt;(&quot;late-data&quot;){}; DataStream&lt;T&gt; input = ...; DataStream&lt;T&gt; result = input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .allowedLateness(&lt;time&gt;) .sideOutputLateData(lateOutputTag) .&lt;windowed transformation&gt;(&lt;window function&gt;); DataStream&lt;T&gt; lateStream = result.getSideOutput(lateOutputTag); Scala代码： val lateOutputTag = OutputTag[T](&quot;late-data&quot;) val input: DataStream[T] = ... val result = input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .allowedLateness(&lt;time&gt;) .sideOutputLateData(lateOutputTag) .&lt;windowed transformation&gt;(&lt;window function&gt;) val lateStream = result.getSideOutput(lateOutputTag) 延迟元素考虑(Late elements considerations) 当指定一个允许延迟大于0时，window以及window中的内容将会继续保持即使水印已经达到了window的最后时间。在这种情况下，当一个延迟事件到来而未丢弃时，它可能会触发window中的其他触发器。这些触发叫做late firings，因为它们是由延迟事件触发的，并相对于window中第一个触发即主触发而言。对于session window而言，late firing还会进一步导致window的合并，因为它们桥接了两个之前存在差距，而未合并的window。 有用状态大小的考虑(Useful state size considerations) window 可以定义一个很长的周期(例如：一天、一周或者一月)，因此积累了相当大的状态。这里有些规则，当估计你的窗口计算的存储要求时，需要记住。 1. Flink会在每个窗口中为每个属于它的元素创建一份备份，鉴于此，滚动窗口保存了每个元素的一个备份，与此相反，滑动窗口会为每个元素创建几个备份，如Window Assigner章节所述。因此，一个窗口大小为1天，滑动大小为1秒的滑动窗口可能就不是个好的策略了。 2. FoldFunction和ReduceFunction可以制定reduce的存储需求，因为它们预聚合元素并且每个窗口只保存一个值。相反，只有WindowFunction需要累积所有的元素。 3. 使用Evitor需要避免任何预聚合操作，因为窗口中的所有元素都需要在应用于计算之前传递到evitor中 ","link":"https://tinaxiawuhao.github.io/post/5uA_yv-aO/"},{"title":"第六章 Flink 中的 Window","content":"Window Window 概述 streaming 流式计算是一种被设计用于处理无限数据集的数据处理引擎， 而无限数据集是指一种不断增长的本质上无限的数据集， 而 window 是一种切割无限数据 为有限块进行处理的手段。 Window 是无限数据流处理的核心， Window 将一个无限的 stream 拆分成有限大小的” buckets” 桶， 我们可以在这些桶上做计算操作。 Window 类型 Window 可以分成两类： CountWindow： 按照指定的数据条数生成一个 Window， 与时间无关。 TimeWindow： 按照时间生成 Window。 对于 TimeWindow， 可以根据窗口实现原理的不同分成三类： 滚动窗口（ TumblingWindow） 、 滑动窗口（ Sliding Window）、 和会话窗口（ Session Window） 。 滚动窗口（Tumbling Windows） 将数据依据固定的窗口长度对数据进行切片。 特点： 时间对齐， 窗口长度固定， 没有重叠。 滚动窗口分配器将每个元素分配到一个指定窗口大小的窗口中， 滚动窗口有一个固定的大小， 并且不会出现重叠。 例如： 如果你指定了一个 5 分钟大小的滚动窗 口， 窗口的创建如下图所示： 图 滚动窗口 适用场景： 适合做 BI 统计等（ 做每个时间段的聚合计算） 。 滑动窗口（Sliding Windows） 滑动窗口是固定窗口的更广义的一种形式， 滑动窗口由固定的窗口长度和滑动间隔组成。 特点：时间对齐， 窗口长度固定， 可以有重叠。 滑动窗口分配器将元素分配到固定长度的窗口中， 与滚动窗口类似， 窗口的大小由窗口大小参数来配置， 另一个窗口滑动参数控制滑动窗口开始的频率。 因此， 滑动窗口如果滑动参数小于窗口大小的话， 窗口是可以重叠的， 在这种情况下元素会被分配到多个窗口中。 例如， 你有 10 分钟的窗口和 5 分钟的滑动， 那么每个窗口中 5 分钟的窗口里包含着上个 10 分钟产生的数据， 如下图所示： 图 滑动窗口 适用场景： 对最近一个时间段内的统计（ 求某接口最近 5min 的失败率来决定是 否要报警） 。 会话窗口（Session Windows） 由一系列事件组合一个指定时间长度的 timeout 间隙组成， 类似于 web 应用的session， 也就是一段时间没有接收到新数据就会生成新的窗口。 特点： 时间无对齐。 session 窗口分配器通过 session 活动来对元素进行分组， session 窗口跟滚动窗口和滑动窗口相比， 不会有重叠和固定的开始时间和结束时间的情况， 相反， 当它在一个固定的时间周期内不再收到元素， 即非活动间隔产生， 那个这个窗口就会关 闭。 一个 session 窗口通过一个 session 间隔来配置， 这个 session 间隔定义了非活跃 周期的长度， 当这个非活跃周期产生， 那么当前的 session 将关闭并且后续的元素将 被分配到新的 session 窗口中去。 图 会话窗口 Window API TimeWindow TimeWindow 是将指定时间范围内的所有数据组成一个 window， 一次对一个 window 里面的所有数据进行计算。 滚动窗口 Flink 默认的时间窗口根据 Processing Time 进行窗口的划分， 将 Flink 获取到的 数据根据进入 Flink 的时间划分到不同的窗口中。 时间间隔可以通过 Time.milliseconds(x)， Time.seconds(x)， Time.minutes(x)等其 中的一个来指定。 import com.tan.flink.bean.SensorReading; import com.tan.flink.source.SourceFromCustom; import org.apache.flink.streaming.api.datastream.DataStreamSource; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.windowing.time.Time; public class TimeWindow_Tumbling { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource&lt;SensorReading&gt; inputDataStream = env.addSource(new SourceFromCustom.CustomSource()); SingleOutputStreamOperator&lt;SensorReading&gt; resultDataStream = inputDataStream.keyBy(&quot;id&quot;) .timeWindow(Time.seconds(5L)) // 滚动时间窗口大小 .maxBy(&quot;temperature&quot;); resultDataStream.print(); env.execute(); } } 滑动窗口（SlidingEventTimeWindows） 滑动窗口和滚动窗口的函数名是完全一致的， 只是在传参数时需要传入两个参 数， 一个是 window_size， 一个是 sliding_size。下面代码中的 sliding_size 设置为了 5s， 也就是说， 每 5s 就计算输出结果一次，每一次计算的 window 范围是 15s 内的所有元素。 时间间隔可以通过 Time.milliseconds(x)， Time.seconds(x)， Time.minutes(x)等其 中的一个来指定。 import com.tan.flink.bean.SensorReading; import com.tan.flink.source.SourceFromCustom; import org.apache.flink.streaming.api.datastream.DataStreamSource; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.windowing.time.Time; public class TimeWindow_Sliding { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource&lt;SensorReading&gt; inputDataStream = env.addSource(new SourceFromCustom.CustomSource()); SingleOutputStreamOperator&lt;SensorReading&gt; resultDataStream = inputDataStream.keyBy(&quot;id&quot;) .timeWindow(Time.seconds(15L), Time.seconds(5)) // 窗口大小15秒 滑动大小5秒 .minBy(&quot;temperature&quot;); resultDataStream.print(); env.execute(); } } CountWindow CountWindow 根据窗口中相同 key 元素的数量来触发执行， 执行时只计算元素数量达到窗口大小的 key 对应的结果。 注意： CountWindow 的 window_size 指的是相同 Key 的元素的个数， 不是输入 的所有元素的总数。 1 滚动窗口 默认的 CountWindow 是一个滚动窗口， 只需要指定窗口大小即可， 当元素数量达到窗口大小时， 就会触发窗口的执行。 import com.tan.flink.bean.SensorReading; import com.tan.flink.source.SourceFromCustom; import org.apache.flink.streaming.api.datastream.DataStreamSource; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; public class CountWindow_Tumbling { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource&lt;SensorReading&gt; inputDataStream = env.addSource(new SourceFromCustom.CustomSource()); SingleOutputStreamOperator&lt;SensorReading&gt; resultDataStream = inputDataStream.keyBy(&quot;id&quot;) .countWindow(3) .maxBy(&quot;temperature&quot;); resultDataStream.print(); env.execute(); } } 2 滑动窗口 滑动窗口和滚动窗口的函数名是完全一致的， 只是在传参数时需要传入两个参数， 一个是 window_size， 一个是 sliding_size。 下面代码中的 sliding_size 设置为了 2， 也就是说， 每收到两个相同 key 的数据就计算一次， 每一次计算的 window 范围是 10 个元素。 import com.tan.flink.bean.SensorReading; import com.tan.flink.source.SourceFromCustom; import org.apache.flink.streaming.api.datastream.DataStreamSource; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; public class CountWindow_Sliding { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource&lt;SensorReading&gt; inputDataStream = env.addSource(new SourceFromCustom.CustomSource()); SingleOutputStreamOperator&lt;SensorReading&gt; resultDataStream = inputDataStream.keyBy(&quot;id&quot;) .countWindow(10, 2) .minBy(&quot;temperature&quot;); resultDataStream.print(); env.execute(); } } window function window function 定义了要对窗口中收集的数据做的计算操作， 主要可以分为两 类： 增量聚合函数（ incremental aggregation functions） 每条数据到来就进行计算， 保持一个简单的状态。 典型的增量聚合函数有 ReduceFunction, AggregateFunction。 全窗口函数（ full window functions） 先把窗口所有数据收集起来， 等到计算的时候会遍历所有数据。 ProcessWindowFunction 就是一个全窗口函数。 其它可选 API trigger() —— 触发器 定义 window 什么时候关闭， 触发计算并输出结果 evitor() —— 移除器,定义移除某些数据的逻辑 allowedLateness() —— 允许处理迟到的数据 sideOutputLateData() —— 将迟到的数据放入侧输出流 getSideOutput() —— 获取侧输出 ","link":"https://tinaxiawuhao.github.io/post/FnbZ5kiHQ/"},{"title":"第五章 Flink 流处理 API","content":"Environment getExecutionEnvironment 创建一个执行环境， 表示当前执行程序的上下文。 如果程序是独立调用的， 则 此方法返回本地执行环境； 如果从命令行客户端调用程序以提交到集群， 则此方法 返回此集群的执行环境， 也就是说， getExecutionEnvironment 会根据查询运行的方 式决定返回什么样的运行环境， 是最常用的一种创建执行环境的方式。 // 批处理环境 ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment() // 流式数据处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment() 如果没有设置并行度， 会以 flink-conf.yaml 中的配置为准， 默认是 1。 createLocalEnvironment 返回本地执行环境， 需要在调用时指定默认的并行度。 LocalStreamEnvironment localEnvironment = StreamExecutionEnvironment.createLocalEnvironment(1); createRemoteEnvironment 返回集群执行环境， 将 Jar 提交到远程服务器。 需要在调用时指定 JobManager的 IP 和端口号， 并指定要在集群中运行的 Jar 包。 final ExecutionEnvironment env = ExecutionEnvironment.createRemoteEnvironment( cluster.getHostname(), cluster.getPort(), config ); scala val env = ExecutionEnvironment.createRemoteEnvironment(&quot;jobmanage-hostname&quot;, 6123,&quot;YOURPATH//wordcount.jar&quot;) setParallelism // 为了打印到控制台的结果不乱序，我们配置全局的并发为1，改变并发对结果正确性没有影响 env.setParallelism(1); Source 从集合读取数据 package myflink; import lombok.*; //传感器温度读数的数据类型 @Data @NoArgsConstructor @AllArgsConstructor @Builder @ToString public class SensorReading { //属性 id,时间戳,温度值 private String id; private Long timestamp; private Double temperature; } public class SourceReading_Collection { public static void main(String[] args) throws Exception { //创建执行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //设置并行调度 //env.setParallelism(1); //从集合中读取数据 //属性 id,时间戳,温度值 DataStream&lt;SensorReading&gt; dataStream = env.fromCollection(Arrays.asList(new SensorReading(&quot;sensor_1&quot;, 1537718199L, 35.8), new SensorReading(&quot;sensor_6&quot;, 1547718201L, 15.4), new SensorReading(&quot;sensor_7&quot;, 1547718202L, 6.7), new SensorReading(&quot;sensor_10&quot;, 1547718205L, 38.1))); DataStream&lt;Integer&gt; integerDataStream = env.fromElements(1, 2, 4, 67, 189); //打印输出 dataStream.print(&quot;data&quot;); integerDataStream.print(&quot;int&quot;); //执行 Flink的jobName env.execute(&quot;SensorReading&quot;); } } 从文件读取数据 public class SourceReading_File { public static void main(String[] args) throws Exception { //创建执行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //读取文件 DataStream&lt;String&gt; dataStream = env.readTextFile(filePath,charsetName); //打印输出 dataStream.print(&quot;data&quot;); //执行 Flink的jobName env.execute(&quot;SensorReading&quot;); } } 以 kafka 消息队列的数据作为来源 需要引入 kafka 连接器的依赖： pom.xml &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka-0.11 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_2.12&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;/dependency&gt; //1. 创建一个Topic名为“test20201217”的主题 kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test20201217 //2. 创建producer(生产者)，生产主题的消息 kafka-console-producer.bat --broker-list localhost:9092 --topic test20201217 //3. 创建consumer(消费者)，消费主题消息 kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic test20201217 具体代码如下： /** * kafkaSource * * 從指定的offset出消费kafka */ public class StreamingKafkaSource { public static void main(String[] args) throws Exception { // 创建流处理的执行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 配置KafKa //配置KafKa和Zookeeper的ip和端口 Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); properties.setProperty(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;); properties.setProperty(&quot;group.id&quot;, &quot;consumer-group&quot;); //将kafka和zookeeper配置信息加载到Flink的执行环境当中StreamExecutionEnvironment FlinkKafkaConsumer011&lt;String&gt; myConsumer = new FlinkKafkaConsumer011&lt;String&gt;(&quot;test20201217&quot;, new SimpleStringSchema(), properties); //添加数据源，此处选用数据流的方式，将KafKa中的数据转换成Flink的DataStream类型 DataStream&lt;String&gt; stream = env.addSource(myConsumer); //打印输出 stream.print(); //执行Job，Flink执行环境必须要有job的执行步骤，而以上的整个过程就是一个Job env.execute(&quot;kafka sink test&quot;); } } 自定义 Source 除了以上的 source 数据来源， 我们还可以自定义 source。 需要做的， 只是传入一个 SourceFunction 就可以。 具体调用如下： env.addSource( new MySensorSource() ) 我们希望可以随机生成传感器数据， MySensorSource 具体的代码实现如下： import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.DataStreamSource; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.datastream.WindowedStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.source.SourceFunction; import org.apache.flink.streaming.api.functions.windowing.WindowFunction; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.streaming.api.windowing.windows.TimeWindow; import org.apache.flink.util.Collector; import org.apache.log4j.Level; import org.apache.log4j.Logger; import org.apache.lucene.analysis.CachingTokenFilter; import java.util.Random; public class MySelfSourceTest01 { public static void main(String[] args) { Logger.getLogger(&quot;org&quot;).setLevel(Level.OFF); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource&lt;String&gt; dataStreamSource = env.addSource(new SourceFunction&lt;String&gt;() { @Override public void run(SourceContext&lt;String&gt; ctx) throws Exception { Random random = new Random(); // 循环可以不停的读取静态数据 while (true) { int nextInt = random.nextInt(100); ctx.collect(&quot;random : &quot; + nextInt); Thread.sleep(1000); } } @Override public void cancel() { } }); WindowedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple, TimeWindow&gt; window = dataStreamSource.map(new MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() { @Override public Tuple2&lt;String, Integer&gt; map(String value) throws Exception { String[] sps = value.split(&quot;:&quot;); return new Tuple2&lt;&gt;(value, Integer.parseInt(sps[1].trim())); } }).keyBy(0).timeWindow(Time.seconds(5)); SingleOutputStreamOperator&lt;String&gt; apply = window.apply(new WindowFunction&lt;Tuple2&lt;String, Integer&gt;, String, Tuple, TimeWindow&gt;() { @Override public void apply(Tuple tuple, TimeWindow window, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; input, Collector&lt;String&gt; out) throws Exception { input.forEach(x -&gt; { System.out.println(&quot;apply function -&gt; &quot; + x.f0); out.collect(x.f0); }); } }); apply.print(); try { env.execute(&quot;myself_source_test01&quot;); } catch (Exception e) { e.printStackTrace(); } } } Transform 转换算子 map stream.map { x =&gt; x * 2 } // 1.map.把string转化成长度输出 //参数T R //T就是传的数据，什么类型都行 //R就是返回的类型 DataStream&lt;Integer&gt; mapStream = stringDataStream.map(new MapFunction&lt;String, Integer&gt;() { @Override public Integer map(String value) throws Exception { return value.length(); } }); flatMap flatMap 的函数签名： def flatMap[A,B](as: List[A])(f: A ⇒ List[B]): List[B] 例如: flatMap(List(1,2,3))(i ⇒ List(i,i)) 结果是 List(1,1,2,2,3,3), 而 List(&quot;a b&quot;, &quot;c d&quot;).flatMap(line ⇒ line.split(&quot; &quot;)) 结果是 List(a, b, c, d)。 stream.flatMap{ x =&gt; x.split(&quot; &quot;) } //2、flatmap,按照逗号分隔 DataStream&lt;String&gt; flatMapStream = stringDataStream.flatMap(new FlatMapFunction&lt;String, String&gt;() { @Override public void flatMap(String value, Collector&lt;String&gt; out) throws Exception { String[] fields = value.split(&quot;,&quot;); for(String field:fields) out.collect(field); } }); Filter stream.filter{ x =&gt; x == 1 } //3.filter,筛选sensor_1开头的的id对应的数据 DataStream&lt;String&gt; filterStream = stringDataStream.filter(new FilterFunction&lt;String&gt;() { @Override public boolean filter(String value) throws Exception { return value.startsWith(&quot;sensor_1&quot;); } }); KeyBy DataStream → KeyedStream： 逻辑地将一个流拆分成不相交的分区， 每个分区包含具有相同 key 的元素， 在内部以 hash 的形式实现的。 DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; windowCounts = text .flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() { @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) { for (String word : value.split(&quot;\\\\s&quot;)) { out.collect(Tuple2.of(word, 1)); } } }) //按Tuple2的第一个属性进行分区 .keyBy(0) //或者根据对象属性进行分区 KeyedStream&lt;SensorReading, String&gt; tempkeyedStream = mapDataStream.keyBy(mpdata -&gt; mpdata.getId()); 滚动聚合算子（Rolling Aggregation） 这些算子可以针对 KeyedStream 的每一个支流做聚合。 sum() min() max() minBy() maxBy() Reduce KeyedStream → DataStream： 一个分组数据流的聚合操作， 合并当前的元素和上次聚合的结果， 产生一个新的值， 返回的流中包含每一次聚合的结果， 而不是只返回最后一次聚合的最终结果。 DataStream&lt;String&gt; stream2 = env.readTextFile(&quot;YOUR_PATH\\\\sensor.txt&quot;) .map( data =&gt; { String[] dataArray = data.split(&quot;,&quot;) SensorReading(dataArray(0).trim, dataArray(1).trim.toLong, dataArray(2).trim.toDouble) }) .keyBy(&quot;id&quot;) .reduce( (x, y) =&gt; SensorReading(x.id, x.timestamp + 1, y.temperature) ) Split 和 Select Split 图 Split DataStream → SplitStream： 根据某些特征把一个 DataStream 拆分成两个或者 多个 DataStream。 Select 图 Select SplitStream→ DataStream： 从一个 SplitStream 中获取一个或者多个 DataStream。 需求： 传感器数据按照温度高低（ 以 30 度为界） ， 拆分成两个流。 public static void main(String[] args) throws Exception { final StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment(); DataStream&lt;Long&gt; input=env.generateSequence(0,10); SplitStream&lt;Long&gt; splitStream = input.split(new OutputSelector&lt;Long&gt;(){ @Override public Iterable&lt;String&gt; select(Long value) { List&lt;String&gt; output = new ArrayList&lt;String&gt;(); if (value % 2 == 0) { output.add(&quot;even&quot;); }else { output.add(&quot;odd&quot;); } return output; } }); //splitStream.print(); DataStream&lt;Long&gt; even = splitStream.select(&quot;even&quot;); DataStream&lt;Long&gt; odd = splitStream.select(&quot;odd&quot;); DataStream&lt;Long&gt; all = splitStream.select(&quot;even&quot;,&quot;odd&quot;); //even.print(); odd.print(); //all.print(); env.execute(); } Connect 和 CoMap 图 Connect 算子 DataStream,DataStream → ConnectedStreams： 连接两个保持他们类型的数 据流， 两个数据流被 Connect 之后， 只是被放在了一个同一个流中， 内部依然保持 各自的数据和形式不发生任何变化， 两个流相互独立。 CoMap,CoFlatMap 图 CoMap/CoFlatMap ConnectedStreams → DataStream： 作用于 ConnectedStreams 上， 功能与 map 和 flatMap 一样， 对 ConnectedStreams 中的每一个 Stream 分别进行 map 和 flatMap 处理。 DataStream warning = high.map( sensorData =&gt; (sensorData.id, sensorData.temperature) ) ConnectedStreams connected = warning.connect(low) DataStream coMap = connected.map( warningData =&gt; (warningData._1, warningData._2, &quot;warning&quot;), lowData =&gt; (lowData.id, &quot;healthy&quot;) ) Union 图 Union DataStream → DataStream： 对两个或者两个以上的 DataStream 进行 union 操 作， 产生一个包含所有 DataStream 元素的新 DataStream。 //合并以后打印 DataStream&lt;StartUpLog&gt; unionStream = appStoreStream.union(otherStream) unionStream.print(&quot;union:::&quot;) Connect 与 Union 区别： Union 之前两个流的类型必须是一样， Connect 可以不一样， 在之后的 coMap 中再去调整成为一样的。 Connect 只能操作两个流， Union 可以操作多个。 支持的数据类型 Flink 流应用程序处理的是以数据对象表示的事件流。 所以在 Flink 内部， 我们 需要能够处理这些对象。 它们需要被序列化和反序列化， 以便通过网络传送它们； 或者从状态后端、 检查点和保存点读取它们。 为了有效地做到这一点， Flink 需要明 确知道应用程序所处理的数据类型。 Flink 使用类型信息的概念来表示数据类型， 并 为每个数据类型生成特定的序列化器、 反序列化器和比较器。 Flink 还具有一个类型提取系统， 该系统分析函数的输入和返回类型， 以自动获 取类型信息， 从而获得序列化器和反序列化器。 但是， 在某些情况下， 例如 lambda 函数或泛型类型， 需要显式地提供类型信息， 才能使应用程序正常工作或提高其性 能。 Flink 支持 Java 和 Scala 中所有常见数据类型。 使用最广泛的类型有以下几种。 基础数据类型 Flink 支持所有的 Java 和 Scala 基础数据类型， Int, Double, Long, String, …​ DataStream&lt;Long&gt; numbers = env.fromElements(1L, 2L, 3L, 4L) numbers.map( n =&gt; n + 1 ) Java 和 Scala 元组（Tuples） DataStream&lt;String, Integer&gt; persons= env.fromElements( (&quot;Adam&quot;, 17), (&quot;Sarah&quot;, 23) ) persons.filter(p =&gt; p._2 &gt; 18) Scala 样例类（case classes） case class Person(name: String, age: Int) val persons: DataStream[Person] = env.fromElements( Person(&quot;Adam&quot;, 17), Person(&quot;Sarah&quot;, 23) ) persons.filter(p =&gt; p.age &gt; 18) Java 简单对象（POJOs） public class Person { public String name; public int age; public Person() {} public Person(String name, int age) { this.name = name; this.age = age; } } DataStream&lt;Person&gt; persons = env.fromElements( new Person(&quot;Alex&quot;, 42), new Person(&quot;Wendy&quot;, 23)); 其它（Arrays, Lists, Maps, Enums, 等等） Flink 对 Java 和 Scala 中的一些特殊目的的类型也都是支持的， 比如 Java 的 ArrayList， HashMap， Enum 等等。 实现 UDF 函数——更细粒度的控制流 函数类（Function Classes） Flink 暴露了所有 udf 函数的接口(实现方式为接口或者抽象类)。 例如 MapFunction, FilterFunction, ProcessFunction 等等。 下面例子实现了 FilterFunction 接口： public class CustomFilterFunction implements FilterFunction&lt;SensorReading&gt; { @Override public boolean filter(SensorReading sensorReading) throws Exception { return sensorReading.temperature&gt;30.0; } } public static void main(String[] args) throws Exception { // 创建流处理的执行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 从文件中读取数据 String inputPath = &quot;F:\\\\Projects\\\\BigData\\\\Flink\\\\FlinkTutorial\\\\src\\\\main\\\\resources\\\\sensor.txt&quot;; // 获取数据 DataStreamSource&lt;String&gt; dataStream = env.readTextFile(inputPath); // 1、先转换成SensorReading类型（简单转换操作） DataStream&lt;SensorReading&gt; stream = dataStream.map(new MapFunction&lt;String, SensorReading&gt;() { @Override public SensorReading map(String data) throws Exception { String[] arr = data.split(&quot;,&quot;); return new SensorReading(arr[0], arr[1], Double.valueOf(arr[2].toString())); } }); // 调用自定义CustomFilterFunction类的，实现过滤 DataStream&lt;SensorReading&gt; dataStreamFilter = stream.filter(new CustomFilterFunction()); dataStreamFilter .print(&quot;CustomFilterFunction&quot;); env.execute(&quot;Function test&quot;); } 还可以将函数实现成匿名类 DataStream&lt;SensorReading&gt; dataStreamFilter = stream.filter(new FilterFunction&lt;SensorReading&gt;() { @Override public boolean filter(SensorReading sensorReading) throws Exception { return sensorReading.temperature&gt;30.0; } ); 我们 filter 的字符串&quot;flink&quot;还可以当作参数传进去。 DataStream&lt;String&gt; tweets = ... DataStream&lt;String&gt; flinkTweets = tweets.filter(new KeywordFilter(&quot;flink&quot;)) public class KeywordFilter(String keyWord) implements FilterFunction&lt;String&gt; { @Override public boolean filter(String value) throws Exception { return value.contains(keyWord) } } 匿名函数（Lambda Functions） DataStream&lt;String&gt; tweets = ... DataStream&lt;String&gt; flinkTweets = tweets.filter((value) -&gt;value.contains(&quot;flink&quot;)) 富函数（Rich Functions） “ 富函数” 是 DataStream API 提供的一个函数类的接口， 所有 Flink 函数类都 有其 Rich 版本。 它与常规函数的不同在于， 可以获取运行环境的上下文， 并拥有一 些生命周期方法， 所以可以实现更复杂的功能。 RichMapFunction RichFlatMapFunction RichFilterFunction …​ Rich Function 有一个生命周期的概念。 典型的生命周期方法有： open()方法是 rich function 的初始化方法， 当一个算子例如 map 或者 filter 被调用之前 open()会被调用。 close()方法是生命周期中的最后一个调用的方法， 做一些清理工作。 getRuntimeContext()方法提供了函数的 RuntimeContext 的一些信息， 例如函 数执行的并行度， 任务的名字， 以及 state 状态 public class MyFlatMap extends RichFlatMapFunction&lt;Tuple2&lt;Integer, Integer&gt;, Tuple2&lt;Integer, Integer&gt;&gt; { Integer subTaskIndex = 0 StreamingRuntimeContext context = (StreamingRuntimeContext) getRuntimeContext() public void open(Configuration parameters) throws Exception { subTaskIndex = context.getIndexOfThisSubtask // 以下可以做一些初始化工作， 例如建立一个和 HDFS 的连接 } @Override public void flatMap(Tuple2&lt;Integer, Integer&gt; integerIntegerTuple2, Collector&lt;Tuple2&lt;Integer, Integer&gt;&gt; collector) throws Exception { if (in % 2 == subTaskIndex) { out.collect((subTaskIndex, in)) } } public void close() throws Exception { // 以下做一些清理工作， 例如断开和 HDFS 的连接。 } } Sink Flink 没有类似于 spark 中 foreach 方法， 让用户进行迭代的操作。 虽有对外的 输出操作都要利用 Sink 完成。 最后通过类似如下方式完成整个任务最终输出操作。 stream.addSink(new MySink(xxxx)) 官方提供了一部分的框架的 sink。 除此以外， 需要用户自定义实现 sink。 Kafka pom.xml &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka-0.11 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_2.12&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;/dependency&gt; 主函数中添加 sink： DataStream&lt;String&gt; union = high.union(low).map(item-&gt;item.temperature.toString) union.addSink(new FlinkKafkaProducer&lt;String&gt;(&quot;localhost:9092&quot;, &quot;test&quot;, new SimpleStringSchema())) Redis pom.xml &lt;!-- https://mvnrepository.com/artifact/org.apache.bahir/flink-connector-redis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.bahir&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-redis_2.11&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;/dependency&gt; 定义一个 redis 的 mapper 类， 用于定义保存到 redis 时调用的命令： public class MyRedisMapper extends RedisMapper&lt;SensorReading&gt;{ public RedisCommandDescription getCommandDescription{ new RedisCommandDescription(RedisCommand.HSET, &quot;sensor_temperature&quot;) } public String getValueFromData(SensorReading t){ t.temperature.toString String getKeyFromData(SensorReading t) { return t.id } } } 在主函数中调用： dataStream.addSink( new RedisSink&lt;SensorReading&gt;( new FlinkJedisPoolConfig.Builder().setHost(&quot;localhost&quot;).setPort(6379).build(), new MyRedisMapper) ) Elasticsearch pom.xml &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-elasticsearch6_2.12&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;/dependency&gt; 在主函数中调用： List httpHosts = new ArrayList&lt;HttpHost&gt;() httpHosts.add(new HttpHost(&quot;localhost&quot;, 9200)) DataStream esSinkBuilder = new ElasticsearchSink.Builder&lt;SensorReading&gt;( httpHosts,new ElasticsearchSinkFunction&lt;SensorReading&gt; { public Unit process(SensorReading t, RuntimeContext runtimeContext, RequestIndexer requestIndexer ) { println(&quot;saving data: &quot; + t) map json = new util.HashMap&lt;String, String&gt;() json.put(&quot;data&quot;, t.toString) IndexRequest indexRequest = Requests.indexRequest().index(&quot;sensor&quot;).`type`(&quot;readingData&quot;).source(json) requestIndexer.add(indexRequest) println(&quot;saved successfully&quot;) } } ) dataStream.addSink( esSinkBuilder.build() ) JDBC 自定义 sink &lt;!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.44&lt;/version&gt; &lt;/dependency&gt; 添加 MyJdbcSink public class MyJdbcSink() extends RichSinkFunction&lt;SensorReading&gt;{ Connection conn; PreparedStatement insertStmt; PreparedStatement updateStmt; // open 主要是创建连接 public Unit open(Configuration parameters) = { super.open(parameters) conn = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/test&quot;, &quot;root&quot;, &quot;123456&quot;) insertStmt = conn.prepareStatement(&quot;INSERT INTO temperatures (sensor, temp) VALUES (?, ?)&quot;) updateStmt = conn.prepareStatement(&quot;UPDATE temperatures SET temp = ? WHERE sensor = ?&quot;) } //调用连接， 执行 sql public Unit invoke(SensorReading value , SinkFunction.Context[] context）{ updateStmt.setDouble(1, value.temperature) updateStmt.setString(2, value.id) updateStmt.execute() if (updateStmt.getUpdateCount == 0) { insertStmt.setString(1, value.id) insertStmt.setDouble(2, value.temperature) insertStmt.execute() } } public Unit close() { insertStmt.close() updateStmt.close() conn.close() } } 在 main 方法中增加， 把明细保存到 mysql 中 dataStream.addSink(new MyJdbcSink()) ","link":"https://tinaxiawuhao.github.io/post/lvf921hnA/"},{"title":"第四章 Flink 运行架构","content":"Flink 运行时的组件 Flink 运行时架构主要包括四个不同的组件， 它们会在运行流处理应用程序时协同工作： 作业管理器（JobManager） 、 资源管理器（ResourceManager） 、 任务管理器（TaskManager），以及分发器（Dispatcher） 。 因为 Flink 是用 Java 和 Scala 实现的， 所以所有组件都会运行在Java 虚拟机上。 每个组件的职责如下： 1. 作业管理器（JobManager） 控制一个应用程序执行的主进程， 也就是说， 每个应用程序都会被一个不同的 JobManager 所控制执行。 JobManager 会先接收到要执行的应用程序， 这个应用程序会包括： 作业图（JobGraph） 、 逻辑数据流图（logical dataflow graph） 和打包了所有的类、 库和其它 资源的 JAR 包。 JobManager 会把 JobGraph 转换成一个物理层面的数据流图， 这个图被叫做 “执行图” （ExecutionGraph） ， 包含了所有可以并发执行的任务。 JobManager 会向资源管 理器（ResourceManager） 请求执行任务必要的资源， 也就是任务管理器（TaskManager） 上 的插槽（ slot） 。 一旦它获取到了足够的资源， 就会将执行图分发到真正运行它们的TaskManager 上。 而在运行过程中， JobManager 会负责所有需要中央协调的操作， 比如说检 查点（checkpoints） 的协调。 2. 资源管理器（ResourceManager） 主要负责管理任务管理器（TaskManager） 的插槽（slot） ， TaskManger 插槽是 Flink 中 定义的处理资源单元。 Flink 为不同的环境和资源管理工具提供了不同资源管理器， 比如 YARN、 Mesos、 K8s， 以及 standalone 部署。 当 JobManager 申请插槽资源时， ResourceManager会将有空闲插槽的 TaskManager 分配给 JobManager。 如果 ResourceManager 没有足够的插槽来满足 JobManager 的请求， 它还可以向资源提供平台发起会话， 以提供启动 TaskManager进程的容器。 另外， ResourceManager 还负责终止空闲的TaskManager， 释放计算资源。 3. 任务管理器（TaskManager） Flink 中的工作进程。 通常在 Flink 中会有多个 TaskManager 运行， 每一个 TaskManager 都包含了一定数量的插槽（slots） 。 插槽的数量限制了 TaskManager 能够执行的任务数量。 启动之后， TaskManager 会向资源管理器注册它的插槽； 收到资源管理器的指令后，TaskManager 就会将一个或者多个插槽提供给 JobManager 调用。 JobManager 就可以向插槽分配任务（tasks） 来执行了。 在执行过程中， 一个 TaskManager 可以跟其它运行同一应用程序的 TaskManager 交换数据。 4. 分发器（Dispatcher） 可以跨作业运行， 它为应用提交提供了 REST 接口。 当一个应用被提交执行时， 分发器 就会启动并将应用移交给一个 JobManager。 由于是 REST 接口， 所以 Dispatcher 可以作为集 群的一个 HTTP 接入点， 这样就能够不受防火墙阻挡。 Dispatcher 也会启动一个 Web UI， 用 来方便地展示和监控作业执行的信息。 Dispatcher 在架构中可能并不是必需的， 这取决于应 用提交运行的方式。 任务提交流程 我们来看看当一个应用提交执行时， Flink 的各个组件是如何交互协作的： 图 任务提交和组件交互流程 上图是从一个较为高层级的视角， 来看应用中各组件的交互协作。 如果部署的集群环境 不同（例如 YARN， Mesos， Kubernetes， standalone 等） ， 其中一些步骤可以被省略， 或是 有些组件会运行在同一个 JVM 进程中。 具体地， 如果我们将 Flink 集群部署到 YARN 上， 那么就会有如下的提交流程： 图 Yarn 模式任务提交流程 Flink 任 务 提 交 后 ， Client 向 HDFS 上 传 Flink 的 Jar 包 和 配 置 ， 之 后 向 YarnResourceManager 提 交 任 务 ， ResourceManager 分 配 Container 资 源 并 通 知 对 应 的NodeManager 启动 ApplicationMaster， ApplicationMaster 启动后加载 Flink 的 Jar 包和配置构建环境， 然后启动 JobManager， 之后 ApplicationMaster 向 ResourceManager申 请 资 源 启 动 TaskManager ， ResourceManager 分 配 Container 资 源 后 ， 由ApplicationMaster 通 知 资 源 所 在 节 点 的 NodeManager 启 动 TaskManager ，NodeManager 加载 Flink 的 Jar 包和配置构建环境并启动 TaskManager， TaskManager启动后向 JobManager 发送心跳包， 并等待 JobManager 向其分配任务。 任务调度原理 图 任务调度原理 客 户 端 不 是 运 行 时 和 程 序 执 行 的 一 部 分 ， 但 它 用 于 准 备 并 发 送 dataflow(JobGraph)给 Master(JobManager)， 然后， 客户端断开连接或者维持连接以 等待接收计算结果。 当 Flink 集 群 启 动 后 ， 首 先 会 启 动 一 个 JobManger 和 一 个 或 多 个 的TaskManager。 由 Client 提交任务给 JobManager， JobManager 再调度任务到各个TaskManager 去执行， 然后 TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。 上述三者均为独立的 JVM 进程。 Client 为提交 Job 的客户端， 可以是运行在任何机器上（ 与 JobManager 环境连通即可） 。 提交 Job 后， Client 可以结束进程（ Streaming 的任务） ， 也可以不结束并等待结果返回。 JobManager`主 要 负 责 调 度 Job 并 协 调 Task 做 checkpoint ， 职 责 上 很 像Storm 的 Nimbus。 从 Client 处接收到 Job 和 JAR 包等资源后， 会生成优化后的执行计划， 并以 Task 的单元调度到各个 TaskManager 去执行。 TaskManager`在启动的时候就设置好了槽位数（ Slot） ， 每个 slot 能启动一个Task， Task 为线程。 从 JobManager 处接收需要部署的 Task， 部署启动后， 与自己的上游建立 Netty 连接， 接收数据并处理。 TaskManger 与 Slots Flink 中每一个 worker(TaskManager)都是一个 `JVM 进程`， 它可能会在`独立的线 程`上执行一个或多个 subtask。 为了控制一个 worker 能接收多少个 task， worker 通 过 task slot 来进行控制（ 一个 worker 至少有一个 task slot） 。 每个 task slot 表示 TaskManager 拥有资源的`一个固定大小的子集` 。 假如一个 TaskManager 有三个 slot， 那么它会将其管理的内存分成三份给各个 slot。 资源 slot 化意味着一个 subtask 将不需要跟来自其他 job 的 subtask 竞争被管理的内存， 取而 代之的是它将拥有一定数量的内存储备。 需要注意的是， 这里不会涉及到 CPU 的隔 离， slot 目前仅仅用来隔离 task 的受管理的内存。 通过调整 task slot 的数量， 允许用户定义 subtask 之间如何互相隔离。 如果一个 TaskManager 一个 slot， 那将意味着每个 task group 运行在独立的 JVM 中（ 该 JVM 可能是通过一个特定的容器启动的） ， 而一个 TaskManager 多个 slot 意味着更多的 subtask 可以共享同一个 JVM。 而在同一个 JVM 进程中的 task 将共享 TCP 连接（ 基 于多路复用） 和心跳消息。 它们也可能共享数据集和数据结构， 因此这减少了每个 task 的负载。 图 TaskManager 与 Slot 图 子任务共享 默认情况下， Flink 允许子任务共享 slot， 即使它们是不同任务的子任务（ 前提是它们来自同一个 job） 。 这样的结果是， 一个 slot 可以保存作业的整个管道。 Task Slot 是静态的概念， 是指 TaskManager 具有的并发执行能力， 可以通过参数 taskmanager.numberOfTaskSlots 进行配置； 而并行度 parallelism 是动态概念，即 TaskManager 运行程序时实际使用的并发能力， 可以通过参数 parallelism.default 进行配置。 也就是说， 假设一共有 3 个 TaskManager， 每一个 TaskManager 中的分配 3 个TaskSlot， 也就是每个 TaskManager 可以接收 3 个 task， 一共 9 个 TaskSlot， 如果我们设置 parallelism.default=1， 即运行程序默认的并行度为 1， 9 个 TaskSlot 只用了 1个， 有 8 个空闲， 因此， 设置合适的并行度才能提高效率。 程序与数据流（DataFlow） 所有的 Flink 程序都是由三部分组成的： `Source` 、 `Transformation` 和 `Sink`。 Source 负责读取数据源， Transformation 利用各种算子进行处理加工， Sink 负责输出。 在运行时， Flink 上运行的程序会被映射成“ 逻辑数据流” （ dataflows） ， 它包含了这三部分。 每一个 dataflow 以一个或多个 sources 开始以一个或多个 sinks 结束。 dataflow 类似于任意的有向无环图（ DAG） 。 在大部分情况下， 程序中的转换 运算（ transformations） 跟 dataflow 中的算子（ operator） 是一一对应的关系， 但有 时候， 一个 transformation 可能对应多个 operator。 图 程序与数据流 执行图（ExecutionGraph） 由 Flink 程序直接映射成的数据流图是 StreamGraph， 也被称为逻辑流图， 因为它们表示的是计算逻辑的高级视图。 为了执行一个流处理程序， Flink 需要将逻辑流 图转换为物理数据流图（ 也叫执行图） ， 详细说明程序的执行方式。 Flink 中的执行图可以分成四层： StreamGraph -> JobGraph -> ExecutionGraph -> 物理执行图。 StreamGraph： 是根据用户通过 Stream API 编写的代码生成的最初的图。 用来表示程序的拓扑结构。 JobGraph： StreamGraph 经过优化后生成了 JobGraph， 提交给 JobManager 的数据结构。 主要的优化为， 将多个符合条件的节点 chain 在一起作为一个节点， 这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。 ExecutionGraph ： JobManager 根 据 JobGraph 生 成 ExecutionGraph 。ExecutionGraph 是 JobGraph 的并行化版本， 是调度层最核心的数据结构。 物理执行图： JobManager 根据 ExecutionGraph 对 Job 进行调度后， 在各个TaskManager 上部署 Task 后形成的“ 图” ， 并不是一个具体的数据结构。 并行度（Parallelism） Flink 程序的执行具有并行、 分布式的特性。 在执行过程中， 一个流（ stream） 包含一个或多个分区（ stream partition） ， 而 每一个算子（ operator） 可以包含一个或多个子任务（ operator subtask） ， 这些子任 务在不同的线程、 不同的物理机或不同的容器中彼此互不依赖地执行。 一个特定算子的子任务（ subtask） 的个数被称之为其并行度（ parallelism） 。 一般情况下， 一个流程序的并行度， 可以认为就是其所有算子中最大的并行度。 一 个程序中， 不同的算子可能具有不同的并行度。 图 并行数据流 Stream 在算子之间传输数据的形式可以是 one-to-one(forwarding)的模式也可以是 redistributing 的模式， 具体是哪一种形式， 取决于算子的种类。 One-to-one： stream(比如在 source 和 map operator 之间)维护着分区以及元素的 顺序。 那意味着 map 算子的子任务看到的元素的个数以及顺序跟 source 算子的子 任务生产的元素的个数、 顺序相同， map、 fliter、 flatMap 等算子都是 one-to-one 的 对应关系。 类似于 spark 中的窄依赖 Redistributing： stream(map()跟 keyBy/window 之间或者 keyBy/window 跟 sink之间)的分区会发生改变。 每一个算子的子任务依据所选择的 transformation 发送数据到不同的目标任务。 例如， keyBy() 基于 hashCode 重分区、 broadcast 和 rebalance会随机重新分区， 这些算子都会引起 redistribute 过程， 而 redistribute 过程就类似于Spark 中的 shuffle 过程。 类似于 spark 中的宽依赖 任务链（Operator Chains） 相同并行度的 one to one 操作， Flink 这样相连的算子链接在一起形成一个 task，原来的算子成为里面的一部分。 将算子链接成 task 是非常有效的优化： 它能减少线程之间的切换和基于缓存区的数据交换， 在减少时延的同时提升吞吐量。 链接的行 为可以在编程 API 中进行指定。 图 task 与 operator chains ","link":"https://tinaxiawuhao.github.io/post/4_EhGqVjY/"},{"title":"第三章 Flink 部署","content":"Standalone 模式 安装 解压缩 flink-1.10.1-bin-scala_2.12.tgz， 进入 conf 目录中。 修改 flink/conf/flink-conf.yaml 文件： 修改 /conf/slaves 文件： 分发给另外两台机子： 启动 访问 http://localhost:8081 可以对 flink 集群和任务进行监控管理。 提交任务 准备数据文件（ 如果需要） 把含数据文件的文件夹， 分发到 taskmanage 机器中 如 果 从 文 件 中 读 取 数 据 ， 由 于 是 从 本 地 磁 盘 读 取 ， 实 际 任 务 会 被 分 发 到taskmanage 的机器中， 所以要把目标文件分发。 执行程序 ./flink run -c com.atguigu.wc.StreamWordCount –p 2 FlinkTutorial-1.0-SNAPSHOT-jar-with-dependencies.jar --host lcoalhost –port 7777 到目标文件夹中查看计算结果 注 意 ： 如 果 计 算 结 果 输 出 到 文 件 ， 会 保 存 到 taskmanage 的 机 器 下 ， 不 会 在jobmanage 下。 在 webui 控制台查看计算过程 Yarn 模式 以 Yarn 模式部署 Flink 任务时， 要求 Flink 是有 Hadoop 支持的版本， Hadoop环境需要保证版本在 2.2 以上， 并且集群中安装有 HDFS 服务 Flink on Yarn Flink 提供了两种在 yarn 上运行的模式， 分别为 Session-Cluster 和 Per-Job-Cluster模式。 Session-cluster 模式： Session-Cluster 模式需要先启动集群， 然后再提交作业， 接着会向 yarn 申请一块空间后， 资源永远保持不变。 如果资源满了， 下一个作业就无法提交， 只能等到 yarn 中的其中一个作业执行完成后， 释放了资源， 下个作业才会正常提交。 所有作 业共享 Dispatcher 和 ResourceManager； 共享资源； 适合规模小执行时间短的作业。 在 yarn 中初始化一个 flink 集群， 开辟指定的资源， 以后提交任务都向这里提交。 这个 flink 集群会常驻在 yarn 集群中， 除非手工停止。 Per-Job-Cluster 模式： 一个 Job 会对应一个集群， 每提交一个作业会根据自身的情况， 都会单独向 yarn申请资源， 直到作业执行完成， 一个作业的失败与否并不会影响下一个作业的正常 提交和运行。 独享 Dispatcher 和 ResourceManager， 按需接受资源申请； 适合规模大 长时间运行的作业。 每次提交都会创建一个新的 flink 集群， 任务之间互相独立， 互不影响， 方便管理。 任务执行完成之后创建的集群也会消失。 Session Cluster 启动 hadoop 集群（ 略） 启动 yarn-session ./yarn-session.sh -n 2 -s 2 -jm 1024 -tm 1024 -nm test -d 其中： -n(--container)： TaskManager 的数量。 -s(--slots)： 每个 TaskManager 的 slot 数量， 默认一个 slot 一个 core， 默认每个 taskmanager 的 slot 的个数为 1， 有时可以多一些 taskmanager， 做冗余。 -jm： JobManager 的内存（ 单位 MB)。 -tm： 每个 taskmanager 的内存（ 单位 MB)。 -nm： yarn 的 appName(现在 yarn 的 ui 上的名字)。 -d： 后台执行。 执行任务 ./flink run -c com.atguigu.wc.StreamWordCount FlinkTutorial-1.0-SNAPSHOT-jar-with-dependencies.jar --host lcoalhost –port 7777 去 yarn 控制台查看任务状态 取消 yarn-session yarn application --kill application_1577588252906_0001 3.2.2 Per Job Cluster 启动 hadoop 集群（ 略） 不启动 yarn-session， 直接执行 job ./flink run –m yarn-cluster -c com.atguigu.wc.StreamWordCount FlinkTutorial-1.0-SNAPSHOT-jar-with-dependencies.jar --host lcoalhost –port 7777 Kubernetes 部署 容器化部署时目前业界很流行的一项技术， 基于 Docker 镜像运行能够让用户更加 方 便 地 对 应 用 进 行 管 理 和 运 维 。 容 器 管 理 工 具 中 最 为 流 行 的 就 是 Kubernetes（ k8s） ， 而 Flink 也在最近的版本中支持了 k8s 部署模式。 搭建 Kubernetes 集群（ 略） 配置各组件的 yaml 文件 在 k8s 上构建 Flink Session Cluster， 需要将 Flink 集群的组件对应的 docker 镜像分别在 k8s 上启动， 包括 JobManager、 TaskManager、 JobManagerService 三个镜像服务。 每个镜像服务都可以从中央镜像仓库中获取。 启动 Flink Session Cluster // 启动 jobmanager-service 服务 kubectl create -f jobmanager-service.yaml // 启动 jobmanager-deployment 服务 kubectl create -f jobmanager-deployment.yaml // 启动 taskmanager-deployment 服务 kubectl create -f taskmanager-deployment.yaml 访问 Flink UI 页面 集群启动后， 就可以通过 JobManagerServicers 中配置的 WebUI 端口， 用浏览器输入以下 url 来访问 Flink UI 页面了： http://{JobManagerHost:Port}/api/v1/namespaces/default/services/flink-jobmanager:ui/proxy ","link":"https://tinaxiawuhao.github.io/post/aPsJFkgVZ/"},{"title":"第二章 快速上手","content":"搭建 maven 工程 FlinkTutorial pom 文件 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.example&lt;/groupId&gt; &lt;artifactId&gt;flink-test&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;scala.version&gt;2.11&lt;/scala.version&gt; &lt;flink.version&gt;1.10.0&lt;/flink.version&gt; &lt;hadoop.version&gt;2.7.7&lt;/hadoop.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_${scala.version}&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 批处理 wordcount WordCount 程序是大数据处理框架的入门程序，俗称“单词计数”。用来统计一段文字每个单词的出现次数，该程序主要分为两个部分：一部分是将文字拆分成单词；另一部分是单词进行分组计数并打印输出结果。 public static void main(String[] args) throws Exception { // 创建Flink运行的上下文环境 final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // 创建DataSet，这里我们的输入是一行一行的文本 DataSet&lt;String&gt; text = env.fromElements( &quot;Flink Spark Storm&quot;, &quot;Flink Flink Flink&quot;, &quot;Spark Spark Spark&quot;, &quot;Storm Storm Storm&quot; ); // 通过Flink内置的转换函数进行计算 DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; counts = text.flatMap(new LineSplitter()) .groupBy(0) .sum(1); //结果打印 counts.printToErr(); } public static final class LineSplitter implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; { @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) { // 将文本分割 String[] tokens = value.toLowerCase().split(&quot;\\\\W+&quot;); for (String token : tokens) { if (token.length() &gt; 0) { out.collect(new Tuple2&lt;String, Integer&gt;(token, 1)); } } } } 实现的整个过程中分为以下几个步骤。 首先，我们需要创建 Flink 的上下文运行环境： 复制ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); 然后，使用 fromElements 函数创建一个 DataSet 对象，该对象中包含了我们的输入，使用 FlatMap、GroupBy、SUM 函数进行转换。 最后，直接在控制台打印输出。 我们可以直接右键运行一下 main 方法，在控制台会出现我们打印的计算结果： 流处理 wordcount 为了模仿一个流式计算环境，我们选择监听一个本地的 Socket 端口，并且使用 Flink 中的滚动窗口，每 5 秒打印一次计算结果。代码如下： public class StreamingJob { public static void main(String[] args) throws Exception { // 创建Flink的流式计算环境 final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 监听本地9000端口 DataStream&lt;String&gt; text = env.socketTextStream(&quot;127.0.0.1&quot;, 9000, &quot;\\n&quot;); // 将接收的数据进行拆分，分组，窗口计算并且进行聚合输出 DataStream&lt;WordWithCount&gt; windowCounts = text .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() { @Override public void flatMap(String value, Collector&lt;WordWithCount&gt; out) { for (String word : value.split(&quot;\\\\s&quot;)) { out.collect(new WordWithCount(word, 1L)); } } }) .keyBy(&quot;word&quot;) .timeWindow(Time.seconds(5), Time.seconds(1)) .reduce(new ReduceFunction&lt;WordWithCount&gt;() { @Override public WordWithCount reduce(WordWithCount a, WordWithCount b) { return new WordWithCount(a.word, a.count + b.count); } }); // 打印结果 windowCounts.print().setParallelism(1); env.execute(&quot;Socket Window WordCount&quot;); } // Data type for words with count public static class WordWithCount { public String word; public long count; public WordWithCount() {} public WordWithCount(String word, long count) { this.word = word; this.count = count; } @Override public String toString() { return word + &quot; : &quot; + count; } } } 整个流式计算的过程分为以下几步。 首先创建一个流式计算环境： 复制StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); 然后进行监听本地 9000 端口，将接收的数据进行拆分、分组、窗口计算并且进行聚合输出。代码中使用了 Flink 的窗口函数，我们在后面的课程中将详细讲解。 我们在本地使用 netcat 命令启动一个端口： nc -lk 9000 然后直接运行我们的 main 方法： 测试——在 linux 系统中用 netcat 命令进行发送测试。 在 nc 中输入： $ nc -lk 9000 Flink Flink Flink Flink Spark Storm 可以在控制台看到： Flink : 4 Spark : 1 Storm : 1 ","link":"https://tinaxiawuhao.github.io/post/pQQ9j5lFD/"},{"title":"第一章 Flink 简介","content":"初识 Flink Flink 起源于 Stratosphere 项目， Stratosphere 是在 2010~2014 年由 3 所地处柏林的大学和欧洲的一些其他的大学共同进行的研究项目， 2014 年 4 月 Stratosphere 的代 码 被 复 制 并 捐 赠 给 了 Apache 软 件 基 金 会 ， 参 加 这 个 孵 化 项 目 的 初 始 成 员 是Stratosphere 系统的核心开发人员， 2014 年 12 月， Flink 一跃成为 Apache 软件基金会的顶级项目。 在德语中， Flink 一词表示快速和灵巧， 项目采用一只松鼠的彩色图案作为 logo，这不仅是因为松鼠具有快速和灵巧的特点， 还因为柏林的松鼠有一种迷人的红棕色， 而 Flink 的松鼠 logo 拥有可爱的尾巴， 尾巴的颜色与 Apache 软件基金会的 logo 颜 色相呼应， 也就是说， 这是一只 Apache 风格的松鼠。 Flink 项目的理念是：“ Apache Flink 是为分布式、 高性能、 随时可用以及准确的流处理应用程序打造的开源流处理框架”。 Apache Flink 是一个框架和分布式处理引擎， 用于对无界和有界数据流进行有状态计算。 Flink 被设计在所有常见的集群环境中运行， 以内存执行速度和任意规模 来执行计算。 Flink 的重要特点 事件驱动型(Event-driven) 事件驱动型应用是一类具有状态的应用， 它从一个或多个事件流提取数据， 并根据到来的事件触发计算、 状态更新或其他外部动作。 比较典型的就是以 kafka 为代表的消息队列几乎都是事件驱动型应用。 与之不同的就是 SparkStreaming 微批次， 如图： 事件驱动型： 流与批的世界观 批处理的特点是有界、 持久、 大量， 非常适合需要访问全套记录才能完成的计算工作， 一般用于离线统计。 流处理的特点是无界、 实时, 无需针对整个数据集执行操作， 而是对通过系统传输的每个数据项执行操作， 一般用于实时统计。在 spark 的世界观中， 一切都是由批次组成的， 离线数据是一个大批次， 而实时数据是由一个一个无限的小批次组成的。而在 flink 的世界观中， 一切都是由流组成的， 离线数据是有界限的流， 实时数据是一个没有界限的流， 这就是所谓的有界流和无界流。 无界数据流： 无界数据流有一个开始但是没有结束， 它们不会在生成时终止并提供数据， 必须连续处理无界流， 也就是说必须在获取后立即处理 event。 对于无界 数据流我们无法等待所有数据都到达， 因为输入是无界的， 并且在任何时间点都不会完成。 处理无界数据通常要求以特定顺序（ 例如事件发生的顺序） 获取 event， 以便能够推断结果完整性。 有界数据流： 有界数据流有明确定义的开始和结束， 可以在执行任何计算之前通过获取所有数据来处理有界流， 处理有界流不需要有序获取， 因为可以始终对有 界数据集进行排序， 有界流的处理也称为批处理。 这种以流为世界观的架构， 获得的最大好处就是具有极低的延迟。 分层 api 最底层级的抽象仅仅提供了有状态流， 它将通过过程函数（ Process Function）被嵌入到 DataStream API 中。 底层过程函数（ Process Function） 与 DataStream API相集成， 使其可以对某些特定的操作进行底层的抽象， 它允许用户可以自由地处理 来自一个或多个数据流的事件， 并使用一致的容错的状态。 除此之外， 用户可以注册事件时间并处理时间回调， 从而使程序可以处理复杂的计算。 实际上， 大多数应用并不需要上述的底层抽象， 而是针对核心 API（ Core APIs）进行编程， 比如 DataStream API（ 有界或无界流数据） 以及 DataSet API（ 有界数据集） 。 这些 API 为数据处理提供了通用的构建模块， 比如由用户定义的多种形式的 转换（ transformations） ， 连接（ joins） ， 聚合（ aggregations） ， 窗口操作（ windows）等等。 DataSet API 为有界数据集提供了额外的支持， 例如循环与迭代。 这些 API 处理的数据类型以类（ classes） 的形式由各自的编程语言所表示。 Table API 是以表为中心的声明式编程， 其中表可能会动态变化（ 在表达流数据时） 。 Table API 遵循（ 扩展的） 关系模型： 表有二维数据结构（ schema） （ 类似于关系数据库中的表）， 同时 API 提供可比较的操作， 例如 select、 project、 join、group-by、aggregate 等。 Table API 程序声明式地定义了什么逻辑操作应该执行， 而不是准确地 确定这些操作代码的看上去如何。 尽管 Table API 可以通过多种类型的用户自定义函数（ UDF） 进行扩展， 其仍不如核心 API 更具表达能力， 但是使用起来却更加简洁（ 代码量更少） 。 除此之外， Table API 程序在执行之前会经过内置优化器进行优化。 你可以在表与 DataStream/DataSet 之间无缝切换， 以允许程序将 Table API 与DataStream 以及 DataSet 混合使用。 Flink 提 供 的 最高 层 级 的 抽 象 是 SQL 。 这 一 层抽 象 在 语 法 与 表 达能 力 上 与Table API 类似， 但是是以 SQL 查询表达式的形式表现程序。 SQL 抽象与 Table API。交互密切， 同时 SQL 查询可以直接在 Table API 定义的表上执行。 目前 Flink 作为批处理还不是主流， 不如 Spark 成熟， 所以 DataSet 使用的并不是很多。 Flink Table API 和 Flink SQL 也并不完善， 大多都由各大厂商自己定制。 所以我们主要学习 DataStream API 的使用。 实际上 Flink 作为最接近 Google DataFlow模型的实现， 是流批统一的观点， 所以基本上使用 DataStream 就可以了。 Flink 几大模块 Flink Table &amp; SQL(还没开发完) Flink Gelly(图计算) Flink CEP(复杂事件处理) ","link":"https://tinaxiawuhao.github.io/post/gam9PGQCS/"},{"title":"Flink 零基础实战教程：如何计算实时热门商品","content":"在上一篇入门教程中，我们已经能够快速构建一个基础的 Flink 程序了。本文会一步步地带领你实现一个更复杂的 Flink 应用程序：实时热门商品。在开始本文前我们建议你先实践一遍上篇文章，因为本文会沿用上文的my-flink-project项目框架。 通过本文你将学到： 1. 如何基于 EventTime 处理，如何指定 Watermark 2. 如何使用 Flink 灵活的 Window API 3. 何时需要用到 State，以及如何使用 4. 如何使用 ProcessFunction 实现 TopN 功能 实战案例介绍 本案例将实现一个“实时热门商品”的需求，我们可以将“实时热门商品”翻译成程序员更好理解的需求：每隔5分钟输出最近一小时内点击量最多的前 N 个商品。将这个需求进行分解我们大概要做这么几件事情： * 抽取出业务时间戳，告诉 Flink 框架基于业务时间做窗口 * 过滤出点击行为数据 * 按一小时的窗口大小，每5分钟统计一次，做滑动窗口聚合（Sliding Window） * 按每个窗口聚合，输出每个窗口中点击量前N名的商品 数据准备 这里我们准备了一份淘宝用户行为数据集（来自阿里云天池公开数据集，特别感谢）。本数据集包含了淘宝上某一天随机一百万用户的所有行为（包括点击、购买、加购、收藏）。数据集的组织形式和MovieLens-20M类似，即数据集的每一行表示一条用户行为，由用户ID、商品ID、商品类目ID、行为类型和时间戳组成，并以逗号分隔。关于数据集中每一列的详细描述如下： 列名称 说明 用户ID 整数类型，加密后的用户ID 商品ID 整数类型，加密后的商品ID 商品类目ID 整数类型，加密后的商品所属类目ID 行为类型 字符串，枚举类型，包括(‘pv’, ‘buy’, ‘cart’, ‘fav’) 时间戳 行为发生的时间戳，单位秒 你可以通过下面的命令下载数据集到项目的 resources 目录下： $ cd my-flink-project/src/main/resources $ curl https://raw.githubusercontent.com/wuchong/my-flink-project/master/src/main/resources/UserBehavior.csv &gt; UserBehavior.csv 这里是否使用 curl 命令下载数据并不重要，你也可以使用 wget 命令或者直接访问链接下载数据。关键是，将数据文件保存到项目的resources 目录下，方便应用程序访问。 编写程序 在 src/main/java/myflink 下创建 HotItems.java 文件： package myflink; public class HotItems { public static void main(String[] args) throws Exception { } } 与上文一样，我们会一步步往里面填充代码。第一步仍然是创建一个 StreamExecutionEnvironment，我们把它添加到 main 函数中。 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 为了打印到控制台的结果不乱序，我们配置全局的并发为1，这里改变并发对结果正确性没有影响 env.setParallelism(1); 创建模拟数据源 在数据准备章节，我们已经将测试的数据集下载到本地了。由于是一个csv文件，我们将使用 CsvInputFormat 创建模拟数据源。 注：虽然一个流式应用应该是一个一直运行着的程序，需要消费一个无限数据源。但是在本案例教程中，为了省去构建真实数据源的繁琐，我们使用了文件来模拟真实数据源，这并不影响下文要介绍的知识点。这也是一种本地验证 Flink 应用程序正确性的常用方式。 我们先创建一个 UserBehavior 的 POJO 类（所有成员变量声明成public便是POJO类），强类型化后能方便后续的处理。 /** 用户行为数据结构 **/ public static class UserBehavior { public long userId; // 用户ID public long itemId; // 商品ID public int categoryId; // 商品类目ID public String behavior; // 用户行为, 包括(&quot;pv&quot;, &quot;buy&quot;, &quot;cart&quot;, &quot;fav&quot;) public long timestamp; // 行为发生的时间戳，单位秒 } 接下来我们就可以创建一个 PojoCsvInputFormat 了， 这是一个读取 csv 文件并将每一行转成指定 POJO 类型（在我们案例中是 UserBehavior）的输入器。 // UserBehavior.csv 的本地文件路径 URL fileUrl = HotItems2.class.getClassLoader().getResource(&quot;UserBehavior.csv&quot;); Path filePath = Path.fromLocalFile(new File(fileUrl.toURI())); // 抽取 UserBehavior 的 TypeInformation，是一个 PojoTypeInfo PojoTypeInfo&lt;UserBehavior&gt; pojoType = (PojoTypeInfo&lt;UserBehavior&gt;) TypeExtractor.createTypeInfo(UserBehavior.class); // 由于 Java 反射抽取出的字段顺序是不确定的，需要显式指定下文件中字段的顺序 String[] fieldOrder = new String[]{&quot;userId&quot;, &quot;itemId&quot;, &quot;categoryId&quot;, &quot;behavior&quot;, &quot;timestamp&quot;}; // 创建 PojoCsvInputFormat PojoCsvInputFormat&lt;UserBehavior&gt; csvInput = new PojoCsvInputFormat&lt;&gt;(filePath, pojoType, fieldOrder); 下一步我们用 PojoCsvInputFormat 创建输入源。 DataStream&lt;UserBehavior&gt; dataSource = env.createInput(csvInput, pojoType); 这就创建了一个 UserBehavior 类型的 DataStream。 EventTime 与 Watermark 当我们说“统计过去一小时内点击量”，这里的“一小时”是指什么呢？ 在 Flink 中它可以是指 ProcessingTime ，也可以是 EventTime，由用户决定。 ProcessingTime：事件被处理的时间。也就是由机器的系统时间来决定。 EventTime：事件发生的时间。一般就是数据本身携带的时间。 在本案例中，我们需要统计业务时间上的每小时的点击量，所以要基于 EventTime 来处理。那么如果让 Flink 按照我们想要的业务时间来处理呢？这里主要有两件事情要做。 第一件是告诉 Flink 我们现在按照 EventTime 模式进行处理，Flink 默认使用 ProcessingTime 处理，所以我们要显式设置下。 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); 第二件事情是指定如何获得业务时间，以及生成 Watermark。Watermark 是用来追踪业务事件的概念，可以理解成 EventTime 世界中的时钟，用来指示当前处理到什么时刻的数据了。由于我们的数据源的数据已经经过整理，没有乱序，即事件的时间戳是单调递增的，所以可以将每条数据的业务时间就当做 Watermark。这里我们用 AscendingTimestampExtractor 来实现时间戳的抽取和 Watermark的生成。 注：真实业务场景一般都是存在乱序的，所以一般使用 BoundedOutOfOrdernessTimestampExtractor。 DataStream&lt;UserBehavior&gt; timedData = dataSource .assignTimestampsAndWatermarks(new AscendingTimestampExtractor&lt;UserBehavior&gt;() { @Override public long extractAscendingTimestamp(UserBehavior userBehavior) { // 原始数据单位秒，将其转成毫秒 return userBehavior.timestamp * 1000; } }); 这样我们就得到了一个带有时间标记的数据流了，后面就能做一些窗口的操作。 过滤出点击事件 在开始窗口操作之前，先回顾下需求“每隔5分钟输出过去一小时内点击量最多的前 N 个商品”。由于原始数据中存在点击、加购、购买、收藏各种行为的数据，但是我们只需要统计点击量，所以先使用 FilterFunction 将点击行为数据过滤出来。 DataStream&lt;UserBehavior&gt; pvData = timedData .filter(new FilterFunction&lt;UserBehavior&gt;() { @Override public boolean filter(UserBehavior userBehavior) throws Exception { // 过滤出只有点击的数据 return userBehavior.behavior.equals(&quot;pv&quot;); } }); 窗口统计点击量 由于要每隔5分钟统计一次最近一小时每个商品的点击量，所以窗口大小是一小时，每隔5分钟滑动一次。即分别要统计 [09:00, 10:00), [09:05, 10:05), [09:10, 10:10)… 等窗口的商品点击量。是一个常见的滑动窗口需求（Sliding Window）。 DataStream&lt;ItemViewCount&gt; windowedData = pvData .keyBy(&quot;itemId&quot;) .timeWindow(Time.minutes(60), Time.minutes(5)) .aggregate(new CountAgg(), new WindowResultFunction()); 我们使用.keyBy(&quot;itemId&quot;)对商品进行分组，使用.timeWindow(Time size, Time slide)对每个商品做滑动窗口（1小时窗口，5分钟滑动一次）。然后我们使用.aggregate(AggregateFunction af, WindowFunction wf)做增量的聚合操作，它能使用AggregateFunction提前聚合掉数据，减少 state 的存储压力。较之.apply(WindowFunction wf)会将窗口中的数据都存储下来，最后一起计算要高效地多。aggregate()方法的第一个参数用于 这里的CountAgg实现了AggregateFunction接口，功能是统计窗口中的条数，即遇到一条数据就加一。 /** COUNT 统计的聚合函数实现，每出现一条记录加一 */ public static class CountAgg implements AggregateFunction&lt;UserBehavior, Long, Long&gt; { @Override public Long createAccumulator() { return 0L; } @Override public Long add(UserBehavior userBehavior, Long acc) { return acc + 1; } @Override public Long getResult(Long acc) { return acc; } @Override public Long merge(Long acc1, Long acc2) { return acc1 + acc2; } } .aggregate(AggregateFunction af, WindowFunction wf)的第二个参数WindowFunction将每个 key每个窗口聚合后的结果带上其他信息进行输出。我们这里实现的WindowResultFunction将主键商品ID，窗口，点击量封装成了ItemViewCount进行输出。 /** 用于输出窗口的结果 */ public static class WindowResultFunction implements WindowFunction&lt;Long, ItemViewCount, Tuple, TimeWindow&gt; { @Override public void apply( Tuple key, // 窗口的主键，即 itemId TimeWindow window, // 窗口 Iterable&lt;Long&gt; aggregateResult, // 聚合函数的结果，即 count 值 Collector&lt;ItemViewCount&gt; collector // 输出类型为 ItemViewCount ) throws Exception { Long itemId = ((Tuple1&lt;Long&gt;) key).f0; Long count = aggregateResult.iterator().next(); collector.collect(ItemViewCount.of(itemId, window.getEnd(), count)); } } /** 商品点击量(窗口操作的输出类型) */ public static class ItemViewCount { public long itemId; // 商品ID public long windowEnd; // 窗口结束时间戳 public long viewCount; // 商品的点击量 public static ItemViewCount of(long itemId, long windowEnd, long viewCount) { ItemViewCount result = new ItemViewCount(); result.itemId = itemId; result.windowEnd = windowEnd; result.viewCount = viewCount; return result; } } 现在我们得到了每个商品在每个窗口的点击量的数据流。 TopN 计算最热门商品 为了统计每个窗口下最热门的商品，我们需要再次按窗口进行分组，这里根据ItemViewCount中的windowEnd进行keyBy()操作。然后使用 ProcessFunction 实现一个自定义的 TopN 函数 TopNHotItems 来计算点击量排名前3名的商品，并将排名结果格式化成字符串，便于后续输出。 DataStream&lt;String&gt; topItems = windowedData .keyBy(&quot;windowEnd&quot;) .process(new TopNHotItems(3)); // 求点击量前3名的商品 ProcessFunction 是 Flink 提供的一个 low-level API，用于实现更高级的功能。它主要提供了定时器 timer 的功能（支持EventTime或ProcessingTime）。本案例中我们将利用 timer 来判断何时收齐了某个 window 下所有商品的点击量数据。由于 Watermark 的进度是全局的， 在 processElement 方法中，每当收到一条数据（ItemViewCount），我们就注册一个 windowEnd+1的定时器（Flink 框架会自动忽略同一时间的重复注册）。windowEnd+1的定时器被触发时，意味着收到了windowEnd+1的 Watermark，即收齐了该windowEnd下的所有商品窗口统计值。我们在onTimer() 中处理将收集的所有商品及点击量进行排序，选出 TopN，并将排名信息格式化成字符串后进行输出。 这里我们还使用了 ListState&lt;ItemViewCount&gt; 来存储收到的每条 ItemViewCount 消息，保证在发生故障时，状态数据的不丢失和一致性。ListState 是 Flink 提供的类似 Java List 接口的 State API，它集成了框架的checkpoint 机制，自动做到了 exactly-once 的语义保证。 /** 求某个窗口中前 N 名的热门点击商品，key 为窗口时间戳，输出为 TopN 的结果字符串 */ public static class TopNHotItems extends KeyedProcessFunction&lt;Tuple, ItemViewCount, String&gt; { private final int topSize; public TopNHotItems(int topSize) { this.topSize = topSize; } // 用于存储商品与点击数的状态，待收齐同一个窗口的数据后，再触发 TopN 计算 private ListState&lt;ItemViewCount&gt; itemState; @Override public void open(Configuration parameters) throws Exception { super.open(parameters); // 状态的注册 ListStateDescriptor&lt;ItemViewCount&gt; itemsStateDesc = new ListStateDescriptor&lt;&gt;( &quot;itemState-state&quot;, ItemViewCount.class); itemState = getRuntimeContext().getListState(itemsStateDesc); } @Override public void processElement( ItemViewCount input, Context context, Collector&lt;String&gt; collector) throws Exception { // 每条数据都保存到状态中 itemState.add(input); // 注册 windowEnd+1 的 EventTime Timer, 当触发时，说明收齐了属于windowEnd窗口的所有商品数据 context.timerService().registerEventTimeTimer(input.windowEnd + 1); } @Override public void onTimer( long timestamp, OnTimerContext ctx, Collector&lt;String&gt; out) throws Exception { // 获取收到的所有商品点击量 List&lt;ItemViewCount&gt; allItems = new ArrayList&lt;&gt;(); for (ItemViewCount item : itemState.get()) { allItems.add(item); } // 提前清除状态中的数据，释放空间 itemState.clear(); // 按照点击量从大到小排序 allItems.sort(new Comparator&lt;ItemViewCount&gt;() { @Override public int compare(ItemViewCount o1, ItemViewCount o2) { return (int) (o2.viewCount - o1.viewCount); } }); // 将排名信息格式化成 String, 便于打印 StringBuilder result = new StringBuilder(); result.append(&quot;====================================\\n&quot;); result.append(&quot;时间: &quot;).append(new Timestamp(timestamp-1)).append(&quot;\\n&quot;); for (int i=0;i&lt;topSize;i++) { ItemViewCount currentItem = allItems.get(i); // No1: 商品ID=12224 浏览量=2413 result.append(&quot;No&quot;).append(i).append(&quot;:&quot;) .append(&quot; 商品ID=&quot;).append(currentItem.itemId) .append(&quot; 浏览量=&quot;).append(currentItem.viewCount) .append(&quot;\\n&quot;); } result.append(&quot;====================================\\n\\n&quot;); out.collect(result.toString()); } } 打印输出 最后一步我们将结果打印输出到控制台，并调用env.execute执行任务。 topItems.print(); env.execute(&quot;Hot Items Job&quot;); 运行程序 直接运行 main 函数，就能看到不断输出的每个时间点的热门商品ID。 更换 Kafka 作为数据源 实际生产环境中， 我们的数据流往往是从 Kafka 获取到的。 如果要让代码更贴 近生产实际， 我们只需将 source 更换为 Kafka 即可： val properties = new Properties() properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;) properties.setProperty(&quot;group.id&quot;, &quot;consumer-group&quot;) properties.setProperty(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;) properties.setProperty(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;) properties.setProperty(&quot;auto.offset.reset&quot;, &quot;latest&quot;) val env = StreamExecutionEnvironment.getExecutionEnvironment env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) env.setParallelism(1) val stream = env .addSource(new FlinkKafkaConsumer[String](&quot;hotitems&quot;, new SimpleStringSchema(), properties)) 当然， 根据实际的需要， 我们还可以将 Sink 指定为 Kafka、 ES、 Redis 或其它 存储， 这里就不一一展开实现了 总结 本文的完整代码可以通过 GitHub 访问到。本文通过实现一个“实时热门商品”的案例，学习和实践了 Flink 的多个核心概念和 API 用法。包括 EventTime、Watermark的使用，State 的使用，Window API 的使用，以及 TopN 的实现。希望本文能加深大家对 Flink 的理解，帮助大家解决实战上遇到的问题。 ","link":"https://tinaxiawuhao.github.io/post/use-flink-calculate-hot-items/"},{"title":"从零构建第一个 Flink 应用","content":" 原文：http://wuchong.me/blog/2018/11/07/5-minutes-build-first-flink-application/ 作者：云邪（Jark） 在本文中，我们将从零开始，教您如何构建第一个 Flink 应用程序。 开发环境准备 Flink 可以运行在 Linux, Max OS X, 或者是 Windows 上。为了开发 Flink 应用程序，在本地机器上需要有 Java 8.x 和 maven 环境。 如果有 Java 8 环境，运行下面的命令会输出如下版本信息： $ java -version java version &quot;1.8.0_65&quot; Java(TM) SE Runtime Environment (build 1.8.0_65-b17) Java HotSpot(TM) 64-Bit Server VM (build 25.65-b01, mixed mode) 如果有 maven 环境，运行下面的命令会输出如下版本信息： $ mvn -version Apache Maven 3.5.4 (1edded0938998edf8bf061f1ceb3cfdeccf443fe; 2018-06-18T02:33:14+08:00) Maven home: /Users/wuchong/dev/maven Java version: 1.8.0_65, vendor: Oracle Corporation, runtime: /Library/Java/JavaVirtualMachines/jdk1.8.0_65.jdk/Contents/Home/jre Default locale: zh_CN, platform encoding: UTF-8 OS name: &quot;mac os x&quot;, version: &quot;10.13.6&quot;, arch: &quot;x86_64&quot;, family: &quot;mac&quot; 另外我们推荐使用 ItelliJ IDEA （社区免费版已够用）作为 Flink 应用程序的开发 IDE。Eclipse 虽然也可以，但是 Eclipse 在 Scala 和 Java 混合型项目下会有些已知问题，所以不太推荐 Eclipse。下一章节，我们会介绍如何创建一个 Flink 工程并将其导入 ItelliJ IDEA。 创建 Maven 项目 我们将使用 Flink Maven Archetype 来创建我们的项目结构和一些初始的默认依赖。在你的工作目录下，运行如下命令来创建项目： mvn archetype:generate \\ -DarchetypeGroupId=org.apache.flink \\ -DarchetypeArtifactId=flink-quickstart-java \\ -DarchetypeVersion=1.6.1 \\ -DgroupId=my-flink-project \\ -DartifactId=my-flink-project \\ -Dversion=0.1 \\ -Dpackage=myflink \\ -DinteractiveMode=false 你可以编辑上面的 groupId, artifactId, package 成你喜欢的路径。使用上面的参数，Maven 将自动为你创建如下所示的项目结构： $ tree my-flink-project my-flink-project ├── pom.xml └── src └── main ├── java │ └── myflink │ ├── BatchJob.java │ └── StreamingJob.java └── resources └── log4j.properties 我们的 pom.xml 文件已经包含了所需的 Flink 依赖，并且在 src/main/java 下有几个示例程序框架。接下来我们将开始编写第一个 Flink 程序。 编写 Flink 程序 启动 IntelliJ IDEA，选择 “Import Project”（导入项目），选择 my-flink-project 根目录下的 pom.xml。根据引导，完成项目导入。 在 src/main/java/myflink 下创建 SocketWindowWordCount.java 文件： package myflink; public class SocketWindowWordCount { public static void main(String[] args) throws Exception { } } 现在这程序还很基础，我们会一步步往里面填代码。注意下文中我们不会将 import 语句也写出来，因为 IDE 会自动将他们添加上去。在本节末尾，我会将完整的代码展示出来，如果你想跳过下面的步骤，可以直接将最后的完整代码粘到编辑器中。 Flink 程序的第一步是创建一个 StreamExecutionEnvironment。这是一个入口类，可以用来设置参数和创建数据源以及提交任务。所以让我们把它添加到 main 函数中： StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment(); 下一步我们将创建一个从本地端口号 9000 的 socket 中读取数据的数据源： DataStream&lt;String&gt; text = env.socketTextStream(&quot;localhost&quot;, 9000, &quot;\\n&quot;); 这创建了一个字符串类型的 DataStream。DataStream 是 Flink 中做流处理的核心 API，上面定义了非常多常见的操作（如，过滤、转换、聚合、窗口、关联等）。在本示例中，我们感兴趣的是每个单词在特定时间窗口中出现的次数，比如说5秒窗口。为此，我们首先要将字符串数据解析成单词和次数（使用Tuple2&lt;String, Integer&gt;表示），第一个字段是单词，第二个字段是次数，次数初始值都设置成了1。我们实现了一个 flatmap 来做解析的工作，因为一行数据中可能有多个单词。 DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; wordCounts = text .flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() { @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) { for (String word : value.split(&quot;\\\\s&quot;)) { out.collect(Tuple2.of(word, 1)); } } }); 接着我们将数据流按照单词字段（即0号索引字段）做分组，这里可以简单地使用 keyBy(int index) 方法，得到一个以单词为 key 的Tuple2&lt;String, Integer&gt;数据流。然后我们可以在流上指定想要的窗口，并根据窗口中的数据计算结果。在我们的例子中，我们想要每5秒聚合一次单词数，每个窗口都是从零开始统计的：。 DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; windowCounts = wordCounts .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1); 第二个调用的.timeWindow()指定我们想要5秒的翻滚窗口（Tumble）。第三个调用为每个key每个窗口指定了sum聚合函数，在我们的例子中是按照次数字段（即1号索引字段）相加。得到的结果数据流，将每5秒输出一次这5秒内每个单词出现的次数。 最后一件事就是将数据流打印到控制台，并开始执行： windowCounts.print().setParallelism(1); env.execute(&quot;Socket Window WordCount&quot;); 最后的 env.execute 调用是启动实际Flink作业所必需的。所有算子操作（例如创建源、聚合、打印）只是构建了内部算子操作的图形。只有在execute()被调用时才会在提交到集群上或本地计算机上执行。 下面是完整的代码，部分代码经过简化（代码在 GitHub 上也能访问到）： package myflink; import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.util.Collector; public class SocketWindowWordCount { public static void main(String[] args) throws Exception { // 创建 execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 通过连接 socket 获取输入数据，这里连接到本地9000端口，如果9000端口已被占用，请换一个端口 DataStream&lt;String&gt; text = env.socketTextStream(&quot;localhost&quot;, 9000, &quot;\\n&quot;); // 解析数据，按 word 分组，开窗，聚合 DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; windowCounts = text .flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() { @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) { for (String word : value.split(&quot;\\\\s&quot;)) { out.collect(Tuple2.of(word, 1)); } } }) .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1); // 将结果打印到控制台，注意这里使用的是单线程打印，而非多线程 windowCounts.print().setParallelism(1); env.execute(&quot;Socket Window WordCount&quot;); } } 运行程序 要运行示例程序，首先我们在终端启动 netcat 获得输入流： nc -lk 9000 如果是 Windows 平台，可以通过 https://nmap.org/ncat/ 安装 ncat 然后运行： ncat -lk 9000 然后直接运行SocketWindowWordCount的 main 方法。 只需要在 netcat 控制台输入单词，就能在 SocketWindowWordCount 的输出控制台看到每个单词的词频统计。如果想看到大于1的计数，请在5秒内反复键入相同的单词。 Cheers! 🎉 ","link":"https://tinaxiawuhao.github.io/post/build-first-flink-application/"}]}